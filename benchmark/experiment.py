import shlex
import tqdm
import datetime
import dataclasses
import pathlib
import tempfile
import collections
import itertools
import util
from mandala.imports import op, Storage  # type: ignore
from workloads import Workload
from prov_collectors import ProvCollector
import measure_resources
# on my machine, importing polars before the others causes a segfault
# TODO: Cite nixpkgs issue in GitHub
import polars


def run_experiments(
        prov_collectors: list[ProvCollector],
        workloads: list[Workload],
        iterations: int,
        seed: int,
        rerun: bool = False,
) -> polars.DataFrame:
    print(f"prov_collectors: ({len(prov_collectors)}) {', '.join(collector.name for collector in prov_collectors)}")
    print(f"workloads: ({len(workloads)}) {', '.join(workload.labels[0][2] for workload in workloads)}")
    print(f"iterations: {iterations}")
    print(f"seed: {seed}")
    print(f"rerun: {rerun}")

    inputs = list(itertools.chain.from_iterable(
        # Shuffling eliminates confounding effects of order dependency
        # E.g., if the CPU overheats and gets throttled, then later runs will be slower
        # We try to prevent this, but if other order effects leak through, at least we randomize the order.
        # However, all iteration=0 come before iteration=1, so that if you stop the program before finishing all n iterations, it may be able to completely finished m < n iterations.
        util.shuffle(
            # each iteration is shuffled using a different seed
            iteration ^ seed,
            tuple(itertools.product([iteration ^ seed], prov_collectors, workloads)),
        )
        for iteration in range(iterations)
    ))

    # progress = dask.diagnostics.ProgressBar()
    # progress.register()
    # records = dask.bag.from_iterable(inputs).map(run_one_experiment).compute()

    storage_file = pathlib.Path(".cache.db")
    with Storage(storage_file) as storage:
        if rerun:
            storage.drop_calls(
                [run_experiment(*args) for args in inputs],
                delete_dependents=True,
            )
        records = [
            storage.unwrap(run_experiment(*args))
            for args in tqdm.tqdm(inputs)
        ]

    return polars.from_dicts(
        [
            {
                "collector": record.prov_collector.name,
                "workload_group": record.workload.labels[0][0],
                "workload_subgroup": record.workload.labels[0][1],
                "workload_subsubgroup": record.workload.labels[0][2],
                "workload_area": record.workload.labels[1][0],
                "workload_subarea": record.workload.labels[1][1],
                "seed": record.seed,
                "returncode": record.process_resources.returncode,
                "walltime": record.process_resources.walltime,
                "user_cpu_time": record.process_resources.user_cpu_time,
                "system_cpu_time": record.process_resources.system_cpu_time,
                "max_memory": record.process_resources.max_memory_usage,
                "n_voluntary_context_switches": record.process_resources.n_voluntary_context_switches,
                "n_involuntary_context_switches": record.process_resources.n_involuntary_context_switches,
                "provenance_size": record.provenance_size,
                "n_ops": record.n_ops,
                "n_unique_files": record.n_unique_files,
                "op_counts": record.op_counts,
            }
            for record in records
        ],
        schema_overrides={
            "collector": polars.Categorical,
            "workload_group": polars.Categorical,
            "workload_subgroup": polars.Categorical,
            "workload_subsubgroup": polars.Categorical,
            "workload_area": polars.Categorical,
            "workload_subarea": polars.Categorical,
        },
    )


@dataclasses.dataclass
class ExperimentStats:
    seed: int
    prov_collector: ProvCollector
    workload: Workload
    process_resources: measure_resources.CompletedProcess
    provenance_size: int = 0
    n_ops: int = 0
    n_unique_files: int = 0
    op_counts: collections.Counter[str] = dataclasses.field(default_factory=collections.Counter[str])


@op
def run_experiment(
    seed: int,
    prov_collector: ProvCollector,
    workload: Workload,
) -> ExperimentStats:
    print(prov_collector.name, workload.labels[0])
    start = datetime.datetime.now()
    ignore_failures = True
    setup_teardown_timeout = datetime.timedelta(seconds=30)

    def ensure_nix_bin(cmd: tuple[str, ...]) -> tuple[str, ...]:
        if not cmd[0].startswith("/nix/store"):
            raise RuntimeError(f"Subprocess binaries should be absolute (generated by Nix) not {cmd[0]}")
        return cmd

    with tempfile.TemporaryDirectory() as _tmp_dir:
        tmp_dir = pathlib.Path(_tmp_dir).resolve()
        work_dir = tmp_dir / "work"
        prov_log = tmp_dir / "prov"

        work_dir.mkdir(exist_ok=True, parents=False)

        context = {
            "work_dir": str(work_dir.resolve()),
            "prov_log": str(prov_log.resolve()),
        }

        if prov_collector.setup_cmd:
            setup_proc = measure_resources.measure_resources(
                ensure_nix_bin(prov_collector.setup_cmd.expand(context)),
                cwd=work_dir,
                env={},
                timeout=setup_teardown_timeout,
            )
            if setup_proc.returncode != 0:
                if ignore_failures:
                    print(str(measure_resources.FailedProcess(setup_proc)))
                    return ExperimentStats(seed, prov_collector, workload, setup_proc)
                else:
                    setup_proc.raise_for_error()

        if workload.setup_cmd:
            setup_proc = measure_resources.measure_resources(
                ensure_nix_bin(workload.setup_cmd.expand(context)),
                cwd=work_dir,
                env={},
                timeout=setup_teardown_timeout,
            )
            if setup_proc.returncode != 0:
                if ignore_failures:
                    print(str(measure_resources.FailedProcess(setup_proc)))
                    return ExperimentStats(seed, prov_collector, workload, setup_proc)
                else:
                    setup_proc.raise_for_error()

        workload_cmd = ensure_nix_bin(workload.cmd.expand(context))
        prov_collector_cmd = prov_collector.run_cmd.expand(context)
        if prov_collector_cmd:
            ensure_nix_bin(prov_collector_cmd)
        cmd = prov_collector_cmd + workload_cmd
        print(shlex.join(cmd))
        workload_proc = measure_resources.measure_resources(
            cmd,
            cwd=work_dir,
            env={},
            timeout=workload.timeout * prov_collector.timeout_multiplier if workload.timeout else None,
        )
        if workload_proc.returncode != 0:
            if ignore_failures:
                print(str(measure_resources.FailedProcess(workload_proc)))
                return ExperimentStats(seed, prov_collector, workload, workload_proc)
            else:
                workload_proc.raise_for_error()

        if prov_collector.teardown_cmd:
            teardown_proc = measure_resources.measure_resources(
                ensure_nix_bin(prov_collector.teardown_cmd.expand(context)),
                cwd=work_dir,
                env={},
                timeout=setup_teardown_timeout,
            )
            if teardown_proc.returncode != 0:
                if ignore_failures:
                    print(str(measure_resources.FailedProcess(teardown_proc)))
                    return ExperimentStats(seed, prov_collector, workload, teardown_proc)
                else:
                    teardown_proc.raise_for_error()

        provenance_size = (util.dir_size(prov_log) if prov_log.is_dir() else prov_log.stat().st_size) if prov_log.exists() else 0
        ops = prov_collector.count(prov_log, pathlib.Path(workload_cmd[0]))

    print(prov_collector.name, workload.labels[0], round((datetime.datetime.now() - start).total_seconds(), 1))

    return ExperimentStats(
        seed,
        prov_collector,
        workload,
        workload_proc,
        provenance_size,
        len(ops),
        len({op.target0 for op in ops if op.target0} | {op.target1 for op in ops if op.target1}),
        collections.Counter(op.type for op in ops),
    )
