* DONE Evaluating system-level provenance tools for practical use
- [[file:docs/provenance_overhead][Paper directory]]
- Computational provenance := how did this file get produced? What binaries, data, libraries, other files that got used? What about the computational provenance of /those/ files?
- System-level provenance collects this data without knowing anything about the underlying programs (black box); just looking at syscalls or the like.
- This paper is a lit review of provenance systems

* DONE Provenance research presentation to GNU/Linux User's Group
- [[file:docs/prov_pres/][Presentation directory]]

* DONE Provenance presentation to UBC
- Presentation on Google Drive

* IN-PROGRESS Measuring provenance overheads
:LOGBOOK:
CLOCK: [2024-01-15 Mon 14:38]--[2024-01-15 Mon 15:15] =>  0:37
:END:
- [[file:docs/low_provenance_overhead/][Paper directory]]
- Take provenance systems and benchmarks from the lit review, apply all prov systems to all benchmarks
- Reproducing: See [[file:benchmark/REPRODUCING.md][REPRODUCING.md]]
- [[file:benchmark/][Code directory]]
  - [[file:benchmark/prov_collectors.py][prov_collectors.py]] contains "provenance collectors"
  - [[file:benchmark/workloads.py][workloads.py]] contains the "workloads"; The workloads have a "setup" and a "run" phase. For example, "setup" may download stuff (we don't want to time the setup; that would just benchmark the internet service provider), whereas "run" will do the compile (we want to time only that).
  - [[file:benchmark/runner.py][runner.py]] will select certain collectors and workloads; if it succeeds, the results get stored in ~.cache/~, so subsequent executions with the same arguments will return instantly
  - [[file:benchmark/experiment.py][experiment.py]] contains the logic to run experiments (especailly cleaning up after them)
  - [[file:benchmark/run_exec_wrapper.py][run_exec_wrapper.py]] knows how to execute commands in a "clean" environment and cgroup
  - [[file:benchmark/notebook/Stats-larger.ipynb][Stats-larger.ipynb]] has the process to extract statistics using bayesian inference from the workflow runs
  - [[file:benchmark/flake.nix][flake.nix]] contains the Nix expressions which describe the environment in which everything runs
  - [[file:benchmark/result/][result/]] directory contains the result of building flake.nix; all binaries and executables should come from result/ in order for the experiment to be reproducible

** Rapid review

*** TODO Redo rapid review with snowballing
- e.g., https://dew-uff.github.io/scripts-provenance/selected.html
- e.g., https://dew-uff.github.io/scripts-provenance/graph.html
- https://joaofelipe.github.io/snowballing/start.html
- https://dl.acm.org/doi/10.1145/2601248.2601268

*** TODO Include record/replay terms
- Add sciunit
- Add reprozip
- Add DetTrace
- Add CDE
- Add Burrito
- Add Sumatra

** Get workloads to work

*** DONE Get Apache to compile
:LOGBOOK:
CLOCK: [2024-01-15 Mon 15:50]--[2024-01-15 Mon 17:30] =>  1:40
:END:
- We need to get src_sh{./result/bin/python runner.py apache} to work

**** DONE Cannot find pcre-config
- I invoke src_sh{./configure --with-pcre-config=/path/to/pcre-config}, and ~./configure~ will still complain ("no pcre-config found").
- I ended up patching with [[file:benchmark/httpd-configure.patch][httpd-configure.patch]].

**** DONE lber.h not found
:PROPERTIES:
:DELEGATED: Sam
:END:
- ~/nix/store/2z0hshv096hhavariih722pckw5v150v-apr-util-1.6.3-dev/include/apr_ldap.h:79:10: fatal error: lber.h: No such file or directory~

*** DONE Get Spack workloads to compile
:LOGBOOK:
CLOCK: [2024-01-14 Sun 21:03]--[2024-01-14 Sun 22:35] =>  1:32
CLOCK: [2024-01-14 Sun 19:42]--[2024-01-14 Sun 19:58] =>  0:16
CLOCK: [2024-01-12 Fri 14:40]--[2024-01-12 Fri 16:13] =>  1:33
CLOCK: [2024-01-11 Thu 15:26]--[2024-01-11 Thu 17:05] =>  1:39
:END:
- We need to get src_sh{./result/bin/python runner.py spack} to work
- See docstring of ~SpackInstall~ in [[file:benchmark/workloads.py][workloads.py]].
- Spack installs a target package (call it $spec) and all of $spec's dependencies. Then it removes $spec, while leaving the dependencies.

*** DONE Write a ~Workload~ class for Apache + ApacheBench
- Compiling Apache is an interesting benchmark, but /running/ Apache with a predefined request load is also an interesting benchmark.
- We should write a new class called ~ApacheLoad~ that installs Apache in its setup() (for simplicity, we won't reuse the version we built earlier), downloads a ~ApacheBench~, and in the run() runs the server with the request load using only tools from ~result/~ or ~.work/~.

*** BACKLOG THTTPD and cherokee
http://www.acme.com/software/thttpd/
https://github.com/larryhe/tinyhttpd
https://github.com/mendsley/tinyhttp
https://cherokee-project.com/

*** BACKLOG SPEC CPU 2006
- Determine if we need just int or also fp benchmarks
- https://www.spec.org/cpu2006/Docs/
- https://www.spec.org/sources/
- https://github.com/miyuki/spec-cpu2006-redist/
- https://www.spec.org/cpu2006/Docs/tools-build.html
- https://www.spec.org/cpu2006/Docs/install-guide-unix.html
- https://www.spec.org/cpu2006/Docs/runspec.html

*** TODO SSH
https://github.com/LineRate/ssh-perf

*** TODO Shellbench
https://github.com/shellspec/shellbench

*** TODO CleanML
https://chu-data-lab.github.io/CleanML/

*** DONE Create Postmark workload
- https://www.filesystems.org/docs/auto-pilot/Postmark.html
- See [[https://doi.org/10.1145/2420950.2420989][Hi-Fi]], [[https://www.usenix.org/legacy/events/usenix09/tech/full_papers/muniswamy-reddy/muniswamy-reddy.pdf][PASSv2]], [[https://www.usenix.org/system/files/conference/usenixsecurity15/sec15-paper-bates.pdf][LPM]], [[https://doi.org/10.1145/3127479.3129249][CamFlow]] for details
- pm>set transactions 400000

*** DONE Create lmbench benchmark
- https://lmbench.sourceforge.net/

*** TODO Create filebench benchmark
- https://github.com/filebench/filebench

*** TODO Native snakemake and nf-core workflows

*** TODO Make browser benchmarks
- Run Chromium and Firefox with Sunspider
- https://github.com/v8/v8/blob/04f51bc70a38fbea743588e41290bea40830a486/test/benchmarks/csuite/csuite.py#L4

*** DONE Create mercurial/VCS workload
- https://savannah.gnu.org/hg/?group=octave
- https://hg.mozilla.org/mozilla-central/
- https://github.com/frej/fast-export
- https://wiki.mercurial-scm.org/ConvertExtension
- https://hg-git.github.io/
- https://repo.mercurial-scm.org/hg

*** TODO Create CVS workload
- http://cvs.savannah.gnu.org/viewvc/cvs/ccvs/

*** TODO Include xz in workload

*** TODO VIC
- Fig 1 of https://arxiv.org/pdf/1707.05731.pdf
- https://github.com/Chicago/food-inspections-evaluation/tree/master/CODE

*** TODO FIE
- Fig 7 of https://arxiv.org/pdf/1707.05731.pdf
- Fig 1 of https://doi.org/10.1016/j.envsoft.2015.12.010
- https://github.com/uva-hydroinformatics/VIC_Pre-Processing_Rules/

*** DONE Write a ProFTPD benchmark
- https://github.com/selectel/ftpbench

*** IN-PROGRESS Write a ~CompileLinux~ class
:PROPERTIES:
:DELEGATED: Faust
:END:
- Write a class that compiles the Linux kernel (just the kernel, no user-space software), using only tools from ~result/~.
- The benchmark should use a specific pin of the Linux kernel and set kernel build options. Both should be customizable and set by files that are checked into Git. However, the Linux source tree should /not/ be checked into Git (see build Apache, where I download the source code in setup() and cache it for future use).

*** DONE Refactor BLAST workloads
- It should be easy to run them a large consistent set of many different BLAST apps.
- Maybe have a 1 min, 10 min, and 60 min randomly-selected, but fixed, configuration

*** TODO Investigate Sysbench
- https://doi.org/10.1145/2508859.2516731

*** TODO investigate BT-IO
https://www.nas.nasa.gov/software/npb.html

*** TODO Run xSDK codes
- https://github.com/xsdk-project/xsdk-examples
- https://github.com/LBL-EESA/alquimia-dev

*** TODO Spark workload
https://www.databricks.com/blog/2017/10/05/build-complex-data-pipelines-with-unified-analytics-platform.html

*** TODO yt workloads
https://yt-project.org/doc/cookbook/index.html
https://prappleizer.github.io/#tutorials
https://trident.readthedocs.io/en/latest/annotated_example.html
https://github.com/PyLCARS/YT_BeyondAstro

** Make API easier to use

*** DONE Refactor ~runner.py~
- Change to ~run_store_analyze.py~
- [[file:benchmark/runner.py][runner.py]] mixes code for selecting benchmarks and prov collectors with code for summarizing statistical outputs.
- Use --benchmarks and --collectors to form a grid
- Accept --iterations, --seed, --fail-first
- Accept --analysis $foo
- Should have an --option to import external workloads and prov_collectors
- Should have --re-run, which removes ~.cache/results_*~ and ~.cache/$hash~

*** TODO Refactor ~run_exec_wrapper.py~
- Should fail gracefully when cgroups are not available

*** DONE Refactor ~stats.py~
- Should have Callable[pandas.DataFrame, None]

*** TODO Refactor ~prov_collectors.py~
- Should have teardown

*** TODO Refactor ~workloads.py~
- Should accept a tempdir
- Should be smaller
- Should have teardown
- Should export instances
- Categories: build Spack, build Deb, BLAST, compile Linux, HTTP bench, FTP bench
- Should have license()

*** DONE Write ~run.py~
- Just runs one workload
- --setup, --main, --teardown

*** TODO Document user interface

** Make easier to install

*** TODO Allow classes to specify Nix packages
- But user should be able to customize lockfile
- setup() should do ~nix build~ and add to path

*** TODO Package Python code for PyPI using Poetry

*** TODO Document installation

** Provenance collectors

*** DONE Fix Sciunits
- We need to get src_sh{./result/bin/python runner.py sciunit} to work.
- Sciunit is a Python package which depends on a binary called ~ptu~.
- Sciunit says "sciunit: /nix/store/7x6rlzd7dqmsa474j8ilc306wlmjb8bp-python3-3.10.13-env/lib/python3.10/site-packages/sciunit2/libexec/ptu: No such file or directory", but on my system, that file does exist! Why can't sciunits find it?
- Answer: That file exists; it is an ELF binary, it's "interpreter" is set to /lib64/linux-something.so. That interpreter does not exist. I replaced this copy of ptu with the nix-built copy of ptu.

*** DONE Fix sciunit

*** DONE Fix strace unparsable lines

*** TODO Fix rr to measure storage overhead

*** TODO Fix Spade+FUSE
- We need to get src_sh{./result/bin/python runner.py spade_fuse} to work.

**** TODO Get SPADE Neo4J database to work
- src_sh{./result/bin/spade start && echo "add storage Neo4J $PWD/db" | ./result/bin/spade control}
- Currently, that fails with "Adding storage Neo4J... error: Unable to find/load class"
- The log can be found in ~~/.local/share/SPADE/current.log~.
- ~/.local/share/SPADE/lib/neo4j-community/lib/*.jar contains Neo4J classes. I believe these are on the classpath. However, this is a different version of Java or something like that, which refuses to load those jars.

*** TODO [#C] Write BPF trace
- We need to write a basic prov collector for BPF trace. The collector should log files read/written by the process and all children processes. Start by writing [[file:benchmark/prov.bt][prov.bt]].

*** TODO Package CARE
https://proot-me.github.io/care/

*** TODO Write a sleepy ptracer

*** TODO Package/write-up PTU
- https://www.usenix.org/system/files/conference/tapp13/tapp13-final18.pdf

*** TODO discuss VAMSA
- https://dl.acm.org/doi/pdf/10.1145/3394486.3403205

*** TODO Build CentOS packages
- See @shiExperienceReportProducing2022. Could leverage https://pypi.org/project/reprotest/

** Stats

*** DONE Measure arithmetic intensity for each
- IO calls / CPU sec, where CPU sec is itself a random variable

*** DONE Measure slowdown as a function of arithmetic intensity
- See [[file:benchmark/notebook/Stats-larger.ipynb][States-larger.ipynb]]

*** DONE [#C] Count dynamic instructions in entire program
- IO calls / 1M dynamic instruction

*** DONE Plot IO vs CPU sec

*** DONE Plot confidence interval of slowdown per arithmetic intensity

*** DONE Evaluate prediction based on arithmetic intensity
- slowdown(prov_collector) * cpu_to_wall_time(workload) * runtime(workload) ~ runtime(workload, prov_collector)
- What is the expected percent error?

*** DONE Characterize benchmarks and benchmark classes by syscall breakdown
- Features: count of each group of syscalls / total time
- Prog should occupy the same point as {Prog, Prog} (that is, analogous to intensive not extensive properties in physics)
- PCA and clustering and dendrogram
  - Sec 3 of https://doi.org/10.1109/ISPASS.2005.1430555
  - Sec 9 of https://doi.org/10.1145/1167473.1167488
- https://doi.org/10.1109/IISWC.2006.302733

*** TODO Revise bayesian model to use benchmark class
- How many classes and benchmarks does one need?

*** TODO Use G-means or X-means to learn the number of clusters

** Writing

*** DONE Write introduction

*** DONE Write background

*** DONE Write literature rapid review section

*** DONE Write benchmark and prov collector collection

*** DONE Revise introduction (60)
- Smoosh Motivation and Background together
- Lead with the problem
- 1 problem -> provenance (vs perf overhead) -> 3 other problems solved -> 3 ways to gather

*** TODO Explain how strace, ltrace, fsatrace, rr got to be there

*** DONE Explain how Sciunits, ReproZip got to be there

*** DONE Describe experimental results

*** TODO Explain the capabilities/features of each prov tracer
- Table of capabilities (vDSO)

*** TODO Discussion
- What provenance methods are most promising?
- Threats to validity
- Mathematical model
- Few of the tools are applicable to comp sci due to methods
- How many work for distributed systems
- How to handle network
- Microbechmark vs applications?
- Non-negative linear regression

*** TODO Story-telling
- Gaps in prior work re comp sci
- Stakeholder perspectives:
  - Tool developers, users, facilities people
- Longterm archiving an execution, such that it is re-executable
- I/O defn? I/O includes stuff like username, clock_gettime

*** TODO Conclusion

*** TODO Threats to validity

*** TODO Background

*** TODO Page-limit

*** TODO Reproducibility appendix
- Need Intel CPU?

*** TODO Why not VMs?

* BACKLOG Record/replay reproducibility with library interposition
- [[file:docs/record_replay/][Paper directory]]
- Record/replay is an easier way to get reproducibility than Docker/Nix/etc.
- Use library interpositioning to make a record/replay tool that is faster than other record/replay tools

** BACKLOG Get global state vars
- Library constructors get called twice (2 copies of library global variables)
- https://stackoverflow.com/questions/77782964/how-to-run-code-exactly-once-in-ld-preloaded-shared-library

* Vars
#+TODO: BACKLOG(b) TODO(t) IN-PROGRESS(p) | DONE(d) BLOCKED(b)

#+BEGIN_SRC elisp
#+END_SRC
