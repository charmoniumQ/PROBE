@misc{acminc.staffArtifactReviewBadging2020,
  title = {Artifact {{Review}} and {{Badging}}},
  author = {{ACM Inc. staff}},
  year = {2020},
  month = aug,
  urldate = {2023-01-19},
  abstract = {Result and Artifact Review documentation and badges - V.1.1},
  howpublished = {https://www.acm.org/publications/policies/artifact-review-and-badging-current},
  langid = {english},
  keywords = {project-acm-rep,project-bugsinpy,project-provenance-pp,reproducibility},
  file = {/home/sam/Zotero/storage/PGDE3ZAE/artifact-review-and-badging-current.html}
}

@article{altschulBasicLocalAlignment1990,
  title = {Basic Local Alignment Search Tool},
  author = {Altschul, Stephen F. and Gish, Warren and Miller, Webb and Myers, Eugene W. and Lipman, David J.},
  year = {1990},
  month = oct,
  journal = {Journal of Molecular Biology},
  volume = {215},
  number = {3},
  pages = {403--410},
  issn = {0022-2836},
  doi = {10.1016/S0022-2836(05)80360-2},
  urldate = {2023-12-04},
  abstract = {A new approach to rapid sequence comparison, basic local alignment search tool (BLAST), directly approximates alignments that optimize a measure of local similarity, the maximal segment pair (MSP) score. Recent mathematical results on the stochastic properties of MSP scores allow an analysis of the performance of this method as well as the statistical significance of alignments it generates. The basic algorithm is simple and robust; it can be implemented in a number of ways and applied in a variety of contexts including straight-forward DNA and protein sequence database searches, motif searches, gene identification searches, and in the analysis of multiple regions of similarity in long DNA sequences. In addition to its flexibility and tractability to mathematical analysis, BLAST is an order of magnitude faster than existing sequence comparison tools of comparable sensitivity.},
  keywords = {project-provenance-pp},
  file = {/home/sam/Zotero/storage/AXZWKX3X/S0022283605803602.html}
}

@inproceedings{arthurKmeansAdvantagesCareful2007,
  title = {K-Means++: The Advantages of Careful Seeding},
  shorttitle = {K-Means++},
  booktitle = {Proceedings of the Eighteenth Annual {{ACM-SIAM}} Symposium on {{Discrete}} Algorithms},
  author = {Arthur, David and Vassilvitskii, Sergei},
  year = {2007},
  month = jan,
  series = {{{SODA}} '07},
  pages = {1027--1035},
  publisher = {{Society for Industrial and Applied Mathematics}},
  address = {{USA}},
  urldate = {2024-02-09},
  abstract = {The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a very simple, randomized seeding technique, we obtain an algorithm that is {$\Theta$}(logk)-competitive with the optimal clustering. Preliminary experiments show that our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.},
  isbn = {978-0-89871-624-5},
  keywords = {machine learning,project-provenance-pp},
  file = {/home/sam/Zotero/storage/P7XBXVNR/Arthur and Vassilvitskii - k-means++ The Advantages of Careful Seeding.pdf}
}

@inproceedings{balakrishnanOPUSLightweightSystem2013,
  title = {\{\vphantom\}{{OPUS}}\vphantom\{\}: {{A Lightweight System}} for {{Observational Provenance}} in {{User Space}}},
  shorttitle = {{{OPUS}}},
  booktitle = {5th {{USENIX Workshop}} on the {{Theory}} and {{Practice}} of {{Provenance}} ({{TaPP}} 13)},
  author = {Balakrishnan, Nikilesh and Bytheway, Thomas and Sohan, Ripduman and Hopper, Andy},
  year = {2013},
  urldate = {2023-07-06},
  langid = {english},
  keywords = {project-provenance-pp,provenance,provenance-tool},
  file = {/home/sam/Zotero/storage/APTAJA64/Balakrishnan et al. - 2013 - OPUS A Lightweight System for Observational Pro.pdf}
}

@article{bartlettXSDKFoundationsExtremescale2017,
  title = {{{xSDK Foundations}}: {{Toward}} an {{Extreme-scale Scientific Software Development Kit}}},
  shorttitle = {{{xSDK Foundations}}},
  author = {Bartlett, Roscoe and Demeshko, Irina and Gamblin, Todd and Hammond, Glenn and Heroux, Michael Allen and Johnson, Jeffrey and Klinvex, Alicia and Li, Xiaoye and McInnes, Lois Curfman and Moulton, J. David and {Osei-Kuffuor}, Daniel and Sarich, Jason and Smith, Barry and Willenbring, James and Yang, Ulrike Meier},
  year = {2017},
  month = feb,
  journal = {Supercomputing Frontiers and Innovations},
  volume = {4},
  number = {1},
  pages = {69--82},
  issn = {2313-8734},
  doi = {10.14529/jsfi170104},
  urldate = {2023-08-31},
  abstract = {Extreme-scale computational science increasingly demands multiscale and multiphysics formulations. Combining software developed by independent groups is imperative: no single team has resources for all predictive science and decision support capabilities. Scientific libraries provide high-quality, reusable software components for constructing applications with improved robustness and portability.~ However, without coordination, many libraries cannot be easily composed.~ Namespace collisions, inconsistent arguments, lack of third-party software versioning, and additional difficulties make composition costly.The Extreme-scale Scientific Software Development Kit (xSDK) defines community policies to improve code quality and compatibility across independently developed packages (hypre, PETSc, SuperLU, Trilinos, and Alquimia) and provides a foundation for addressing broader issues in software interoperability, performance portability, and sustainability.~ The xSDK provides turnkey installation of member software and seamless combination of aggregate capabilities, and it marks first steps toward extreme-scale scientific software ecosystems from which future applications can be composed rapidly with assured quality and scalability.},
  copyright = {Copyright (c)},
  langid = {english},
  keywords = {high-performance computing,project-provenance-pp},
  file = {/home/sam/Zotero/storage/C3CDRTF5/Bartlett et al. - 2017 - xSDK Foundations Toward an Extreme-scale Scientif.pdf}
}

@inproceedings{batesTrustworthyWholeSystemProvenance2015,
  title = {Trustworthy \{\vphantom\}{{Whole-System}}\vphantom\{\} {{Provenance}} for the {{Linux Kernel}}},
  booktitle = {24th {{USENIX Security Symposium}} ({{USENIX Security}} 15)},
  author = {Bates, Adam and Tian, Dave (Jing) and Butler, Kevin R. B. and Moyer, Thomas},
  year = {2015},
  pages = {319--334},
  urldate = {2023-08-25},
  isbn = {978-1-939133-11-3},
  langid = {english},
  keywords = {project-provenance-pp,prov-tool},
  file = {/home/sam/Zotero/storage/5QPU4S6E/Bates et al. - 2015 - Trustworthy Whole-System Provenance for the Linu.pdf}
}

@article{berradaBaselineUnsupervisedAdvanced2020,
  title = {A Baseline for Unsupervised Advanced Persistent Threat Detection in System-Level Provenance},
  author = {Berrada, Ghita and Cheney, James and Benabderrahmane, Sidahmed and Maxwell, William and Mookherjee, Himan and Theriault, Alec and Wright, Ryan},
  year = {2020},
  month = jul,
  journal = {Future Generation Computer Systems},
  volume = {108},
  pages = {401--413},
  issn = {0167-739X},
  doi = {10.1016/j.future.2020.02.015},
  urldate = {2023-08-23},
  abstract = {Advanced persistent threats (APTs) are stealthy, sophisticated, and unpredictable cyberattacks that can steal intellectual property, damage critical infrastructure, or cause millions of dollars in damage. Detecting APTs by monitoring system-level activity is difficult because manually inspecting the high volume of normal system activity is overwhelming for security analysts. We evaluate the effectiveness of unsupervised batch and streaming anomaly detection algorithms over multiple gigabytes of provenance traces recorded on four different operating systems to determine whether they can detect realistic APT-like attacks reliably and efficiently. This article is the first detailed study of the effectiveness of generic unsupervised anomaly detection techniques in this setting.},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/6LH8V7HU/Berrada et al. - 2020 - A baseline for unsupervised advanced persistent th.pdf;/home/sam/Zotero/storage/J7DXZAI5/S0167739X19320448.html}
}

@article{beyerReliableBenchmarkingRequirements2019,
  title = {Reliable Benchmarking: Requirements and Solutions},
  shorttitle = {Reliable Benchmarking},
  author = {Beyer, Dirk and L{\"o}we, Stefan and Wendler, Philipp},
  year = {2019},
  month = feb,
  journal = {Int J Softw Tools Technol Transfer},
  volume = {21},
  number = {1},
  pages = {1--29},
  issn = {1433-2779, 1433-2787},
  doi = {10.1007/s10009-017-0469-y},
  urldate = {2022-06-30},
  abstract = {Benchmarking is a widely used method in experimental computer science, in particular, for the comparative evaluation of tools and algorithms. As a consequence, a number of questions need to be answered in order to ensure proper benchmarking, resource measurement, and presentation of results, all of which is essential for researchers, tool developers, and users, as well as for tool competitions. We identify a set of requirements that are indispensable for reliable benchmarking and resource measurement of time and memory usage of automatic solvers, verifiers, and similar tools, and discuss limitations of existing methods and benchmarking tools. Fulfilling these requirements in a benchmarking framework can (on Linux systems) currently only be done by using the cgroup and namespace features of the kernel. We developed BenchExec, a ready-to-use, tool-independent, and open-source implementation of a benchmarking framework that fulfills all presented requirements, making reliable benchmarking and resource measurement easy. Our framework is able to work with a wide range of different tools, has proven its reliability and usefulness in the International Competition on Software Verification, and is used by several research groups worldwide to ensure reliable benchmarking. Finally, we present guidelines on how to present measurement results in a scientifically valid and comprehensible way.},
  langid = {english},
  keywords = {metascience,project-provenance-pp,reproducibility engineering,software benchmarking},
  annotation = {interest: 90},
  file = {/home/sam/Zotero/storage/5XXRH3IC/Beyer2019_Article_ReliableBenchmarkingRequiremen.pdf;/home/sam/Zotero/storage/XX3G42I4/Current_ReliableBenchmarking.pdf}
}

@article{billahUsingDataGrid2016,
  title = {Using a Data Grid to Automate Data Preparation Pipelines Required for Regional-Scale Hydrologic Modeling},
  author = {Billah, Mirza M. and Goodall, Jonathan L. and Narayan, Ujjwal and Essawy, Bakinam T. and Lakshmi, Venkat and Rajasekar, Arcot and Moore, Reagan W.},
  year = {2016},
  month = apr,
  journal = {Environmental Modelling \& Software},
  volume = {78},
  pages = {31--39},
  issn = {1364-8152},
  doi = {10.1016/j.envsoft.2015.12.010},
  urldate = {2024-02-05},
  abstract = {Modeling a regional-scale hydrologic system introduces major data challenges related to the access and transformation of heterogeneous datasets into the information needed to execute a hydrologic model. These data preparation activities are difficult to automate, making the reproducibility and extensibility of model simulations conducted by others difficult or even impossible. This study addresses this challenge by demonstrating how the integrated Rule Oriented Data Management System (iRODS) can be used to support data processing pipelines needed when using data-intensive models to simulate regional-scale hydrologic systems. Focusing on the Variable Infiltration Capacity (VIC) model as a case study, data preparation steps are sequenced using rules within iRODS. VIC and iRODS are applied to study hydrologic conditions in the Carolinas, USA during the period 1998{\textendash}2007 to better understand impacts of drought within the region. The application demonstrates how iRODS can support hydrologic modelers to create more reproducible and extensible model-based analyses.},
  keywords = {hydrology,project-provenance-pp},
  file = {/home/sam/Zotero/storage/LBM32NEM/S1364815215301249.html}
}

@misc{BPFDocumentation,
  title = {{{BPF Documentation}}},
  journal = {The Linux Kernel documentation},
  urldate = {2023-08-24},
  howpublished = {https://docs.kernel.org/bpf/index.html},
  keywords = {operating systems,project-provenance-pp},
  file = {/home/sam/Zotero/storage/G8CJH67B/index.html}
}

@incollection{braunIssuesAutomaticProvenance2006,
  title = {Issues in {{Automatic Provenance Collection}}},
  booktitle = {Provenance and {{Annotation}} of {{Data}}},
  author = {Braun, Uri and Garfinkel, Simson and Holland, David A. and {Muniswamy-Reddy}, Kiran-Kumar and Seltzer, Margo I.},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Moreau, Luc and Foster, Ian},
  year = {2006},
  volume = {4145},
  pages = {171--183},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11890850_18},
  urldate = {2023-08-23},
  abstract = {Automatic provenance collection describes systems that observe processes and data transformations inferring, collecting, and maintaining provenance about them. Automatic collection is a powerful tool for analysis of objects and processes, providing a level of transparency and pervasiveness not found in more conventional provenance systems. Unfortunately, automatic collection is also difficult. We discuss the challenges we encountered and the issues we exposed as we developed an automatic provenance collector that runs at the operating system level.},
  isbn = {978-3-540-46302-3 978-3-540-46303-0},
  langid = {english},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/KXYB7A6B/Braun et al. - 2006 - Issues in Automatic Provenance Collection.pdf}
}

@inproceedings{burtonWorkloadCharacterizationUsing1998,
  title = {Workload Characterization Using Lightweight System Call Tracing and Reexecution},
  booktitle = {1998 {{IEEE International Performance}}, {{Computing}} and {{Communications Conference}}. {{Proceedings}} ({{Cat}}. {{No}}.{{98CH36191}})},
  author = {Burton, A.N. and Kelly, P.H.J.},
  year = {1998},
  month = feb,
  pages = {260--266},
  issn = {1097-2641},
  doi = {10.1109/PCCC.1998.659975},
  urldate = {2024-02-13},
  abstract = {This paper shows how system call traces can be obtained with minimal interference to the system being characterized, and used as realistic, repeatable workloads for experiments to evaluate operating system and file system designs and configuration alternatives. Our system call trace mechanism, called ULTra, captures a complete trace of each UNIX process's calls to the operating system. The performance impact is normally small, and it runs in user mode without special privileges. We show how the resulting traces can be used to drive full, repeatable reexecution of the captured behaviour, and present a case study which shows the usefulness and accuracy of the tool for predicting the impact of file system caching on a WWW server's performance.},
  keywords = {operating systems,project-provenance-pp},
  file = {/home/sam/Zotero/storage/VNTWDTX7/Burton and Kelly - 1998 - Workload characterization using lightweight system.pdf}
}

@inproceedings{buttProvONEProvenanceModel2020,
  title = {{{ProvONE}}+: {{A Provenance Model}} for {{Scientific Workflows}}},
  shorttitle = {{{ProvONE}}+},
  booktitle = {Web {{Information Systems Engineering}} {\textendash} {{WISE}} 2020},
  author = {Butt, Anila Sahar and Fitch, Peter},
  editor = {Huang, Zhisheng and Beek, Wouter and Wang, Hua and Zhou, Rui and Zhang, Yanchun},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {431--444},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-62008-0_30},
  abstract = {The provenance of workflows is essential, both for the data they derive and for their specification, to allow for the reproducibility, sharing and reuse of information in the scientific community. Although the formal modelling of scientific workflow provenance was of interest and studied, in many fields like semantic web, yet no provenance model has existed, we are aware of, to model control-flow driven scientific workflows. The provenance models proposed by the semantic web community for data-driven scientific workflows may capture the provenance of control-flow driven workflows execution traces (i.e., retrospective provenance) but underspecify the workflow structure (i.e., workflow provenance). An underspecified or incomplete structure of a workflow results in the misinterpretation of a scientific experiment and precludes conformance checking of the workflow, thereby restricting the gains of provenance. To overcome the limitation, we present a formal, lightweight and general-purpose specification model for the control-flows involved scientific workflows. The proposed model can be combined with the existing provenance models and easy to extend to specify the common control-flow patterns. In this article, we inspire the need for control-flow driven scientific workflow provenance model, provide an overview of its key classes and properties, and briefly discuss its integration with the ProvONE provenance model as well as its compatibility to PROV-DM. We will also focus on the sample modelling using the proposed model and present a comprehensive implementation scenario from the agricultural domain for validating the model.},
  isbn = {978-3-030-62008-0},
  langid = {english},
  keywords = {project-acm-rep,project-provenance-pp,provenance,semantic web},
  file = {/home/sam/Zotero/storage/XIV45RJ9/Butt and Fitch - 2020 - ProvONE+ A Provenance Model for Scientific Workfl.pdf}
}

@inproceedings{bzeznikNixHPCPackage2017,
  title = {Nix as {{HPC}} Package Management System},
  booktitle = {Proceedings of the {{Fourth International Workshop}} on {{HPC User Support Tools}}},
  author = {Bzeznik, Bruno and Henriot, Oliver and Reis, Valentin and Richard, Olivier and Tavard, Laure},
  year = {2017},
  month = nov,
  series = {{{HUST}}'17},
  pages = {1--6},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3152493.3152556},
  urldate = {2023-05-05},
  abstract = {Modern High Performance Computing systems are becoming larger and more heterogeneous. The proper management of software for the users of such systems poses a significant challenge. These users run very diverse applications that may be compiled with proprietary tools for specialized hardware. Moreover, the application life-cycle of these software may exceed the lifetime of the HPC systems themselves. These difficulties motivate the use of specialized package management systems. In this paper, we outline an approach to HPC package development, deployment, management, sharing, and reuse based on the Nix functional package manager. We report our experience with this approach inside the GRICAD HPC center[GRICAD 2017a] in Grenoble over a 12 month period and compare it to other existing approaches.},
  isbn = {978-1-4503-5130-0},
  keywords = {project-acm-rep,project-provenance-pp,research software engineering},
  annotation = {interest: 99},
  file = {/home/sam/Zotero/storage/Z788J5GN/Bzeznik et al. - 2017 - Nix as HPC package management system.pdf}
}

@article{carataPrimerProvenance2014,
  title = {A Primer on Provenance},
  author = {Carata, Lucian and Akoush, Sherif and Balakrishnan, Nikilesh and Bytheway, Thomas and Sohan, Ripduman and Seltzer, Margo and Hopper, Andy},
  year = {2014},
  month = may,
  journal = {Commun. ACM},
  volume = {57},
  number = {5},
  pages = {52--60},
  issn = {0001-0782},
  doi = {10.1145/2596628},
  urldate = {2023-08-23},
  abstract = {Better understanding data requires tracking its history and context.},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/K2EIEAY9/Carata et al. - 2014 - A primer on provenance.pdf}
}

@incollection{cartaxoRapidReviewsSoftware2020,
  title = {Rapid {{Reviews}} in {{Software Engineering}}},
  booktitle = {Contemporary {{Empirical Methods}} in {{Software Engineering}}},
  author = {Cartaxo, Bruno and Pinto, Gustavo and Soares, Sergio},
  editor = {Felderer, Michael and Travassos, Guilherme Horta},
  year = {2020},
  pages = {357--384},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-32489-6_13},
  urldate = {2023-10-27},
  abstract = {Integrating research evidence into practice is one of the main goals of evidence-based software engineering (EBSE). Secondary studies, one of the main EBSE products, are intended to summarize the ``best'' research evidence and make them easily consumable by practitioners. However, recent studies show that some secondary studies lack connections with software engineering practice. In this chapter, we present the concept of Rapid Reviews, which are lightweight secondary studies focused on delivering evidence to practitioners in a timely manner. Rapid reviews support practitioners in their decision-making, and should be conducted bounded to a practical problem, inserted into a practical context. Thus, Rapid Reviews can be easily integrated in a knowledge/technology transfer initiative. After describing the basic concepts, we present the results and experiences of conducting two Rapid Reviews. We also provide guidelines to help researchers and practitioners who want to conduct Rapid Reviews, and we finally discuss topics that may concern the research community about the feasibility of Rapid Reviews as an evidence-based method. In conclusion, we believe Rapid Reviews might be of interest to researchers and practitioners working on the intersection of software engineering research and practice.},
  isbn = {978-3-030-32489-6},
  langid = {english},
  keywords = {project-provenance-pp,rapid reviews},
  file = {/home/sam/Zotero/storage/4XQIHKDN/Cartaxo et al. - 2020 - Rapid Reviews in Software Engineering.pdf}
}

@inproceedings{cartaxoRoleRapidReviews2018,
  title = {The {{Role}} of {{Rapid Reviews}} in {{Supporting Decision-Making}} in {{Software Engineering Practice}}},
  booktitle = {Proceedings of the 22nd {{International Conference}} on {{Evaluation}} and {{Assessment}} in {{Software Engineering}} 2018},
  author = {Cartaxo, Bruno and Pinto, Gustavo and Soares, Sergio},
  year = {2018},
  month = jun,
  series = {{{EASE}} '18},
  pages = {24--34},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3210459.3210462},
  urldate = {2023-10-26},
  abstract = {Context: Recent work on Evidence Based Software Engineering (EBSE) suggests that systematic reviews lack connection with Software Engineering (SE) practice. In Evidence Based Medicine there is a growing initiative to address this kind of problem, in particular through what has been named as Rapid Reviews (RRs). They are adaptations of regular systematic reviews made to fit practitioners constraints. Goal: Evaluate the perceptions from SE practitioners on the use of Rapid Reviews to support decision-making in SE practice. Method: We conducted an Action Research to evaluate RRs insertion in a real-world software development project. Results: Our results show that practitioners are rater positive about Rapid Reviews. They reported to have learned new concepts, reduced time and cost of decision-making, improved their understanding about the problem their facing, among other benefits. Additionally, two months after the introduction of the Rapid Review, in a follow up visit, we perceived that the practitioners have indeed adopted the evidence provided. Conclusions: Based on the positive results we obtained with this study, and the experiences reported in medicine, we believe RRs could play an important role towards knowledge transfer and decision-making support in SE practice.},
  isbn = {978-1-4503-6403-4},
  keywords = {project-provenance-pp,rapid reviews},
  file = {/home/sam/Zotero/storage/QDQC8QUF/Cartaxo et al. - 2018 - The Role of Rapid Reviews in Supporting Decision-M.pdf}
}

@misc{cespedesLtrace,
  title = {Ltrace},
  author = {Cespedes, Juan},
  abstract = {ltrace intercepts and records dynamic library calls which are called by an executed process and the signals received by that process. It can also intercept and print the system calls executed by the program.},
  keywords = {project-provenance-pp}
}

@inproceedings{chanExpressivenessBenchmarkingSystemLevel2017,
  title = {Expressiveness {{Benchmarking}} for \{\vphantom\}{{System-Level}}\vphantom\{\} {{Provenance}}},
  booktitle = {9th {{USENIX Workshop}} on the {{Theory}} and {{Practice}} of {{Provenance}} ({{TaPP}} 2017)},
  author = {Chan, Sheung Chi and Gehani, Ashish and Cheney, James and Sohan, Ripduman and Irshad, Hassaan},
  year = {2017},
  urldate = {2023-08-23},
  langid = {english},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/YWH8KUWW/Chan et al. - 2017 - Expressiveness Benchmarking for System-Level Pro.pdf}
}

@inproceedings{chanProvMarkProvenanceExpressiveness2019a,
  title = {{{ProvMark}}: {{A Provenance Expressiveness Benchmarking System}}},
  shorttitle = {{{ProvMark}}},
  booktitle = {Proceedings of the 20th {{International Middleware Conference}}},
  author = {Chan, Sheung Chi and Cheney, James and Bhatotia, Pramod and Pasquier, Thomas and Gehani, Ashish and Irshad, Hassaan and Carata, Lucian and Seltzer, Margo},
  year = {2019},
  month = dec,
  series = {Middleware '19},
  pages = {268--279},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3361525.3361552},
  urldate = {2023-08-21},
  abstract = {System level provenance is of widespread interest for applications such as security enforcement and information protection. However, testing the correctness or completeness of provenance capture tools is challenging and currently done manually. In some cases there is not even a clear consensus about what behavior is correct. We present an automated tool, ProvMark, that uses an existing provenance system as a black box and reliably identifies the provenance graph structure recorded for a given activity, by a reduction to subgraph isomorphism problems handled by an external solver. ProvMark is a beginning step in the much needed area of testing and comparing the expressiveness of provenance systems. We demonstrate ProvMark's usefuless in comparing three capture systems with different architectures and distinct design philosophies.},
  isbn = {978-1-4503-7009-7},
  keywords = {project-provenance-pp,provenance},
  file = {/home/sam/Zotero/storage/Q4ITJ896/Chan et al. - 2019 - ProvMark A Provenance Expressiveness Benchmarking.pdf}
}

@article{chengCompressionLowRank2005,
  title = {On the {{Compression}} of {{Low Rank Matrices}}},
  author = {Cheng, H. and Gimbutas, Z. and Martinsson, P. G. and Rokhlin, V.},
  year = {2005},
  month = jan,
  journal = {SIAM J. Sci. Comput.},
  volume = {26},
  number = {4},
  pages = {1389--1404},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {1064-8275},
  doi = {10.1137/030602678},
  urldate = {2024-02-05},
  abstract = {Randomized sampling has recently been proven a highly efficient technique for computing approximate factorizations of matrices that have low numerical rank. This paper describes an extension of such techniques to a wider class of matrices that are not themselves rank-deficient but have off-diagonal blocks that are; specifically, the class of so-called hierarchically semiseparable (HSS) matrices. HSS matrices arise frequently in numerical analysis and signal processing, particularly in the construction of fast methods for solving differential and integral equations numerically. The HSS structure admits algebraic operations (matrix-vector multiplications, matrix factorizations, matrix inversion, etc.) to be performed very rapidly, but only once the HSS representation of the matrix has been constructed. How to rapidly compute this representation in the first place is much less well understood. The present paper demonstrates that if an \$N{\textbackslash}times N\$ matrix can be applied to a vector in \$O(N)\$ time, and if individual entries of the matrix can be computed rapidly, then provided that an HSS representation of the matrix exists, it can be constructed in \$O(N{\textbackslash},k\^\{2\})\$ operations, where k is an upper bound for the numerical rank of the off-diagonal blocks. The point is that when legacy codes (based on, e.g., the fast multipole method) can be used for the fast matrix-vector multiply, the proposed algorithm can be used to obtain the HSS representation of the matrix, and then well-established techniques for HSS matrices can be used to invert or factor the matrix.},
  keywords = {linear algebra,project-provenance-pp},
  file = {/home/sam/Zotero/storage/AP4Z4PNZ/Cheng et al. - 2005 - On the Compression of Low Rank Matrices.pdf}
}

@inproceedings{chirigatiReproZipComputationalReproducibility2016,
  title = {{{ReproZip}}: {{Computational Reproducibility With Ease}}},
  shorttitle = {{{ReproZip}}},
  booktitle = {Proceedings of the 2016 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Chirigati, Fernando and Rampin, R{\'e}mi and Shasha, Dennis and Freire, Juliana},
  year = {2016},
  month = jun,
  series = {{{SIGMOD}} '16},
  pages = {2085--2088},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2882903.2899401},
  urldate = {2023-10-16},
  abstract = {We present ReproZip, the recommended packaging tool for the SIGMOD Reproducibility Review. ReproZip was designed to simplify the process of making an existing computational experiment reproducible across platforms, even when the experiment was put together without reproducibility in mind. The tool creates a self-contained package for an experiment by automatically tracking and identifying all its required dependencies. The researcher can share the package with others, who can then use ReproZip to unpack the experiment, reproduce the findings on their favorite operating system, as well as modify the original experiment for reuse in new research, all with little effort. The demo will consist of examples of non-trivial experiments, showing how these can be packed in a Linux machine and reproduced on different machines and operating systems. Demo visitors will also be able to pack and reproduce their own experiments.},
  isbn = {978-1-4503-3531-7},
  keywords = {project-provenance-pp,record-replay,reproducibility engineering},
  file = {/home/sam/Zotero/storage/LL2ND3RF/Chirigati et al. - 2016 - ReproZip Computational Reproducibility With Ease.pdf}
}

@techreport{collbergRepeatabilityBenefactionComputer2015,
  title = {Repeatability and {{Benefaction}} in {{Computer Systems Research}}{\textemdash}{{A Study}} and a {{Modest Proposal}}},
  author = {Collberg, Christian and Proebsting, Todd and Warren, Alex M},
  year = {2015},
  month = feb,
  number = {14-04},
  institution = {{University of Arizona}},
  abstract = {We describe a study into the extent to which Computer Systems researchers share their code and data and the extent to which such code builds. Starting with 601 papers from ACM conferences and journals, we examine 402 papers whose results were backed by code. For 32.3\% of these papers we were able to obtain the code and build it within 30 minutes; for 48.3\% of the papers we managed to build the code, but it may have required extra effort; for 54.0\% of the papers either we managed to build the code or the authors stated the code would build with reasonable effort. We also propose a novel sharing specification scheme that requires researchers to specify the level of sharing that reviewers and readers can assume from a paper.},
  keywords = {project-provenance-pp,reproducibility engineering},
  file = {/home/sam/Zotero/storage/K9CA8GSQ/RepeatabilityTR.pdf}
}

@article{collbergRepeatabilityComputerSystems2016,
  title = {Repeatability in Computer Systems Research},
  author = {Collberg, Christian and Proebsting, Todd A.},
  year = {2016},
  month = feb,
  journal = {Commun. ACM},
  volume = {59},
  number = {3},
  pages = {62--69},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/2812803},
  urldate = {2022-05-27},
  abstract = {To encourage repeatable research, fund repeatability engineering and reward commitments to sharing research artifacts.},
  langid = {english},
  keywords = {internship-project,project-acm-rep,project-provenance-pp,research software engineering},
  file = {/home/sam/Zotero/storage/JGDDR733/2812803.pdf}
}

@misc{ConfirmationDepthMeasure2014,
  title = {Confirmation {{Depth}} as a Measure of Reproducible Scientific Research.},
  year = {2014},
  month = oct,
  journal = {David Soergel},
  urldate = {2023-02-23},
  abstract = {What does it mean to reproduce a scientific study?  Confirmation Depth provides a guiding principle.},
  howpublished = {http://davidsoergel.com/posts/confirmation-depth-as-a-measure-of-reproducible-scientific-research},
  langid = {english},
  keywords = {project-provenance-pp},
  file = {/home/sam/Zotero/storage/KNRLR2QW/confirmation-depth-as-a-measure-of-reproducible-scientific-research.html}
}

@article{constantinDocumentComponentsOntology2016,
  title = {The {{Document Components Ontology}} ({{DoCO}})},
  author = {Constantin, Alexandru and Peroni, Silvio and Pettifer, Steve and Shotton, David and Vitali, Fabio},
  year = {2016},
  month = jan,
  journal = {Semantic Web},
  volume = {7},
  number = {2},
  pages = {167--181},
  publisher = {{IOS Press}},
  issn = {1570-0844},
  doi = {10.3233/SW-150177},
  urldate = {2023-05-25},
  abstract = {The availability in machine-readable form of descriptions of the structure of documents, as well as of the document discourse (e.g. the scientific discourse within scholarly articles), is crucial for facilitating semantic publishing and the overall c},
  langid = {english},
  keywords = {project-provenance-pp,semantic web},
  file = {/home/sam/Zotero/storage/AK4CAYTZ/Constantin et al. - 2016 - The&nbsp;Document&nbsp;Components&nbsp;Ontology&nb.pdf}
}

@misc{coulourisBlastBenchmark2016,
  title = {Blast {{Benchmark}}},
  author = {Coulouris, George and NIH Staff},
  year = {2016},
  journal = {Fiehn Lab},
  urldate = {2023-12-04},
  howpublished = {https://fiehnlab.ucdavis.edu/staff/kind/Collector/Benchmark/blast-benchmark},
  keywords = {project-provenance-pp},
  file = {/home/sam/Zotero/storage/N6N7WD2M/blast-benchmark.html}
}

@inproceedings{courtesReproducibleUserControlledSoftware2015,
  title = {Reproducible and {{User-Controlled Software Environments}} in {{HPC}} with {{Guix}}},
  booktitle = {Euro-{{Par}} 2015: {{Parallel Processing Workshops}}},
  author = {Court{\`e}s, Ludovic and Wurmus, Ricardo},
  editor = {Hunold, Sascha and Costan, Alexandru and Gim{\'e}nez, Domingo and Iosup, Alexandru and Ricci, Laura and G{\'o}mez Requena, Mar{\'i}a Engracia and Scarano, Vittorio and Varbanescu, Ana Lucia and Scott, Stephen L. and Lankes, Stefan and Weidendorfer, Josef and Alexander, Michael},
  year = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {579--591},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-27308-2_47},
  abstract = {Support teams of high-performance computing (HPC) systems often find themselves between a rock and a hard place: on one hand, they understandably administrate these large systems in a conservative way, but on the other hand, they try to satisfy their users by deploying up-to-date tool chains as well as libraries and scientific software. HPC system users often have no guarantee that they will be able to reproduce results at a later point in time, even on the same system{\textemdash}software may have been upgraded, removed, or recompiled under their feet, and they have little hope of being able to reproduce the same software environment elsewhere. We present GNU~Guix and the functional package management paradigm and show how it can improve reproducibility and sharing among researchers with representative use cases.},
  isbn = {978-3-319-27308-2},
  langid = {english},
  keywords = {project-acm-rep,project-provenance-pp,research software engineering},
  annotation = {interest: 99},
  file = {/home/sam/Zotero/storage/BWSF7UIR/Courtès and Wurmus - 2015 - Reproducible and User-Controlled Software Environm.pdf}
}

@inproceedings{daiLightweightProvenanceService2017,
  title = {Lightweight {{Provenance Service}} for {{High-Performance Computing}}},
  booktitle = {2017 26th {{International Conference}} on {{Parallel Architectures}} and {{Compilation Techniques}} ({{PACT}})},
  author = {Dai, Dong and Chen, Yong and Carns, Philip and Jenkins, John and Ross, Robert},
  year = {2017},
  month = sep,
  pages = {117--129},
  doi = {10.1109/PACT.2017.14},
  urldate = {2024-02-14},
  abstract = {Provenance describes detailed information about the history of a piece of data, containing the relationships among elements such as users, processes, jobs, and workflows that contribute to the existence of data. Provenance is key to supporting many data management functionalities that are increasingly important in operations such as identifying data sources, parameters, or assumptions behind a given result; auditing data usage; or understanding details about how inputs are transformed into outputs. Despite its importance, however, provenance support is largely underdeveloped in highly parallel architectures and systems. One major challenge is the demanding requirements of providing provenance service in situ. The need to remain lightweight and to be always on often conflicts with the need to be transparent and offer an accurate catalog of details regarding the applications and systems. To tackle this challenge, we introduce a lightweight provenance service, called LPS, for high-performance computing (HPC) systems. LPS leverages a kernel instrument mechanism to achieve transparency and introduces representative execution and flexible granularity to capture comprehensive provenance with controllable overhead. Extensive evaluations and use cases have confirmed its efficiency and usability. We believe that LPS can be integrated into current and future HPC systems to support a variety of data management needs.},
  keywords = {hpc,project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/7RQBN6AC/8091224.html}
}

@phdthesis{davisTechnologyAcceptanceModel1985,
  type = {Thesis},
  title = {A Technology Acceptance Model for Empirically Testing New End-User Information Systems: Theory and Results},
  shorttitle = {A Technology Acceptance Model for Empirically Testing New End-User Information Systems},
  author = {Davis, Fred D.},
  year = {1985},
  urldate = {2022-06-03},
  abstract = {The goal of this research is to develop and test a theoretical model of the effect of system characteristics on user acceptance of computer-based information systems. The model, referred to as the technology acceptance model (TAM), is being developed with two major objectives in mind. First, it should improve our understanding of user acceptance processes, providing new theoretical insights into the successful design and implementation of information systems. Second, TAM should provide the theoretical basis for a practical "user acceptance testing" methodology that would enable system designers and implementors to evaluate proposed new systems prior to their implementation. Applying the proposed model in user acceptance testing would involve demonstrating system prototypes to potential users and measuring their motivation to use the alternative systems. Such user acceptance testing could provide useful information about the relative likelihood of success of proposed systems early in their development, where such information has greatest value. Based on these objectives, key questions guiding this research include: 1. What are the major motivational variables that mediate between system characteristics and actual use of computer-based systems by end-users in organiza ional settings? 2. How are these variables causally related to one another, to system characteristics, and to user behavior? 3. How can user motivation be measured prior to organizational implementation in order to evaluate the rebtive likelihood of user acceptance for proposed new systems? For user acceptance testing to be viable, the associated model of user motivation must be valid. The present research takes several steps toward establishing a valid motivational model of the user, and aims to provide the foundation for future research that will tend to lead toward this end. Research steps taken in the present thesis include: 1. a fairly general, well-established theoretical model of human behavior from psychology was chosen as a paradigm within which to formulate the proposed technology acceptance model; 2. several adaptations to this paradigm were introduced in order to render it applicable to the present context; 3. published literature in the Management Information Systems and Human Factors fields was reviewed to demonstrate that empirical support exists for various elements of the proposed model, while at the same time the model goes beyond existing theoretical specifications, building upon and integrating previous research in a cumulative manner; 4. measures for the model's psychological variables were developed and pre-tested; 5. a field survey of 100 organizational users was conducted in order to validate the measures of the model's variables, and to test the model's structure, and 6. a laboratory user acceptance experiment of two business graphics systems involving 40 MBA student subjects was performed to further test the proposed model's structure, to test the ability to substitute videotape presentation for hands-on interaction in user acceptance tests, to evaluate the specific graphics systems being tested, and to test several theoretical extensions and refinements to the propose},
  copyright = {M.I.T. theses are protected by copyright. They may be viewed from this source for any purpose, but reproduction or distribution in any format is prohibited without written permission. See provided URL for inquiries about permission.},
  langid = {english},
  school = {Massachusetts Institute of Technology},
  keywords = {internship-project,project-provenance-pp,technology-acceptance},
  file = {/home/sam/Zotero/storage/VIZ75ECA/14927137-MIT.pdf}
}

@misc{desnoyersUsingLinuxKernel,
  title = {Using the {{Linux Kernel Tracepoints}}},
  author = {Desnoyers, Matthieu},
  journal = {The Linux Kernel documentation},
  urldate = {2023-08-24},
  howpublished = {https://www.kernel.org/doc/html/latest/trace/tracepoints.html},
  keywords = {operating systems,project-provenance-pp},
  file = {/home/sam/Zotero/storage/VCQP6JNQ/tracepoints.html}
}

@phdthesis{dolstraPurelyFunctionalSoftware2006,
  title = {The {{Purely Functional Software Deployment Model}}},
  author = {Dolstra, Eelco},
  year = {2006},
  month = jan,
  urldate = {2023-08-24},
  abstract = {Software deployment is the set of activities related to getting software components to work on the machines of end users. It includes activities such as installation, upgrading, uninstallation, and so on. Many tools have been developed to support deployment, but they all have serious limitations with respect to correctness. For instance, the installation of a component can lead to the failure of previously installed components; a component might require other components that are not present; and it is generally difficult to undo deployment actions. The fundamental causes of these problems are a lack of isolation between components, the difficulty in identifying the dependencies between components, and incompatibilities between versions and variants of components. This thesis describes a better approach based on a purely functional deployment model, implemented in a deployment system called Nix. Components are stored in isolation from each other in a Nix store. Each component has a name that contains a cryptographic hash of all inputs that contributed to its build process, and the content of a component never changes after it has been built. Hence the model is purely functional. This storage scheme provides several important advantages. First, it ensures isolation between components: if two components differ in any way, they will be stored in different locations and will not overwrite each other. Second, it allows us to identify component dependencies. Undeclared build time dependencies are prevented due to the absence of "global" component directories used in other deployment systems. Runtime dependencies can be found by scanning for cryptographic hashes in the binary contents of components, a technique analogous to conservative garbage collection in programming language implementation. Since dependency information is complete, complete deployment can be performed by copying closures of components under the dependency relation. Developers and users are not confronted with components' cryptographic hashes directly. Components are built automatically from Nix expressions, which describe how to build and compose arbitrary software components; hashes are computed as part of this process. Components are automatically made available to users through "user environments", which are synthesised sets of activated components. User environments enable atomic upgrades and rollbacks, as well as different sets of activated components for different users. Nix expressions provide a source-based deployment model. However, source-based deployment can be transparently optimised into binary deployment by making pre-built binaries (keyed on their cryptographic hashes) available in a shared location such as a network server. This is referred to as transparent source/binary deployment. The purely functional deployment model has been validated by applying it to the deployment of more than 278 existing Unix packages. In addition, this thesis shows that the model can be applied naturally to the related activities of continuous integration using build farms, service deployment and build management.},
  langid = {english},
  school = {Utrecht University},
  keywords = {operating systems,project-provenance-pp}
}

@misc{DTrace,
  title = {About {{DTrace}}},
  urldate = {2023-08-23},
  langid = {english},
  keywords = {operating systems,project-provenance-pp,prov-tool},
  file = {/home/sam/Zotero/storage/TDNCI4W3/about.html}
}

@inproceedings{elsnerEmpiricallyEvaluatingReadily2021,
  title = {Empirically Evaluating Readily Available Information for Regression Test Optimization in Continuous Integration},
  booktitle = {Proceedings of the 30th {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Elsner, Daniel and Hauer, Florian and Pretschner, Alexander and Reimer, Silke},
  year = {2021},
  month = jul,
  series = {{{ISSTA}} 2021},
  pages = {491--504},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3460319.3464834},
  urldate = {2023-01-19},
  abstract = {Regression test selection (RTS) and prioritization (RTP) techniques aim to reduce testing efforts and developer feedback time after a change to the code base. Using various information sources, including test traces, build dependencies, version control data, and test histories, they have been shown to be effective. However, not all of these sources are guaranteed to be available and accessible for arbitrary continuous integration (CI) environments. In contrast, metadata from version control systems (VCSs) and CI systems are readily available and inexpensive. Yet, corresponding RTP and RTS techniques are scattered across research and often only evaluated on synthetic faults or in a specific industrial context. It is cumbersome for practitioners to identify insights that apply to their context, let alone to calibrate associated parameters for maximum cost-effectiveness. This paper consolidates existing work on RTP and unsafe RTS into an actionable methodology to build and evaluate such approaches that exclusively rely on CI and VCS metadata. To investigate how these approaches from prior research compare in heterogeneous settings, we apply the methodology in a large-scale empirical study on a set of 23 projects covering 37,000 CI logs and 76,000 VCS commits. We find that these approaches significantly outperform established RTP baselines and, while still triggering 90\% of the failures, we show that practitioners can expect to save on average 84\% of test execution time for unsafe RTS. We also find that it can be beneficial to limit training data, features from test history work better than change-based features, and, somewhat surprisingly, simple and well-known heuristics often outperform complex machine-learned models.},
  isbn = {978-1-4503-8459-9},
  keywords = {continuous integration,project-acm-rep,project-provenance-pp,regression testing},
  annotation = {interest: 99},
  file = {/home/sam/Zotero/storage/CG4ZZ7MN/Elsner et al. - 2021 - Empirically evaluating readily available informati.pdf}
}

@inproceedings{erxlebenIntroducingWikidataLinked2014,
  title = {Introducing {{Wikidata}} to the {{Linked Data Web}}},
  booktitle = {The {{Semantic Web}} {\textendash} {{ISWC}} 2014},
  author = {Erxleben, Fredo and G{\"u}nther, Michael and Kr{\"o}tzsch, Markus and Mendez, Julian and Vrande{\v c}i{\'c}, Denny},
  editor = {Mika, Peter and Tudorache, Tania and Bernstein, Abraham and Welty, Chris and Knoblock, Craig and Vrande{\v c}i{\'c}, Denny and Groth, Paul and Noy, Natasha and Janowicz, Krzysztof and Goble, Carole},
  year = {2014},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {50--65},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-11964-9_4},
  abstract = {Wikidata is the central data management platform of Wikipedia. By the efforts of thousands of volunteers, the project has produced a large, open knowledge base with many interesting applications. The data is highly interlinked and connected to many other datasets, but it is also very rich, complex, and not available in RDF. To address this issue, we introduce new RDF exports that connect Wikidata to the Linked Data Web. We explain the data model of Wikidata and discuss its encoding in RDF. Moreover, we introduce several partial exports that provide more selective or simplified views on the data. This includes a class hierarchy and several other types of ontological axioms that we extract from the site. All datasets we discuss here are freely available online and updated regularly.},
  isbn = {978-3-319-11964-9},
  langid = {english},
  keywords = {project-provenance-pp,semantic web},
  file = {/home/sam/Zotero/storage/QZACD8IN/Erxleben et al. - 2014 - Introducing Wikidata to the Linked Data Web.pdf}
}

@misc{EventTracingWin322021,
  title = {Event {{Tracing}} - {{Win32}} Apps},
  year = {2021},
  month = jan,
  urldate = {2023-08-23},
  abstract = {This documentation is for user-mode applications that want to use ETW. For information about instrumenting device drivers that run in kernel mode, see WPP Software Tracing and Adding Event Tracing to Kernel-Mode Drivers in the Windows Driver Kit (WDK).},
  howpublished = {https://learn.microsoft.com/en-us/windows/win32/etw/event-tracing-portal},
  langid = {american},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/FCT2GL8J/event-tracing-portal.html}
}

@inproceedings{fadolalkarimPANDDEProvenancebasedANomaly2016,
  title = {{{PANDDE}}: {{Provenance-based ANomaly Detection}} of {{Data Exfiltration}}},
  shorttitle = {{{PANDDE}}},
  booktitle = {Proceedings of the {{Sixth ACM Conference}} on {{Data}} and {{Application Security}} and {{Privacy}}},
  author = {Fadolalkarim, Daren and Sallam, Asmaa and Bertino, Elisa},
  year = {2016},
  month = mar,
  series = {{{CODASPY}} '16},
  pages = {267--276},
  publisher = {{Association for Computing Machinery}},
  address = {{New Orleans Louisiana USA}},
  doi = {10.1145/2857705.2857710},
  urldate = {2023-08-24},
  abstract = {Preventing data exfiltration by insiders is a challenging process since insiders are users that have access permissions to the data. Existing mechanisms focus on tracking users' activities while they are connected to the database, and are unable to detect anomalous actions that the users perform on the data once they gain access to it. Being able to detect anomalous actions on the data is critical as these actions are often sign of attempts to misuse data. In this paper, we propose an approach to detect anomalous actions executed on data returned to the users from a database. The approach has been implemented as part of the Provenancebased ANomaly Detection of Data Exfiltration (PANDDE) tool. PANDDE leverages data provenance information captured at the operating system level. Such information is then used to create profiles of users' actions on the data once retrieved from the database. The profiles indicate actions that are consistent with the tasks of the users. Actions recorded in the profiles include data printing, emailing, and storage. Profiles are then used at run-time to detect anomalous actions.},
  isbn = {978-1-4503-3935-3},
  langid = {english},
  keywords = {project-provenance-pp},
  file = {/home/sam/Zotero/storage/6KESJXH2/Fadolalkarim et al. - 2016 - PANDDE Provenance-based ANomaly Detection of Data.pdf}
}

@inproceedings{ferreiradasilvaWorkflowHubCommunityFramework2020,
  title = {{{WorkflowHub}}: {{Community Framework}} for {{Enabling Scientific Workflow Research}} and {{Development}}},
  shorttitle = {{{WorkflowHub}}},
  booktitle = {2020 {{IEEE}}/{{ACM Workflows}} in {{Support}} of {{Large-Scale Science}} ({{WORKS}})},
  author = {{Ferreira da Silva}, Rafael and Pottier, Lo{\"i}c and Coleman, Tain{\~a} and Deelman, Ewa and Casanova, Henri},
  year = {2020},
  month = nov,
  pages = {49--56},
  publisher = {{IEEE}},
  address = {{Georgia, USA}},
  doi = {10.1109/WORKS51914.2020.00012},
  abstract = {Scientific workflows are a cornerstone of modern scientific computing. They are used to describe complex computational applications that require efficient and robust management of large volumes of data, which are typically stored/processed on heterogeneous, distributed resources. The workflow research and development community has employed a number of methods for the quantitative evaluation of existing and novel workflow algorithms and systems. In particular, a common approach is to simulate workflow executions. In previous work, we have presented a collection of tools that have been used for aiding research and development activities in the Pegasus project, and that have been adopted by others for conducting workflow research. Despite their popularity, there are several shortcomings that prevent easy adoption, maintenance, and consistency with the evolving structures and computational requirements of production workflows. In this work, we present WorkflowHub, a community framework that provides a collection of tools for analyzing workflow execution traces, producing realistic synthetic workflow traces, and simulating workflow executions. We demonstrate the realism of the generated synthetic traces by comparing simulated executions of these traces with actual workflow executions. We also contrast these results with those obtained when using the previously available collection of tools. We find that our framework not only can be used to generate representative synthetic workflow traces (i.e., with workflow structures and task characteristics distributions that resemble those in traces obtained from real-world workflow executions), but can also generate representative workflow traces at larger scales than that of available workflow traces.},
  keywords = {project-acm-rep,project-provenance-pp,workflow managers},
  file = {/home/sam/Zotero/storage/262NG8DJ/Silva et al. - 2020 - WorkflowHub Community Framework for Enabling Scie.pdf;/home/sam/Zotero/storage/8UFHN6M6/Silva et al. - 2020 - WorkflowHub Community Framework for Enabling Scie.pdf;/home/sam/Zotero/storage/C33VHWM8/9308170.html}
}

@inproceedings{fosterChimeraVirtualData2002,
  title = {Chimera: A Virtual Data System for Representing, Querying, and Automating Data Derivation},
  shorttitle = {Chimera},
  booktitle = {Proceedings 14th {{International Conference}} on {{Scientific}} and {{Statistical Database Management}}},
  author = {Foster, I. and Vockler, J. and Wilde, M. and Zhao, Yong},
  year = {2002},
  month = jul,
  pages = {37--46},
  issn = {1099-3371},
  doi = {10.1109/SSDM.2002.1029704},
  urldate = {2024-01-21},
  abstract = {A lot of scientific data is not obtained from measurements but rather derived from other data by the application of computational procedures. We hypothesize that explicit representation of these procedures can enable documentation of data provenance, discovery of available methods, and on-demand data generation (so-called "virtual data"). To explore this idea, we have developed the Chimera virtual data system, which combines a virtual data catalog for representing data derivation procedures and derived data, with a virtual data language interpreter that translates user requests into data definition and query operations on the database. We couple the Chimera system with distributed "data grid" services to enable on-demand execution of computation schedules constructed from database queries. We have applied this system to two challenge problems, the reconstruction of simulated collision event data from a high-energy physics experiment, and searching digital sky survey data for galactic clusters, with promising results.},
  keywords = {project-provenance-pp},
  file = {/home/sam/Zotero/storage/K4GKCHY6/1029704.html}
}

@article{freireProvenanceComputationalTasks2008,
  title = {Provenance for {{Computational Tasks}}: {{A Survey}}},
  shorttitle = {Provenance for {{Computational Tasks}}},
  author = {Freire, Juliana and Koop, David and Santos, Emanuele and Silva, Cl{\'a}udio T.},
  year = {2008},
  month = may,
  journal = {Comput. Sci. Eng.},
  volume = {10},
  number = {3},
  pages = {11--21},
  issn = {1521-9615},
  doi = {10.1109/MCSE.2008.79},
  urldate = {2022-07-08},
  abstract = {The problem of systematically capturing and managing provenance for computational tasks has recently received significant attention because of its relevance to a wide range of domains and applications. The authors give an overview of important concepts related to provenance management, so that potential users can make informed decisions when selecting or designing a provenance solution.},
  keywords = {project-provenance-pp,provenance},
  annotation = {interest: 97}
}

@inproceedings{frewES3DemonstrationTransparent2008,
  title = {{{ES3}}: {{A Demonstration}} of {{Transparent Provenance}} for {{Scientific Computation}}},
  shorttitle = {{{ES3}}},
  booktitle = {Provenance and {{Annotation}} of {{Data}} and {{Processes}}},
  author = {Frew, James and Slaughter, Peter},
  editor = {Freire, Juliana and Koop, David and Moreau, Luc},
  year = {2008},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {200--207},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-89965-5_21},
  abstract = {The Earth System Science Server (ES3) is a software environment for data-intensive Earth science, with unique capabilities for automatically and transparently capturing and managing the provenance of arbitrary computations. Transparent acquisition avoids the scientist having to express their computations in specific languages or schemas for provenance to be available. ES3 models provenance as relationships between processes and their input and output files. These relationships are captured by monitoring read and write accesses at various levels in the science software and asynchronously converting them to time-ordered streams of provenance events which are stored in an XML database. An ES3 provenance query returns an XML serialization of a provenance graph, forward or backwards from a specified process or file. We demonstrate ES3 provenance by generating complex data products from Earth satellite imagery.},
  isbn = {978-3-540-89965-5},
  langid = {english},
  keywords = {project-provenance-pp},
  file = {/home/sam/Zotero/storage/QREK9HHT/Frew and Slaughter - 2008 - ES3 A Demonstration of Transparent Provenance for.pdf}
}

@misc{FUSE,
  title = {{{FUSE}}},
  journal = {The Linux Kernel documentation},
  urldate = {2023-08-24},
  howpublished = {https://www.kernel.org/doc/html/latest/filesystems/fuse.html},
  keywords = {operating systems,project-provenance-pp},
  file = {/home/sam/Zotero/storage/V8S6UDQG/fuse.html}
}

@inproceedings{gamblinSpackPackageManager2015,
  title = {The {{Spack}} Package Manager: Bringing Order to {{HPC}} Software Chaos},
  shorttitle = {The {{Spack}} Package Manager},
  booktitle = {Proceedings of the {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  author = {Gamblin, Todd and LeGendre, Matthew and Collette, Michael R. and Lee, Gregory L. and Moody, Adam and {de Supinski}, Bronis R. and Futral, Scott},
  year = {2015},
  month = nov,
  series = {{{SC}} '15},
  pages = {1--12},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2807591.2807623},
  urldate = {2022-04-10},
  abstract = {Large HPC centers spend considerable time supporting software for thousands of users, but the complexity of HPC software is quickly outpacing the capabilities of existing software management tools. Scientific applications require specific versions of compilers, MPI, and other dependency libraries, so using a single, standard software stack is infeasible. However, managing many configurations is difficult because the configuration space is combinatorial in size. We introduce Spack, a tool used at Lawrence Livermore National Laboratory to manage this complexity. Spack provides a novel, recursive specification syntax to invoke parametric builds of packages and dependencies. It allows any number of builds to coexist on the same system, and it ensures that installed packages can find their dependencies, regardless of the environment. We show through real-world use cases that Spack supports diverse and demanding applications, bringing order to HPC software chaos.},
  isbn = {978-1-4503-3723-6},
  keywords = {high-performance computing,operating systems,package managers,project-acm-rep,project-astrophysics,project-provenance-pp,reproducibility engineering,research software engineering},
  annotation = {interest: 80},
  file = {/home/sam/Zotero/storage/7RMEVM2B/Gamblin et al. - 2015 - The Spack package manager bringing order to HPC s.pdf}
}

@misc{gandonRDFXMLSyntax2014,
  title = {{{RDF}} 1.1 {{XML Syntax}}},
  author = {Gandon, Fabian and Shcreiber, Guus and Beckett, David},
  year = {2014},
  month = feb,
  journal = {W3C Standards},
  urldate = {2023-05-26},
  abstract = {This document defines an XML syntax for RDF called RDF/XML in terms of Namespaces in XML, the XML Information Set and XML Base.},
  howpublished = {https://www.w3.org/TR/rdf-syntax-grammar/\#section-Syntax-blank-nodes},
  keywords = {project-provenance-pp,semantic web},
  file = {/home/sam/Zotero/storage/XP5APX3P/rdf-syntax-grammar.html}
}

@inproceedings{garijoNewApproachPublishing2011a,
  title = {A New Approach for Publishing Workflows: Abstractions, Standards, and Linked Data},
  shorttitle = {A New Approach for Publishing Workflows},
  booktitle = {Proceedings of the 6th Workshop on {{Workflows}} in Support of Large-Scale Science},
  author = {Garijo, Daniel and Gil, Yolanda},
  year = {2011},
  month = nov,
  series = {{{WORKS}} '11},
  pages = {47--56},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2110497.2110504},
  urldate = {2023-05-26},
  abstract = {In recent years, a variety of systems have been developed that export the workflows used to analyze data and make them part of published articles. We argue that the workflows that are published in current approaches are dependent on the specific codes used for execution, the specific workflow system used, and the specific workflow catalogs where they are published. In this paper, we describe a new approach that addresses these shortcomings and makes workflows more reusable through: 1) the use of abstract workflows to complement executable workflows to make them reusable when the execution environment is different, 2) the publication of both abstract and executable workflows using standards such as the Open Provenance Model that can be imported by other workflow systems, 3) the publication of workflows as Linked Data that results in open web accessible workflow repositories. We illustrate this approach using a complex workflow that we re-created from an influential publication that describes the generation of 'drugomes'.},
  isbn = {978-1-4503-1100-7},
  keywords = {project-provenance-pp,semantic web,workflow managers},
  file = {/home/sam/Zotero/storage/5FU8H8X6/Garijo and Gil - 2011 - A new approach for publishing workflows abstracti.pdf}
}

@inproceedings{gehaniSPADESupportProvenance2012,
  title = {{{SPADE}}: {{Support}} for {{Provenance Auditing}} in {{Distributed Environments}}},
  shorttitle = {{{SPADE}}},
  booktitle = {Middleware 2012},
  author = {Gehani, Ashish and Tariq, Dawood},
  editor = {Narasimhan, Priya and Triantafillou, Peter},
  year = {2012},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {101--120},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-35170-9_6},
  abstract = {SPADE is an open source software infrastructure for data provenance collection and management. The underlying data model used throughout the system is graph-based, consisting of vertices and directed edges that are modeled after the node and relationship types described in the Open Provenance Model. The system has been designed to decouple the collection, storage, and querying of provenance metadata. At its core is a novel provenance kernel that mediates between the producers and consumers of provenance information, and handles the persistent storage of records. It operates as a service, peering with remote instances to enable distributed provenance queries. The provenance kernel on each host handles the buffering, filtering, and multiplexing of incoming metadata from multiple sources, including the operating system, applications, and manual curation. Provenance elements can be located locally with queries that use wildcard, fuzzy, proximity, range, and Boolean operators. Ancestor and descendant queries are transparently propagated across hosts until a terminating expression is satisfied, while distributed path queries are accelerated with provenance sketches.},
  isbn = {978-3-642-35170-9},
  langid = {english},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/S8LZBZWF/Gehani and Tariq - 2012 - SPADE Support for Provenance Auditing in Distribu.pdf}
}

@inproceedings{gomez-perezWhenHistoryMatters2013,
  title = {When {{History Matters}} - {{Assessing Reliability}} for the {{Reuse}} of {{Scientific Workflows}}},
  booktitle = {The {{Semantic Web}} {\textendash} {{ISWC}} 2013},
  author = {{G{\'o}mez-P{\'e}rez}, Jos{\'e} Manuel and {Garc{\'i}a-Cuesta}, Esteban and Garrido, Aleix and Ruiz, Jos{\'e} Enrique and Zhao, Jun and Klyne, Graham},
  editor = {Alani, Harith and Kagal, Lalana and Fokoue, Achille and Groth, Paul and Biemann, Chris and Parreira, Josiane Xavier and Aroyo, Lora and Noy, Natasha and Welty, Chris and Janowicz, Krzysztof},
  year = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {81--97},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-41338-4_6},
  abstract = {Scientific workflows play an important role in computational research as essential artifacts for communicating the methods used to produce research findings. We are witnessing a growing number of efforts that treat workflows as first-class artifacts for sharing and exchanging scientific knowledge, either as part of scholarly articles or as stand-alone objects. However, workflows are not born to be reliable, which can seriously damage their reusability and trustworthiness as knowledge exchange instruments. Scientific workflows are commonly subject to decay, which consequently undermines their reliability over their lifetime. The reliability of workflows can be notably improved by advocating scientists to preserve a minimal set of information that is essential to assist the interpretations of these workflows and hence improve their potential for reproducibility and reusability. In this paper we show how, by measuring and monitoring the completeness and stability of scientific workflows over time we are able to provide scientists with a measure of their reliability, supporting the reuse of trustworthy scientific knowledge.},
  isbn = {978-3-642-41338-4},
  langid = {english},
  keywords = {project-acm-rep,project-provenance-pp,reproducibility,workflow managers},
  file = {/home/sam/Zotero/storage/5ARRBNBG/Gómez-Pérez et al. - 2013 - When History Matters - Assessing Reliability for t.pdf}
}

@misc{goochOverviewLinuxVirtual,
  title = {Overview of the {{Linux Virtual File System}}},
  author = {Gooch},
  journal = {The Linux Kernel documentation},
  urldate = {2023-08-24},
  howpublished = {https://docs.kernel.org/filesystems/vfs.html},
  keywords = {operating systems,project-provenance-pp},
  file = {/home/sam/Zotero/storage/CLVRT56M/vfs.html}
}

@misc{grandeNfprov2023,
  title = {Nf-Prov},
  author = {Grande, Bruno and Sherman, Ben and Di Tomasso, Paolo},
  year = {2023},
  month = may,
  urldate = {2023-05-25},
  copyright = {Apache-2.0},
  howpublished = {Sage-Bionetworks-Workflows},
  keywords = {project-provenance-pp,provenance}
}

@inproceedings{grayBioschemasPotatoSalad2017,
  title = {Bioschemas: {{From Potato Salad}} to {{Protein Annotation}}},
  shorttitle = {Bioschemas},
  booktitle = {{{ISWC}} 2017 {{Posters}} \& {{Demonstrations}} and {{Industry Tracks}}: {{Proceedings}} of the {{ISWC}} 2017 {{Posters}} \& {{Demonstrations}} and {{Industry Tracks}} Co-Located with 16th {{International Semantic Web Conference}} ({{ISWC}} 2017)},
  author = {Gray, Alasdair J. G. and Goble, Carole and Jimenez, Rafael C.},
  year = {2017},
  month = oct,
  publisher = {{RWTH Aachen University}},
  urldate = {2023-05-26},
  langid = {english},
  keywords = {project-provenance-pp,semantic web},
  file = {/home/sam/Zotero/storage/XU99KNK2/Gray et al. - 2017 - Bioschemas From Potato Salad to Protein Annotatio.pdf}
}

@inproceedings{graysonAutomaticReproductionWorkflows2023,
  title = {Automatic {{Reproduction}} of {{Workflows}} in the {{Snakemake Workflow Catalog}} and Nf-Core {{Registries}}},
  booktitle = {Proceedings of the 2023 {{ACM Conference}} on {{Reproducibility}} and {{Replicability}}},
  author = {Grayson, Samuel and Marinov, Darko and Katz, Daniel S. and Milewicz, Reed},
  year = {2023},
  month = jun,
  series = {{{ACM REP}} '23},
  pages = {74--84},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3589806.3600037},
  urldate = {2024-01-20},
  abstract = {Workflows make it easier for scientists to assemble computational experiments consisting of many disparate components. However, those disparate components also increase the probability that the computational experiment fails to be reproducible. Even if software is reproducible today, it may become irreproducible tomorrow without the software itself changing at all, because of the constantly changing software environment in which the software is run. To alleviate irreproducibility, workflow engines integrate with container engines. Additionally, communities that sprung up around workflow engines started to host registries for workflows that follow standards. These standards reduce the effort needed to make workflows automatically reproducible. In this paper, we study automatic reproduction of workflows from two registries, focusing on non-crashing executions. The experimental data lets us analyze the upper bound to which workflow engines could achieve reproducibility. We identify lessons learned in achieving reproducibility in practice.},
  isbn = {9798400701764},
  keywords = {project-provenance-pp,reproducibility,reproducibility engineering,research software engineering,workflow engines},
  file = {/home/sam/Zotero/storage/W6NHPEL2/Grayson et al. - 2023 - Automatic Reproduction of Workflows in the Snakema.pdf}
}

@techreport{graysonEvaluatingSystemLevelProvenance2023,
  title = {Evaluating {{System-Level Provenance Tools}} for {{Practical Use}}},
  author = {Grayson, Samuel and Reed, Milewicz},
  year = {2023},
  number = {SAND2023-13916R},
  pages = {172--181},
  address = {{Albuquerque, NM and Livermore, CA}},
  institution = {{Sandia National Laboratories}},
  abstract = {Tracking provenance has many applications in computational science, especially for im- proving reproducibility, but it is not yet widely used in practice. In this report, we execute a literature rapid review to find system-level provenance tools and methods for use in practice based on the method of collection, source availability, and platform compatibility.},
  langid = {english},
  keywords = {project-provenance-pp},
  file = {/home/sam/Zotero/storage/G2VJ9G2I/Seritan and Reuter - 2023 - CSRI Summer Program The Center for Computing Resea.pdf}
}

@article{grothAnatomyNanopublication2010,
  title = {The Anatomy of a Nanopublication},
  author = {Groth, Paul and Gibson, Andrew and Velterop, Jan},
  year = {2010},
  month = jan,
  journal = {Information Services \& Use},
  volume = {30},
  number = {1-2},
  pages = {51--56},
  publisher = {{IOS Press}},
  issn = {0167-5265},
  doi = {10.3233/ISU-2010-0613},
  urldate = {2023-05-26},
  abstract = {As the amount of scholarly communication increases, it is increasingly difficult for specific core scientific statements to be found, connected and curated. Additionally, the redundancy of these statements in multiple fora makes it difficult to deter},
  langid = {english},
  keywords = {academic publishing,project-provenance-pp,semantic web},
  file = {/home/sam/Zotero/storage/LLFTU3XH/Groth et al. - 2010 - The anatomy of a nanopublication.pdf}
}

@inproceedings{gruberEmpiricalStudyFlaky2021a,
  title = {An {{Empirical Study}} of {{Flaky Tests}} in {{Python}}},
  booktitle = {2021 14th {{IEEE Conference}} on {{Software Testing}}, {{Verification}} and {{Validation}} ({{ICST}})},
  author = {Gruber, Martin and Lukasczyk, Stephan and Kroi{\ss}, Florian and Fraser, Gordon},
  year = {2021},
  month = apr,
  pages = {148--158},
  issn = {2159-4848},
  doi = {10.1109/ICST49551.2021.00026},
  abstract = {Tests that cause spurious failures without any code changes, i.e., flaky tests, hamper regression testing, increase maintenance costs, may shadow real bugs, and decrease trust in tests. While the prevalence and importance of flakiness is well established, prior research focused on Java projects, thus raising the question of how the findings generalize. In order to provide a better understanding of the role of flakiness in software development beyond Java, we empirically study the prevalence, causes, and degree of flakiness within software written in Python, one of the currently most popular programming languages. For this, we sampled 22 352 open source projects from the popular PyPI package index, and analyzed their 876 186 test cases for flakiness. Our investigation suggests that flakiness is equally prevalent in Python as it is in Java. The reasons, however, are different: Order dependency is a much more dominant problem in Python, causing 59 \% of the 7 571 flaky tests in our dataset. Another 28 \% were caused by test infrastructure problems, which represent a previously undocumented cause of flakiness. The remaining 13 \% can mostly be attributed to the use of network and randomness APIs by the projects, which is indicative of the type of software commonly written in Python. Our data also suggests that finding flaky tests requires more runs than are often done in the literature: A 95 \% confidence that a passing test case is not flaky on average would require 170 reruns.},
  keywords = {project-provenance-pp,software mining,software testing},
  file = {/home/sam/Zotero/storage/A2WGSNPH/Gruber et al. - 2021 - An Empirical Study of Flaky Tests in Python.pdf;/home/sam/Zotero/storage/ZM52F9IF/9438576.html}
}

@inproceedings{guoCDEUsingSystem2011,
  title = {{{CDE}}: {{Using System Call Interposition}} to {{Automatically Create Portable Software Packages}}},
  booktitle = {2011 {{USENIX Annual Technical Conference}}},
  author = {Guo, Philip and Engler, Dawson},
  year = {2011},
  month = jun,
  publisher = {{USENIX}},
  address = {{Portland, OR, USA}},
  abstract = {It can be painfully hard to take software that runs on one person's machine and get it to run on another machine. Online forums and mailing lists are filled with discussions of users' troubles with compiling, installing, and configuring software and their myriad of dependencies. To eliminate this dependency problem, we created a system called CDE that uses system call interposition to monitor the execution of x86-Linux programs and package up the Code, Data, and Environment required to run them on other x86-Linux machines. Creating a CDE package is completely automatic, and running programs within a package requires no installation, configuration, or root permissions. Hundreds of people in both academia and industry have used CDE to distribute software, demo prototypes, make their scientific experiments reproducible, run software natively on older Linux distributions, and deploy experiments to compute clusters.},
  keywords = {project-provenance-pp,record-replay,reproducibility engineering}
}

@inproceedings{hanPROVIOOCentricProvenance2022,
  title = {{{PROV-IO}}: {{An I}}/{{O-Centric Provenance Framework}} for {{Scientific Data}} on {{HPC Systems}}},
  shorttitle = {{{PROV-IO}}},
  booktitle = {Proceedings of the 31st {{International Symposium}} on {{High-Performance Parallel}} and {{Distributed Computing}}},
  author = {Han, Runzhou and Byna, Suren and Tang, Houjun and Dong, Bin and Zheng, Mai},
  year = {2022},
  month = jun,
  series = {{{HPDC}} '22},
  pages = {213--226},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3502181.3531477},
  urldate = {2024-02-14},
  abstract = {cData provenance, or data lineage, describes the life cycle of data. In scientific workflows on HPC systems, scientists often seek diverse provenance (e.g., origins of data products, usage patterns of datasets). Unfortunately, existing provenance solutions cannot address the challenges due to their incompatible provenance models and/or system implementations. In this paper, we analyze three representative scientific workflows in collaboration with the domain scientists to identify concrete provenance needs. Based on the first-hand analysis, we propose a provenance framework called PROV-IO, which includes an I/O-centric provenance model for describing scientific data and the associated I/O operations and environments precisely. Moreover, we build a prototype of PROV-IO to enable end-to-end provenance support on real HPC systems with little manual effort. The PROV-IO framework provides flexibility in selecting various classes of provenance. Our experiments with realistic workflows show that PROV-IO can address the provenance needs of the domain scientists effectively with reasonable performance (e.g., less than 3.5\% tracking overhead for most experiments). Moreover, PROV-IO outperforms a state-of-the-art system (i.e., ProvLake) in our experiments.},
  isbn = {978-1-4503-9199-3},
  keywords = {hpc,project-provenance-pp,provenance},
  file = {/home/sam/Zotero/storage/FTWVMNMW/Han et al. - 2022 - PROV-IO An IO-Centric Provenance Framework for S.pdf}
}

@article{hassanOmegaLogHighFidelityAttack2020,
  title = {{{OmegaLog}}: {{High-Fidelity Attack Investigation}} via {{Transparent Multi-layer Log Analysis}}},
  shorttitle = {{{OmegaLog}}},
  author = {Hassan, Wajih Ul and Noureddine, Mohammad Ali and Datta, Pubali and Bates, Adam},
  year = {2020},
  month = jan,
  journal = {Network and Distributed System Security Symposium},
  urldate = {2023-08-23},
  abstract = {Recent advances in causality analysis have enabled investigators to trace multi-stage attacks using whole- system provenance graphs. Based on system-layer audit logs (e.g., syscalls), these approaches omit vital sources of application context (e.g., email addresses, HTTP response codes) that can found in higher layers of the system. Although this information is often essential to understanding attack behaviors, incorporating this evidence into causal analysis engines is difficult due to the semantic gap that exists between system layers. To address this shortcoming, we propose the notion of universal provenance, which encodes all forensically-relevant causal dependencies regardless of their layer of origin. To transparently realize this vision on commodity systems, we present {$\omega$}LOG (``Omega Log''), a provenance tracking mechanism that bridges the semantic gap between system and application logging contexts. {$\omega$}LOG analyzes program binaries to identify and model application-layer logging behaviors, enabling application events to be accurately reconciled with system-layer accesses. {$\omega$}LOG then intercepts applications' runtime logging activities and grafts those events onto the system-layer provenance graph, allowing investigators to reason more precisely about the nature of attacks. We demonstrate that {$\omega$}LOG is widely-applicable to existing software projects and can transparently facilitate execution partitioning of dependency graphs without any training or developer intervention. Evaluation on real-world attack scenarios shows that universal provenance graphs are concise and rich with semantic information as compared to the state-of-the-art, with 12\% average runtime overhead.},
  langid = {english},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/B7Y8LDR8/Hassan et al. - 2020 - OmegaLog High-Fidelity Attack Investigation via T.pdf}
}

@inproceedings{hassanScalableClusterAuditing2018,
  title = {Towards {{Scalable Cluster Auditing}} through {{Grammatical Inference}} over {{Provenance Graphs}}},
  booktitle = {Proceedings 2018 {{Network}} and {{Distributed System Security Symposium}}},
  author = {Hassan, Wajih Ul and Lemay, Mark and Aguse, Nuraini and Bates, Adam and Moyer, Thomas},
  year = {2018},
  publisher = {{Internet Society}},
  address = {{San Diego, CA}},
  doi = {10.14722/ndss.2018.23141},
  urldate = {2023-08-23},
  abstract = {Investigating the nature of system intrusions in large distributed systems remains a notoriously difficult challenge. While monitoring tools (e.g., Firewalls, IDS) provide preliminary alerts through easy-to-use administrative interfaces, attack reconstruction still requires that administrators sift through gigabytes of system audit logs stored locally on hundreds of machines. At present, two fundamental obstacles prevent synergy between system-layer auditing and modern cluster monitoring tools: 1) the sheer volume of audit data generated in a data center is prohibitively costly to transmit to a central node, and 2) systemlayer auditing poses a ``needle-in-a-haystack'' problem, such that hundreds of employee hours may be required to diagnose a single intrusion.},
  isbn = {978-1-891562-49-5},
  langid = {english},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/JRBUPWS8/Hassan et al. - 2018 - Towards Scalable Cluster Auditing through Grammati.pdf}
}

@article{henningSPECCPU2006Benchmark2006,
  title = {{{SPEC CPU2006}} Benchmark Descriptions},
  author = {Henning, John L.},
  year = {2006},
  month = sep,
  journal = {SIGARCH Comput. Archit. News},
  volume = {34},
  number = {4},
  pages = {1--17},
  issn = {0163-5964},
  doi = {10.1145/1186736.1186737},
  urldate = {2024-02-08},
  abstract = {On August 24, 2006, the Standard Performance Evaluation Corporation (SPEC) announced CPU2006 [2], which replaces CPU2000. The SPEC CPU benchmarks are widely used in both industry and academia [3].},
  langid = {english},
  keywords = {benchmarking,project-provenance-pp},
  file = {/home/sam/Zotero/storage/LA6IRPY6/Henning - 2006 - SPEC CPU2006 benchmark descriptions.pdf}
}

@article{hollandPASSingProvenanceChallenge2008,
  title = {{{PASSing}} the Provenance Challenge},
  author = {Holland, David A. and Seltzer, Margo I. and Braun, Uri and {Muniswamy-Reddy}, Kiran-Kumar},
  year = {2008},
  journal = {Concurrency and Computation: Practice and Experience},
  volume = {20},
  number = {5},
  pages = {531--540},
  issn = {1532-0634},
  doi = {10.1002/cpe.1227},
  urldate = {2023-08-23},
  abstract = {Provenance-aware storage systems (PASS) are a new class of storage system treating provenance as a first-class object, providing automatic collection, storage, and management of provenance as well as query capabilities. We developed the first PASS prototype between 2005 and 2006, targeting scientific end users. Prior to undertaking the provenance challenge, we had focused on provenance collection and storage, without much emphasis on a query model or language. The challenge forced us to (quickly) develop a query model and infrastructure implementing this model. We present a brief overview of the PASS prototype and a discussion of the evolution of the query model that we developed for the challenge. Copyright {\copyright} 2007 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2007 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/VD4HIMW2/Holland et al. - 2008 - PASSing the provenance challenge.pdf;/home/sam/Zotero/storage/8HRDN6SV/cpe.html}
}

@article{howisonRetractBitrottenPublications2014,
  title = {Retract Bit-Rotten Publications: {{Aligning}} Incentives for Sustaining Scientific Software},
  shorttitle = {Retract Bit-Rotten Publications},
  author = {Howison, James},
  year = {2014},
  month = jul,
  publisher = {{figshare}},
  doi = {10.6084/m9.figshare.1111632.v1},
  urldate = {2023-02-23},
  abstract = {A provocation for the WSSSPE2 workshop},
  langid = {english},
  keywords = {continuous integration,project-provenance-pp,reproducibility engineering},
  file = {/home/sam/Zotero/storage/7VJNSRVB/1111632.html}
}

@inproceedings{janinCAREComprehensiveArchiver2014,
  title = {{{CARE}}, the Comprehensive Archiver for Reproducible Execution},
  booktitle = {Proceedings of the 1st {{ACM SIGPLAN Workshop}} on {{Reproducible Research Methodologies}} and {{New Publication Models}} in {{Computer Engineering}}},
  author = {Janin, Yves and Vincent, C{\'e}dric and Duraffort, R{\'e}mi},
  year = {2014},
  month = jun,
  series = {{{TRUST}} '14},
  pages = {1--7},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2618137.2618138},
  urldate = {2024-02-14},
  abstract = {We present CARE, the Comprehensive Archiver for Reproducible Execution on Linux. CARE runs in userland, requires no setup and performs a single task: building an archive that contains selected executables and files accessed by a given application during an observation run. To reproduce computational results from this initial run, it is then enough to unpack the archive that comes equipped with all necessary tools for re-execution in a confined environment. Technically, CARE leverages on PRoot, a generic system call interposition engine that relies on the ptrace mechanism to monitor (and if needed to modify) system calls emitted by applications under scrutiny. PRoot is extensible and CARE is properly speaking an extension of PRoot. CARE is available on x86\_64, x86 and ARM processors, and benefits from a new history-based algorithm that automatically selects files to be stored in a CARE archive.},
  isbn = {978-1-4503-2951-4},
  keywords = {operating systems,project-provenance-pp,provenance},
  file = {/home/sam/Zotero/storage/L8GLA9K4/Janin et al. - 2014 - CARE, the comprehensive archiver for reproducible .pdf}
}

@inproceedings{jiEnablingRefinableCrossHost2018,
  title = {Enabling {{Refinable}} \{\vphantom\}{{Cross-Host}}\vphantom\{\} {{Attack Investigation}} with {{Efficient Data Flow Tagging}} and {{Tracking}}},
  booktitle = {27th {{USENIX Security Symposium}} ({{USENIX Security}} 18)},
  author = {Ji, Yang and Lee, Sangho and Fazzini, Mattia and Allen, Joey and Downing, Evan and Kim, Taesoo and Orso, Alessandro and Lee, Wenke},
  year = {2018},
  pages = {1705--1722},
  urldate = {2023-08-23},
  abstract = {Investigating attacks across multiple hosts is challenging. The true dependencies between security-sensitive files, network endpoints, or memory objects from different hosts can be easily concealed by dependency explosion or undefined program behavior (e.g., memory corruption). Dynamic information flow tracking (DIFT) is a potential solution to this problem, but, existing DIFT techniques only track information flow within a single host and lack an efficient mechanism to maintain and synchronize the data flow tags globally across multiple hosts.  In this paper, we propose RTAG, an efficient data flow tagging and tracking mechanism that enables practical cross-host attack investigations. RTAG is based on three novel techniques. First, by using a record-and-replay technique, it decouples the dependencies between different data flow tags from the analysis, enabling lazy synchronization between independent and parallel DIFT instances of different hosts. Second, it takes advantage of systemcall-level provenance information to calculate and allocate the optimal tag map in terms of memory consumption. Third, it embeds tag information into network packets to track cross-host data flows with less than 0.05\% network bandwidth overhead. Evaluation results show that RTAG is able to recover the true data flows of realistic cross-host attack scenarios. Performance wise, RTAG reduces the memory consumption of DIFT-based analysis by up to 90\% and decreases the overall analysis time by 60\%{\textendash}90\% compared with previous investigation systems.},
  isbn = {978-1-939133-04-5},
  langid = {english},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/Y526VM3S/Ji et al. - 2018 - Enabling Refinable Cross-Host Attack Investigati.pdf}
}

@inproceedings{jiRAINRefinableAttack2017,
  title = {{{RAIN}}: {{Refinable Attack Investigation}} with {{On-demand Inter-Process Information Flow Tracking}}},
  shorttitle = {{{RAIN}}},
  booktitle = {Proceedings of the 2017 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Ji, Yang and Lee, Sangho and Downing, Evan and Wang, Weiren and Fazzini, Mattia and Kim, Taesoo and Orso, Alessandro and Lee, Wenke},
  year = {2017},
  month = oct,
  series = {{{CCS}} '17},
  pages = {377--390},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3133956.3134045},
  urldate = {2023-08-23},
  abstract = {As modern attacks become more stealthy and persistent, detecting or preventing them at their early stages becomes virtually impossible. Instead, an attack investigation or provenance system aims to continuously monitor and log interesting system events with minimal overhead. Later, if the system observes any anomalous behavior, it analyzes the log to identify who initiated the attack and which resources were affected by the attack and then assess and recover from any damage incurred. However, because of a fundamental tradeoff between log granularity and system performance, existing systems typically record system-call events without detailed program-level activities (e.g., memory operation) required for accurately reconstructing attack causality or demand that every monitored program be instrumented to provide program-level information. To address this issue, we propose RAIN, a Refinable Attack INvestigation system based on a record-replay technology that records system-call events during runtime and performs instruction-level dynamic information flow tracking (DIFT) during on-demand process replay. Instead of replaying every process with DIFT, RAIN conducts system-call-level reachability analysis to filter out unrelated processes and to minimize the number of processes to be replayed, making inter-process DIFT feasible. Evaluation results show that RAIN effectively prunes out unrelated processes and determines attack causality with negligible false positive rates. In addition, the runtime overhead of RAIN is similar to existing system-call level provenance systems and its analysis overhead is much smaller than full-system DIFT.},
  isbn = {978-1-4503-4946-8},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/T2R2UAKP/Ji et al. - 2017 - RAIN Refinable Attack Investigation with On-deman.pdf}
}

@inproceedings{jiRecProvProvenanceAwareUser2016,
  title = {{{RecProv}}: {{Towards Provenance-Aware User Space Record}} and {{Replay}}},
  shorttitle = {{{RecProv}}},
  booktitle = {Provenance and {{Annotation}} of {{Data}} and {{Processes}}},
  author = {Ji, Yang and Lee, Sangho and Lee, Wenke},
  editor = {Mattoso, Marta and Glavic, Boris},
  year = {2016},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {3--15},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-40593-3_1},
  abstract = {Deterministic record and replay systems have widely been used in software debugging, failure diagnosis, and intrusion detection. In order to detect the Advanced Persistent Threat (APT), online execution needs to be recorded with acceptable runtime overhead; then, investigators can analyze the replayed execution with heavy dynamic instrumentation. While most record and replay systems rely on kernel module or OS virtualization, those running at user space are favoured for being lighter weight and more portable without any of the changes needed for OS/Kernel virtualization. On the other hand, higher level provenance data at a higher level provides dynamic analysis with system causalities and hugely increases its efficiency. Considering both benefits, we propose a provenance-aware user space record and replay system, called RecProv. RecProv is designed to provide high provenance fidelity; specifically, with versioning files from the recorded trace logs and integrity protection to provenance data through real-time trace isolation. The collected provenance provides the high-level system dependency that helps pinpoint suspicious activities where further analysis can be applied. We show that RecProv is able to output accurate provenance in both visualized graph and W3C standardized PROV-JSON formats.},
  isbn = {978-3-319-40593-3},
  langid = {english},
  keywords = {project-provenance-pp,prov-tool},
  file = {/home/sam/Zotero/storage/R4637A8D/Ji et al. - 2016 - RecProv Towards Provenance-Aware User Space Recor.pdf}
}

@misc{kaliberaQuantifyingPerformanceChanges2020,
  title = {Quantifying {{Performance Changes}} with {{Effect Size Confidence Intervals}}},
  author = {Kalibera, Tomas and Jones, Richard},
  year = {2020},
  month = jul,
  number = {arXiv:2007.10899},
  eprint = {2007.10899},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2007.10899},
  urldate = {2023-08-22},
  abstract = {Measuring performance \& quantifying a performance change are core evaluation techniques in programming language and systems research. Of 122 recent scientific papers, as many as 65 included experimental evaluation that quantified a performance change using a ratio of execution times. Few of these papers evaluated their results with the level of rigour that has come to be expected in other experimental sciences. The uncertainty of measured results was largely ignored. Scarcely any of the papers mentioned uncertainty in the ratio of the mean execution times, and most did not even mention uncertainty in the two means themselves. Most of the papers failed to address the non-deterministic execution of computer programs (caused by factors such as memory placement, for example), and none addressed non-deterministic compilation. It turns out that the statistical methods presented in the computer systems performance evaluation literature for the design and summary of experiments do not readily allow this either. This poses a hazard to the repeatability, reproducibility and even validity of quantitative results. Inspired by statistical methods used in other fields of science, and building on results in statistics that did not make it to introductory textbooks, we present a statistical model that allows us both to quantify uncertainty in the ratio of (execution time) means and to design experiments with a rigorous treatment of those multiple sources of non-determinism that might impact measured performance. Better still, under our framework summaries can be as simple as "system A is faster than system B by 5.5\% \${\textbackslash}pm\$ 2.5\%, with 95\% confidence", a more natural statement than those derived from typical current practice, which are often misinterpreted. November 2013},
  archiveprefix = {arxiv},
  keywords = {project-provenance-pp,software benchmarking},
  file = {/home/sam/Zotero/storage/U4JKMEN2/Kalibera and Jones - 2020 - Quantifying Performance Changes with Effect Size C.pdf;/home/sam/Zotero/storage/U9LM4IQU/2007.html}
}

@techreport{katcherPostMarkNewFile2005,
  title = {{{PostMark}}: {{A New File System Benchmark}}},
  shorttitle = {Network {{Appliance}} - {{PostMark}}},
  author = {Katcher, Jeffrey},
  year = {2005},
  month = sep,
  number = {TR3022},
  urldate = {2024-01-22},
  abstract = {Existing file system benchmarks are deficient in portraying performance in the ephemeral small-file regime used by Internet software, especially:     electronic mail;     netnews; and     web-based commerce.  PostMark is a new benchmark to measure performance for this class of application. In this paper, PostMark test results are presented and analyzed for both UNIX and Windows NT application servers. Network Appliance Filers (file server appliances) are shown to provide superior performance (via NFS or CIFS) compared to local disk alternatives, especially at higher loads. Such results are consistent with reports from ISPs (Internet Service Providers) who have deployed NetApp filers to support such applications on a large scale.},
  keywords = {benchmarking,project-provenance-pp},
  file = {/home/sam/Zotero/storage/FVZZJ693/3022.html}
}

@inproceedings{kemerlisLibdftPracticalDynamic2012,
  title = {Libdft: Practical Dynamic Data Flow Tracking for Commodity Systems},
  shorttitle = {Libdft},
  booktitle = {Proceedings of the 8th {{ACM SIGPLAN}}/{{SIGOPS}} Conference on {{Virtual Execution Environments}}},
  author = {Kemerlis, Vasileios P. and Portokalidis, Georgios and Jee, Kangkook and Keromytis, Angelos D.},
  year = {2012},
  month = mar,
  series = {{{VEE}} '12},
  pages = {121--132},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2151024.2151042},
  urldate = {2023-08-23},
  abstract = {Dynamic data flow tracking (DFT) deals with tagging and tracking data of interest as they propagate during program execution. DFT has been repeatedly implemented by a variety of tools for numerous purposes, including protection from zero-day and cross-site scripting attacks, detection and prevention of information leaks, and for the analysis of legitimate and malicious software. We present libdft, a dynamic DFT framework that unlike previous work is at once fast, reusable, and works with commodity software and hardware. libdft provides an API for building DFT-enabled tools that work on unmodified binaries, running on common operating systems and hardware, thus facilitating research and rapid prototyping. We explore different approaches for implementing the low-level aspects of instruction-level data tracking, introduce a more efficient and 64-bit capable shadow memory, and identify (and avoid) the common pitfalls responsible for the excessive performance overhead of previous studies. We evaluate libdft using real applications with large codebases like the Apache and MySQL servers, and the Firefox web browser. We also use a series of benchmarks and utilities to compare libdft with similar systems. Our results indicate that it performs at least as fast, if not faster, than previous solutions, and to the best of our knowledge, we are the first to evaluate the performance overhead of a fast dynamic DFT implementation in such depth. Finally, libdft is freely available as open source software.},
  isbn = {978-1-4503-1176-2},
  keywords = {project-provenance-pp,prov-tool},
  file = {/home/sam/Zotero/storage/R4P4ZSXE/Kemerlis et al. - 2012 - libdft practical dynamic data flow tracking for c.pdf}
}

@misc{kenistonKernelProbesKprobes,
  title = {Kernel {{Probes}} ({{Kprobes}})},
  shorttitle = {Kernel {{Probes}} ({{Kprobes}})},
  author = {Keniston, Jim and Panchamukhi, Prasanna S and Hiramatsu, Masami},
  journal = {The Linux Kernel documentation},
  urldate = {2023-08-24},
  howpublished = {https://www.kernel.org/doc/html/latest/trace/kprobes.html},
  langid = {american},
  keywords = {operating systems,project-provenance-pp},
  file = {/home/sam/Zotero/storage/PALWK2QM/kprobes.html}
}

@misc{kleenIntelProcessorTrace2015,
  title = {{{Intel}}{\textregistered} {{Processor Trace}} on {{Linux}}},
  author = {Kleen, Andi and Strong, Beeman},
  year = {2015},
  month = aug,
  address = {{Seattle, Washington, USA}},
  langid = {english},
  keywords = {computer architecture,project-provenance-pp},
  file = {/home/sam/Zotero/storage/LLWIKPGQ/Kleen and Strong - Intel® Processor Trace on Linux.pdf}
}

@article{krafczykLearningReproducingComputational2021,
  title = {Learning from Reproducing Computational Results: Introducing Three Principles and the {{Reproduction Package}}},
  shorttitle = {Learning from Reproducing Computational Results},
  author = {Krafczyk, M. S. and Shi, A. and Bhaskar, A. and Marinov, D. and Stodden, V.},
  year = {2021},
  month = mar,
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {379},
  number = {2197},
  pages = {20200069},
  publisher = {{Royal Society}},
  doi = {10.1098/rsta.2020.0069},
  urldate = {2023-01-31},
  abstract = {We carry out efforts to reproduce computational results for seven published articles and identify barriers to computational reproducibility. We then derive three principles to guide the practice and dissemination of reproducible computational research: (i) Provide transparency regarding how computational results are produced; (ii) When writing and releasing research software, aim for ease of (re-)executability; (iii) Make any code upon which the results rely as deterministic as possible. We then exemplify these three principles with 12 specific guidelines for their implementation in practice. We illustrate the three principles of reproducible research with a series of vignettes from our experimental reproducibility work. We define a novel Reproduction Package, a formalism that specifies a structured way to share computational research artifacts that implements the guidelines generated from our reproduction efforts to allow others to build, reproduce and extend computational science. We make our reproduction efforts in this paper publicly available as exemplar Reproduction Packages. This article is part of the theme issue `Reliability and reproducibility in computational science: implementing verification, validation and uncertainty quantification in silico'.},
  keywords = {project-acm-rep,project-provenance-pp,reproducibility engineering},
  file = {/home/sam/Zotero/storage/P3RYMMMA/Krafczyk et al. - 2021 - Learning from reproducing computational results i.pdf}
}

@article{kurtzerSingularityScientificContainers2017,
  title = {Singularity: {{Scientific}} Containers for Mobility of Compute},
  shorttitle = {Singularity},
  author = {Kurtzer, Gregory M. and Sochat, Vanessa and Bauer, Michael W.},
  year = {2017},
  month = may,
  journal = {PLOS ONE},
  volume = {12},
  number = {5},
  pages = {e0177459},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0177459},
  urldate = {2023-01-29},
  abstract = {Here we present Singularity, software developed to bring containers and reproducibility to scientific computing. Using Singularity containers, developers can work in reproducible environments of their choosing and design, and these complete environments can easily be copied and executed on other platforms. Singularity is an open source initiative that harnesses the expertise of system and software engineers and researchers alike, and integrates seamlessly into common workflows for both of these groups. As its primary use case, Singularity brings mobility of computing to both users and HPC centers, providing a secure means to capture and distribute software and compute environments. This ability to create and deploy reproducible environments across these centers, a previously unmet need, makes Singularity a game changing development for computational science.},
  langid = {english},
  keywords = {project-acm-rep,project-provenance-pp,reproducibility engineering,research software engineering},
  file = {/home/sam/Zotero/storage/MYW2MIFY/Kurtzer et al. - 2017 - Singularity Scientific containers for mobility of.pdf}
}

@inproceedings{kwonLDXCausalityInference2016,
  title = {{{LDX}}: {{Causality Inference}} by {{Lightweight Dual Execution}}},
  shorttitle = {{{LDX}}},
  booktitle = {Proceedings of the {{Twenty-First International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}},
  author = {Kwon, Yonghwi and Kim, Dohyeong and Sumner, William Nick and Kim, Kyungtae and Saltaformaggio, Brendan and Zhang, Xiangyu and Xu, Dongyan},
  year = {2016},
  month = mar,
  series = {{{ASPLOS}} '16},
  pages = {503--515},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2872362.2872395},
  urldate = {2023-08-23},
  abstract = {Causality inference, such as dynamic taint anslysis, has many applications (e.g., information leak detection). It determines whether an event e is causally dependent on a preceding event c during execution. We develop a new causality inference engine LDX. Given an execution, it spawns a slave execution, in which it mutates c and observes whether any change is induced at e. To preclude non-determinism, LDX couples the executions by sharing syscall outcomes. To handle path differences induced by the perturbation, we develop a novel on-the-fly execution alignment scheme that maintains a counter to reflect the progress of execution. The scheme relies on program analysis and compiler transformation. LDX can effectively detect information leak and security attacks with an average overhead of 6.08\% while running the master and the slave concurrently on separate CPUs, much lower than existing systems that require instruction level monitoring. Furthermore, it has much better accuracy in causality inference.},
  isbn = {978-1-4503-4091-5},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/LJ8B4UR9/Kwon et al. - 2016 - LDX Causality Inference by Lightweight Dual Execu.pdf}
}

@inproceedings{kwonMCIModelingbasedCausality2018,
  title = {{{MCI}} : {{Modeling-based Causality Inference}} in {{Audit Logging}} for {{Attack Investigation}}},
  shorttitle = {{{MCI}}},
  booktitle = {Proceedings 2018 {{Network}} and {{Distributed System Security Symposium}}},
  author = {Kwon, Yonghwi and Wang, Fei and Wang, Weihang and Lee, Kyu Hyung and Lee, Wen-Chuan and Ma, Shiqing and Zhang, Xiangyu and Xu, Dongyan and Jha, Somesh and Ciocarlie, Gabriela and Gehani, Ashish and Yegneswaran, Vinod},
  year = {2018},
  publisher = {{Internet Society}},
  address = {{San Diego, CA}},
  doi = {10.14722/ndss.2018.23306},
  urldate = {2023-08-23},
  abstract = {In this paper, we develop a model based causality inference technique for audit logging that does not require any application instrumentation or kernel modification. It leverages a recent dynamic analysis, dual execution (LDX), that can infer precise causality between system calls but unfortunately requires doubling the resource consumption such as CPU time and memory consumption. For each application, we use LDX to acquire precise causal models for a set of primitive operations. Each model is a sequence of system calls that have inter-dependences, some of them caused by memory operations and hence implicit at the system call level. These models are described by a language that supports various complexity such as regular, context-free, and even context-sensitive. In production run, a novel parser is deployed to parse audit logs (without any enhancement) to model instances and hence derive causality. Our evaluation on a set of real-world programs shows that the technique is highly effective. The generated models can recover causality with 0\% false-positives (FP) and false-negatives (FN) for most programs and only 8.3\% FP and 5.2\% FN in the worst cases. The models also feature excellent composibility, meaning that the models derived from primitive operations can be composed together to describe causality for large and complex real world missions. Applying our technique to attack investigation shows that the system-wide attack causal graphs are highly precise and concise, having better quality than the state-of-the-art.},
  isbn = {978-1-891562-49-5},
  langid = {english},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/9I4EY8H5/Kwon et al. - 2018 - MCI  Modeling-based Causality Inference in Audit .pdf}
}

@inproceedings{lattnerLLVMCompilationFramework2004,
  title = {{{LLVM}}: A Compilation Framework for Lifelong Program Analysis \& Transformation},
  shorttitle = {{{LLVM}}},
  booktitle = {International {{Symposium}} on {{Code Generation}} and {{Optimization}}, 2004. {{CGO}} 2004.},
  author = {Lattner, C. and Adve, V.},
  year = {2004},
  month = mar,
  pages = {75--86},
  doi = {10.1109/CGO.2004.1281665},
  abstract = {We describe LLVM (low level virtual machine), a compiler framework designed to support transparent, lifelong program analysis and transformation for arbitrary programs, by providing high-level information to compiler transformations at compile-time, link-time, run-time, and in idle time between runs. LLVM defines a common, low-level code representation in static single assignment (SSA) form, with several novel features: a simple, language-independent type-system that exposes the primitives commonly used to implement high-level language features; an instruction for typed address arithmetic; and a simple mechanism that can be used to implement the exception handling features of high-level languages (and setjmp/longjmp in C) uniformly and efficiently. The LLVM compiler framework and code representation together provide a combination of key capabilities that are important for practical, lifelong analysis and transformation of programs. To our knowledge, no existing compilation approach provides all these capabilities. We describe the design of the LLVM representation and compiler framework, and evaluate the design in three ways: (a) the size and effectiveness of the representation, including the type information it provides; (b) compiler performance for several interprocedural problems; and (c) illustrative examples of the benefits LLVM provides for several challenging compiler problems.},
  keywords = {compilers,project-provenance-pp},
  file = {/home/sam/Zotero/storage/PEAVHRLJ/Lattner and Adve - 2004 - LLVM a compilation framework for lifelong program.pdf}
}

@inproceedings{leeHighAccuracyAttack2017,
  title = {High {{Accuracy Attack Provenance}} via {{Binary-based Execution Partition}}},
  booktitle = {Proceedings of the 2017 {{Network}} and {{Distributed System Security}} ({{NDSS}}) {{Symposium}}},
  author = {Lee, Kyu Hyung and Zhang, Xiangyu and Xu, Dongyan},
  year = {2017},
  abstract = {An important aspect of cyber attack forensics is to understand the provenance of suspicious events, as it discloses the root cause and ramifications of cyber attacks. Traditionally, this is done by analyzing audit log. However, the presence of long running programs makes a live process receiving a large volume of inputs and produce many outputs and each output may be causally related to all the preceding inputs, leading to dependence explosion and making attack investigations almost infeasible. We observe that a long running execution can be partitioned into individual units by monitoring the execution of the program's event-handling loops, with each iteration corresponding to the processing of an independent input/request. We reverse engineer such loops from application binaries. We also reverse engineer instructions that could cause workflows between units. Detecting such a workflow is critical to disclosing causality between units. We then perform selective logging for unit boundaries and unit dependences. Our experiments show that our technique, called BEEP, has negligible runtime overhead ({$<$} 1.4\%) and low space overhead (12.28\% on average). It is effective in capturing the minimal causal graph for every attack case we have studied, without any dependence explosion.},
  langid = {english},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/ABDE2U7Q/Lee et al. - High Accuracy Attack Provenance via Binary-based E.pdf}
}

@inproceedings{leeLogGCGarbageCollecting2013,
  title = {{{LogGC}}: Garbage Collecting Audit Log},
  shorttitle = {{{LogGC}}},
  booktitle = {Proceedings of the 2013 {{ACM SIGSAC}} Conference on {{Computer}} \& Communications Security},
  author = {Lee, Kyu Hyung and Zhang, Xiangyu and Xu, Dongyan},
  year = {2013},
  month = nov,
  series = {{{CCS}} '13},
  pages = {1005--1016},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2508859.2516731},
  urldate = {2024-01-21},
  abstract = {System-level audit logs capture the interactions between applications and the runtime environment. They are highly valuable for forensic analysis that aims to identify the root cause of an attack, which may occur long ago, or to determine the ramifications of an attack for recovery from it. A key challenge of audit log-based forensics in practice is the sheer size of the log files generated, which could grow at a rate of Gigabytes per day. In this paper, we propose LogGC, an audit logging system with garbage collection (GC) capability. We identify and overcome the unique challenges of garbage collection in the context of computer forensic analysis, which makes LogGC different from traditional memory GC techniques. We also develop techniques that instrument user applications at a small number of selected places to emit additional system events so that we can substantially reduce the false dependences between system events to improve GC effectiveness. Our results show that LogGC can reduce audit log size by 14 times for regular user systems and 37 times for server systems, without affecting the accuracy of forensic analysis.},
  isbn = {978-1-4503-2477-9},
  keywords = {project-provenance-pp},
  file = {/home/sam/Zotero/storage/E9P3C9QY/Lee et al. - 2013 - LogGC garbage collecting audit log.pdf}
}

@inproceedings{leeSecureProvenanceCloud2015,
  title = {Towards {{Secure Provenance}} in the {{Cloud}}: {{A Survey}}},
  shorttitle = {Towards {{Secure Provenance}} in the {{Cloud}}},
  booktitle = {2015 {{IEEE}}/{{ACM}} 8th {{International Conference}} on {{Utility}} and {{Cloud Computing}} ({{UCC}})},
  author = {Lee, Brian and Awad, Abir and Awad, Mirna},
  year = {2015},
  month = dec,
  pages = {577--582},
  doi = {10.1109/UCC.2015.102},
  abstract = {Provenance information are meta-data that summarize the history of the creation and the actions performed on an artefact e.g. data, process etc. Secure provenance is essential to improve data forensics, ensure accountability and increase the trust in the cloud. In this paper, we survey the existing cloud provenance management schemes and proposed security solutions. We investigate the current related security challenges resulting from the nature of the provenance model and the characteristics of the cloud and we finally identify potential research directions which we feel necessary t should be covered in order to build a secure cloud provenance for the next generation.},
  keywords = {project-provenance-pp},
  file = {/home/sam/Zotero/storage/AN3W669M/Lee et al. - 2015 - Towards Secure Provenance in the Cloud A Survey.pdf}
}

@article{libertyRandomizedAlgorithmsLowrank2007,
  title = {Randomized Algorithms for the Low-Rank Approximation of Matrices},
  author = {Liberty, Edo and Woolfe, Franco and Martinsson, Per-Gunnar and Rokhlin, Vladimir and Tygert, Mark},
  year = {2007},
  month = dec,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {104},
  number = {51},
  pages = {20167--20172},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.0709640104},
  urldate = {2024-01-25},
  abstract = {We describe two recently proposed randomized algorithms for the construction of low-rank approximations to matrices, and demonstrate their application (inter alia) to the evaluation of the singular value decompositions of numerically low-rank matrices. Being probabilistic, the schemes described here have a finite probability of failure; in most cases, this probability is rather negligible (10-17 is a typical value). In many situations, the new procedures are considerably more efficient and reliable than the classical (deterministic) ones; they also parallelize naturally. We present several numerical examples to illustrate the performance of the schemes.},
  keywords = {linear algebra,project-provenance-pp},
  file = {/home/sam/Zotero/storage/95J2R7CS/Liberty et al. - 2007 - Randomized algorithms for the low-rank approximati.pdf}
}

@article{liThreatDetectionInvestigation2021,
  title = {Threat Detection and Investigation with System-Level Provenance Graphs: {{A}} Survey},
  shorttitle = {Threat Detection and Investigation with System-Level Provenance Graphs},
  author = {Li, Zhenyuan and Chen, Qi Alfred and Yang, Runqing and Chen, Yan and Ruan, Wei},
  year = {2021},
  month = jul,
  journal = {Computers \& Security},
  volume = {106},
  pages = {102282},
  issn = {0167-4048},
  doi = {10.1016/j.cose.2021.102282},
  urldate = {2023-08-23},
  abstract = {With the development of information technology, the border of the cyberspace gets much broader and thus also exposes increasingly more vulnerabilities to attackers. Traditional mitigation-based defence strategies are challenging to cope with the current complicated situation. Security practitioners urgently need better tools to describe and modelling attacks for defense. The provenance graph seems like an ideal method for threat modelling with powerful semantic expression ability and attacks historic correlation ability. In this paper, we firstly introduce the basic concepts about system-level provenance graph and present a typical system architecture for provenance graph-based threat detection and investigation. A comprehensive provenance graph-based threat detection system can be divided into three modules: data collection module, data management module, and threat detection modules. Each module contains several components and involves different research problems. We systematically taxonomize and compare the existing algorithms and designs involved in them. Based on these comparisons, we identify the strategy of technology selection for real-world deployment. We also provide insights and challenges about the existing work to guide future research in this area.},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/EXE45PJ5/Li et al. - 2021 - Threat detection and investigation with system-lev.pdf;/home/sam/Zotero/storage/4NB2FLWF/S0167404821001061.html}
}

@article{lukPinBuildingCustomized2005,
  title = {Pin: Building Customized Program Analysis Tools with Dynamic Instrumentation},
  shorttitle = {Pin},
  author = {Luk, Chi-Keung and Cohn, Robert and Muth, Robert and Patil, Harish and Klauser, Artur and Lowney, Geoff and Wallace, Steven and Reddi, Vijay Janapa and Hazelwood, Kim},
  year = {2005},
  month = jun,
  journal = {SIGPLAN Not.},
  volume = {40},
  number = {6},
  pages = {190--200},
  issn = {0362-1340},
  doi = {10.1145/1064978.1065034},
  urldate = {2023-08-24},
  abstract = {Robust and powerful software instrumentation tools are essential for program analysis tasks such as profiling, performance evaluation, and bug detection. To meet this need, we have developed a new instrumentation system called Pin. Our goals are to provide easy-to-use, portable, transparent, and efficient instrumentation. Instrumentation tools (called Pintools) are written in C/C++ using Pin's rich API. Pin follows the model of ATOM, allowing the tool writer to analyze an application at the instruction level without the need for detailed knowledge of the underlying instruction set. The API is designed to be architecture independent whenever possible, making Pintools source compatible across different architectures. However, a Pintool can access architecture-specific details when necessary. Instrumentation with Pin is mostly transparent as the application and Pintool observe the application's original, uninstrumented behavior. Pin uses dynamic compilation to instrument executables while they are running. For efficiency, Pin uses several techniques, including inlining, register re-allocation, liveness analysis, and instruction scheduling to optimize instrumentation. This fully automated approach delivers significantly better instrumentation performance than similar tools. For example, Pin is 3.3x faster than Valgrind and 2x faster than DynamoRIO for basic-block counting. To illustrate Pin's versatility, we describe two Pintools in daily use to analyze production software. Pin is publicly available for Linux platforms on four architectures: IA32 (32-bit x86), EM64T (64-bit x86), Itanium{\textregistered}, and ARM. In the ten months since Pin 2 was released in July 2004, there have been over 3000 downloads from its website.},
  keywords = {computer architecture,project-provenance-pp},
  file = {/home/sam/Zotero/storage/LQ986PRH/Luk et al. - 2005 - Pin building customized program analysis tools wi.pdf}
}

@inproceedings{maAccurateLowCost2015,
  title = {Accurate, {{Low Cost}} and {{Instrumentation-Free Security Audit Logging}} for {{Windows}}},
  booktitle = {Proceedings of the 31st {{Annual Computer Security Applications Conference}}},
  author = {Ma, Shiqing and Lee, Kyu Hyung and Kim, Chung Hwan and Rhee, Junghwan and Zhang, Xiangyu and Xu, Dongyan},
  year = {2015},
  month = dec,
  series = {{{ACSAC}} '15},
  pages = {401--410},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2818000.2818039},
  urldate = {2023-08-23},
  abstract = {Audit logging is an important approach to cyber attack investigation. However, traditional audit logging either lacks accuracy or requires expensive and complex binary instrumentation. In this paper, we propose a Windows based audit logging technique that features accuracy and low cost. More importantly, it does not require instrumenting the applications, which is critical for commercial software with IP protection. The technique is build on Event Tracing for Windows (ETW). By analyzing ETW log and critical parts of application executables, a model can be constructed to parse ETW log to units representing independent sub-executions in a process. Causality inferred at the unit level renders much higher accuracy, allowing us to perform accurate attack investigation and highly effective log reduction.},
  isbn = {978-1-4503-3682-6},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/8Q2P53GV/Ma et al. - 2015 - Accurate, Low Cost and Instrumentation-Free Securi.pdf}
}

@inproceedings{mackoCollectingProvenanceXen2011,
  title = {Collecting {{Provenance}} via the {{Xen Hypervisor}}},
  booktitle = {Proceedings of the {{Theory}} and {{Practice}} of {{Provenance}} ({{TaPP}})},
  author = {Macko, Peter and Chiarini, Marc and Seltzer, Margo},
  year = {2011},
  publisher = {{USENIX}},
  abstract = {The Provenance Aware Storage Systems project (PASS) currently collects system-level provenance by intercept- ing system calls in the Linux kernel and storing the provenance in a stackable filesystem. While this ap- proach is reasonably efficient, it suffers from two sig- nificant drawbacks: each new revision of the kernel re- quires reintegration of PASS changes, the stability of which must be continually tested; also, the use of a stack- able filesystem makes it difficult to collect provenance on root volumes, especially during early boot. In this pa- per we describe an approach to collecting system-level provenance from virtual guest machines running under the Xen hypervisor. We make the case that our approach alleviates the aforementioned difficulties and promotes adoption of provenance collection within cloud comput- ing platforms.},
  keywords = {project-provenance-pp,provenance-tool}
}

@inproceedings{macqueenMethodsClassificationAnalysis1965,
  title = {Some Methods for Classification and Analysis of Multivariate Observatiosn},
  booktitle = {Proceedings of the {{Fifth Berkeley Symposium}} on {{Mathematical Statistics}} and {{Probability}}},
  author = {Macqueen, J},
  year = {1965},
  month = jun,
  volume = {1},
  pages = {281},
  publisher = {{University of California Press}},
  address = {{Los Angeles, CA}},
  langid = {english},
  keywords = {machine learning,project-provenance-pp},
  file = {/home/sam/Zotero/storage/VT8M33UC/Macqueen - SOME METHODS FOR CLASSIFICATION AND ANALYSIS OF MU.pdf}
}

@misc{madabhushanaConfigureLinuxSystem2021,
  title = {Configure {{Linux}} System Auditing with Auditd},
  author = {Madabhushana, Ashish Bharadwaj},
  year = {2021},
  month = oct,
  publisher = {{Red Hat, Inc.}},
  urldate = {2024-01-21},
  abstract = {Learn how to install, configure, and manage the audit daemon to track security-related information on your Linux systems.},
  chapter = {Enable Sysadmin},
  langid = {english},
  keywords = {project-provenance-pp},
  file = {/home/sam/Zotero/storage/VX3ZS8EH/configure-linux-auditing-auditd.html}
}

@inproceedings{maMPIMultiplePerspective2017,
  title = {\{\vphantom\}{{MPI}}\vphantom\{\}: {{Multiple Perspective Attack Investigation}} with {{Semantic Aware Execution Partitioning}}},
  shorttitle = {\{\vphantom\}{{MPI}}\vphantom\{\}},
  booktitle = {26th {{USENIX Security Symposium}} ({{USENIX Security}} 17)},
  author = {Ma, Shiqing and Zhai, Juan and Wang, Fei and Lee, Kyu Hyung and Zhang, Xiangyu and Xu, Dongyan},
  year = {2017},
  pages = {1111--1128},
  urldate = {2023-08-23},
  isbn = {978-1-931971-40-9},
  langid = {english},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/UXCKL2W3/Ma et al. - 2017 - MPI Multiple Perspective Attack Investigation w.pdf}
}

@inproceedings{maProTracerPracticalProvenance2016,
  title = {{{ProTracer}}: {{Towards Practical Provenance Tracing}} by {{Alternating Between Logging}} and {{Tainting}}},
  shorttitle = {{{ProTracer}}},
  booktitle = {Proceedings 2016 {{Network}} and {{Distributed System Security Symposium}}},
  author = {Ma, Shiqing and Zhang, Xiangyu and Xu, Dongyan},
  year = {2016},
  publisher = {{Internet Society}},
  address = {{San Diego, CA}},
  doi = {10.14722/ndss.2016.23350},
  urldate = {2023-08-23},
  abstract = {Provenance tracing is a very important approach to Advanced Persistent Threat (APT) attack detection and investigation. Existing techniques either suffer from the dependence explosion problem or have non-trivial space and runtime overhead, which hinder their application in practice. We propose ProTracer, a lightweight provenance tracing system that alternates between system event logging and unit level taint propagation. The technique is built on an on-the-fly system event processing infrastructure that features a very lightweight kernel module and a sophisticated user space daemon that performs concurrent and out-of-order event processing. The evaluation with different realistic system workloads and a number of attack cases show that ProTracer only produces 13MB log data per day, and 0.84GB(Server)/2.32GB(Client) in 3 months without losing any important information. The space consumption is only {$<$} 1.28\% of the state-of-the-art, 7 times smaller than an off-line garbage collection technique. The run-time overhead averages {$<$}7\% for servers and {$<$}5\% for regular applications. The generated attack causal graphs are a few times smaller than those by existing techniques while they are equally informative.},
  isbn = {978-1-891562-41-9},
  langid = {english},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/JW5VPETI/Ma et al. - 2016 - ProTracer Towards Practical Provenance Tracing by.pdf}
}

@misc{markrussSysmonSysinternals2023,
  title = {Sysmon - {{Sysinternals}}},
  author = {{markruss}},
  year = {2023},
  month = apr,
  urldate = {2023-08-23},
  abstract = {Monitors and reports key system activity via the Windows event log.},
  howpublished = {https://learn.microsoft.com/en-us/sysinternals/downloads/sysmon},
  langid = {american},
  keywords = {operating systems,project-provenance-pp,prov-tool},
  file = {/home/sam/Zotero/storage/VNS2CHDN/sysmon.html}
}

@inproceedings{mcvoyLmbenchPortableTools1996,
  title = {Lmbench: {{Portable Tools}} for {{Performance Analysis}}},
  booktitle = {Proceedings of the {{USENIX}} 1996 {{Annual Technical Conference}}},
  author = {McVoy, Larry and Staelin, Carl},
  year = {1996},
  month = jan,
  publisher = {{USENIX}},
  address = {{San Diego, CA}},
  abstract = {lmbench is a micro-benchmark suite designed to focus attention on the basic building blocks of many common system applications, such as databases, simulations, software development, and networking. In almost all cases, the individual tests are the result of analysis and isolation of a customer's actual performance problem. These tools can be, and currently are, used to compare different system implementations from different vendors. In several cases, the benchmarks have uncovered previously unknown bugs and design flaws. The results have shown a strong correlation between memory system performance and overall performance. lmbench includes an extensible database of results from systems current as of late  1995.},
  langid = {english},
  keywords = {project-provenance-pp},
  file = {/home/sam/Zotero/storage/7N67DY3L/McVoy et al. - lmbench Portable Tools for Performance Analysis.pdf}
}

@misc{mesnardReproducibleReplicableCFD2016,
  title = {Reproducible and Replicable {{CFD}}: It's Harder than You Think},
  shorttitle = {Reproducible and Replicable {{CFD}}},
  author = {Mesnard, Olivier and Barba, Lorena A.},
  year = {2016},
  month = oct,
  number = {arXiv:1605.04339},
  eprint = {1605.04339},
  primaryclass = {physics},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1605.04339},
  urldate = {2023-02-23},
  abstract = {Completing a full replication study of our previously published findings on bluff-body aerodynamics was harder than we thought. Despite the fact that we have good reproducible-research practices, sharing our code and data openly. Here's what we learned from three years, four CFD codes and hundreds of runs.},
  archiveprefix = {arxiv},
  keywords = {project-provenance-pp,reproducibility engineering},
  annotation = {interest: 94},
  file = {/home/sam/Zotero/storage/4C9ZRCHY/Mesnard and Barba - 2016 - Reproducible and replicable CFD it's harder than .pdf;/home/sam/Zotero/storage/7KT47NJL/1605.html}
}

@misc{meurerCondaCrossPlatform2014,
  title = {Conda: {{A Cross Platform Package Manager}} for Any {{Binary Distribution}}},
  shorttitle = {Conda},
  author = {Meurer, Aaron},
  year = {2014},
  month = jul,
  urldate = {2023-04-06},
  keywords = {project-provenance-pp}
}

@article{moreauSpecialIssueFirst2008,
  title = {Special {{Issue}}: {{The First Provenance Challenge}}},
  shorttitle = {Special {{Issue}}},
  author = {Moreau, Luc and Lud{\"a}scher, Bertram and Altintas, Ilkay and Barga, Roger S. and Bowers, Shawn and Callahan, Steven and Chin, George and Clifford, Ben and Cohen, Shirley and {Cohen-Boulakia}, Sarah and Davidson, Susan and Deelman, Ewa and Digiampietri, Luciano and Foster, Ian and Freire, Juliana and Frew, James and Futrelle, Joe and Gibson, Tara and Gil, Yolanda and Goble, Carole and Golbeck, Jennifer and Groth, Paul and Holland, David A. and Jiang, Sheng and Kim, Jihie and Koop, David and Krenek, Ales and McPhillips, Timothy and Mehta, Gaurang and Miles, Simon and Metzger, Dominic and Munroe, Steve and Myers, Jim and Plale, Beth and Podhorszki, Norbert and Ratnakar, Varun and Santos, Emanuele and Scheidegger, Carlos and Schuchardt, Karen and Seltzer, Margo and Simmhan, Yogesh L. and Silva, Claudio and Slaughter, Peter and Stephan, Eric and Stevens, Robert and Turi, Daniele and Vo, Huy and Wilde, Mike and Zhao, Jun and Zhao, Yong},
  year = {2008},
  month = apr,
  journal = {Concurrency Computat.: Pract. Exper.},
  volume = {20},
  number = {5},
  pages = {409--418},
  issn = {15320626, 15320634},
  doi = {10.1002/cpe.1233},
  urldate = {2022-07-08},
  langid = {english},
  keywords = {project-provenance-pp,provenance},
  annotation = {interest: 87},
  file = {/home/sam/Zotero/storage/A8LJX7P3/Moreau et al. - 2008 - Special Issue The First Provenance Challenge.pdf}
}

@inproceedings{muniswamy-reddyLayeringProvenanceSystems2009,
  title = {Layering in Provenance Systems},
  booktitle = {Proceedings of the 2009 Conference on {{USENIX Annual}} Technical Conference},
  author = {{Muniswamy-Reddy}, Kiran-Kumar and Braun, Uri and Holland, David A. and Macko, Peter and Maclean, Diana and Margo, Daniel and Seltzer, Margo and Smogor, Robin},
  year = {2009},
  month = jun,
  series = {{{USENIX}}'09},
  pages = {10},
  publisher = {{USENIX Association}},
  address = {{USA}},
  doi = {10.5555/1855807.1855817},
  urldate = {2023-07-18},
  abstract = {Digital provenance describes the ancestry or history of a digital object. Most existing provenance systems, however, operate at only one level of abstraction: the system call layer, a workflow specification, or the high-level constructs of a particular application. The provenance collectable in each of these layers is different, and all of it can be important. Single-layer systems fail to account for the different levels of abstraction at which users need to reason about their data and processes. These systems cannot integrate data provenance across layers and cannot answer questions that require an integrated view of the provenance. We have designed a provenance collection structure facilitating the integration of provenance across multiple levels of abstraction, including a workflow engine, a web browser, and an initial runtime Python provenance tracking wrapper. We layer these components atop provenance-aware network storage (NFS) that builds upon a Provenance-Aware Storage System (PASS). We discuss the challenges of building systems that integrate provenance across multiple layers of abstraction, present how we augmented systems in each layer to integrate provenance, and present use cases that demonstrate how provenance spanning multiple layers provides functionality not available in existing systems. Our evaluation shows that the overheads imposed by layering provenance systems are reasonable.},
  langid = {english},
  keywords = {project-provenance-pp,provenance,provenance-tool},
  file = {/home/sam/Zotero/storage/CB9P9RSJ/Muniswamy-Reddy et al. - Layering in Provenance Systems.pdf;/home/sam/Zotero/storage/GUGJKZNA/Muniswamy-Reddy et al. - Layering in Provenance Systems.pdf;/home/sam/Zotero/storage/EPL6YLK6/Muniswamy-Reddy et al. - Layering in Provenance Systems.html}
}

@inproceedings{muniswamy-reddyProvenanceAwareStorageSystems2006,
  title = {Provenance-{{Aware Storage Systems}}},
  booktitle = {2006 {{USENIX Annual Technical Conference}}},
  author = {{Muniswamy-Reddy}, Kiran-Kumar and Holland, David A and Braun, Uri and Seltzer, Margo},
  year = {2006},
  abstract = {A Provenance-Aware Storage System (PASS) is a storage system that automatically collects and maintains provenance or lineage, the complete history or ancestry of an item. We discuss the advantages of treating provenance as meta-data collected and maintained by the storage system, rather than as manual annotations stored in a separately administered database. We describe a PASS implementation, discussing the challenges it presents, performance cost it incurs, and the new functionality it enables. We show that with reasonable overhead, we can provide useful functionality not available in today's file systems or provenance management systems.},
  copyright = {open},
  langid = {american},
  keywords = {project-provenance-pp,provenance-tool},
  annotation = {Accepted: 2015-12-07T19:07:43Z},
  file = {/home/sam/Zotero/storage/2NZWHDHT/Muniswamy-Reddy et al. - 2006 - Provenance-Aware Storage Systems.pdf;/home/sam/Zotero/storage/BYWE4X2D/Muniswamy-Reddy et al. - 2006 - Provenance-Aware Storage Systems.pdf}
}

@article{mytkowiczProducingWrongData2009,
  title = {Producing Wrong Data without Doing Anything Obviously Wrong!},
  author = {Mytkowicz, Todd and Diwan, Amer and Hauswirth, Matthias and Sweeney, Peter F.},
  year = {2009},
  month = mar,
  journal = {SIGARCH Comput. Archit. News},
  volume = {37},
  number = {1},
  pages = {265--276},
  issn = {0163-5964},
  doi = {10.1145/2528521.1508275},
  urldate = {2022-04-11},
  abstract = {This paper presents a surprising result: changing a seemingly innocuous aspect of an experimental setup can cause a systems researcher to draw wrong conclusions from an experiment. What appears to be an innocuous aspect in the experimental setup may in fact introduce a significant bias in an evaluation. This phenomenon is called measurement bias in the natural and social sciences. Our results demonstrate that measurement bias is significant and commonplace in computer system evaluation. By significant we mean that measurement bias can lead to a performance analysis that either over-states an effect or even yields an incorrect conclusion. By commonplace we mean that measurement bias occurs in all architectures that we tried (Pentium 4, Core 2, and m5 O3CPU), both compilers that we tried (gcc and Intel's C compiler), and most of the SPEC CPU2006 C programs. Thus, we cannot ignore measurement bias. Nevertheless, in a literature survey of 133 recent papers from ASPLOS, PACT, PLDI, and CGO, we determined that none of the papers with experimental results adequately consider measurement bias. Inspired by similar problems and their solutions in other sciences, we describe and demonstrate two methods, one for detecting (causal analysis) and one for avoiding (setup randomization) measurement bias.},
  keywords = {project-acm-rep,project-provenance-pp,software benchmarking,software engineering},
  annotation = {score: 95 interest: 70},
  file = {/home/sam/Zotero/storage/WRXTAESN/1508284.1508275.pdf}
}

@inproceedings{namikiMethodConstructingResearch2023,
  title = {A {{Method}} for {{Constructing Research Data Provenance}} in {{High-Performance Computing Systems}}},
  booktitle = {2023 {{IEEE}} 19th {{International Conference}} on E-{{Science}} (e-{{Science}})},
  author = {Namiki, Yuta and Hosomi, Takeo and Tanushi, Hideyuki and Yamashita, Akihiro and Date, Susumu},
  year = {2023},
  month = oct,
  pages = {1--10},
  issn = {2325-3703},
  doi = {10.1109/e-Science58273.2023.10254932},
  urldate = {2024-02-14},
  abstract = {Research must be reproducible to be verifiable. Provenance, which describes how data was produced, is one of the metadata that can improve reproducibility. In this paper, we propose a method to construct the provenance of research data produced in high-performance computing (HPC) systems. Our method can construct a high-level and user-perspective provenance by integrating information available in HPC systems, such as a workload manager, with low-level data about running programs' behavior captured in an operating system kernel. The method enables users of HPC systems to collect the provenance without modifying assets such as programs and scripts.},
  keywords = {hpc,project-provenance-pp,provenance}
}

@misc{ocallahanEngineeringRecordReplay2017,
  title = {Engineering {{Record And Replay For Deployability}}: {{Extended Technical Report}}},
  shorttitle = {Engineering {{Record And Replay For Deployability}}},
  author = {O'Callahan, Robert and Jones, Chris and Froyd, Nathan and Huey, Kyle and Noll, Albert and Partush, Nimrod},
  year = {2017},
  month = may,
  number = {arXiv:1705.05937},
  eprint = {1705.05937},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1705.05937},
  urldate = {2024-01-26},
  abstract = {The ability to record and replay program executions with low overhead enables many applications, such as reverse-execution debugging, debugging of hard-to-reproduce test failures, and "black box" forensic analysis of failures in deployed systems. Existing record-and-replay approaches limit deployability by recording an entire virtual machine (heavyweight), modifying the OS kernel (adding deployment and maintenance costs), requiring pervasive code instrumentation (imposing significant performance and complexity overhead), or modifying compilers and runtime systems (limiting generality). We investigated whether it is possible to build a practical record-and-replay system avoiding all these issues. The answer turns out to be yes - if the CPU and operating system meet certain non-obvious constraints. Fortunately modern Intel CPUs, Linux kernels and user-space frameworks do meet these constraints, although this has only become true recently. With some novel optimizations, our system 'rr' records and replays real-world low-parallelism workloads with low overhead, with an entirely user-space implementation, using stock hardware, compilers, runtimes and operating systems. "rr" forms the basis of an open-source reverse-execution debugger seeing significant use in practice. We present the design and implementation of 'rr', describe its performance on a variety of workloads, and identify constraints on hardware and operating system design required to support our approach.},
  archiveprefix = {arxiv},
  keywords = {project-provenance-pp,reproducibility},
  file = {/home/sam/Zotero/storage/Z8KUXE23/O'Callahan et al. - 2017 - Engineering Record And Replay For Deployability E.pdf;/home/sam/Zotero/storage/TVC4MPID/1705.html}
}

@article{oliveiraProvenanceAnalyticsWorkflowBased2018,
  title = {Provenance {{Analytics}} for {{Workflow-Based Computational Experiments}}: {{A Survey}}},
  shorttitle = {Provenance {{Analytics}} for {{Workflow-Based Computational Experiments}}},
  author = {Oliveira, Wellington and Oliveira, Daniel De and Braganholo, Vanessa},
  year = {2018},
  month = may,
  journal = {ACM Comput. Surv.},
  volume = {51},
  number = {3},
  pages = {53:1--53:25},
  issn = {0360-0300},
  doi = {10.1145/3184900},
  urldate = {2023-02-23},
  abstract = {Until not long ago, manually capturing and storing provenance from scientific experiments were constant concerns for scientists. With the advent of computational experiments (modeled as scientific workflows) and Scientific Workflow Management Systems, produced and consumed data, as well as the provenance of a given experiment, are automatically managed, so provenance capturing and storing in such a context is no longer a major concern. Similarly to several existing big data problems, the bottom line is now on how to analyze the large amounts of provenance data generated by workflow executions and how to be able to extract useful knowledge of this data. In this context, this article surveys the current state of the art on provenance analytics by presenting the key initiatives that have been taken to support provenance data analysis. We also contribute by proposing a taxonomy to classify elements related to provenance analytics.},
  keywords = {project-provenance-pp,provenance},
  annotation = {interest: 95},
  file = {/home/sam/Zotero/storage/IT8P4NF7/Oliveira et al. - 2018 - Provenance Analytics for Workflow-Based Computatio.pdf}
}

@inproceedings{pasquierPracticalWholesystemProvenance2017,
  title = {Practical Whole-System Provenance Capture},
  booktitle = {Proceedings of the 2017 {{Symposium}} on {{Cloud Computing}}},
  author = {Pasquier, Thomas and Han, Xueyuan and Goldstein, Mark and Moyer, Thomas and Eyers, David and Seltzer, Margo and Bacon, Jean},
  year = {2017},
  month = sep,
  series = {{{SoCC}} '17},
  pages = {405--418},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3127479.3129249},
  urldate = {2023-07-07},
  abstract = {Data provenance describes how data came to be in its present form. It includes data sources and the transformations that have been applied to them. Data provenance has many uses, from forensics and security to aiding the reproducibility of scientific experiments. We present CamFlow, a whole-system provenance capture mechanism that integrates easily into a PaaS offering. While there have been several prior whole-system provenance systems that captured a comprehensive, systemic and ubiquitous record of a system's behavior, none have been widely adopted. They either A) impose too much overhead, B) are designed for long-outdated kernel releases and are hard to port to current systems, C) generate too much data, or D) are designed for a single system. CamFlow addresses these shortcoming by: 1) leveraging the latest kernel design advances to achieve efficiency; 2) using a self-contained, easily maintainable implementation relying on a Linux Security Module, NetFilter, and other existing kernel facilities; 3) providing a mechanism to tailor the captured provenance data to the needs of the application; and 4) making it easy to integrate provenance across distributed systems. The provenance we capture is streamed and consumed by tenant-built auditor applications. We illustrate the usability of our implementation by describing three such applications: demonstrating compliance with data regulations; performing fault/intrusion detection; and implementing data loss prevention. We also show how CamFlow can be leveraged to capture meaningful provenance without modifying existing applications.},
  isbn = {978-1-4503-5028-0},
  keywords = {project-provenance-pp,provenance,provenance-tool},
  file = {/home/sam/Zotero/storage/DYSRHKYG/Pasquier et al. - 2017 - Practical whole-system provenance capture.pdf}
}

@misc{perryCrossValidationUnsupervisedLearning2009,
  title = {Cross-{{Validation}} for {{Unsupervised Learning}}},
  author = {Perry, Patrick O.},
  year = {2009},
  month = sep,
  number = {arXiv:0909.3052},
  eprint = {0909.3052},
  primaryclass = {math, stat},
  publisher = {{arXiv}},
  urldate = {2024-02-10},
  abstract = {Cross-validation (CV) is a popular method for model-selection. Unfortunately, it is not immediately obvious how to apply CV to unsupervised or exploratory contexts. This thesis discusses some extensions of cross-validation to unsupervised learning, specifically focusing on the problem of choosing how many principal components to keep. We introduce the latent factor model, define an objective criterion, and show how CV can be used to estimate the intrinsic dimensionality of a data set. Through both simulation and theory, we demonstrate that cross-validation is a valuable tool for unsupervised learning.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {machine learning,project-provenance-pp},
  file = {/home/sam/Zotero/storage/5M3Y9SK7/Perry - 2009 - Cross-Validation for Unsupervised Learning.pdf}
}

@inproceedings{phamUsingProvenanceRepeatability2013,
  title = {Using {{Provenance}} for {{Repeatability}}},
  booktitle = {5th {{USENIX Workshop}} on the {{Theory}} and {{Practice}} of {{Provenance}} ({{TaPP}} 13)},
  author = {Pham, Quan and Malik, Tanu and Foster, Ian},
  year = {2013},
  urldate = {2024-02-14},
  langid = {english},
  keywords = {project-provenance-pp,provenance},
  file = {/home/sam/Zotero/storage/SNTQWLYD/Pham et al. - 2013 - Using Provenance for Repeatability.pdf}
}

@article{pimentelSurveyCollectingManaging2019,
  title = {A {{Survey}} on {{Collecting}}, {{Managing}}, and {{Analyzing Provenance}} from {{Scripts}}},
  author = {Pimentel, Jo{\~a}o Felipe and Freire, Juliana and Murta, Leonardo and Braganholo, Vanessa},
  year = {2019},
  month = jun,
  journal = {ACM Comput. Surv.},
  volume = {52},
  number = {3},
  pages = {47:1--47:38},
  issn = {0360-0300},
  doi = {10.1145/3311955},
  urldate = {2024-01-20},
  abstract = {Scripts are widely used to design and run scientific experiments. Scripting languages are easy to learn and use, and they allow complex tasks to be specified and executed in fewer steps than with traditional programming languages. However, they also have important limitations for reproducibility and data management. As experiments are iteratively refined, it is challenging to reason about each experiment run (or trial), to keep track of the association between trials and experiment instances as well as the differences across trials, and to connect results to specific input data and parameters. Approaches have been proposed that address these limitations by collecting, managing, and analyzing the provenance of scripts. In this article, we survey the state of the art in provenance for scripts. We have identified the approaches by following an exhaustive protocol of forward and backward literature snowballing. Based on a detailed study, we propose a taxonomy and classify the approaches using this taxonomy.},
  keywords = {project-provenance-pp,provenance},
  file = {/home/sam/Zotero/storage/YF83IWDC/Pimentel et al. - 2019 - A Survey on Collecting, Managing, and Analyzing Pr.pdf}
}

@inproceedings{pohlyHiFiCollectingHighfidelity2012,
  title = {Hi-{{Fi}}: Collecting High-Fidelity Whole-System Provenance},
  shorttitle = {Hi-{{Fi}}},
  booktitle = {Proceedings of the 28th {{Annual Computer Security Applications Conference}}},
  author = {Pohly, Devin J. and McLaughlin, Stephen and McDaniel, Patrick and Butler, Kevin},
  year = {2012},
  month = dec,
  series = {{{ACSAC}} '12},
  pages = {259--268},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2420950.2420989},
  urldate = {2023-08-23},
  abstract = {Data provenance---a record of the origin and evolution of data in a system---is a useful tool for forensic analysis. However, existing provenance collection mechanisms fail to achieve sufficient breadth or fidelity to provide a holistic view of a system's operation over time. We present Hi-Fi, a kernel-level provenance system which leverages the Linux Security Modules framework to collect high-fidelity whole-system provenance. We demonstrate that Hi-Fi is able to record a variety of malicious behavior within a compromised system. In addition, our benchmarks show the collection overhead from Hi-Fi to be less than 1\% for most system calls and 3\% in a representative workload, while simultaneously generating a system measurement that fully reflects system evolution. In this way, we show that we can collect broad, high-fidelity provenance data which is capable of supporting detailed forensic analysis.},
  isbn = {978-1-4503-1312-4},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/HBBVHQER/Pohly et al. - 2012 - Hi-Fi collecting high-fidelity whole-system prove.pdf}
}

@inproceedings{prasadLocatingSystemProblems2005,
  title = {Locating {{System Problems Using Dynamic Instrumentation}}},
  booktitle = {Proceedings of the {{Linux Symposium}}},
  author = {Prasad, Vara and Cohen, William and Eigler, Frank and Hunt, Martin and Keniston, Jim and Chen, Brad},
  year = {2005},
  month = jul,
  volume = {2},
  pages = {49--64},
  publisher = {{kernel.org}},
  address = {{Ottawa, Ontario, Canada}},
  abstract = {Diagnosing complex performance or kernel debugging problems often requires kernel modifications with multiple rebuilds and reboots. This is tedious, time-consuming work that most developers would prefer to minimize. Systemtap uses the kprobes infrastructure to dynamically instrument the kernel and user applications. Systemtap instrumentation incurs low overhead when enabled, and zero overhead when disabled. SystemTap provides facilities to define instrumentation points in a high-level language, and to aggregate and analyze the instrumentation data. Details of the SystemTap architecture and implementation are presented, along with an example of its application.},
  keywords = {operating systems,project-provenance-pp},
  file = {/home/sam/Zotero/storage/ENCHXEJI/ols2005v2.pdf}
}

@inproceedings{priedhorskyCharliecloudUnprivilegedContainers2017,
  title = {Charliecloud: Unprivileged Containers for User-Defined Software Stacks in {{HPC}}},
  shorttitle = {Charliecloud},
  booktitle = {Proceedings of the {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  author = {Priedhorsky, Reid and Randles, Tim},
  year = {2017},
  month = nov,
  pages = {1--10},
  publisher = {{ACM}},
  address = {{Denver Colorado}},
  doi = {10.1145/3126908.3126925},
  urldate = {2022-05-26},
  abstract = {Supercomputing centers are seeing increasing demand for user-defined software stacks (UDSS), instead of or in addition to the stack provided by the center. These UDSS support user needs such as complex dependencies or build requirements, externally required configurations, portability, and consistency. The challenge for centers is to provide these services in a usable manner while minimizing the risks: security, support burden, missing functionality, and performance. We present Charliecloud, which uses the Linux user and mount namespaces to run industry-standard Docker containers with no privileged operations or daemons on center resources. Our simple approach avoids most security risks while maintaining access to the performance and functionality already on offer, doing so in just 800 lines of code. Charliecloud promises to bring an industry-standard UDSS user workflow to existing, minimally altered HPC resources.},
  isbn = {978-1-4503-5114-0},
  langid = {english},
  keywords = {containers,hpc,operating systems,project-acm-rep,project-provenance-pp},
  file = {/home/sam/Zotero/storage/T2VWSVNT/3126908.3126925.pdf}
}

@misc{Ptrace,
  title = {Ptrace},
  journal = {Linux manual page},
  urldate = {2023-08-24},
  howpublished = {https://man7.org/linux/man-pages/man2/ptrace.2.html},
  keywords = {operating systems,project-provenance-pp},
  file = {/home/sam/Zotero/storage/NVK5RHL3/ptrace.2.html}
}

@inproceedings{rougierReScienceJournalReproducible2019,
  title = {{{ReScience C}}: {{A Journal}} for {{Reproducible Replications}} in {{Computational Science}}},
  shorttitle = {{{ReScience C}}},
  booktitle = {Reproducible {{Research}} in {{Pattern Recognition}}},
  author = {Rougier, Nicolas P. and Hinsen, Konrad},
  editor = {Kerautret, Bertrand and Colom, Miguel and Lopresti, Daniel and Monasse, Pascal and Talbot, Hugues},
  year = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {150--156},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-23987-9_14},
  abstract = {Independent replication is one of the most powerful methods to verify published scientific studies. In computational science, it requires the reimplementation of the methods described in the original article by a different team of researchers. Replication is often performed by scientists who wish to gain a better understanding of a published method, but its results are rarely made public. ReScience~C is a peer-reviewed journal dedicated to the publication of high-quality computational replications that provide added value to the scientific community. To this end, ReScience~C requires replications to be reproducible and implemented using Open Source languages and libraries. In this article, we provide an overview of ReScience~C's goals and quality standards, outline the submission and reviewing processes, and summarize the experience of its first three years of operation, concluding with an outlook towards evolutions envisaged for the near future.},
  isbn = {978-3-030-23987-9},
  langid = {english},
  keywords = {project-acm-rep,project-provenance-pp,reproducibility engineering},
  file = {/home/sam/Zotero/storage/3YZGFA62/Rougier and Hinsen - 2019 - ReScience C A Journal for Reproducible Replicatio.pdf}
}

@article{rupprechtImprovingReproducibilityData2020,
  title = {Improving Reproducibility of Data Science Pipelines through Transparent Provenance Capture},
  author = {Rupprecht, Lukas and Davis, James C. and Arnold, Constantine and Gur, Yaniv and Bhagwat, Deepavali},
  year = {2020},
  month = aug,
  journal = {Proc. VLDB Endow.},
  volume = {13},
  number = {12},
  pages = {3354--3368},
  issn = {2150-8097},
  doi = {10.14778/3415478.3415556},
  urldate = {2023-08-24},
  abstract = {Data science has become prevalent in a large variety of domains. Inherent in its practice is an exploratory, probing, and fact finding journey, which consists of the assembly, adaptation, and execution of complex data science pipelines. The trustworthiness of the results of such pipelines rests entirely on their ability to be reproduced with fidelity, which is difficult if pipelines are not documented or recorded minutely and consistently. This difficulty has led to a reproducibility crisis and presents a major obstacle to the safe adoption of the pipeline results in production environments. The crisis can be resolved if the provenance for each data science pipeline is captured transparently as pipelines are executed. However, due to the complexity of modern data science pipelines, transparently capturing sufficient provenance to allow for reproducibility is challenging. As a result, most existing systems require users to augment their code or use specific tools to capture provenance, which hinders productivity and results in a lack of adoption. In this paper, we present Ursprung,1 a transparent provenance collection system designed for data science environments.2 The Ursprung philosophy is to capture provenance and build lineage by integrating with the execution environment to automatically track static and runtime configuration parameters of data science pipelines. Rather than requiring data scientists to make changes to their code, Ursprung records basic provenance information from system-level sources and combines it with provenance from application-level sources (e.g., log files, stdout), which can be accessed and recorded through a domain-specific language. In our evaluation, we show that Ursprung is able to capture sufficient provenance for a variety of use cases and only adds an overhead of up to 4\%.},
  keywords = {project-provenance-pp,prov-tool},
  file = {/home/sam/Zotero/storage/YVLSLMIG/Rupprecht et al. - 2020 - Improving reproducibility of data science pipeline.pdf}
}

@inproceedings{sakalisSplash3ProperlySynchronized2016,
  title = {Splash-3: {{A}} Properly Synchronized Benchmark Suite for Contemporary Research},
  shorttitle = {Splash-3},
  booktitle = {2016 {{IEEE International Symposium}} on {{Performance Analysis}} of {{Systems}} and {{Software}} ({{ISPASS}})},
  author = {Sakalis, Christos and Leonardsson, Carl and Kaxiras, Stefanos and Ros, Alberto},
  year = {2016},
  month = apr,
  pages = {101--111},
  doi = {10.1109/ISPASS.2016.7482078},
  urldate = {2024-02-08},
  abstract = {Benchmarks are indispensable in evaluating the performance implications of new research ideas. However, their usefulness is compromised if they do not work correctly on a system under evaluation or, in general, if they cannot be used consistently to compare different systems. A well-known benchmark suite of parallel applications is the Splash-2 suite. Since its creation in the context of the DASH project, Splash-2 benchmarks have been widely used in research. However, Splash-2 was released over two decades ago and does not adhere to the recent C memory consistency model. This leads to unexpected and often incorrect behavior when some Splash-2 benchmarks are used in conjunction with contemporary compilers and hardware (simulated or real). Most importantly, we discovered critical performance bugs that may question some of the reported benchmark results. In this work, we analyze the Splash-2 benchmarks and expose data races and related performance bugs. We rectify the problematic benchmarks and evaluate the resulting performance. Our work contributes to the community a new sanitized version of the Splash-2 benchmarks, called the Splash-3 benchmark suite.},
  keywords = {benchmarking,project-provenance-pp},
  file = {/home/sam/Zotero/storage/8GZ8FS6H/Sakalis et al. - 2016 - Splash-3 A properly synchronized benchmark suite .pdf;/home/sam/Zotero/storage/GBWPKZA6/7482078.html}
}

@unpublished{sarLineageFileSystem,
  title = {Lineage {{File System}}},
  author = {Sar, Can and Cao, Pei},
  urldate = {2023-08-23},
  keywords = {project-provenance-pp,provenance-tool}
}

@article{schunemannReviewsRapidRapid2015,
  title = {Reviews: {{Rapid}}! {{Rapid}}! {{Rapid}}! {\dots}and Systematic},
  shorttitle = {Reviews},
  author = {Sch{\"u}nemann, Holger J. and Moja, Lorenzo},
  year = {2015},
  month = jan,
  journal = {Systematic Reviews},
  volume = {4},
  number = {1},
  pages = {4},
  issn = {2046-4053},
  doi = {10.1186/2046-4053-4-4},
  urldate = {2023-10-27},
  keywords = {project-provenance-pp,rapid reviews},
  file = {/home/sam/Zotero/storage/SVE98P9A/Schünemann and Moja - 2015 - Reviews Rapid! Rapid! Rapid! …and systematic.pdf;/home/sam/Zotero/storage/TJK8MEUF/2046-4053-4-4.html}
}

@article{shiExperienceReportProducing2022,
  title = {An {{Experience Report}} on {{Producing Verifiable Builds}} for {{Large-Scale Commercial Systems}}},
  author = {Shi, Yong and Wen, Mingzhi and Cogo, Filipe R. and Chen, Boyuan and Jiang, Zhen Ming},
  year = {2022},
  month = sep,
  journal = {IEEE Transactions on Software Engineering},
  volume = {48},
  number = {9},
  pages = {3361--3377},
  issn = {1939-3520},
  doi = {10.1109/TSE.2021.3092692},
  urldate = {2023-12-18},
  abstract = {Build verifiability is a safety property for a software system which can be used to check against various security-related issues during the build process. In summary, a verifiable build generates equivalent build artifacts for every build instance, allowing independent auditors to verify that the generated artifacts correspond to their source code. Producing a verifiable build is a very challenging problem, as non-equivalences in the build artifacts can be caused by non-determinsm from the build environment, the build toolchain, or the system implementation. Existing research and practices on build verifiability mainly focus on remediating sources of non-determinism. However, such a process does not work well with large-scale commercial systems (LSCSs) due to their stringent security requirements, complex third party dependencies, and large volumes of code changes. In this paper, we present an experience report on using a unified process and a toolkit to produce verifiable builds for LSCSs. A unified process contrasts with the existing practices in which recommendations to mitigate sources of non-determinism are proposed on a case-by-case basis and are not codified in a comprehensive tool. Our approach supports the following three strategies to systematically mitigate non-equivalences in the build artifacts: remediation, controlling, and interpretation. Case study on three LSCSs within {\textbackslash}sf HuaweiHuawei shows that our approach is able to increase the proportion of verified build artifacts from less than 50 to 100 percent. To cross-validate our approach, we successfully applied our approach to build 2,218 open source packages distributed under {\textbackslash}sf CentOSCentOS 7.8, increasing the proportion of verified build artifacts from 85 to 99 percent with minimal human intervention. We also provide an overview of our mitigation guideline, which describes the recommended strategies to mitigate various non-equivalences. Finally, we present some discussions and open research problems in this area based on our experience and lessons learned in the past few years of applying our approach within the company. This paper will be useful for practitioners and software engineering researchers who are interested in build verifiability.},
  keywords = {project-provenance-pp,reproducibility},
  file = {/home/sam/Zotero/storage/GGEJY2E8/Shi et al. - 2022 - An Experience Report on Producing Verifiable Build.pdf;/home/sam/Zotero/storage/359TFW2E/9465650.html}
}

@article{shottonCiTOCitationTyping2010a,
  title = {{{CiTO}}, the {{Citation Typing Ontology}}},
  author = {Shotton, David},
  year = {2010},
  month = jun,
  journal = {J Biomed Semant},
  volume = {1},
  number = {1},
  pages = {S6},
  issn = {2041-1480},
  doi = {10.1186/2041-1480-1-S1-S6},
  urldate = {2023-05-25},
  abstract = {CiTO, the Citation Typing Ontology, is an ontology for describing the nature of reference citations in scientific research articles and other scholarly works, both to other such publications and also to Web information resources, and for publishing these descriptions on the Semantic Web. Citation are described in terms of the factual and rhetorical relationships between citing publication and cited publication, the in-text and global citation frequencies of each cited work, and the nature of the cited work itself, including its publication and peer review status. This paper describes CiTO and illustrates its usefulness both for the annotation of bibliographic reference lists and for the visualization of citation networks. The latest version of CiTO, which this paper describes, is CiTO Version 1.6, published on 19 March 2010. CiTO is written in the Web Ontology Language OWL, uses the namespace http://purl.org/net/cito/, and is available from http://purl.org/net/cito/. This site uses content negotiation to deliver to the user an OWLDoc Web version of the ontology if accessed via a Web browser, or the OWL ontology itself if accessed from an ontology management tool such as Prot{\'e}g{\'e} 4 (http://protege.stanford.edu/). Collaborative work is currently under way to harmonize CiTO with other ontologies describing bibliographies and the rhetorical structure of scientific discourse.},
  langid = {english},
  keywords = {project-provenance-pp,semantic web},
  file = {/home/sam/Zotero/storage/NBZPC9S5/Shotton - 2010 - CiTO, the Citation Typing Ontology.pdf}
}

@misc{smalleyLinuxSecurityModules,
  title = {Linux {{Security Modules}}: {{General Security Hooks}} for {{Linux}}},
  author = {Smalley, Stephen and Fraser, Timothy and Vance, Chris},
  journal = {The Linux Kernel documentation},
  urldate = {2023-08-24},
  howpublished = {https://docs.kernel.org/security/lsm.html},
  keywords = {operating systems,project-provenance-pp},
  file = {/home/sam/Zotero/storage/ZPXUQP9J/lsm.html}
}

@article{soiland-reyesPackagingResearchArtefacts2022,
  title = {Packaging Research Artefacts with {{RO-Crate}}},
  author = {{Soiland-Reyes}, Stian and Sefton, Peter and Crosas, Merc{\`e} and Castro, Leyla Jael and Coppens, Frederik and Fern{\'a}ndez, Jos{\'e} M. and Garijo, Daniel and Gr{\"u}ning, Bj{\"o}rn and La Rosa, Marco and Leo, Simone and {\'O} Carrag{\'a}in, Eoghan and Portier, Marc and Trisovic, Ana and {RO-Crate Community} and Groth, Paul and Goble, Carole},
  year = {2022},
  month = jan,
  journal = {Data Science},
  volume = {5},
  number = {2},
  pages = {97--138},
  publisher = {{IOS Press}},
  issn = {2451-8484},
  doi = {10.3233/DS-210053},
  urldate = {2023-05-26},
  abstract = {An increasing number of researchers support reproducibility by including pointers to and descriptions of datasets, software and methods in their publications. However, scientific articles may be ambiguous, incomplete and difficult to process by autom},
  langid = {english},
  keywords = {project-acm-rep,project-provenance-pp,reproducibility engineering},
  annotation = {interest: 99},
  file = {/home/sam/Zotero/storage/YHYB8T2Y/Soiland-Reyes et al. - 2022 - Packaging research artefacts with RO-Crate.pdf}
}

@article{soiland-reyesWf4EverResearchObject2013,
  title = {{{Wf4Ever Research Object Model}}},
  author = {{Soiland-Reyes}, Stian and Bechhofer, Sean and Belhajjame, Khalid and Klyne, Graham and Garijo, Daniel and Coricho, Oscar and Garc{\'i}a Cuesta, Esteban and Palma, Raul},
  year = {2013},
  month = nov,
  publisher = {{Zenodo}},
  doi = {10.5281/ZENODO.12744},
  urldate = {2023-05-26},
  abstract = {The Wf4Ever Research Object Model provides a vocabulary for the description of workflow-centric Research Objects: aggregations of resources relating to scientific workflows. {$<$}strong{$>$}Permalink{$<$}/strong{$>$}: https://w3id.org/ro/2013-11-30/},
  copyright = {Creative Commons Attribution 4.0, Open Access},
  keywords = {project-provenance-pp,provenance,semantic web}
}

@inproceedings{stamatogiannakisDecouplingProvenanceCapture2015,
  title = {Decoupling Provenance Capture and Analysis from Execution},
  booktitle = {Proceedings of the 7th {{USENIX Conference}} on {{Theory}} and {{Practice}} of {{Provenance}}},
  author = {Stamatogiannakis, Manolis and Groth, Paul and Bos, Herbert},
  year = {2015},
  month = jul,
  series = {{{TaPP}}'15},
  pages = {3},
  publisher = {{USENIX Association}},
  address = {{USA}},
  urldate = {2024-01-21},
  abstract = {Capturing provenance usually involves the direct observation and instrumentation of the execution of a program or workflow. However, this approach restricts provenance analysis to pre-determined programs and methods. This may not pose a problem when one is interested in the provenance of a well-defined workflow, but may limit the analysis of unstructured processes such as interactive desktop computing. In this paper, we present a new approach to capturing provenance based on full execution record and replay. Our approach leverages full-system execution trace logging and replay, which allows the complete decoupling of analysis from the original execution. This enables the selective analysis of the execution using progressively heavier instrumentation.},
  langid = {english},
  keywords = {introspection,project-provenance-pp,provenance,provenance-tool,reverse engineering},
  file = {/home/sam/Zotero/storage/XVTAMHUA/Stamatogiannakis et al. - Decoupling Provenance Capture and Analysis from Ex.pdf}
}

@inproceedings{stamatogiannakisLookingBlackBoxCapturing2015,
  title = {Looking {{Inside}} the {{Black-Box}}: {{Capturing Data Provenance Using Dynamic Instrumentation}}},
  shorttitle = {Looking {{Inside}} the {{Black-Box}}},
  booktitle = {Provenance and {{Annotation}} of {{Data}} and {{Processes}}},
  author = {Stamatogiannakis, Manolis and Groth, Paul and Bos, Herbert},
  editor = {Lud{\"a}scher, Bertram and Plale, Beth},
  year = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {155--167},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-16462-5_12},
  abstract = {Knowing the provenance of a data item helps in ascertaining its trustworthiness. Various approaches have been proposed to track or infer data provenance. However, these approaches either treat an executing program as a black-box, limiting the fidelity of the captured provenance, or require developers to modify the program to make it provenance-aware. In this paper, we introduce DataTracker, a new approach to capturing data provenance based on taint tracking, a technique widely used in the security and reverse engineering fields. Our system is able to identify data provenance relations through dynamic instrumentation of unmodified binaries, without requiring access to, or knowledge of, their source code. Hence, we can track provenance for a variety of well-known applications. Because DataTracker looks inside the executing program, it captures high-fidelity and accurate data provenance.},
  isbn = {978-3-319-16462-5},
  langid = {english},
  keywords = {project-provenance-pp,prov-tool},
  file = {/home/sam/Zotero/storage/ULXR9GF5/Stamatogiannakis et al. - 2015 - Looking Inside the Black-Box Capturing Data Prove.pdf}
}

@inproceedings{suenS2LoggerEndtoEndData2013,
  title = {{{S2Logger}}: {{End-to-End Data Tracking Mechanism}} for {{Cloud Data Provenance}}},
  shorttitle = {{{S2Logger}}},
  booktitle = {2013 12th {{IEEE International Conference}} on {{Trust}}, {{Security}} and {{Privacy}} in {{Computing}} and {{Communications}}},
  author = {Suen, Chun Hui and Ko, Ryan K.L. and Tan, Yu Shyang and Jagadpramana, Peter and Lee, Bu Sung},
  year = {2013},
  month = jul,
  pages = {594--602},
  issn = {2324-9013},
  doi = {10.1109/TrustCom.2013.73},
  abstract = {The inability to effectively track data in cloud computing environments is becoming one of the top concerns for cloud stakeholders. This inability is due to two main reasons. Firstly, the lack of data tracking tools built for clouds. Secondly, current logging mechanisms are only designed from a system-centric perspective. There is a need for data-centric logging techniques which can trace data activities (e.g. file creation, edition, duplication, transfers, deletions, etc.) within and across all cloud servers. This will effectively enable full transparency and accountability for data movements in the cloud. In this paper, we introduce S2Logger, a data event logging mechanism which captures, analyses and visualizes data events in the cloud from the data point of view. By linking together atomic data events captured at both file and block level, the resulting sequence of data events depicts the cloud data provenance records throughout the data lifecycle. With this information, we can then detect critical data-related cloud security problems such as malicious actions, data leakages and data policy violations by analysing the data provenance. S2Logger also enables us to address the gaps and inadequacies of existing system-centric security tools.},
  keywords = {project-provenance-pp,prov-tool},
  file = {/home/sam/Zotero/storage/IV6JM6YF/Suen et al. - 2013 - S2Logger End-to-End Data Tracking Mechanism for C.pdf}
}

@article{suhEMPExecutionTime2017,
  title = {{{EMP}}: Execution Time Measurement Protocol for Compute-Bound Programs},
  shorttitle = {{{EMP}}},
  author = {Suh, Young-Kyoon and Snodgrass, Richard T. and Kececioglu, John D. and Downey, Peter J. and Maier, Robert S. and Yi, Cheng},
  year = {2017},
  month = apr,
  journal = {Software: Practice and Experience},
  volume = {47},
  number = {4},
  pages = {559--597},
  issn = {0038-0644, 1097-024X},
  doi = {10.1002/spe.2476},
  urldate = {2023-08-22},
  abstract = {Measuring execution time is one of the most used performance evaluation techniques in computer science research. Inaccurate measurements cannot be used for a fair performance comparison between programs. Despite the prevalence of its use, the intrinsic variability in the time measurement makes it hard to obtain repeatable and accurate timing results of a program running on an operating system. We propose a novel execution time measurement protocol (termed EMP) for measuring the execution time of a compute-bound program on Linux, while minimizing that measurement's variability. During the development of execution time measurement protocol, we identified several factors that disturb execution time measurement. We introduce successive refinements to the protocol by addressing each of these factors, in concert, reducing variability by more than an order of magnitude. We also introduce a new visualization technique, what we term `dual-execution scatter plot' that highlights infrequent, long-running daemons, differentiating them from frequent and/or short-running daemons. Our empirical results show that the proposed protocol successfully achieves three major aspects{\textemdash}precision, accuracy, and scalability{\textemdash}in execution time measurement that can work for open-source and proprietary software. Copyright {\copyright} 2017 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2017 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {benchmarking,project-provenance-pp,software benchmarking},
  annotation = {interest: 71},
  file = {/home/sam/Zotero/storage/FKY79VS6/Suh et al. - 2017 - EMP execution time measurement protocol for compu.pdf;/home/sam/Zotero/storage/LERXZLRE/spe.html}
}

@inproceedings{sultanaFileProvenanceSystem2013,
  title = {A File Provenance System},
  booktitle = {Proceedings of the Third {{ACM}} Conference on {{Data}} and Application Security and Privacy},
  author = {Sultana, Salmin and Bertino, Elisa},
  year = {2013},
  month = feb,
  series = {{{CODASPY}} '13},
  pages = {153--156},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2435349.2435368},
  urldate = {2023-08-23},
  abstract = {A file provenance system supports the automatic collection and management of provenance i.e. the complete processing history of a data object. File system level provenance provides functionality unavailable in the existing provenance systems. In this paper, we discuss the design objectives for a flexible and efficient file provenance system and then propose the design of such a system, called FiPS. We design FiPS as a thin stackable file system for capturing provenance in a portable manner. FiPS can capture provenance at various degrees of granularity, can transform provenance records into secure information, and can direct the resulting provenance data to various persistent storage systems.},
  isbn = {978-1-4503-1890-7},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/L543Q7CC/Sultana and Bertino - 2013 - A file provenance system.pdf}
}

@inproceedings{tariqAutomatedCollectionApplicationLevel2012,
  title = {Towards {{Automated Collection}} of \{\vphantom\}{{Application-Level}}\vphantom\{\} {{Data Provenance}}},
  booktitle = {4th {{USENIX Workshop}} on the {{Theory}} and {{Practice}} of {{Provenance}} ({{TaPP}} 12)},
  author = {Tariq, Dawood and Masaim, Ali and Gehani, Ashish},
  year = {2012},
  urldate = {2023-08-23},
  langid = {english},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/PG74HTCE/2012 - Towards Automated Collection of Application-Level.pdf}
}

@misc{thalheimInspectorDataProvenance2016a,
  title = {Inspector: {{A Data Provenance Library}} for {{Multithreaded Programs}}},
  shorttitle = {Inspector},
  author = {Thalheim, J{\"o}rg and Bhatotia, Pramod and Fetzer, Christof},
  year = {2016},
  month = may,
  number = {arXiv:1605.00498},
  eprint = {1605.00498},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1605.00498},
  urldate = {2024-01-21},
  abstract = {Data provenance strives for explaining how the computation was performed by recording a trace of the execution. The provenance trace is useful across a wide-range of workflows to improve the dependability, security, and efficiency of software systems. In this paper, we present Inspector, a POSIX-compliant data provenance library for shared-memory multithreaded programs. The Inspector library is completely transparent and easy to use: it can be used as a replacement for the pthreads library by a simple exchange of libraries linked, without even recompiling the application code. To achieve this result, we present a parallel provenance algorithm that records control, data, and schedule dependencies using a Concurrent Provenance Graph (CPG). We implemented our algorithm to operate at the compiled binary code level by leveraging a combination of OS-specific mechanisms, and recently released Intel PT ISA extensions as part of the Broadwell micro-architecture. Our evaluation on a multicore platform using applications from multithreaded benchmarks suites (PARSEC and Phoenix) shows reasonable provenance overheads for a majority of applications. Lastly, we briefly describe three case-studies where the generic interface exported by Inspector is being used to improve the dependability, security, and efficiency of systems. The Inspector library is publicly available for further use in a wide range of other provenance workflows.},
  archiveprefix = {arxiv},
  keywords = {computer architecture,project-provenance-pp},
  file = {/home/sam/Zotero/storage/VGWTF9CE/Thalheim et al. - 2016 - Inspector A Data Provenance Library for Multithre.pdf;/home/sam/Zotero/storage/GSDDFS9S/1605.html}
}

@article{timperleyUnderstandingImprovingArtifact2021,
  title = {Understanding and {{Improving Artifact Sharing}} in {{Software Engineering Research}}},
  author = {Timperley, Christopher S. and Herckis, Lauren and Goues, Claire Le and Hilton, Michael},
  year = {2021},
  month = jul,
  journal = {Empir Software Eng},
  volume = {26},
  number = {4},
  eprint = {2008.01046},
  primaryclass = {cs},
  pages = {67},
  issn = {1382-3256, 1573-7616},
  doi = {10.1007/s10664-021-09973-5},
  urldate = {2023-05-06},
  abstract = {In recent years, many software engineering researchers have begun to include artifacts alongside their research papers. Ideally, artifacts, including tools, benchmarks, and data, support the dissemination of ideas, provide evidence for research claims, and serve as a starting point for future research. However, in practice, artifacts suffer from a variety of issues that prevent the realization of their full potential. To help the software engineering community realize the full potential of artifacts, we seek to understand the challenges involved in the creation, sharing, and use of artifacts. To that end, we perform a mixed-methods study including a survey of artifacts in software engineering publications, and an online survey of 153 software engineering researchers. By analyzing the perspectives of artifact creators, users, and reviewers, we identify several high-level challenges that affect the quality of artifacts including mismatched expectations between these groups, and a lack of sufficient reward for both creators and reviewers. Using Diffusion of Innovations as an analytical framework, we examine how these challenges relate to one another, and build an understanding of the factors that affect the sharing and success of artifacts. Finally, we make recommendations to improve the quality of artifacts based on our results and existing best practices.},
  archiveprefix = {arxiv},
  keywords = {artifact evaluation,project-provenance-pp,reproducibility engineering,research software engineering},
  annotation = {interest: 90},
  file = {/home/sam/Zotero/storage/BM7CHTK8/Timperley et al. - 2021 - Understanding and Improving Artifact Sharing in So.pdf;/home/sam/Zotero/storage/ID3NVV6E/2008.html}
}

@inproceedings{tonthatSciunitsReusableResearch2017,
  title = {Sciunits: {{Reusable Research Objects}}},
  shorttitle = {Sciunits},
  booktitle = {2017 {{IEEE}} 13th {{International Conference}} on E-{{Science}} (e-{{Science}})},
  author = {Ton That, Dai Hai and Fils, Gabriel and Yuan, Zhihao and Malik, Tanu},
  year = {2017},
  month = oct,
  pages = {374--383},
  doi = {10.1109/eScience.2017.51},
  abstract = {Science is conducted collaboratively, often requiring knowledge sharing about computational experiments. When experiments include only datasets, they can be shared using Uniform Resource Identifiers (URIs) or Digital Object Identifiers (DOIs). An experiment, however, seldom includes only datasets, but more often includes software, its past execution, provenance, and associated documentation. The Research Object has recently emerged as a comprehensive and systematic method for aggregation and identification of diverse elements of computational experiments. While a necessary method, mere aggregation is not sufficient for the sharing of computational experiments. Other users must be able to easily recompute on these shared research objects. In this paper, we present the sciunit, a reusable research object in which aggregated content is recomputable. We describe a Git-like client that efficiently creates, stores, and repeats sciunits. We show through analysis that sciunits repeat computational experiments with minimal storage and processing overhead. Finally, we provide an overview of sharing and reproducible cyberinfrastructure based on sciunits gaining adoption in the domain of geosciences.},
  keywords = {project-provenance-pp,provenance,record-replay},
  file = {/home/sam/Zotero/storage/4RA2L32H/Ton That et al. - 2017 - Sciunits Reusable Research Objects.pdf;/home/sam/Zotero/storage/ILYKBCUH/8109156.html}
}

@article{trisovicLargescaleStudyResearch2022,
  title = {A Large-Scale Study on Research Code Quality and Execution},
  author = {Trisovic, Ana and Lau, Matthew K. and Pasquier, Thomas and Crosas, Merc{\`e}},
  year = {2022},
  month = feb,
  journal = {Sci Data},
  volume = {9},
  number = {1},
  pages = {60},
  publisher = {{Nature Publishing Group}},
  issn = {2052-4463},
  doi = {10.1038/s41597-022-01143-6},
  urldate = {2022-12-13},
  abstract = {This article presents a study on the quality and execution of research code from publicly-available replication datasets at the Harvard Dataverse repository. Research code is typically created by a group of scientists and published together with academic papers to facilitate research transparency and reproducibility. For this study, we define ten questions to address aspects impacting research reproducibility and reuse. First, we retrieve and analyze more than 2000 replication datasets with over 9000 unique R files published from 2010 to 2020. Second, we execute the code in a clean runtime environment to assess its ease of reuse. Common coding errors were identified, and some of them were solved with automatic code cleaning to aid code execution. We find that 74\% of R files failed to complete without error in the initial execution, while 56\% failed when code cleaning was applied, showing that many errors can be prevented with good coding practices. We also analyze the replication datasets from journals' collections and discuss the impact of the journal policy strictness on the code re-execution rate. Finally, based on our results, we propose a set of recommendations for code dissemination aimed at researchers, journals, and repositories.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {project-acm-rep,project-provenance-pp,reproducibility engineering},
  file = {/home/sam/Zotero/storage/YI4U9WQW/Trisovic et al. - 2022 - A large-scale study on research code quality and e.pdf}
}

@inproceedings{vahdatTransparentResultCaching1998,
  title = {Transparent Result Caching},
  booktitle = {Proceedings of the Annual Conference on {{USENIX Annual Technical Conference}}},
  author = {Vahdat, Amin and Anderson, Thomas},
  year = {1998},
  month = jun,
  series = {{{ATEC}} '98},
  pages = {3},
  publisher = {{USENIX Association}},
  address = {{USA}},
  urldate = {2023-08-23},
  abstract = {The goal of this work is to develop a general framework for transparently managing the interactions and dependencies among input files, development tools, and output files. By unobtrusively monitoring the execution of unmodified programs, we are able to track process lineage--each process's parent, children, input files, and output files, and file dependency--for each file, the sequence of operations and the set of input files used to create the file. We use this information to implement Transparent Result Caching (TREC) and describe how TREC is used to build a number of useful user utilities. Unmake allows users to query TREC for file lineage information, including the full sequence of programs executed to create a particular output file. Transparent Make uses TREC to automatically generate dependency information by observing program execution, freeing end users from the need to explicitly specify dependency information (i.e., Makefiles can be replaced by shell scripts). Dynamic Web Object Caching allows for the caching of certain dynamically generated web pages, improving server performance and client latency.},
  langid = {english},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/UWUW3GY2/Vahdat and Anderson - Transparent Result Caching.pdf}
}

@article{valletPracticalTransparentVerifiable2022,
  title = {Toward Practical Transparent Verifiable and Long-Term Reproducible Research Using {{Guix}}},
  author = {Vallet, Nicolas and Michonneau, David and Tournier, Simon},
  year = {2022},
  month = oct,
  journal = {Sci Data},
  volume = {9},
  number = {1},
  pages = {597},
  publisher = {{Nature Publishing Group}},
  issn = {2052-4463},
  doi = {10.1038/s41597-022-01720-9},
  urldate = {2023-05-06},
  abstract = {Reproducibility crisis urge scientists to promote transparency which allows peers to draw same conclusions after performing identical steps from hypothesis to results. Growing resources are developed to open the access to methods, data and source codes. Still, the computational environment, an interface between data and source code running analyses, is not addressed. Environments are usually described with software and library names associated with version labels or provided as an opaque container image. This is not enough to describe the complexity of the dependencies on which they rely to operate on. We describe this issue and illustrate how open tools like Guix can be used by any scientist to share their environment and allow peers to reproduce it. Some steps of research might not be fully reproducible, but at least, transparency for computation is technically addressable. These tools should be considered by scientists willing to promote transparency and open science.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {project-acm-rep,project-provenance-pp,reproducibility,research software engineering},
  annotation = {interest: 99},
  file = {/home/sam/Zotero/storage/NTCF5E96/Vallet et al. - 2022 - Toward practical transparent verifiable and long-t.pdf}
}

@inproceedings{vangoorFUSENotFUSE2017,
  title = {To \{\vphantom\}{{FUSE}}\vphantom\{\} or {{Not}} to \{\vphantom\}{{FUSE}}\vphantom\{\}: {{Performance}} of \{\vphantom\}{{User-Space}}\vphantom\{\} {{File Systems}}},
  shorttitle = {To \{\vphantom\}{{FUSE}}\vphantom\{\} or {{Not}} to \{\vphantom\}{{FUSE}}\vphantom\{\}},
  booktitle = {15th {{USENIX Conference}} on {{File}} and {{Storage Technologies}} ({{FAST}} 17)},
  author = {Vangoor, Bharath Kumar Reddy and Tarasov, Vasily and Zadok, Erez},
  year = {2017},
  pages = {59--72},
  urldate = {2023-07-25},
  isbn = {978-1-931971-36-2},
  langid = {english},
  keywords = {filesystems,operating systems,project-provenance-pp},
  file = {/home/sam/Zotero/storage/3N3VXAWI/Vangoor et al. - 2017 - To FUSE or Not to FUSE Performance of User-S.pdf;/home/sam/Zotero/storage/5RCSUA7B/fast17_slides_vangoor.pdf}
}

@inproceedings{wangLprovPracticalLibraryaware2018,
  title = {Lprov: {{Practical Library-aware Provenance Tracing}}},
  shorttitle = {Lprov},
  booktitle = {Proceedings of the 34th {{Annual Computer Security Applications Conference}}},
  author = {Wang, Fei and Kwon, Yonghwi and Ma, Shiqing and Zhang, Xiangyu and Xu, Dongyan},
  year = {2018},
  month = dec,
  series = {{{ACSAC}} '18},
  pages = {605--617},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3274694.3274751},
  urldate = {2023-08-24},
  abstract = {With the continuing evolution of sophisticated APT attacks, provenance tracking is becoming an important technique for efficient attack investigation in enterprise networks. Most of existing provenance techniques are operating on system event auditing that discloses dependence relationships by scrutinizing syscall traces. Unfortunately, such auditing-based provenance is not able to track the causality of another important dimension in provenance, the shared libraries. Different from other data-only system entities like files and sockets, dynamic libraries are linked at runtime and may get executed, which poses new challenges in provenance tracking. For example, library provenance cannot be tracked by syscalls and mapping; whether a library function is called and how it is called within an execution context is invisible at syscall level; linking a library does not promise their execution at runtime. Addressing these challenges is critical to tracking sophisticated attacks leveraging libraries. In this paper, to facilitate fine-grained investigation inside the execution of library binaries, we develop Lprov, a novel provenance tracking system which combines library tracing and syscall tracing. Upon a syscall, Lprov identifies the library calls together with the stack which induces it so that the library execution provenance can be accurately revealed. Our evaluation shows that Lprov can precisely identify attack provenance involving libraries, including malicious library attack and library vulnerability exploitation, while syscall-based provenance tools fail to identify. It only incurs 7.0\% (in geometric mean) runtime overhead and consumes 3 times less storage space of a state-of-the-art provenance tool.},
  isbn = {978-1-4503-6569-7},
  keywords = {project-provenance-pp,prov-tool},
  file = {/home/sam/Zotero/storage/3YFG9E7U/Wang et al. - 2018 - Lprov Practical Library-aware Provenance Tracing.pdf}
}

@article{wardjr.HierarchicalGroupingOptimize1963,
  title = {Hierarchical {{Grouping}} to {{Optimize}} an {{Objective Function}}},
  author = {Ward Jr., Joe H.},
  year = {1963},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {58},
  number = {301},
  pages = {236--244},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.1963.10500845},
  urldate = {2024-02-10},
  abstract = {A procedure for forming hierarchical groups of mutually exclusive subsets, each of which has members that are maximally similar with respect to specified characteristics, is suggested for use in large-scale (n {$>$} 100) studies when a precise optimal solution for a specified number of groups is not practical. Given n sets, this procedure permits their reduction to n - 1 mutually exclusive sets by considering the union of all possible n(n - 1)/2 pairs and selecting a union having a maximal value for the functional relation, or objective function, that reflects the criterion chosen by the investigator. By repeating this process until only one group remains, the complete hierarchical structure and a quantitative estimate of the loss associated with each stage in the grouping can be obtained. A general flowchart helpful in computer programming and a numerical example are included.},
  keywords = {machine learning,project-provenance-pp},
  file = {/home/sam/Zotero/storage/83MVHXS6/Ward Jr. - 1963 - Hierarchical Grouping to Optimize an Objective Fun.pdf}
}

@article{weibelDublinCoreMetadata2000,
  title = {The {{Dublin Core Metadata Initiative}}: {{Mission}}, {{Current Activities}}, and {{Future Directions}}},
  shorttitle = {The {{Dublin Core Metadata Initiative}}},
  author = {Weibel, Stuart L. and Koch, Traugott},
  year = {2000},
  month = dec,
  journal = {D-Lib Magazine},
  volume = {6},
  number = {12},
  issn = {1082-9873},
  doi = {10.1045/december2000-weibel},
  urldate = {2023-05-25},
  langid = {english},
  keywords = {project-provenance-pp,semantic web}
}

@misc{wilder-jamesDescriptionProjectWiki2017,
  title = {Description of a {{Project}} Wiki},
  author = {{Wilder-James}, Edd},
  year = {2017},
  month = jan,
  urldate = {2023-05-26},
  abstract = {DOAP is a project to create an XML/RDF vocabulary to describe software projects, and in particular open source projects. In addition to developing an RDF schema and examples, the DOAP project aims to provide tool support in all the popular programming languages.},
  howpublished = {https://github.com/ewilderj/doap/wiki},
  keywords = {project-provenance-pp,semantic web},
  file = {/home/sam/Zotero/storage/6H2FHPPR/wiki.html}
}

@techreport{xuDXTDarshanEXtended2017,
  title = {{{DXT}}: {{Darshan eXtended Tracing}}},
  shorttitle = {{{DXT}}},
  author = {Xu, Cong and Snyder, Shane and Venkatesan, Vishwanath and Carns, Philip and Kulkarni, Omkar and Byna, Suren and Sisneros, Roberto and Chadalavada, Kalyana},
  year = {2017},
  month = may,
  institution = {{Argonne National Lab. (ANL), Argonne, IL (United States)}},
  urldate = {2023-10-27},
  abstract = {The U.S. Department of Energy's Office of Scientific and Technical Information},
  langid = {english},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/3WW37QBM/Xu et al. - 2017 - DXT Darshan eXtended Tracing.pdf}
}

@inproceedings{yangUISCOPEAccurateInstrumentationfree2020,
  title = {{{UISCOPE}}: {{Accurate}}, {{Instrumentation-free}}, and {{Visible Attack Investigation}} for {{GUI Applications}}},
  shorttitle = {{{UISCOPE}}},
  booktitle = {Proceedings 2020 {{Network}} and {{Distributed System Security Symposium}}},
  author = {Yang, Runqing and Ma, Shiqing and Xu, Haitao and Zhang, Xiangyu and Chen, Yan},
  year = {2020},
  publisher = {{Internet Society}},
  address = {{San Diego, CA}},
  doi = {10.14722/ndss.2020.24329},
  urldate = {2023-08-23},
  abstract = {Existing attack investigation solutions for GUI applications suffer from a few limitations such as inaccuracy (because of the dependence explosion problem), requiring instrumentation, and providing very low visibility. Such limitations have hindered their widespread and practical deployment. In this paper, we present UISCOPE, a novel accurate, instrumentationfree, and visible attack investigation system for GUI applications. The core idea of UISCOPE is to perform causality analysis on both UI elements/events which represent users' perspective and low-level system events which provide detailed information of what happens under the hood, and then correlate system events with UI events to provide high accuracy and visibility. Long running processes are partitioned to individual UI transitions, to which low-level system events are attributed, making the results accurate. The produced graphs contain (causally related) UI elements with which users are very familiar, making them easily accessible. We deployed UISCOPE on 7 machines for a week, and also utilized UISCOPE to conduct an investigation of 6 realworld attacks. Our evaluation shows that compared to existing works, UISCOPE introduces neglibible overhead (less than 1\% runtime overhead and 3.05 MB event logs per hour on average) while UISCOPE can precisely identify attack provenance while offering users thorough visibility into the attack context.},
  isbn = {978-1-891562-61-7},
  langid = {english},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/IUQN7XVC/Yang et al. - 2020 - UISCOPE Accurate, Instrumentation-free, and Visib.pdf}
}

@inproceedings{yiEvaluatingBenchmarkSubsetting2006,
  title = {Evaluating {{Benchmark Subsetting Approaches}}},
  booktitle = {2006 {{IEEE International Symposium}} on {{Workload Characterization}}},
  author = {Yi, Joshua J. and Sendag, Resit and Eeckhout, Lieven and Joshi, Ajay and Lilja, David J. and John, Lizy K.},
  year = {2006},
  month = oct,
  pages = {93--104},
  doi = {10.1109/IISWC.2006.302733},
  urldate = {2024-01-22},
  abstract = {To reduce the simulation time to a tractable amount or due to compilation (or other related) problems, computer architects often simulate only a subset of the benchmarks in a benchmark suite. However, if the architect chooses a subset of benchmarks that is not representative, the subsequent simulation results will, at best, be misleading or, at worst, yield incorrect conclusions. To address this problem, computer architects have recently proposed several statistically-based approaches to subset a benchmark suite. While some of these approaches are well-grounded statistically, what has not yet been thoroughly evaluated is the: 1) absolute accuracy; 2) relative accuracy across a range of processor and memory subsystem enhancements; and 3) representativeness and coverage of each approach for a range of subset sizes. Specifically, this paper evaluates statistically-based subsetting approaches based on principal components analysis (PCA) and the Plackett and Burman (P\&B) design, in addition to prevailing approaches such as integer vs. floating-point, core vs. memory-bound, by language, and at random. Our results show that the two statistically-based approaches, PCA and P\&B, have the best absolute and relative accuracy for CPI and energy-delay product (EDP), produce subsets that are the most representative, and choose benchmark and input set pairs that are most well-distributed across the benchmark space. To achieve a 5\% absolute CPI and EDP error, across a wide range of configurations, PCA and P\&B typically need about 17 benchmark and input set pairs, while the other five approaches often choose more than 30 benchmark and input set pairs},
  keywords = {project-provenance-pp,software benchmarking},
  file = {/home/sam/Zotero/storage/ITHSVQR4/Yi et al. - 2006 - Evaluating Benchmark Subsetting Approaches.pdf;/home/sam/Zotero/storage/E3UB24EQ/4086137.html}
}

@inproceedings{yinPanoramaCapturingSystemwide2007,
  title = {Panorama: Capturing System-Wide Information Flow for Malware Detection and Analysis},
  shorttitle = {Panorama},
  booktitle = {Proceedings of the 14th {{ACM}} Conference on {{Computer}} and Communications Security},
  author = {Yin, Heng and Song, Dawn and Egele, Manuel and Kruegel, Christopher and Kirda, Engin},
  year = {2007},
  month = oct,
  series = {{{CCS}} '07},
  pages = {116--127},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1315245.1315261},
  urldate = {2023-08-23},
  abstract = {Malicious programs spy on users' behavior and compromise their privacy. Even software from reputable vendors, such as Google Desktop and Sony DRM media player, may perform undesirable actions. Unfortunately, existing techniques for detecting malware and analyzing unknown code samples are insufficient and have significant shortcomings. We observe that malicious information access and processing behavior is the fundamental trait of numerous malware categories breaching users' privacy (including keyloggers, password thieves, network sniffers, stealth backdoors, spyware and rootkits), which separates these malicious applications from benign software. We propose a system, Panorama, to detect and analyze malware by capturing this fundamental trait. In our extensive experiments, Panorama successfully detected all the malware samples and had very few false positives. Furthermore, by using Google Desktop as a case study, we show that our system can accurately capture its information access and processing behavior, and we can confirm that it does send back sensitive information to remote servers in certain settings. We believe that a system such as Panorama will offer indispensable assistance to code analysts and malware researchers by enabling them to quickly comprehend the behavior and innerworkings of an unknown sample.},
  isbn = {978-1-59593-703-2},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/L7K5I9UH/Yin et al. - 2007 - Panorama capturing system-wide information flow f.pdf}
}

@inproceedings{zhaoApplyingVirtualData2006,
  title = {Applying the {{Virtual Data Provenance Model}}},
  booktitle = {Provenance and {{Annotation}} of {{Data}}},
  author = {Zhao, Yong and Wilde, Michael and Foster, Ian},
  editor = {Moreau, Luc and Foster, Ian},
  year = {2006},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {148--161},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11890850_16},
  abstract = {In many domains of science, engineering, and commerce, data analysis systems are employed to derive new data (and ultimately, one hopes, knowledge) from datasets describing experimental results or simulated phenomena. To support such analyses, we have developed a ``virtual data system'' that allows users first to define, then to invoke, and finally explore the provenance of procedures (and workflows comprising multiple procedure calls) that perform such data derivations. The underlying execution model is ``functional'' in the sense that procedures read (but do not modify) their input and produce output via deterministic computations. This property makes it straightforward for the virtual data system to record not only the recipe for producing any given data object but also sufficient information about the environment in which the recipe has been executed, all with sufficient fidelity that the steps used to create a data object can be re-executed to reproduce the data object at a later time or a different location. The virtual data system maintains this information in an integrated schema alongside semantic annotations, and thus enables a powerful query capability in which the rich semantic information implied by knowledge of the structure of data derivation procedures can be exploited to provide an information environment that fuses recipe, history, and application-specific semantics. We provide here an overview of this integration, the queries and transformations that it enables, and examples of how these capabilities can serve scientific processes.},
  isbn = {978-3-540-46303-0},
  langid = {english},
  keywords = {project-provenance-pp,provenance},
  file = {/home/sam/Zotero/storage/XFYDNEA7/Zhao et al. - 2006 - Applying the Virtual Data Provenance Model.pdf}
}

@inproceedings{zhaoWhyWorkflowsBreak2012,
  title = {Why Workflows Break {\textemdash} Understanding and Combating Decay in {{Taverna}} Workflows},
  booktitle = {2012 {{IEEE}} 8th {{International Conference}} on {{E-Science}} (e-{{Science}})},
  author = {Zhao, Jun and {Gomez-Perez}, Jose-Manuel and Belhajjame, Khalid and Klyne, Graham and {Garcia-cuesta}, Esteban and Garrido, Aleix and Hettne, Kristina and Roos, Marco and De Roure, David and Goble, Carole},
  year = {2012},
  month = oct,
  pages = {9},
  publisher = {{IEEE}},
  address = {{Chicago, IL}},
  doi = {10.1109/eScience.2012.6404482},
  abstract = {Workflows provide a popular means for preserving scientific methods by explicitly encoding their process. However, some of them are subject to a decay in their ability to be re-executed or reproduce the same results over time, largely due to the volatility of the resources required for workflow executions. This paper provides an analysis of the root causes of workflow decay based on an empirical study of a collection of Taverna workflows from the myExperiment repository. Although our analysis was based on a specific type of workflow, the outcomes and methodology should be applicable to workflows from other systems, at least those whose executions also rely largely on accessing third-party resources. Based on our understanding about decay we recommend a minimal set of auxiliary resources to be preserved together with the workflows as an aggregation object and provide a software tool for end-users to create such aggregations and to assess their completeness},
  keywords = {internship-project,project-acm-rep,project-provenance-pp,workflow managers},
  file = {/home/sam/Zotero/storage/2BQSSKJF/Why_workflows_break__Understanding_and_combating_decay_in_Taverna_workflows.pdf}
}

@article{zipperleProvenancebasedIntrusionDetection2022,
  title = {Provenance-Based {{Intrusion Detection Systems}}: {{A Survey}}},
  shorttitle = {Provenance-Based {{Intrusion Detection Systems}}},
  author = {Zipperle, Michael and Gottwalt, Florian and Chang, Elizabeth and Dillon, Tharam},
  year = {2022},
  month = dec,
  journal = {ACM Comput. Surv.},
  volume = {55},
  number = {7},
  pages = {135:1--135:36},
  issn = {0360-0300},
  doi = {10.1145/3539605},
  urldate = {2023-08-23},
  abstract = {Traditional Intrusion Detection Systems (IDS) cannot cope with the increasing number and sophistication of cyberattacks such as Advanced Persistent Threats (APT). Due to their high false-positive rate and the required effort of security experts to validate them, incidents can remain undetected for up to several months. As a result, enterprises suffer from data loss and severe financial damage. Recent research explored data provenance for Host-based Intrusion Detection Systems (HIDS) as one promising data source to tackle this issue. Data provenance represents information flows between system entities as Direct Acyclic Graph (DAG). Provenance-based Intrusion Detection Systems (PIDS) utilize data provenance to enhance the detection performance of intrusions and reduce false-alarm rates compared to traditional IDS. This survey demonstrates the potential of PIDS by providing a detailed evaluation of recent research in the field, proposing a novel taxonomy for PIDS, discussing current issues, and potential future research directions. This survey aims to help and motivate researchers to get started in the field of PIDS by tackling issues of data collection, graph summarization, intrusion detection, and developing real-world benchmark datasets.},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/AZFUMA8Z/Zipperle et al. - 2022 - Provenance-based Intrusion Detection Systems A Su.pdf}
}
