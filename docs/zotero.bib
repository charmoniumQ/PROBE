@article{abbottUphillStruggle2006,
  title = {Uphill {{Struggle}}},
  author = {Abbott, Alison},
  date = {2006-02-01},
  journaltitle = {Nature},
  volume = {439},
  number = {7076},
  pages = {524--525},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/439524a},
  url = {https://www.nature.com/articles/439524a},
  urldate = {2022-08-26},
  abstract = {The first vaccine against Lyme disease was withdrawn because patients distrusted it. Should market forces be allowed to shape the next one, asks Alison Abbott.},
  issue = {7076},
  langid = {english},
  keywords = {public health},
  file = {/home/sam/Zotero/storage/BDRNUYQL/Abbott - 2006 - Uphill Struggle.pdf}
}

@inproceedings{abdalkareemWhyDevelopersUse2017,
  title = {Why Do Developers Use Trivial Packages? An Empirical Case Study on Npm},
  shorttitle = {Why Do Developers Use Trivial Packages?},
  booktitle = {Proceedings of the 2017 11th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  author = {Abdalkareem, Rabe and Nourry, Olivier and Wehaibi, Sultan and Mujahid, Suhaib and Shihab, Emad},
  date = {2017-08-21},
  series = {{{ESEC}}/{{FSE}} 2017},
  pages = {385--395},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3106237.3106267},
  url = {https://doi.org/10.1145/3106237.3106267},
  urldate = {2022-04-13},
  abstract = {Code reuse is traditionally seen as good practice. Recent trends have pushed the concept of code reuse to an extreme, by using packages that implement simple and trivial tasks, which we call `trivial packages'. A recent incident where a trivial package led to the breakdown of some of the most popular web applications such as Facebook and Netflix made it imperative to question the growing use of trivial packages. Therefore, in this paper, we mine more than 230,000 npm packages and 38,000 JavaScript applications in order to study the prevalence of trivial packages. We found that trivial packages are common and are increasing in popularity, making up 16.8\% of the studied npm packages. We performed a survey with 88 Node.js developers who use trivial packages to understand the reasons and drawbacks of their use. Our survey revealed that trivial packages are used because they are perceived to be well implemented and tested pieces of code. However, developers are concerned about maintaining and the risks of breakages due to the extra dependencies trivial packages introduce. To objectively verify the survey results, we empirically validate the most cited reason and drawback and find that, contrary to developers' beliefs, only 45.2\% of trivial packages even have tests. However, trivial packages appear to be `deployment tested' and to have similar test, usage and community interest as non-trivial packages. On the other hand, we found that 11.5\% of the studied trivial packages have more than 20 dependencies. Hence, developers should be careful about which trivial packages they decide to use.},
  isbn = {978-1-4503-5105-8},
  keywords = {industry practices,software engineering},
  file = {/home/sam/Zotero/storage/FNGR5RR7/Abdalkareem et al. - 2017 - Why do developers use trivial packages an empiric.pdf;/home/sam/Zotero/storage/U7QPCPYC/why-do-developers-use-trivial-packages-npm.html}
}

@article{abeykoonHPTMTParallelOperators2022,
  title = {{{HPTMT Parallel Operators}} for {{High Performance Data Science}} and {{Data Engineering}}},
  author = {Abeykoon, Vibhatha and Kamburugamuve, Supun and Widanage, Chathura and Perera, Niranda and Uyar, Ahmet and Kanewala, Thejaka Amila and family=Laszewski, given=Gregor, prefix=von, useprefix=true and Fox, Geoffrey},
  date = {2022-02-07},
  journaltitle = {Frontiers in Big Data},
  shortjournal = {Front. Big Data},
  volume = {4},
  pages = {756041},
  issn = {2624-909X},
  doi = {10.3389/fdata.2021.756041},
  url = {https://www.frontiersin.org/articles/10.3389/fdata.2021.756041/full},
  urldate = {2022-05-25},
  abstract = {Data-intensive applications are becoming commonplace in all science disciplines. They are comprised of a rich set of sub-domains such as data engineering, deep learning, and machine learning. These applications are built around efficient data abstractions and operators that suit the applications of different domains. Often lack of a clear definition of data structures and operators in the field has led to other implementations that do not work well together. The HPTMT architecture that we proposed recently, identifies a set of data structures, operators, and an execution model for creating rich data applications that links all aspects of data engineering and data science together efficiently. This paper elaborates and illustrates this architecture using an end-to-end application with deep learning and data engineering parts working together. Our analysis show that the proposed system architecture is better suited for high performance computing environments compared to the current big data processing systems. Furthermore our proposed system emphasizes the importance of efficient compact data structures such as Apache Arrow tabular data representation defined for high performance. Thus the system integration we proposed scales a sequential computation to a distributed computation retaining optimum performance along with highly usable application programming interface.},
  keywords = {data science},
  file = {/home/sam/Zotero/storage/8NFXT6BU/fdata-04-756041.pdf}
}

@article{abeysooriyaGeneNameErrors2021,
  title = {Gene Name Errors: {{Lessons}} Not Learned},
  shorttitle = {Gene Name Errors},
  author = {Abeysooriya, Mandhri and Soria, Megan and Kasu, Mary Sravya and Ziemann, Mark},
  date = {2021-07-30},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  volume = {17},
  number = {7},
  pages = {e1008984},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1008984},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008984},
  urldate = {2022-08-25},
  abstract = {Erroneous conversion of gene names into other dates and other data types has been a frustration for computational biologists for years. We hypothesized that such errors in supplementary files might diminish after a report in 2016 highlighting the extent of the problem. To assess this, we performed a scan of supplementary files published in PubMed Central from 2014 to 2020. Overall, gene name errors continued to accumulate unabated in the period after 2016. An improved scanning software we developed identified gene name errors in 30.9\% (3,436/11,117) of articles with supplementary Excel gene lists; a figure significantly higher than previously estimated. This is due to gene names being converted not just to dates and floating-point numbers, but also to internal date format (five-digit numbers). These findings further reinforce that spreadsheets are ill-suited to use with large genomic data.},
  langid = {english},
  keywords = {genomics,metascience,research software engineering},
  file = {/home/sam/Zotero/storage/LG3MS4V7/Abeysooriya et al. - 2021 - Gene name errors Lessons not learned.pdf;/home/sam/Zotero/storage/ZRMDQSZU/article.html}
}

@online{AboutCodeScanCode,
  title = {{{AboutCode}} - {{ScanCode}}},
  url = {https://www.aboutcode.org/projects/scancode.html},
  urldate = {2022-09-06},
  keywords = {software},
  annotation = {interest: 91},
  file = {/home/sam/Zotero/storage/LTCNAICY/scancode.html}
}

@article{abramsonRelativeDebuggingNew1996,
  title = {Relative Debugging: A New Methodology for Debugging Scientific Applications},
  shorttitle = {Relative Debugging},
  author = {Abramson, David and Foster, Ian and Michalakes, John and Sosič, Rok},
  date = {1996-11-01},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {39},
  number = {11},
  pages = {69--77},
  issn = {0001-0782},
  doi = {10.1145/240455.240475},
  url = {https://doi.org/10.1145/240455.240475},
  urldate = {2022-10-18},
  keywords = {research software engineering},
  annotation = {interest: 80},
  file = {/home/sam/Zotero/storage/WTHBSRXD/Abramson et al. - 1996 - Relative debugging a new methodology for debuggin.pdf}
}

@article{abramsonTranslationalResearchComputer2019,
  title = {Translational {{Research}} in {{Computer Science}}},
  author = {Abramson, David and Parashar, Manish},
  date = {2019-09},
  journaltitle = {Computer},
  volume = {52},
  number = {9},
  pages = {16--23},
  issn = {1558-0814},
  doi = {10.1109/MC.2019.2925650},
  abstract = {There are benefits to formalizing translational computer science (TCS) to complement traditional modes of computer science research, as has been done for translational medicine. TCS has the potential to accelerate the impact of computer science research overall.},
  eventtitle = {Computer},
  annotation = {interset: 89}
}

@online{acminc.staffArtifactReviewBadging2020,
  title = {Artifact {{Review}} and {{Badging}}},
  author = {ACM Inc. staff},
  date = {2020-08-24},
  url = {https://www.acm.org/publications/policies/artifact-review-and-badging-current},
  urldate = {2023-01-19},
  abstract = {Result and Artifact Review documentation and badges - V.1.1},
  langid = {english},
  keywords = {project-acm-rep,project-bugsinpy,project-provenance-pp,reproducibility},
  file = {/home/sam/Zotero/storage/PGDE3ZAE/artifact-review-and-badging-current.html}
}

@report{adamsResearchRiskRansomware2021,
  title = {Research at {{Risk}}: {{Ransomware}} Attack on {{Physics}} and {{Astronomy Case Study}}},
  shorttitle = {Research at {{Risk}}},
  author = {Adams, Andrew and Siu, Tom and Songer, Julie and Welch, Von},
  date = {2021-07},
  institution = {NSF Cybersecurity Center of Excellence, Trusted CI, trustedci.org},
  url = {http://hdl.handle.net/2022/26638},
  urldate = {2022-05-23},
  langid = {english},
  keywords = {cybersecurity,internship-project,project-devsecops},
  file = {/home/sam/Zotero/storage/QJPMBJVC/Trusted CI - MSU Ransomware Lessons Learned.pdf}
}

@online{adeeBadBugsWorst2013,
  title = {Bad Bugs: {{The}} Worst Disasters Caused by Software Fails},
  shorttitle = {Bad Bugs},
  author = {Adee, Sally},
  date = {2013-06-05},
  url = {https://www.newscientist.com/gallery/software-bugs/},
  urldate = {2022-04-07},
  abstract = {Clever software makes our lives easier but a glitch can have disastrous consequences – here are six of the worst},
  langid = {american},
  organization = {New Scientist},
  keywords = {software engineering},
  file = {/home/sam/Zotero/storage/5U3KLT95/software-bugs.html}
}

@article{adorfHowProfessionallyDevelop2019,
  title = {How to {{Professionally Develop Reusable Scientific Software}}—{{And When Not To}}},
  author = {Adorf, Carl S. and Ramasubramani, Vyas and Anderson, Joshua A. and Glotzer, Sharon C.},
  date = {2019-03},
  journaltitle = {Computing in Science \& Engineering},
  volume = {21},
  number = {2},
  pages = {66--79},
  issn = {1558-366X},
  doi = {10.1109/MCSE.2018.2882355},
  abstract = {A critical challenge in scientific computing is balancing developing high-quality software with the need for immediate scientific progress. We present a flexible approach that emphasizes writing specialized code that is refactored only when future immediate scientific goals demand it. Our lazy refactoring technique, which calls for code with clearly defined interfaces and sharply delimited scopes to maximize reuse and integrability, helps reduce total development time and accelerates the production of scientific results. We offer guidelines for how to implement such code, as well as criteria to aid in the evaluation of existing tools. To demonstrate their application, we showcase the development progression of tools for particle simulations originating from the Glotzer Group at the University of Michigan. We emphasize the evolution of these tools into a loosely integrated software stack of highly reusable software that can be maintained to ensure the long-term stability of established research workflows.},
  eventtitle = {Computing in {{Science}} \& {{Engineering}}},
  keywords = {research software engineering},
  annotation = {interest: 87},
  file = {/home/sam/Zotero/storage/DJ7L4VWN/Adorf et al. - 2019 - How to Professionally Develop Reusable Scientific .pdf;/home/sam/Zotero/storage/7GDTJD5N/8558687.html}
}

@article{ahujaImplementationFOAFAIISO2019,
  title = {Implementation of {{FOAF}}, {{AIISO}} and {{DOAP}} Ontologies for Creating an Academic Community Network Using Semantic Frameworks},
  author = {Ahuja, Himanshu and R, Sivakumar},
  date = {2019-10-01},
  journaltitle = {International Journal of Electrical and Computer Engineering (IJECE)},
  shortjournal = {IJECE},
  volume = {9},
  number = {5},
  pages = {4302},
  issn = {2722-2578, 2088-8708},
  doi = {10.11591/ijece.v9i5.pp4302-4310},
  url = {http://ijece.iaescore.com/index.php/IJECE/article/view/18022},
  urldate = {2023-06-05},
  abstract = {Web 2.0 delivers the information which is then displayed in human readable content, omitting the crucial information which can be drawn from the data by the applications. Web 3.0 or semantic web is an extension to the current web, with an ambition to determine the drawbacks of the current web. The semantic web has already proven its influence in several communities around the globe, such as social media, music industry, healthcare domain, online blogs or articles, etc.; Among the several tools and technologies, ontologies or vocabularies are the foundation pillar for the semantic web. In this paper, the developed system aims at improving the collaboration and academic relations among staff which is directly related to our education community by providing a better networking platform which lets the agents discuss their achievements, titles, domain interests, and various other activities. Results have been analyzed to show how new facts, information can be implied from the presented knowledge of several agents and help generate a relationship graph by utilizing various semantic tools. The system discussed in this paper processes all the information in a format which can be understood by both humans and the machines, to interpret the underlying meaning about it and provide effective results.},
  keywords = {semantic web},
  file = {/home/sam/Zotero/storage/PKX78PJN/Ahuja and R - 2019 - Implementation of FOAF, AIISO and DOAP ontologies .pdf}
}

@inproceedings{al-saadiExaWorksWorkflowsExascale2021,
  title = {{{ExaWorks}}: {{Workflows}} for {{Exascale}}},
  shorttitle = {{{ExaWorks}}},
  booktitle = {2021 {{IEEE Workshop}} on {{Workflows}} in {{Support}} of {{Large-Scale Science}} ({{WORKS}})},
  author = {Al-Saadi, Aymen and Ahn, Dong H. and Babuji, Yadu and Chard, Kyle and Corbett, James and Hategan, Mihael and Herbein, Stephen and Jha, Shantenu and Laney, Daniel and Merzky, Andre and Munson, Todd and Salim, Michael and Titov, Mikhail and Turilli, Matteo and Uram, Thomas D. and Wozniak, Justin M.},
  date = {2021-11},
  pages = {50--57},
  publisher = {IEEE},
  location = {St. Louis, MO, USA},
  doi = {10.1109/WORKS54523.2021.00012},
  url = {https://ieeexplore.ieee.org/document/9652623/},
  urldate = {2022-05-25},
  abstract = {Exascale computers will offer transformative capabilities to combine data-driven and learning-based approaches with traditional simulation applications to accelerate scientific discovery and insight. These software combinations and integrations, however, are difficult to achieve due to challenges of coordination and deployment of heterogeneous software components on diverse and massive platforms. We present the ExaWorks project, which can address many of these challenges: ExaWorks is leading a co-design process to create a workflow Software Development Toolkit (SDK) consisting of a wide range of workflow management tools that can be composed and interoperate through common interfaces. We describe the initial set of tools and interfaces supported by the SDK, efforts to make them easier to apply to complex science challenges, and examples of their application to exemplar cases. Furthermore, we discuss how our project is working with the workflows community, large computing facilities as well as HPC platform vendors to sustainably address the requirements of workflows at the exascale.},
  eventtitle = {2021 {{IEEE Workshop}} on {{Workflows}} in {{Support}} of {{Large-Scale Science}} ({{WORKS}})},
  isbn = {978-1-6654-1136-3},
  keywords = {research software engineering,workflow managers},
  file = {/home/sam/Zotero/storage/K352RHRM/ExaWorks_Workflows_for_Exascale.pdf}
}

@online{alamReusabilityChallengesScientific2023,
  title = {Reusability {{Challenges}} of {{Scientific Workflows}}: {{A Case Study}} for {{Galaxy}}},
  shorttitle = {Reusability {{Challenges}} of {{Scientific Workflows}}},
  author = {Alam, Khairul and Roy, Banani and Serebrenik, Alexander},
  date = {2023-09-13},
  eprint = {2309.07291},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2309.07291},
  url = {http://arxiv.org/abs/2309.07291},
  urldate = {2023-10-16},
  abstract = {Scientific workflow has become essential in software engineering because it provides a structured approach to designing, executing, and analyzing scientific experiments. Software developers and researchers have developed hundreds of scientific workflow management systems so scientists in various domains can benefit from them by automating repetitive tasks, enhancing collaboration, and ensuring the reproducibility of their results. However, even for expert users, workflow creation is a complex task due to the dramatic growth of tools and data heterogeneity. Thus, scientists attempt to reuse existing workflows shared in workflow repositories. Unfortunately, several challenges prevent scientists from reusing those workflows. In this study, we thus first attempted to identify those reusability challenges. We also offered an action list and evidence-based guidelines to promote the reusability of scientific workflows. Our intensive manual investigation examined the reusability of existing workflows and exposed several challenges. The challenges preventing reusability include tool upgrading, tool support unavailability, design flaws, incomplete workflows, failure to load a workflow, etc. Such challenges and our action list offered guidelines to future workflow composers to create better workflows with enhanced reusability. In the future, we plan to develop a recommender system using reusable workflows that can assist scientists in creating effective and error-free workflows.},
  pubstate = {prepublished},
  file = {/home/sam/Zotero/storage/4GMSIK9M/Alam et al. - 2023 - Reusability Challenges of Scientific Workflows A .pdf;/home/sam/Zotero/storage/D76BETGX/2309.html}
}

@online{allberyMissingSysVinitSupport2016,
  title = {Is Missing {{SysV-init}} Support a Bug?},
  author = {Allbery, Russ},
  date = {2016-08-30},
  url = {https://lwn.net/Articles/698822/},
  urldate = {2023-09-27},
  organization = {LWN.net},
  file = {/home/sam/Zotero/storage/PUTLGU74/698822.html}
}

@online{allenAstrophysicsSourceCode2012,
  title = {Astrophysics {{Source Code Library}}},
  author = {Allen, Alice and DuPrie, Kimberly and Berriman, Bruce and Hanisch, Robert J. and Mink, Jessica and Teuben, Peter J.},
  date = {2012-12-09},
  eprint = {1212.1916},
  eprinttype = {arXiv},
  eprintclass = {astro-ph},
  doi = {10.48550/arXiv.1212.1916},
  url = {http://arxiv.org/abs/1212.1916},
  urldate = {2025-01-13},
  abstract = {The Astrophysics Source Code Library (ASCL), founded in 1999, is a free on-line registry for source codes of interest to astronomers and astrophysicists. The library is housed on the discussion forum for Astronomy Picture of the Day (APOD) and can be accessed at http://ascl.net. The ASCL has a comprehensive listing that covers a significant number of the astrophysics source codes used to generate results published in or submitted to refereed journals and continues to grow. The ASCL currently has entries for over 500 codes; its records are citable and are indexed by ADS. The editors of the ASCL and members of its Advisory Committee were on hand at a demonstration table in the ADASS poster room to present the ASCL, accept code submissions, show how the ASCL is starting to be used by the astrophysics community, and take questions on and suggestions for improving the resource.},
  pubstate = {prepublished},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics,Computer Science - Digital Libraries},
  file = {/home/sam/Zotero/storage/Q5A7YM3B/Allen et al. - 2012 - Astrophysics Source Code Library.pdf;/home/sam/Zotero/storage/4MMN4WTD/1212.html}
}

@online{allenLookingLeapingCreating2015,
  title = {Looking before Leaping: {{Creating}} a Software Registry},
  shorttitle = {Looking before Leaping},
  author = {Allen, Alice and Schmidt, Judy},
  date = {2015-08-18},
  eprint = {1407.5378},
  eprinttype = {arXiv},
  eprintclass = {astro-ph},
  doi = {10.48550/arXiv.1407.5378},
  url = {http://arxiv.org/abs/1407.5378},
  urldate = {2023-06-16},
  abstract = {What lessons can be learned from examining numerous efforts to create a repository or directory of scientist-written software for a discipline? Astronomy has seen a number of efforts to build such a resource, one of which is the Astrophysics Source Code Library (ASCL). The ASCL (ascl.net) was founded in 1999, had a period of dormancy, and was restarted in 2010. When taking over responsibility for the ASCL in 2010, the new editor sought to answer the opening question, hoping this would better inform the work to be done. We also provide specific steps the ASCL is taking to try to improve code sharing and discovery in astronomy and share recent improvements to the resource.},
  pubstate = {prepublished},
  version = {3},
  keywords = {library science,software archiving},
  file = {/home/sam/Zotero/storage/96CVIEHF/Allen and Schmidt - 2015 - Looking before leaping Creating a software regist.pdf;/home/sam/Zotero/storage/WEWSVJHX/1407.html}
}

@article{alliezAttributingReferencingResearch2020,
  title = {Attributing and {{Referencing}} ({{Research}}) {{Software}}: {{Best Practices}} and {{Outlook From Inria}}},
  shorttitle = {Attributing and {{Referencing}} ({{Research}}) {{Software}}},
  author = {Alliez, Pierre and Cosmo, Roberto Di and Guedj, Benjamin and Girault, Alain and Hacid, Mohand-Saïd and Legrand, Arnaud and Rougier, Nicolas},
  date = {2020-01},
  journaltitle = {Computing in Science \& Engineering},
  volume = {22},
  number = {1},
  pages = {39--52},
  issn = {1558-366X},
  doi = {10.1109/MCSE.2019.2949413},
  abstract = {Software is a fundamental pillar of modern scientific research, across all fields and disciplines. However, there is a lack of adequate means to cite and reference software due to the complexity of the problem in terms of authorship, roles, and credits. This complexity is further increased when it is considered over the lifetime of a software that can span up to several decades. Building upon the internal experience of Inria, the French research institute for digital sciences, we provide in this article a contribution to the ongoing efforts in order to develop proper guidelines and recommendations for software citation and reference. Namely, we recommend: first, a richer taxonomy for software contributions with a qualitative scale; second, to put humans at the heart of the evaluation; and third, to distinguish citation from reference.},
  eventtitle = {Computing in {{Science}} \& {{Engineering}}},
  keywords = {library science,software archiving},
  file = {/home/sam/Zotero/storage/7ZSDHP95/Alliez et al. - 2020 - Attributing and Referencing (Research) Software B.pdf;/home/sam/Zotero/storage/NZ4FNKW6/stamp.html}
}

@article{allisonReproducibilityTragedyErrors2016,
  title = {Reproducibility: {{A}} Tragedy of Errors},
  shorttitle = {Reproducibility},
  author = {Allison, David B. and Brown, Andrew W. and George, Brandon J. and Kaiser, Kathryn A.},
  date = {2016-02},
  journaltitle = {Nature},
  volume = {530},
  number = {7588},
  pages = {27--29},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/530027a},
  url = {https://www.nature.com/articles/530027a},
  urldate = {2022-09-27},
  abstract = {Mistakes in peer-reviewed papers are easy to find but hard to fix, report David B. Allison and colleagues.},
  issue = {7588},
  langid = {english},
  keywords = {academic publishing,metascience,reproducibility},
  annotation = {interest: 76},
  file = {/home/sam/Zotero/storage/NFA2W972/Allison et al. - 2016 - Reproducibility A tragedy of errors.pdf;/home/sam/Zotero/storage/CF4XZWVB/530027a.html}
}

@article{altschulBasicLocalAlignment1990,
  title = {Basic Local Alignment Search Tool},
  author = {Altschul, Stephen F. and Gish, Warren and Miller, Webb and Myers, Eugene W. and Lipman, David J.},
  date = {1990-10-05},
  journaltitle = {Journal of Molecular Biology},
  shortjournal = {Journal of Molecular Biology},
  volume = {215},
  number = {3},
  pages = {403--410},
  issn = {0022-2836},
  doi = {10.1016/S0022-2836(05)80360-2},
  url = {https://www.sciencedirect.com/science/article/pii/S0022283605803602},
  urldate = {2023-12-04},
  abstract = {A new approach to rapid sequence comparison, basic local alignment search tool (BLAST), directly approximates alignments that optimize a measure of local similarity, the maximal segment pair (MSP) score. Recent mathematical results on the stochastic properties of MSP scores allow an analysis of the performance of this method as well as the statistical significance of alignments it generates. The basic algorithm is simple and robust; it can be implemented in a number of ways and applied in a variety of contexts including straight-forward DNA and protein sequence database searches, motif searches, gene identification searches, and in the analysis of multiple regions of similarity in long DNA sequences. In addition to its flexibility and tractability to mathematical analysis, BLAST is an order of magnitude faster than existing sequence comparison tools of comparable sensitivity.},
  keywords = {project-provenance-pp},
  file = {/home/sam/Zotero/storage/AXZWKX3X/S0022283605803602.html}
}

@unpublished{amarnathHetSchedQualityofMissionAware2022,
  title = {{{HetSched}}: {{Quality-of-Mission Aware Scheduling}} for {{Autonomous Vehicle SoCs}}},
  shorttitle = {{{HetSched}}},
  author = {Amarnath, Aporva and Pal, Subhankar and Kassa, Hiwot and Vega, Augusto and Buyuktosunoglu, Alper and Franke, Hubertus and Wellman, John-David and Dreslinski, Ronald and Bose, Pradip},
  date = {2022-03-24},
  eprint = {2203.13396},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2203.13396},
  urldate = {2022-04-17},
  abstract = {Systems-on-Chips (SoCs) that power autonomous vehicles (AVs) must meet stringent performance and safety requirements prior to deployment. With increasing complexity in AV applications, the system needs to meet these real-time demands of multiple safety-critical applications simultaneously. A typical AV-SoC is a heterogeneous multiprocessor consisting of accelerators supported by general-purpose cores. Such heterogeneity, while needed for power-performance efficiency, complicates the art of task scheduling. In this paper, we demonstrate that hardware heterogeneity impacts the scheduler's effectiveness and that optimizing for only the real-time aspect of applications is not sufficient in AVs. Therefore, a more holistic approach is required -- one that considers global Quality-of-Mission (QoM) metrics, as defined in the paper. We then propose HetSched, a multi-step scheduler that leverages dynamic runtime information about the underlying heterogeneous hardware platform, along with the applications' real-time constraints and the task traffic in the system to optimize overall mission performance. HetSched proposes two scheduling policies: MSstat and MSdyn and scheduling optimizations like task pruning, hybrid heterogeneous ranking and rank update. HetSched improves overall mission performance on average by 4.6x, 2.6x and 2.6x when compared against CPATH, ADS and 2lvl-EDF (state-of-the-art real-time schedulers built for heterogeneous systems), respectively, and achieves an average of 53.3\% higher hardware utilization, while meeting 100\% critical deadlines for real-world applications of autonomous vehicles. Furthermore, when used as part of an SoC design space exploration loop, in comparison to prior schedulers, HetSched reduces the number of processing elements required by an SoC to safely complete AV's missions by 35\% on average while achieving 2.7x lower energy-mission time product.},
  keywords = {scheduling},
  file = {/home/sam/Zotero/storage/SI88F4NY/Amarnath et al. - 2022 - HetSched Quality-of-Mission Aware Scheduling for .pdf}
}

@article{amrheinScientistsRiseStatistical2019,
  title = {Scientists Rise up against Statistical Significance},
  author = {Amrhein, Valentin and Greenland, Sander and McShane, Blake},
  date = {2019-03},
  journaltitle = {Nature},
  volume = {567},
  number = {7748},
  pages = {305--307},
  publisher = {Nature Publishing Group},
  doi = {10.1038/d41586-019-00857-9},
  url = {https://www.nature.com/articles/d41586-019-00857-9},
  urldate = {2024-01-29},
  abstract = {Valentin Amrhein, Sander Greenland, Blake McShane and more than 800 signatories call for an end to hyped claims and the dismissal of possibly crucial effects.},
  issue = {7748},
  langid = {english},
  keywords = {statistics},
  annotation = {Bandiera\_abtest: a\\
Cg\_type: Comment\\
Subject\_term: Research data, Research management},
  file = {/home/sam/Zotero/storage/ICNI3KL7/Amrhein et al. - 2019 - Scientists rise up against statistical significanc.pdf}
}

@unpublished{amstadtWineNotEmulator,
  title = {Wine {{Is Not}} an {{Emulator}}},
  author = {Amstadt, Bob and Johnson, Michael K.},
  url = {https://dl.acm.org/doi/fullHtml/10.5555/324681.324684},
  urldate = {2024-01-13}
}

@article{andaVariabilityReproducibilitySoftware2009,
  title = {Variability and {{Reproducibility}} in {{Software Engineering}}: {{A Study}} of {{Four Companies}} That {{Developed}} the {{Same System}}},
  shorttitle = {Variability and {{Reproducibility}} in {{Software Engineering}}},
  author = {Anda, Bente C.D. and Sjøberg, Dag I.K. and Mockus, Audris},
  date = {2009-05},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {35},
  number = {3},
  pages = {407--429},
  issn = {1939-3520},
  doi = {10.1109/TSE.2008.89},
  abstract = {The scientific study of a phenomenon requires it to be reproducible. Mature engineering industries are recognized by projects and products that are, to some extent, reproducible. Yet, reproducibility in software engineering (SE) has not been investigated thoroughly, despite the fact that lack of reproducibility has both practical and scientific consequences. We report a longitudinal multiple-case study of variations and reproducibility in software development, from bidding to deployment, on the basis of the same requirement specification. In a call for tender to 81 companies, 35 responded. Four of them developed the system independently. The firm price, planned schedule, and planned development process, had, respectively, “low,” “low,” and “medium” reproducibilities. The contractor's costs, actual lead time, and schedule overrun of the projects had, respectively, “medium,” “high,” and “low” reproducibilities. The quality dimensions of the delivered products, reliability, usability, and maintainability had, respectively, “low,” "high,” and “low” reproducibilities. Moreover, variability for predictable reasons is also included in the notion of reproducibility. We found that the observed outcome of the four development projects matched our expectations, which were formulated partially on the basis of SE folklore. Nevertheless, achieving more reproducibility in SE remains a great challenge for SE research, education, and industry.},
  eventtitle = {{{IEEE Transactions}} on {{Software Engineering}}},
  keywords = {reproducibility},
  annotation = {interest: 93},
  file = {/home/sam/Zotero/storage/NJGBA7X4/4693714.html}
}

@article{annanEditorialPublicationGeoscientific2013,
  title = {Editorial: {{The}} Publication of Geoscientific Model Developments v1.0},
  shorttitle = {Editorial},
  author = {Annan, J. and Hargreaves, D. and Lunt, D. and Ridgwell, A. and Rutt, I. and Sander, R.},
  date = {2013-08-14},
  journaltitle = {Geoscientific Model Development},
  volume = {6},
  number = {4},
  pages = {1233--1242},
  publisher = {Copernicus GmbH},
  issn = {1991-959X},
  doi = {10.5194/gmd-6-1233-2013},
  url = {https://gmd.copernicus.org/articles/6/1233/2013/},
  urldate = {2023-01-30},
  abstract = {{$<$}p{$><$}strong class="journal-contentHeaderColor"{$>$}Abstract.{$<$}/strong{$>$} In 2008, the first volume of the European Geosciences Union (EGU) journal Geoscientific Model Development (GMD) was published. GMD was founded because we perceived there to be a need for a space to publish comprehensive descriptions of numerical models in the geosciences. The journal is now well established, with the submission rate increasing over time. However, there are several aspects of model publication that we believe could be further improved. In this editorial we assess the lessons learned over the first few years of the journal's life, and describe some changes to GMD's editorial policy, which will ensure that the models and model developments are published in such a way that they are of maximum value to the community. {$<$}br{$><$}br{$>$} These changes to editorial policy mostly focus on improving the rigour of the review process through a stricter requirement for access to the materials necessary to test the behaviour of the models. {$<$}br{$><$}br{$>$} Throughout this editorial, "must" means that the stated actions are required, and the paper cannot be published without them; "strongly encouraged" means that we encourage the action, but papers can still be published if the criteria are not met; "may" means that the action may be carried out by the authors or referees, if they so wish. {$<$}br{$><$}br{$>$} We have reviewed and rationalised the manuscript types into five new categories. For all papers which are primarily based on a specific numerical model, the changes are as follows: - The paper must be accompanied by the code, or means of accessing the code, for the purpose of peer-review. If the code is normally distributed in a way which could compromise the anonymity of the referees, then the code must be made available to the editor. The referee/editor is not required to review the code in any way, but they may do so if they so wish. - All papers must include a section at the end of the paper entitled "Code availability". In this section, instructions for obtaining the code (e.g. from a supplement, or from a website) should be included; alternatively, contact information should be given where the code can be obtained on request, or the reasons why the code is not available should be clearly stated. - We strongly encourage authors to upload any user manuals associated with the code. - For models where this is practicable, we strongly encourage referees to compile the code, and run test cases supplied by the authors where appropriate.For models which have been previously described in the "grey" literature (e.g. as internal institutional documents), we strongly encourage authors to include this grey literature as a supplement, when this is allowed by the original authors.  - All papers must include a model name and version number (or other unique identifier) in the title.  -It is our perception that, since Geoscientific Model Development (GMD) was founded, it has become increasingly common to see model descriptions published in other more traditional journals, so we hope that our insights may be of general value to the wider geoscientific community.},
  langid = {english},
  keywords = {open data,project-acm-rep,reproducibility engineering},
  file = {/home/sam/Zotero/storage/AHL95BFI/GMD Executive Editors - 2013 - Editorial The publication of geoscientific model .pdf}
}

@unpublished{armstrongMessWeRe2014,
  title = {The {{Mess We}}'re {{In}}},
  author = {Armstrong, Joe},
  date = {2014-09-19},
  url = {https://www.youtube.com/watch?v=lKXe3HUG2l4},
  urldate = {2024-02-22},
  abstract = {Joe Armstrong is one of the inventors of Erlang. When at the Ericsson computer science lab in 1986, he was part of the team who designed and implemented the first version of Erlang. He has written several Erlang books including Programming Erlang Software for a Concurrent World. Joe has a PhD in computer science from the Royal Institute of Technology in Stockholm, Sweden.},
  venue = {Strange Loop Conference}
}

@book{armstrongPerformanceOpenSource,
  title = {The {{Performance}} of {{Open Source Applications}}},
  editor = {Armstrong, Tavish},
  url = {http://aosabook.org/en/index.html},
  langid = {english},
  pagetotal = {182}
}

@article{arrasSaBReLoadtimeSelective2022,
  title = {{{SaBRe}}: Load-Time Selective Binary Rewriting},
  shorttitle = {{{SaBRe}}},
  author = {Arras, Paul-Antoine and Andronidis, Anastasios and Pina, Luís and Mituzas, Karolis and Shu, Qianyi and Grumberg, Daniel and Cadar, Cristian},
  date = {2022-04-01},
  journaltitle = {International Journal on Software Tools for Technology Transfer},
  shortjournal = {Int J Softw Tools Technol Transfer},
  volume = {24},
  number = {2},
  pages = {205--223},
  issn = {1433-2787},
  doi = {10.1007/s10009-021-00644-w},
  url = {https://doi.org/10.1007/s10009-021-00644-w},
  urldate = {2025-01-14},
  abstract = {Binary rewriting consists in disassembling a program to modify its instructions. However, existing solutions suffer from shortcomings in terms of soundness and performance. We present SaBRe, a load-time system for selective binary rewriting. SaBRe rewrites specific constructs—particularly system calls and functions—when the program is loaded into memory, and intercepts them using plugins through a simple API. We also discuss the theoretical underpinnings of disassembling and rewriting. We developed two backends—for x86\_64 and RISC-V—which were used to implement three plugins: a fast system call tracer, a multi-version executor, and a fault injector. Our evaluation shows that SaBRe imposes little overhead, typically below 3\%.},
  langid = {english},
  keywords = {Fault injection,Multi-version execution,RISC-V,Selective binary rewriting,System call tracing,x86_64},
  file = {/home/sam/Zotero/storage/R42U8G58/Arras et al. - 2022 - SaBRe load-time selective binary rewriting.pdf}
}

@inproceedings{arthurKmeansAdvantagesCareful2007,
  title = {K-Means++: The Advantages of Careful Seeding},
  shorttitle = {K-Means++},
  booktitle = {Proceedings of the Eighteenth Annual {{ACM-SIAM}} Symposium on {{Discrete}} Algorithms},
  author = {Arthur, David and Vassilvitskii, Sergei},
  date = {2007-01-07},
  series = {{{SODA}} '07},
  pages = {1027--1035},
  publisher = {{Society for Industrial and Applied Mathematics}},
  location = {USA},
  abstract = {The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a very simple, randomized seeding technique, we obtain an algorithm that is Θ(logk)-competitive with the optimal clustering. Preliminary experiments show that our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.},
  isbn = {978-0-89871-624-5},
  keywords = {machine learning,project-provenance-pp},
  file = {/home/sam/Zotero/storage/P7XBXVNR/Arthur and Vassilvitskii - k-means++ The Advantages of Careful Seeding.pdf}
}

@article{arvanitouSoftwareEngineeringPractices2021,
  title = {Software Engineering Practices for Scientific Software Development: {{A}} Systematic Mapping Study},
  shorttitle = {Software Engineering Practices for Scientific Software Development},
  author = {Arvanitou, Elvira-Maria and Ampatzoglou, Apostolos and Chatzigeorgiou, Alexander and Carver, Jeffrey C.},
  date = {2021-02},
  journaltitle = {Journal of Systems and Software},
  shortjournal = {Journal of Systems and Software},
  volume = {172},
  pages = {110848},
  issn = {01641212},
  doi = {10.1016/j.jss.2020.110848},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0164121220302387},
  urldate = {2022-06-03},
  abstract = {Background: The development of scientific software applications is far from trivial, due to the constant increase in the necessary complexity of these applications, their increasing size, and their need for intensive maintenance and reuse. Aim: To this end, developers of scientific software (who usually lack a formal computer science background) need to use appropriate software engineering (SE) practices. This paper describes the results of a systematic mapping study on the use of SE for scientific application development and their impact on software quality. Method: To achieve this goal we have performed a systematic mapping study on 359 papers. We first describe a catalog of SE practices used in scientific software development. Then, we discuss the quality attributes of interest that drive the application of these practices, as well as tentative side-effects of applying the practices on qualities. Results: The main findings indicate that scientific software developers are focusing on practices that improve implementation productivity, such as code reuse, use of third-party libraries, and the application of “good” programming techniques. In addition, apart from the finding that performance is a key-driver for many of these applications, scientific software developers also find maintainability and productivity to be important. Conclusions: The results of the study are compared to existing literature, are interpreted under a software engineering prism, and various implications for researchers and practitioners are provided. One of the key findings of the study, which is considered as important for driving future research endeavors is the lack of evidence on the trade-offs that need to be made when applying a software practice, i.e., negative (indirect) effects on other quality attributes.},
  langid = {english},
  keywords = {industry practices,internship-project,research software engineering,software quality},
  annotation = {interest: 95},
  file = {/home/sam/Zotero/storage/4QFJHQM3/1-s2.0-S0164121220302387-main.pdf}
}

@article{atkinsonScientificWorkflowsPresent2017,
  title = {Scientific Workflows: {{Past}}, Present and Future},
  shorttitle = {Scientific Workflows},
  author = {Atkinson, Malcolm and Gesing, Sandra and Montagnat, Johan and Taylor, Ian},
  date = {2017-10},
  journaltitle = {Future Generation Computer Systems},
  shortjournal = {Future Generation Computer Systems},
  volume = {75},
  pages = {216--227},
  issn = {0167739X},
  doi = {10.1016/j.future.2017.05.041},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167739X17311202},
  urldate = {2022-07-07},
  langid = {english},
  keywords = {workflow managers},
  annotation = {interest: 95}
}

@online{aug24FallDatacenterSoftware2022,
  title = {The {{Fall}} of {{Datacenter Software}}},
  author = {family=Aug 24, given=Irene Zhang, prefix=on, useprefix=false and {2022}},
  date = {2022-08-24T13:00:28+00:00},
  url = {https://www.sigarch.org/the-fall-of-datacenter-software/},
  urldate = {2022-08-31},
  abstract = {The research community has long predicted the death of Moore’s law and attendant growth in datacenter hardware speeds. In a few years, datacenter networks will grow an order of magnitude from 40Gb …},
  langid = {american},
  organization = {SIGARCH},
  keywords = {computer architecture,operating systems},
  file = {/home/sam/Zotero/storage/WHYMLABY/the-fall-of-datacenter-software.html}
}

@article{avilaSUSSINGMERGERTREES2014,
  title = {{{SUSSING MERGER TREES}}: The Influence of the Halo Finder},
  shorttitle = {{{SUSSING MERGER TREES}}},
  author = {Avila, Santiago and Knebe, Alexander and Pearce, Frazer R. and Schneider, Aurel and Srisawat, Chaichalit and Thomas, Peter A. and Behroozi, Peter and Elahi, Pascal J. and Han, Jiaxin and Mao, Yao-Yuan and Onions, Julian and Rodriguez-Gomez, Vicente and Tweed, Dylan},
  date = {2014-07-11},
  journaltitle = {Monthly Notices of the Royal Astronomical Society},
  volume = {441},
  number = {4},
  pages = {3488--3501},
  issn = {1365-2966, 0035-8711},
  doi = {10.1093/mnras/stu799},
  url = {http://academic.oup.com/mnras/article/441/4/3488/1223418/SUSSING-MERGER-TREES-the-influence-of-the-halo},
  urldate = {2022-07-22},
  abstract = {Merger tree codes are routinely used to follow the growth and merger of dark matter haloes in simulations of cosmic structure formation. Whereas in Srisawat et. al. we compared the trees built using a wide variety of such codes, here we study the influence of the underlying halo catalogue upon the resulting trees. We observe that the specifics of halo finding itself greatly influences the constructed merger trees. We find that the choices made to define the halo mass are of prime importance. For instance, amongst many potential options different finders select self-bound objects or spherical regions of defined overdensity, decide whether or not to include substructures within the mass returned and vary in their initial particle selection. The impact of these decisions is seen in tree length (the period of time a particularly halo can be traced back through the simulation), branching ratio (essentially the merger rate of subhaloes) and mass evolution. We therefore conclude that the choice of the underlying halo finder is more relevant to the process of building merger trees than the tree builder itself. We also report on some built-in features of specific merger tree codes that (sometimes) help to improve the quality of the merger trees produced.},
  langid = {english},
  keywords = {astrophysics,dark matter halos}
}

@online{AxiomComputerAlgebra,
  title = {Axiom {{Computer Algebra System}}},
  url = {http://www.axiom-developer.org/},
  urldate = {2022-09-06},
  keywords = {formal verification,software},
  file = {/home/sam/Zotero/storage/K58XZE84/www.axiom-developer.org.html}
}

@article{aycockGoodWormsHuman2008,
  title = {"{{Good}}" Worms and Human Rights},
  author = {Aycock, John and Maurushat, Alana},
  date = {2008-03},
  journaltitle = {ACM SIGCAS Computers and Society},
  shortjournal = {SIGCAS Comput. Soc.},
  volume = {38},
  number = {1},
  pages = {28--39},
  issn = {0095-2737},
  doi = {10.1145/1361255.1361256},
  url = {https://dl.acm.org/doi/10.1145/1361255.1361256},
  urldate = {2023-10-12},
  abstract = {The extent of Internet censorship in countries like China is regularly tested, but the testing methods used from within a censored country can entail risk for humans. A benevolent worm can be used for testing instead: the worm’s selfreplication, long the bane of suggested benevolent viruses and worms, is shown to be essential here. We describe the design of this benevolent worm, along with some other related applications for it. A full technical, ethical, and legal analysis is provided.},
  langid = {english},
  file = {/home/sam/Zotero/storage/NWIUTS2L/Aycock and Maurushat - 2008 - Good worms and human rights.pdf}
}

@inproceedings{babujiParslPervasiveParallel2019,
  title = {Parsl: {{Pervasive Parallel Programming}} in {{Python}}},
  shorttitle = {Parsl},
  booktitle = {Proceedings of the 28th {{International Symposium}} on {{High-Performance Parallel}} and {{Distributed Computing}}},
  author = {Babuji, Yadu and Woodard, Anna and Li, Zhuozhao and Katz, Daniel S. and Clifford, Ben and Kumar, Rohan and Lacinski, Lukasz and Chard, Ryan and Wozniak, Justin M. and Foster, Ian and Wilde, Michael and Chard, Kyle},
  date = {2019-06-17},
  series = {{{HPDC}} '19},
  pages = {25--36},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3307681.3325400},
  url = {https://doi.org/10.1145/3307681.3325400},
  urldate = {2022-05-12},
  abstract = {High-level programming languages such as Python are increasingly used to provide intuitive interfaces to libraries written in lower-level languages and for assembling applications from various components. This migration towards orchestration rather than implementation, coupled with the growing need for parallel computing (e.g., due to big data and the end of Moore's law), necessitates rethinking how parallelism is expressed in programs. Here, we present Parsl, a parallel scripting library that augments Python with simple, scalable, and flexible constructs for encoding parallelism. These constructs allow Parsl to construct a dynamic dependency graph of components that it can then execute efficiently on one or many processors. Parsl is designed for scalability, with an extensible set of executors tailored to different use cases, such as low-latency, high-throughput, or extreme-scale execution. We show, via experiments on the Blue Waters supercomputer, that Parsl executors can allow Python scripts to execute components with as little as 5 ms of overhead, scale to more than 250000 workers across more than 8000 nodes, and process upward of 1200 tasks per second. Other Parsl features simplify the construction and execution of composite programs by supporting elastic provisioning and scaling of infrastructure, fault-tolerant execution, and integrated wide-area data management. We show that these capabilities satisfy the needs of many-task, interactive, online, and machine learning applications in fields such as biology, cosmology, and materials science.},
  isbn = {978-1-4503-6670-0},
  keywords = {project-acm-rep,project-charmonium.cache,workflow managers},
  file = {/home/sam/Zotero/storage/8RGEIJIE/Babuji et al. - 2019 - Parsl Pervasive Parallel Programming in Python.pdf}
}

@inproceedings{babujiScalableParallelProgramming2019,
  title = {Scalable {{Parallel Programming}} in {{Python}} with {{Parsl}}},
  booktitle = {Proceedings of the {{Practice}} and {{Experience}} in {{Advanced Research Computing}} on {{Rise}} of the {{Machines}} (Learning)},
  author = {Babuji, Yadu and Woodard, Anna and Li, Zhuozhao and Katz, Daniel S. and Clifford, Ben and Foster, Ian and Wilde, Michael and Chard, Kyle},
  date = {2019-07-28},
  pages = {1--8},
  publisher = {ACM},
  location = {Chicago IL USA},
  doi = {10.1145/3332186.3332231},
  url = {https://dl.acm.org/doi/10.1145/3332186.3332231},
  urldate = {2022-05-25},
  abstract = {Python is increasingly the lingua franca of scientific computing. It is used as a higher level language to wrap lower-level libraries and to compose scripts from various independent components. However, scaling and moving Python programs from laptops to supercomputers remains a challenge. Here we present Parsl, a parallel scripting library for Python. Parsl makes it straightforward for developers to implement parallelism in Python by annotating functions that can be executed asynchronously and in parallel, and to scale analyses from a laptop to thousands of nodes on a supercomputer or distributed system. We examine how Parsl is implemented, focusing on syntax and usage. We describe two scientific use cases in which Parsl's intuitive and scalable parallelism is used.},
  eventtitle = {{{PEARC}} '19: {{Practice}} and {{Experience}} in {{Advanced Research Computing}}},
  isbn = {978-1-4503-7227-5},
  langid = {english},
  keywords = {hpc,research software engineering},
  file = {/home/sam/Zotero/storage/ACK35PBV/3332186.3332231.pdf}
}

@article{badiaWorkflowsScienceChallenge2017,
  title = {Workflows for Science: A Challenge When Facing the Convergence of {{HPC}} and {{Big Data}}},
  shorttitle = {Workflows for Science},
  author = {Badia, Rosa M and Ayguade, Eduard and Labarta, Jesus},
  date = {2017-03},
  journaltitle = {Supercomputing Frontiers and Innovations},
  shortjournal = {JSFI},
  volume = {4},
  number = {1},
  issn = {23138734},
  doi = {10.14529/jsfi170102},
  url = {https://superfri.org/index.php/superfri/article/view/125},
  urldate = {2022-06-28},
  abstract = {Workflows have been used traditionally as a mean to describe and implement the computing usually parametric studies and explorations searching for the best solution  that  scientific researchers want to perform.  A workflow is not only the computing application, but a way of documenting a process.  Science workflows may be of very different nature depending on the area of research, matching the actual experiment that the scientist want to perform.  Workflow Management Systems are environments that offer the researchers tools to define, publish, execute and document their workflows.  In some cases, the science workflows are used to generate data; in other cases are used to analyse existing data; only in a few cases, workflows are used both to generate and analyse  data. The design of experiments is in some cases generated blindly, without a clear idea of which points are relevant to be computed/simulated, ending up with huge amount of computation that is performed following a brute-force strategy.  However, the evolution of systems and the large amount of data generated by the applications require an in-situ analysis of the data, thus requiring new solutions to develop workflows that includes both the simulation/computational part and the analytic part. What is more, the fact that both components, computation and analytics, can be run together  will enable the possibility of defining more dynamic workflows, with new computations being decided by the analytics in a more efficient way. The first part of the paper will review current approaches that a set of scientific communities follows in the development of their workflows. Due to the election of several scientific communities and use cases using a specific Workflow Management System, this survey maybe incomplete with regard a complete revision of the literature about workflows, but we expect that the reader appreaciates the effort performed in trying to see the scientific communities needs and requirements.  The second part of the paper will propose a new software architecture to develop a new  family of end-to-end workflows that enables the management of  dynamic workflows composed of simulations, analytics and visualization, including inputs/outputs from streams.},
  keywords = {internship-project,workflow managers},
  annotation = {interest: 95}
}

@online{baezWhatWeCan,
  title = {What {{We Can Do About Science Journals}}},
  author = {Baez, John},
  url = {https://math.ucr.edu/home/baez/journals.html},
  urldate = {2022-08-30},
  keywords = {academic publishing},
  file = {/home/sam/Zotero/storage/DQDL8CNY/journals.html}
}

@article{baggerlyDerivingChemosensitivityCell2009,
  title = {Deriving Chemosensitivity from Cell Lines: Forensic Bioinformatics and Reproducible Research in High-Throughput Biology},
  author = {Baggerly, Keith A. and Coombes, Kevin R.},
  date = {2009},
  journaltitle = {The Annals of Applied Statistics},
  volume = {3},
  number = {4},
  eprint = {27801549},
  eprinttype = {jstor},
  pages = {1309--1334},
  publisher = {Institute of Mathematical Statistics},
  issn = {1932-6157},
  url = {https://www.jstor.org/stable/27801549},
  urldate = {2022-09-06},
  abstract = {High-throughput biological assays such as microarrays let us ask very detailed questions about how diseases operate, and promise to let us personalize therapy. Data processing, however, is often not described well enough to allow for exact reproduction of the results, leading to exercises in "forensic bioinformatics" where aspects of raw data and reported results are used to infer what methods must have been employed. Unfortunately, poor documentation can shift from an inconvenience to an active danger when it obscures not just methods but errors. In this report we examine several related papers purporting to use microarray-based signatures of drug sensitivity derived from cell lines to predict patient response. Patients in clinical trials are currently being allocated to treatment arms on the basis of these results. However, we show in five case studies that the results incorporate several simple errors that may be putting patients at risk. One theme that emerges is that the most common errors are simple (e.g., row or column offsets); conversely, it is our experience that the most simple errors are common. We then discuss steps we are taking to avoid such errors in our own investigations.},
  keywords = {reproducibility engineering,research software engineering},
  annotation = {interest: 74}
}

@article{bagozziLegacyTechnologyAcceptance2007,
  title = {The {{Legacy}} of the {{Technology Acceptance Model}} and a {{Proposal}} for a {{Paradigm Shift}}},
  author = {Bagozzi, Richard},
  date = {2007-04},
  journaltitle = {Journal of the Association for Information Systems},
  shortjournal = {JAIS},
  volume = {8},
  number = {4},
  pages = {244--254},
  issn = {15369323},
  doi = {10.17705/1jais.00122},
  url = {https://aisel.aisnet.org/jais/vol8/iss4/12/},
  urldate = {2022-05-27},
  abstract = {This article presents a critique of a number of shortcomings with the technology acceptance model (TAM) and points to specific remedies in each case. In addition, I present a model for the purposes of providing a foundation for a paradigm shift. The model consists first of a decision making core (goal desire → goal intention → action desire → action intention) that is grounded in basic decision making variables/processes of a universal nature. The decision core also contains a mechanism for self-regulation that moderates the effects of desires on intentions. Second, added to the decision making core are a number of causes and effects of decisions and self-regulatory reasoning, with the aim of introducing potential contingent, contextual nuances for understanding decision making. Many of the causal variables here are contained within TAM or its extensions; also considered are new variables grounded in emotional, group/social/cultural, and goal-directed behavior research.},
  keywords = {internship-project,technology-acceptance},
  file = {/home/sam/Zotero/storage/FHEUAUCJ/The Legacy of the Technology Acceptance Model and a Proposal for.pdf;/home/sam/Zotero/storage/SMKEUKZ4/Screenshot from 2022-06-02 11-25-34.png}
}

@article{baileyDangerDeathAre,
  entrysubtype = {newspaper},
  title = {Danger of Death: Are We Programmed to Miscalculate Risk?},
  shorttitle = {Danger of Death},
  author = {Bailey, David H. and Borwein, Jonathan},
  journaltitle = {The Conversation},
  url = {http://theconversation.com/danger-of-death-are-we-programmed-to-miscalculate-risk-4598},
  urldate = {2022-08-25},
  abstract = {Assessing risk is something everyone must do every day. Yet few are very good at it, and there are significant consequences of the public’s collective inability to accurately assess risk. As a first and…},
  langid = {english},
  keywords = {psychology},
  file = {/home/sam/Zotero/storage/U8C7H5J3/danger-of-death-are-we-programmed-to-miscalculate-risk-4598.html}
}

@article{baileyHowStopMedia,
  entrysubtype = {newspaper},
  title = {How to Stop the Media Reporting Science Fiction as Fact},
  author = {Bailey, David H. and Borwein, Jonathan},
  journaltitle = {The Conversation},
  url = {http://theconversation.com/how-to-stop-the-media-reporting-science-fiction-as-fact-10252},
  urldate = {2022-08-25},
  abstract = {Few of us have the time or expertise to sift through all of the scientific papers published every day to determine which research is important and relevant to our lives. In this sense, science journalists…},
  langid = {english},
  keywords = {journalism},
  file = {/home/sam/Zotero/storage/G9I7GHWH/how-to-stop-the-media-reporting-science-fiction-as-fact-10252.html}
}

@article{bakerHowQualityControl2016,
  title = {How Quality Control Could Save Your Science},
  author = {Baker, Monya},
  date = {2016-01-01},
  journaltitle = {Nature},
  volume = {529},
  number = {7587},
  pages = {456--458},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/529456a},
  url = {https://www.nature.com/articles/529456a},
  urldate = {2022-09-27},
  abstract = {It may not be sexy, but quality assurance is becoming a crucial part of lab life.},
  issue = {7587},
  langid = {english},
  keywords = {metascience},
  annotation = {interest: 71},
  file = {/home/sam/Zotero/storage/IJC4G3TJ/Baker - 2016 - How quality control could save your science.pdf;/home/sam/Zotero/storage/8HTSN2WQ/529456a.html}
}

@online{bakhvalovHowGetConsistent,
  title = {How to Get Consistent Results When Benchmarking on {{Linux}}?},
  author = {Bakhvalov, Denis},
  url = {https://easyperf.net/blog/2019/08/02/Perf-measurement-environment-on-Linux},
  urldate = {2022-04-11},
  organization = {EasyPerf},
  keywords = {software benchmarking,software engineering},
  annotation = {score: 70},
  file = {/home/sam/Zotero/storage/3Z3IXZVF/Perf-measurement-environment-on-Linux.html}
}

@inproceedings{balakrishnanOPUSLightweightSystem2013,
  title = {\{\vphantom\}{{OPUS}}\vphantom\{\}: {{A Lightweight System}} for {{Observational Provenance}} in {{User Space}}},
  shorttitle = {{{OPUS}}},
  author = {Balakrishnan, Nikilesh and Bytheway, Thomas and Sohan, Ripduman and Hopper, Andy},
  date = {2013},
  url = {https://www.usenix.org/conference/tapp13/technical-sessions/presentation/balakrishnan},
  urldate = {2023-07-06},
  eventtitle = {5th {{USENIX Workshop}} on the {{Theory}} and {{Practice}} of {{Provenance}} ({{TaPP}} 13)},
  langid = {english},
  keywords = {project-provenance-pp,provenance,provenance-tool},
  file = {/home/sam/Zotero/storage/APTAJA64/Balakrishnan et al. - 2013 - OPUS A Lightweight System for Observational Pro.pdf}
}

@online{balasubramanianRADICALCybertoolsMiddlewareBuilding2019,
  title = {{{RADICAL-Cybertools}}: {{Middleware Building Blocks}} for {{Scalable Science}}},
  shorttitle = {{{RADICAL-Cybertools}}},
  author = {Balasubramanian, Vivek and Jha, Shantenu and Merzky, Andre and Turilli, Matteo},
  date = {2019-04-05},
  eprint = {1904.03085},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1904.03085},
  url = {http://arxiv.org/abs/1904.03085},
  urldate = {2023-05-06},
  abstract = {RADICAL-Cybertools (RCT) are a set of software systems that serve as middleware to develop efficient and effective tools for scientific computing. Specifically, RCT enable executing many-task applications at extreme scale and on a variety of computing infrastructures. RCT are building blocks, designed to work as stand-alone systems, integrated among themselves or integrated with third-party systems. RCT enables innovative science in multiple domains, including but not limited to biophysics, climate science and particle physics, consuming hundreds of millions of core hours. This paper provides an overview of RCT systems, their impact, and the architectural principles and software engineering underlying RCT},
  pubstate = {prepublished},
  keywords = {research software engineering},
  annotation = {interest: 85},
  file = {/home/sam/Zotero/storage/YAPEE5WF/Balasubramanian et al. - 2019 - RADICAL-Cybertools Middleware Building Blocks for.pdf;/home/sam/Zotero/storage/VMUVFT9Q/1904.html}
}

@article{barbaHardRoadReproducibility2016,
  title = {The Hard Road to Reproducibility},
  author = {Barba, Lorena A.},
  date = {2016-10-07},
  journaltitle = {Science},
  volume = {354},
  number = {6308},
  pages = {142--142},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.354.6308.142},
  url = {https://www.science.org/doi/10.1126/science.354.6308.142},
  urldate = {2023-01-24},
  keywords = {reproducibility engineering,research software engineering},
  file = {/home/sam/Zotero/storage/VM4YNVRU/Barba - 2016 - The hard road to reproducibility.pdf}
}

@unpublished{barbaReproducibilityPIManifesto2012,
  type = {presentation},
  title = {Reproducibility {{PI Manifesto}}},
  author = {Barba, Lorena A.},
  date = {2012-12-13},
  publisher = {figshare},
  doi = {10.6084/m9.figshare.104539.v1},
  url = {https://figshare.com/articles/presentation/Reproducibility_PI_Manifesto/104539/1},
  urldate = {2023-01-24},
  abstract = {Slides for lightning talk at the ICERM workshop on "Reproducibility in Computational and Experimental Mathematics", December 2012. Shared under CC-BY.},
  langid = {english},
  keywords = {reproducibility engineering},
  file = {/home/sam/Zotero/storage/BAZDE4ZZ/Barba - 2012 - Reproducibility PI Manifesto.pdf}
}

@unpublished{bardramWritingComputerScience2007,
  title = {Writing a ({{Computer Science}}) {{Paper}}},
  author = {Bardram, Jakob E},
  date = {2007},
  langid = {english},
  keywords = {academic writing},
  file = {/home/sam/Zotero/storage/IJG955FU/Bardram - Writing a (Computer Science) Paper.pdf}
}

@article{bargaAutomaticCaptureEfficient2008,
  title = {Automatic Capture and Efficient Storage of E-{{Science}} Experiment Provenance},
  author = {Barga, Roger S. and Digiampietri, Luciano A.},
  date = {2008},
  journaltitle = {Concurrency and Computation: Practice and Experience},
  volume = {20},
  number = {5},
  pages = {419--429},
  issn = {1532-0634},
  doi = {10.1002/cpe.1235},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.1235},
  urldate = {2023-07-18},
  abstract = {For the first provenance challenge, we introduce a layered model to represent workflow provenance that allows navigation from an abstract model of the experiment to instance data collected during a specific experiment run. We outline modest extensions to a commercial workflow engine so it will automatically capture provenance at workflow runtime. We also present an approach to store this provenance data in a relational database. Finally, we demonstrate how core provenance queries in the challenge can be expressed in SQL and discuss the merits of our layered representation. Copyright © 2007 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {/home/sam/Zotero/storage/958YTBAZ/Barga and Digiampietri - 2008 - Automatic capture and efficient storage of e-Scien.pdf;/home/sam/Zotero/storage/UPLVKT46/cpe.html}
}

@article{barnesPublishYourComputer2010,
  title = {Publish Your Computer Code: It Is Good Enough},
  shorttitle = {Publish Your Computer Code},
  author = {Barnes, Nick},
  date = {2010-10},
  journaltitle = {Nature},
  volume = {467},
  number = {7317},
  pages = {753--753},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/467753a},
  url = {https://www.nature.com/articles/467753a},
  urldate = {2023-01-19},
  abstract = {Freely provided working code — whatever its quality — improves programming and enables others to engage with your research, says Nick Barnes.},
  issue = {7317},
  langid = {english},
  keywords = {opensource software,reproducibility engineering,research software engineering},
  file = {/home/sam/Zotero/storage/D6VGNURJ/Barnes - 2010 - Publish your computer code it is good enough.pdf}
}

@article{bartlettXSDKFoundationsExtremescale2017,
  title = {{{xSDK Foundations}}: {{Toward}} an {{Extreme-scale Scientific Software Development Kit}}},
  shorttitle = {{{xSDK Foundations}}},
  author = {Bartlett, Roscoe and Demeshko, Irina and Gamblin, Todd and Hammond, Glenn and Heroux, Michael Allen and Johnson, Jeffrey and Klinvex, Alicia and Li, Xiaoye and McInnes, Lois Curfman and Moulton, J. David and Osei-Kuffuor, Daniel and Sarich, Jason and Smith, Barry and Willenbring, James and Yang, Ulrike Meier},
  date = {2017-02-25},
  journaltitle = {Supercomputing Frontiers and Innovations},
  volume = {4},
  number = {1},
  pages = {69--82},
  issn = {2313-8734},
  doi = {10.14529/jsfi170104},
  url = {https://superfri.org/index.php/superfri/article/view/127},
  urldate = {2023-08-31},
  abstract = {Extreme-scale computational science increasingly demands multiscale and multiphysics formulations. Combining software developed by independent groups is imperative: no single team has resources for all predictive science and decision support capabilities. Scientific libraries provide high-quality, reusable software components for constructing applications with improved robustness and portability.~ However, without coordination, many libraries cannot be easily composed.~ Namespace collisions, inconsistent arguments, lack of third-party software versioning, and additional difficulties make composition costly.The Extreme-scale Scientific Software Development Kit (xSDK) defines community policies to improve code quality and compatibility across independently developed packages (hypre, PETSc, SuperLU, Trilinos, and Alquimia) and provides a foundation for addressing broader issues in software interoperability, performance portability, and sustainability.~ The xSDK provides turnkey installation of member software and seamless combination of aggregate capabilities, and it marks first steps toward extreme-scale scientific software ecosystems from which future applications can be composed rapidly with assured quality and scalability.},
  issue = {1},
  langid = {english},
  keywords = {high-performance computing,project-provenance-pp},
  file = {/home/sam/Zotero/storage/C3CDRTF5/Bartlett et al. - 2017 - xSDK Foundations Toward an Extreme-scale Scientif.pdf}
}

@inproceedings{batesTrustworthyWholeSystemProvenance2015,
  title = {Trustworthy \{\vphantom\}{{Whole-System}}\vphantom\{\} {{Provenance}} for the {{Linux Kernel}}},
  author = {Bates, Adam and Tian, Dave (Jing) and Butler, Kevin R. B. and Moyer, Thomas},
  date = {2015},
  pages = {319--334},
  url = {https://www.usenix.org/conference/usenixsecurity15/technical-sessions/presentation/bates},
  urldate = {2023-08-25},
  eventtitle = {24th {{USENIX Security Symposium}} ({{USENIX Security}} 15)},
  isbn = {978-1-939133-11-3},
  langid = {english},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/5QPU4S6E/Bates et al. - 2015 - Trustworthy Whole-System Provenance for the Linu.pdf}
}

@article{beaulieu-jonesReproducibilityComputationalWorkflows2017,
  title = {Reproducibility of Computational Workflows Is Automated Using Continuous Analysis},
  author = {Beaulieu-Jones, Brett K. and Greene, Casey S.},
  date = {2017-04},
  journaltitle = {Nature Biotechnology},
  shortjournal = {Nat Biotechnol},
  volume = {35},
  number = {4},
  pages = {342--346},
  publisher = {Nature Publishing Group},
  issn = {1546-1696},
  doi = {10.1038/nbt.3780},
  url = {https://www.nature.com/articles/nbt.3780},
  urldate = {2023-02-20},
  abstract = {The application of continuous integration, an approach common in software development, enables the automatic reproduction of computational analyses.},
  issue = {4},
  langid = {english},
  keywords = {project-acm-rep,project-continuous-analysis,reproducibility engineering,workflow managers},
  file = {/home/sam/Zotero/storage/B26RFVP4/Beaulieu-Jones and Greene - 2017 - Reproducibility of computational workflows is auto.pdf}
}

@inproceedings{bechhoferWhyLinkedData2010,
  title = {Why {{Linked Data}} Is {{Not Enough}} for {{Scientists}}},
  booktitle = {2010 {{IEEE Sixth International Conference}} on E-{{Science}}},
  author = {Bechhofer, Sean and Ainsworth, John and Bhagat, Jiten and Buchan, Iain and Couch, Philip and Cruickshank, Don and Roure, David De and Delderfield, Mark and Dunlop, Ian and Gamble, Matthew and Goble, Carole and Michaelides, Danius and Missier, Paolo and Owen, Stuart and Newman, David and Sufi, Shoaib},
  date = {2010-12},
  pages = {300--307},
  doi = {10.1109/eScience.2010.21},
  abstract = {Scientific data stands to represent a significant portion of the linked open data cloud and science itself stands to benefit from the data fusion capability that this will afford. However, simply publishing linked data into the cloud does not necessarily meet the requirements of reuse. Publishing has requirements of provenance, quality, credit, attribution, methods in order to provide the \textbackslash emphreproducibility that allows validation of results. In this paper we make the case for a scientific data publication model on top of linked data and introduce the notion of \textbackslash emphResearch Objects as first class citizens for sharing and publishing.},
  eventtitle = {2010 {{IEEE Sixth International Conference}} on E-{{Science}}},
  keywords = {provenance,semantic web}
}

@article{begleyDrugDevelopmentRaise2012,
  title = {Drug Development: {{Raise}} Standards for Preclinical Cancer Research},
  shorttitle = {Drug Development},
  author = {Begley, C. Glenn and Ellis, Lee M.},
  date = {2012-03-28},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {483},
  number = {7391},
  eprint = {22460880},
  eprinttype = {pmid},
  pages = {531--533},
  issn = {1476-4687},
  doi = {10.1038/483531a},
  langid = {english},
  keywords = {drug development,open data},
  annotation = {interest: 85}
}

@article{behrooziMajorMergersGoing2015,
  title = {Major Mergers Going {{Notts}}: Challenges for Modern Halo Finders},
  shorttitle = {Major Mergers Going {{Notts}}},
  author = {Behroozi, Peter and Knebe, Alexander and Pearce, Frazer R. and Elahi, Pascal and Han, Jiaxin and Lux, Hanni and Mao, Yao-Yuan and Muldrew, Stuart I. and Potter, Doug and Srisawat, Chaichalit},
  date = {2015-12-11},
  journaltitle = {Monthly Notices of the Royal Astronomical Society},
  shortjournal = {Mon. Not. R. Astron. Soc.},
  volume = {454},
  number = {3},
  pages = {3020--3029},
  issn = {0035-8711, 1365-2966},
  doi = {10.1093/mnras/stv2046},
  url = {https://academic.oup.com/mnras/article-lookup/doi/10.1093/mnras/stv2046},
  urldate = {2022-07-22},
  abstract = {Merging haloes with similar masses (i.e. major mergers) pose significant challenges for halo finders. We compare five halo-finding algorithms’ (ahf, hbt, rockstar, subfind, and velociraptor) recovery of halo properties for both isolated and cosmological major mergers. We find that halo positions and velocities are often robust, but mass biases exist for every technique. The algorithms also show strong disagreement in the prevalence and duration of major mergers, especially at high redshifts (z {$>$} 1). This raises significant uncertainties for theoretical models that require major mergers for, e.g. galaxy morphology changes, size changes, or black hole growth, as well as for finding Bullet Cluster analogues. All finders not using temporal information also show host halo and subhalo relationship swaps over successive timesteps, requiring careful merger tree construction to avoid problematic mass accretion histories. We suggest that future algorithms should combine phase-space and temporal information to avoid the issues presented.},
  langid = {english},
  keywords = {astrophysics,dark matter halos}
}

@online{beingessnerHowSwiftAchieved2019,
  title = {How {{Swift Achieved Dynamic Linking Where Rust Couldn}}'t},
  author = {Beingessner, Aria},
  date = {2019-11-07},
  url = {https://gankra.github.io/blah/swift-abi},
  urldate = {2023-03-10},
  langid = {english},
  organization = {Faultlore},
  keywords = {programming languages},
  annotation = {interest: 75},
  file = {/home/sam/Zotero/storage/A29AW25I/swift-abi.html}
}

@online{beingessnerPrePoopingYourPants2015,
  title = {Pre-{{Pooping Your Pants With Rust}}},
  author = {Beingessner, Alexis},
  date = {2015-04-27},
  url = {https://cglab.ca/~abeinges/blah/everyone-poops/},
  urldate = {2023-12-17},
  abstract = {Why Rust is not guarantee when destructors get called},
  organization = {The Miscellaneous Screamings of Alexis Beingessner}
}

@online{belhajjameWf4EverResearchObject2013,
  title = {{{Wf4Ever Research Object Model}} 1.0},
  author = {Belhajjame, Khalid and Klyne, Graham and Garijo, Daniel and Corch, Oscar and García Cuesta, Esteban and Palma, Raul},
  date = {2013-11-30},
  url = {https://wf4ever.github.io/ro/},
  abstract = {The Wf4Ever Research Object Model provides a vocabulary for the description of workflow-centric Research Objects: aggregations of resources relating to scientific workflows.},
  keywords = {provenance,semantic web}
}

@inproceedings{belhajjameWorkflowPROVcorpusBased2013,
  title = {A Workflow {{PROV-corpus}} Based on Taverna and Wings},
  booktitle = {Proceedings of the {{Joint EDBT}}/{{ICDT}} 2013 {{Workshops}} on - {{EDBT}} '13},
  author = {Belhajjame, Khalid and Zhao, Jun and Garijo, Daniel and Garrido, Aleix and Soiland-Reyes, Stian and Alper, Pinar and Corcho, Oscar},
  date = {2013},
  pages = {331},
  publisher = {ACM Press},
  location = {Genoa, Italy},
  doi = {10.1145/2457317.2457376},
  url = {http://dl.acm.org/citation.cfm?doid=2457317.2457376},
  urldate = {2022-08-02},
  abstract = {We describe a corpus of provenance traces that we have collected by executing 120 real world scientific workflows. The workflows are from two different workflow systems: Taverna [5] and Wings [3], and 12 different application domains (see Figure 1). Table 1 provides a summary of this PROV-corpus.},
  eventtitle = {The {{Joint EDBT}}/{{ICDT}} 2013 {{Workshops}}},
  isbn = {978-1-4503-1599-9},
  langid = {english},
  keywords = {provenance,semantic web}
}

@article{bentonLegalitySanctuaryCities2018,
  title = {The {{Legality}} of {{Sanctuary Cities}}},
  author = {Benton, Grace},
  date = {2018/2019},
  journaltitle = {Georgetown Immigration Law Journal},
  shortjournal = {Geo. Immigr. L.J.},
  volume = {33},
  pages = {139},
  url = {https://heinonline.org/HOL/Page?handle=hein.journals/geoimlj33&id=141&div=&collection=},
  file = {/home/sam/Zotero/storage/QJ8ZWHP5/Benton - The Legality of Sanctuary Cities.pdf}
}

@article{berganDeterministicProcessGroups,
  title = {Deterministic {{Process Groups}} in {{dOS}}},
  author = {Bergan, Tom and Hunt, Nicholas and Ceze, Luis and Gribble, Steven D},
  abstract = {Current multiprocessor systems execute parallel and concurrent software nondeterministically: even when given precisely the same input, two executions of the same program may produce different output. This severely complicates debugging, testing, and automatic replication for fault-tolerance. Previous efforts to address this issue have focused primarily on record and replay, but making execution actually deterministic would address the problem at the root.},
  langid = {english},
  keywords = {reproducibility},
  file = {/home/sam/Zotero/storage/I7N5ZURA/Bergan et al. - Deterministic Process Groups in dOS.pdf}
}

@inproceedings{bergerPerformanceMatters2017,
  title = {Performance Matters},
  author = {Berger, Emery},
  date = {2017-09-10},
  pages = {53--55},
  publisher = {Barcelona Supercomputing Center},
  url = {https://upcommons.upc.edu/handle/2117/108842},
  urldate = {2022-04-11},
  abstract = {Performance clearly matters to users. The most common software update on the AppStore *by far* is "Bug fixes and performance enhancements." Now that Moore's Law Free Lunch has ended, programmers have to work hard to get high performance for their applications. But why is performance so hard to deliver? I will first explain why our current approaches to evaluating and optimizing performance don't work, especially on modern hardware and for modern applications. I will then present two systems that address these challenges. Stabilizer is a tool that enables statistically sound performance evaluation, making it possible to understand the impact of optimizations and conclude things like the fact that the -O2 and -O3 optimization levels are indistinguishable from noise (unfortunately true). Since compiler optimizations have largely run out of steam, we need better profiling support, especially for modern concurrent, multi-threaded applications. Coz is a novel "causal profiler" that lets programmers optimize for throughput or latency, and which pinpoints and accurately predicts the impact of optimizations. Coz's approach unlocks numerous previously unknown optimization opportunities. Guided by Coz, we improved the performance of Memcached by 9\%, SQLite by 25\%, and accelerated six Parsec applications by as much as 68\%; in most cases, these optimizations involved modifying under 10 lines of code. This talk is based on work with Charlie Curtsinger published at ASPLOS 2013 (Stabilizer) and SOSP 2015 (Coz), which received a Best Paper Award and was selected as a CACM Research Highlight.},
  eventtitle = {Book of Abstracts},
  langid = {english},
  keywords = {software benchmarking,software engineering},
  annotation = {Accepted: 2017-10-19T08:36:31Z},
  file = {/home/sam/Zotero/storage/PGHD6WDJ/Berger - 2017 - Performance matters.pdf}
}

@unpublished{bergerPerformanceMatters2019,
  title = {Performance {{Matters}}},
  author = {Berger, Emery},
  date = {2019-09-15},
  url = {https://www.youtube.com/watch?v=r-TLSBdHe1A},
  urldate = {2022-04-11},
  abstract = {Performance clearly matters to users. For example, the most common software update on the AppStore is "Bug fixes and performance enhancements." Now that Moore's Law has ended, programmers have to work hard to get high performance for their applications. But why is performance hard to deliver? I will first explain why current approaches to evaluating and optimizing performance don't work, especially on modern hardware and for modern applications. I then present two systems that address these challenges. Stabilizer is a tool that enables statistically sound performance evaluation, making it possible to understand the impact of optimizations and conclude things like the fact that the -O2 and -O3 optimization levels are indistinguishable from noise (sadly true). Since compiler optimizations have run out of steam, we need better profiling support, especially for modern concurrent, multi-threaded applications. Coz is a new "causal profiler" that lets programmers optimize for throughput or latency, and which pinpoints and accurately predicts the impact of optimizations. Coz's approach unlocks previously unknown optimization opportunities. Guided by Coz, we improved the performance of Memcached (9\%), SQLite (25\%), and accelerated six other applications by as much as 68\%; in most cases, this involved modifying less than 10 lines of code and took under half an hour (without any prior understanding of the programs!). Coz now ships as part of standard Linux distros (apt install coz-profiler).},
  eventtitle = {Strange {{Loop}} 2019},
  keywords = {software benchmarking,software engineering},
  annotation = {score: 80}
}

@article{berners-leeSemanticWeb2001,
  title = {The {{Semantic Web}}},
  author = {Berners-Lee, Tim and Hendler, James and Lassila, Ora},
  date = {2001},
  journaltitle = {Scientific American},
  volume = {284},
  number = {5},
  eprint = {26059207},
  eprinttype = {jstor},
  pages = {34--43},
  publisher = {Scientific American, a division of Nature America, Inc.},
  issn = {0036-8733},
  url = {https://www.jstor.org/stable/26059207},
  urldate = {2023-06-07},
  keywords = {semantic web},
  file = {/home/sam/Zotero/storage/YUR7ULPT/Berners-Lee et al. - 2001 - The Semantic Web.pdf}
}

@article{berradaBaselineUnsupervisedAdvanced2020,
  title = {A Baseline for Unsupervised Advanced Persistent Threat Detection in System-Level Provenance},
  author = {Berrada, Ghita and Cheney, James and Benabderrahmane, Sidahmed and Maxwell, William and Mookherjee, Himan and Theriault, Alec and Wright, Ryan},
  date = {2020-07-01},
  journaltitle = {Future Generation Computer Systems},
  shortjournal = {Future Generation Computer Systems},
  volume = {108},
  pages = {401--413},
  issn = {0167-739X},
  doi = {10.1016/j.future.2020.02.015},
  url = {https://www.sciencedirect.com/science/article/pii/S0167739X19320448},
  urldate = {2023-08-23},
  abstract = {Advanced persistent threats (APTs) are stealthy, sophisticated, and unpredictable cyberattacks that can steal intellectual property, damage critical infrastructure, or cause millions of dollars in damage. Detecting APTs by monitoring system-level activity is difficult because manually inspecting the high volume of normal system activity is overwhelming for security analysts. We evaluate the effectiveness of unsupervised batch and streaming anomaly detection algorithms over multiple gigabytes of provenance traces recorded on four different operating systems to determine whether they can detect realistic APT-like attacks reliably and efficiently. This article is the first detailed study of the effectiveness of generic unsupervised anomaly detection techniques in this setting.},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/6LH8V7HU/Berrada et al. - 2020 - A baseline for unsupervised advanced persistent th.pdf;/home/sam/Zotero/storage/J7DXZAI5/S0167739X19320448.html}
}

@article{bertholdKNIMEKonstanzInformation2009,
  title = {{{KNIME}} - the {{Konstanz}} Information Miner: Version 2.0 and Beyond},
  shorttitle = {{{KNIME}} - the {{Konstanz}} Information Miner},
  author = {Berthold, Michael R. and Cebron, Nicolas and Dill, Fabian and Gabriel, Thomas R. and Kötter, Tobias and Meinl, Thorsten and Ohl, Peter and Thiel, Kilian and Wiswedel, Bernd},
  date = {2009-11-16},
  journaltitle = {ACM SIGKDD Explorations Newsletter},
  shortjournal = {SIGKDD Explor. Newsl.},
  volume = {11},
  number = {1},
  pages = {26--31},
  issn = {1931-0145, 1931-0153},
  doi = {10.1145/1656274.1656280},
  url = {https://dl.acm.org/doi/10.1145/1656274.1656280},
  urldate = {2024-10-04},
  abstract = {The Konstanz Information Miner is a modular environment, which enables easy visual assembly and interactive execution of a data pipeline. It is designed as a teaching, research and collaboration platform, which enables simple integration of new algorithms and tools as well as data manipulation or visualization methods in the form of new modules or nodes. In this paper we describe some of the design aspects of the underlying architecture, briey sketch how new nodes can be incorporated, and highlight some of the new features of version 2.0.},
  langid = {english},
  file = {/home/sam/Zotero/storage/B7D8GQ99/Berthold et al. - 2009 - KNIME - the Konstanz information miner version 2.0 and beyond.pdf}
}

@article{besseyFewBillionLines2010,
  title = {A Few Billion Lines of Code Later: Using Static Analysis to Find Bugs in the Real World},
  shorttitle = {A Few Billion Lines of Code Later},
  author = {Bessey, Al and Block, Ken and Chelf, Ben and Chou, Andy and Fulton, Bryan and Hallem, Seth and Henri-Gros, Charles and Kamsky, Asya and McPeak, Scott and Engler, Dawson},
  date = {2010-02-01},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {53},
  number = {2},
  pages = {66--75},
  issn = {0001-0782},
  doi = {10.1145/1646353.1646374},
  url = {https://dl.acm.org/doi/10.1145/1646353.1646374},
  urldate = {2023-11-01},
  abstract = {How Coverity built a bug-finding tool, and a business, around the unlimited supply of bugs in software systems.},
  keywords = {formal verification,industry practices,translational research},
  file = {/home/sam/Zotero/storage/E8LW8VK5/Bessey et al. - 2010 - A few billion lines of code later using static an.pdf}
}

@online{bestaBuildCloudDistributing2011,
  title = {Build in the {{Cloud}}: {{Distributing Build Outputs}}},
  author = {Besta, Milos and Miretskiy, Yevgeniy and Cox, Jeff},
  date = {2011-10-27},
  url = {https://google-engtools.blogspot.com/2011/10/build-in-cloud-distributing-build.html},
  organization = {Google Engineering Tools},
  keywords = {build systems,continuous integration,industry practices,software engineering},
  file = {/home/sam/Zotero/storage/ITFI4NMS/build-in-cloud-distributing-build.html}
}

@book{beuzenPythonPackages2022,
  title = {Python {{Packages}}},
  author = {Beuzen, Tomas and Timbers, Tiffany},
  date = {2022-04-21},
  edition = {1st edition},
  publisher = {{Chapman and Hall/CRC}},
  location = {Boca Raton},
  url = {https://py-pkgs.org/},
  abstract = {Python packages are a core element of the Python programming language and are how you create organized, reusable, and shareable code in Python. Python Packages is an open source book that describes modern and efficient workflows for creating Python packages.},
  isbn = {978-1-032-02944-3},
  langid = {english},
  pagetotal = {222},
  keywords = {industry practices,package versioning},
  annotation = {interest: 87}
}

@article{beyerApprenticingCustomer1995,
  title = {Apprenticing with the Customer},
  author = {Beyer, Hugh R. and Holtzblatt, Karen},
  date = {1995-05},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {38},
  number = {5},
  pages = {45--52},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/203356.203365},
  url = {https://dl.acm.org/doi/10.1145/203356.203365},
  urldate = {2022-06-07},
  langid = {english},
  keywords = {internship-project,requirements gathering,software engineering process}
}

@article{beyerReliableBenchmarkingRequirements2019,
  title = {Reliable Benchmarking: Requirements and Solutions},
  shorttitle = {Reliable Benchmarking},
  author = {Beyer, Dirk and Löwe, Stefan and Wendler, Philipp},
  date = {2019-02-06},
  journaltitle = {International Journal on Software Tools for Technology Transfer},
  shortjournal = {Int J Softw Tools Technol Transfer},
  volume = {21},
  number = {1},
  pages = {1--29},
  issn = {1433-2779, 1433-2787},
  doi = {10.1007/s10009-017-0469-y},
  url = {https://link.springer.com/10.1007/s10009-017-0469-y},
  urldate = {2022-06-30},
  abstract = {Benchmarking is a widely used method in experimental computer science, in particular, for the comparative evaluation of tools and algorithms. As a consequence, a number of questions need to be answered in order to ensure proper benchmarking, resource measurement, and presentation of results, all of which is essential for researchers, tool developers, and users, as well as for tool competitions. We identify a set of requirements that are indispensable for reliable benchmarking and resource measurement of time and memory usage of automatic solvers, verifiers, and similar tools, and discuss limitations of existing methods and benchmarking tools. Fulfilling these requirements in a benchmarking framework can (on Linux systems) currently only be done by using the cgroup and namespace features of the kernel. We developed BenchExec, a ready-to-use, tool-independent, and open-source implementation of a benchmarking framework that fulfills all presented requirements, making reliable benchmarking and resource measurement easy. Our framework is able to work with a wide range of different tools, has proven its reliability and usefulness in the International Competition on Software Verification, and is used by several research groups worldwide to ensure reliable benchmarking. Finally, we present guidelines on how to present measurement results in a scientifically valid and comprehensible way.},
  langid = {english},
  keywords = {metascience,project-provenance-pp,reproducibility engineering,software benchmarking},
  annotation = {interest: 90},
  file = {/home/sam/Zotero/storage/5XXRH3IC/Beyer2019_Article_ReliableBenchmarkingRequiremen.pdf;/home/sam/Zotero/storage/XX3G42I4/Current_ReliableBenchmarking.pdf}
}

@online{bilderDOIlikeStringsFake2016,
  type = {website},
  title = {{{DOI-like}} Strings and Fake {{DOIs}}},
  author = {Bilder, Geoffrey},
  date = {2016-06-29},
  url = {https://www.crossref.org/blog/doi-like-strings-and-fake-dois/},
  urldate = {2023-06-16},
  abstract = {TL;DR Crossref discourages our members from using DOI-like strings or fake DOIs. Details Recently we have seen quite a bit of debate around the use of so-called “fake-DOIs.” We have also been quoted as saying that we discourage the use of “fake DOIs” or “DOI-like strings”. This post outlines some of the cases in which we’ve seen fake DOIs used and why we recommend against doing so. Using DOI-like strings as internal identifiers Some of our members use DOI-like strings as internal identifiers for their manuscript tracking systems.},
  langid = {english},
  organization = {Crossref},
  keywords = {library science,software archiving},
  file = {/home/sam/Zotero/storage/UK726PDF/doi-like-strings-and-fake-dois.html}
}

@online{bilderStructureORCIDIdentifiers2012,
  title = {Structure of {{ORCID Identifiers V7}}},
  author = {Bilder, Geoffrey},
  date = {2012-08-02},
  url = {https://docs.google.com/document/d/1awd6PPguRAdZsC6CKpFSSSu1dulliT8E3kHwIJ3tD5o/edit?usp=embed_facebook},
  urldate = {2023-05-30},
  langid = {english},
  organization = {Google Docs},
  keywords = {semantic web},
  file = {/home/sam/Zotero/storage/QQ36LA7H/edit.html}
}

@article{billahUsingDataGrid2016,
  title = {Using a Data Grid to Automate Data Preparation Pipelines Required for Regional-Scale Hydrologic Modeling},
  author = {Billah, Mirza M. and Goodall, Jonathan L. and Narayan, Ujjwal and Essawy, Bakinam T. and Lakshmi, Venkat and Rajasekar, Arcot and Moore, Reagan W.},
  date = {2016-04-01},
  journaltitle = {Environmental Modelling \& Software},
  shortjournal = {Environmental Modelling \& Software},
  volume = {78},
  pages = {31--39},
  issn = {1364-8152},
  doi = {10.1016/j.envsoft.2015.12.010},
  url = {https://www.sciencedirect.com/science/article/pii/S1364815215301249},
  urldate = {2024-02-05},
  abstract = {Modeling a regional-scale hydrologic system introduces major data challenges related to the access and transformation of heterogeneous datasets into the information needed to execute a hydrologic model. These data preparation activities are difficult to automate, making the reproducibility and extensibility of model simulations conducted by others difficult or even impossible. This study addresses this challenge by demonstrating how the integrated Rule Oriented Data Management System (iRODS) can be used to support data processing pipelines needed when using data-intensive models to simulate regional-scale hydrologic systems. Focusing on the Variable Infiltration Capacity (VIC) model as a case study, data preparation steps are sequenced using rules within iRODS. VIC and iRODS are applied to study hydrologic conditions in the Carolinas, USA during the period 1998–2007 to better understand impacts of drought within the region. The application demonstrates how iRODS can support hydrologic modelers to create more reproducible and extensible model-based analyses.},
  keywords = {hydrology,project-provenance-pp},
  file = {/home/sam/Zotero/storage/LBM32NEM/S1364815215301249.html}
}

@incollection{blosteinIssuesPracticalUse1996,
  title = {Issues in the Practical Use of Graph Rewriting},
  booktitle = {Graph {{Grammars}} and {{Their Application}} to {{Computer Science}}},
  author = {Blostein, Dorothea and Fahmy, Hoda and Grbavec, Ann},
  editor = {Cuny, Janice and Ehrig, Hartmut and Engels, Gregor and Rozenberg, Grzegorz},
  editora = {Goos, Gerhard and Hartmanis, Juris and family=Leeuwen, given=Jan, prefix=van, useprefix=true},
  editoratype = {redactor},
  date = {1996},
  volume = {1073},
  pages = {38--55},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/3-540-61228-9_78},
  url = {http://link.springer.com/10.1007/3-540-61228-9_78},
  urldate = {2022-08-03},
  isbn = {978-3-540-61228-5 978-3-540-68388-9},
  keywords = {graph rewriting,programming languages}
}

@article{bohannonWhoAfraidPeer2013,
  title = {Who's {{Afraid}} of {{Peer Review}}?},
  author = {Bohannon, John},
  date = {2013-10-04},
  journaltitle = {Science},
  shortjournal = {Science},
  volume = {342},
  number = {6154},
  pages = {60--65},
  publisher = {American Association for the Advancement of Science},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.342.6154.60},
  url = {https://www.science.org/doi/10.1126/science.342.6154.60},
  urldate = {2022-07-07},
  abstract = {A spoof paper concocted by                Science                reveals little or no scrutiny at many open-access journals.                          ,                               Dozens of open-access journals targeted in an elaborate                Science                sting accepted a spoof research article, raising questions about peer-review practices in much of the open-access world.},
  langid = {english},
  keywords = {academic publishing,metascience,predatory journals},
  file = {/home/sam/Zotero/storage/5YJDU9QD/Bohannon - 2013 - Who's Afraid of Peer Review.pdf}
}

@inproceedings{bontchevAreGoodComputer1994,
  title = {Are '{{Good}}' {{Computer Viruses Still}} a {{Bad Idea}}?},
  booktitle = {Proc. {{EICAR}}’94 {{Conf}}.},
  author = {Bontchev, Vesselin},
  date = {1994},
  pages = {25--47},
  url = {https://cryptohub.nl/zines/vxheavens/lib/avb02.html},
  urldate = {2023-10-12},
  eventtitle = {{{EICAR}}},
  file = {/home/sam/Zotero/storage/JRK634JQ/avb02.html}
}

@article{borgmanWhoGotData2012,
  title = {Who’s {{Got}} the {{Data}}? {{Interdependencies}} in {{Science}} and {{Technology Collaborations}}},
  shorttitle = {Who’s {{Got}} the {{Data}}?},
  author = {Borgman, Christine L. and Wallis, Jillian C. and Mayernik, Matthew S.},
  date = {2012-12-01},
  journaltitle = {Computer Supported Cooperative Work (CSCW)},
  shortjournal = {Comput Supported Coop Work},
  volume = {21},
  number = {6},
  pages = {485--523},
  issn = {1573-7551},
  doi = {10.1007/s10606-012-9169-z},
  url = {https://doi.org/10.1007/s10606-012-9169-z},
  urldate = {2022-08-25},
  abstract = {Science and technology always have been interdependent, but never more so than with today’s highly instrumented data collection practices. We report on a long-term study of collaboration between environmental scientists (biology, ecology, marine sciences), computer scientists, and engineering research teams as part of a five-university distributed science and technology research center devoted to embedded networked sensing. The science and technology teams go into the field with mutual interests in gathering scientific data. “Data” are constituted very differently between the research teams. What are data to the science teams may be context to the technology teams, and vice versa. Interdependencies between the teams determine the ability to collect, use, and manage data in both the short and long terms. Four types of data were identified, which are managed separately, limiting both reusability of data and replication of research. Decisions on what data to curate, for whom, for what purposes, and for how long, should consider the interdependencies between scientific and technical processes, the complexities of data collection, and the disposition of the resulting data.},
  langid = {english},
  keywords = {research software engineering},
  annotation = {interest: 80},
  file = {/home/sam/Zotero/storage/CCRWG8VX/Borgman et al. - 2012 - Who’s Got the Data Interdependencies in Science a.pdf}
}

@book{bourqueSWEBOKGuideSoftware2014,
  title = {{{SWEBOK}}: Guide to the Software Engineering Body of Knowledge},
  shorttitle = {{{SWEBOK}}},
  author = {Bourque, Pierre and Fairley, R. E and {IEEE Computer Society}},
  date = {2014},
  isbn = {978-0-7695-5166-1},
  langid = {english},
  keywords = {industry practices,internship-project,software engineering},
  annotation = {OCLC: 880350861},
  file = {/home/sam/Zotero/storage/BT2DJVCF/swebok-v3.pdf}
}

@online{BPFDocumentation,
  title = {{{BPF Documentation}}},
  url = {https://docs.kernel.org/bpf/index.html},
  urldate = {2023-08-24},
  organization = {The Linux Kernel documentation},
  keywords = {operating systems,project-provenance-pp},
  file = {/home/sam/Zotero/storage/G8CJH67B/index.html}
}

@incollection{braunIssuesAutomaticProvenance2006,
  title = {Issues in {{Automatic Provenance Collection}}},
  booktitle = {Provenance and {{Annotation}} of {{Data}}},
  author = {Braun, Uri and Garfinkel, Simson and Holland, David A. and Muniswamy-Reddy, Kiran-Kumar and Seltzer, Margo I.},
  editor = {Moreau, Luc and Foster, Ian},
  editora = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard},
  editoratype = {redactor},
  date = {2006},
  volume = {4145},
  pages = {171--183},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/11890850_18},
  url = {http://link.springer.com/10.1007/11890850_18},
  urldate = {2023-08-23},
  abstract = {Automatic provenance collection describes systems that observe processes and data transformations inferring, collecting, and maintaining provenance about them. Automatic collection is a powerful tool for analysis of objects and processes, providing a level of transparency and pervasiveness not found in more conventional provenance systems. Unfortunately, automatic collection is also difficult. We discuss the challenges we encountered and the issues we exposed as we developed an automatic provenance collector that runs at the operating system level.},
  isbn = {978-3-540-46302-3 978-3-540-46303-0},
  langid = {english},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/KXYB7A6B/Braun et al. - 2006 - Issues in Automatic Provenance Collection.pdf}
}

@article{brinckmanComputingEnvironmentsReproducibility2019,
  title = {Computing Environments for Reproducibility: {{Capturing}} the “{{Whole Tale}}”},
  shorttitle = {Computing Environments for Reproducibility},
  author = {Brinckman, Adam and Chard, Kyle and Gaffney, Niall and Hategan, Mihael and Jones, Matthew B. and Kowalik, Kacper and Kulasekaran, Sivakumar and Ludäscher, Bertram and Mecum, Bryce D. and Nabrzyski, Jarek and Stodden, Victoria and Taylor, Ian J. and Turk, Matthew J. and Turner, Kandace},
  date = {2019-05},
  journaltitle = {Future Generation Computer Systems},
  shortjournal = {Future Generation Computer Systems},
  volume = {94},
  pages = {854--867},
  issn = {0167739X},
  doi = {10.1016/j.future.2017.12.029},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167739X17310695},
  urldate = {2024-09-04},
  langid = {english},
  file = {/home/sam/Zotero/storage/XY5DNI9L/Brinckman et al. - 2019 - Computing environments for reproducibility Captur.pdf}
}

@article{brinckmanComputingEnvironmentsReproducibility2019a,
  title = {Computing Environments for Reproducibility: {{Capturing}} the “{{Whole Tale}}”},
  shorttitle = {Computing Environments for Reproducibility},
  author = {Brinckman, Adam and Chard, Kyle and Gaffney, Niall and Hategan, Mihael and Jones, Matthew B. and Kowalik, Kacper and Kulasekaran, Sivakumar and Ludäscher, Bertram and Mecum, Bryce D. and Nabrzyski, Jarek and Stodden, Victoria and Taylor, Ian J. and Turk, Matthew J. and Turner, Kandace},
  date = {2019-05},
  journaltitle = {Future Generation Computer Systems},
  shortjournal = {Future Generation Computer Systems},
  volume = {94},
  pages = {854--867},
  issn = {0167739X},
  doi = {10.1016/j.future.2017.12.029},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167739X17310695},
  urldate = {2024-10-04},
  langid = {english},
  file = {/home/sam/Zotero/storage/4B77FRQC/Brinckman et al. - 2019 - Computing environments for reproducibility Capturing the “Whole Tale”.pdf}
}

@article{bromanDataOrganizationSpreadsheets2018,
  title = {Data {{Organization}} in {{Spreadsheets}}},
  author = {Broman, Karl W. and Woo, Kara H.},
  date = {2018-01-02},
  journaltitle = {The American Statistician},
  volume = {72},
  number = {1},
  pages = {2--10},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2017.1375989},
  url = {https://doi.org/10.1080/00031305.2017.1375989},
  urldate = {2022-08-25},
  abstract = {Spreadsheets are widely used software tools for data entry, storage, analysis, and visualization. Focusing on the data entry and storage aspects, this article offers practical recommendations for organizing spreadsheet data to reduce errors and ease later analyses. The basic principles are: be consistent, write dates like YYYY-MM-DD, do not leave any cells empty, put just one thing in a cell, organize the data as a single rectangle (with subjects as rows and variables as columns, and with a single header row), create a data dictionary, do not include calculations in the raw data files, do not use font color or highlighting as data, choose good names for things, make backups, use data validation to avoid data entry errors, and save the data in plain text files.},
  keywords = {research software engineering},
  file = {/home/sam/Zotero/storage/ISFLLCJB/Broman and Woo - 2018 - Data Organization in Spreadsheets.pdf}
}

@incollection{brookeSUSQuickDirty1996,
  title = {{{SUS}}: {{A}} '{{Quick}} and {{Dirty}}' {{Usability Scale}}},
  shorttitle = {{{SUS}}},
  booktitle = {Usability {{Evaluation In Industry}}},
  author = {Brooke, John},
  editor = {Jordan, Patrick W. and Thomas, B. and McClelland, Ian Lyall and Weerdmeester, Bernard},
  date = {1996-06-11},
  edition = {1},
  pages = {207--212},
  publisher = {CRC Press},
  doi = {10.1201/9781498710411-35},
  url = {https://www.taylorfrancis.com/books/9781498710411/chapters/10.1201/9781498710411-35},
  urldate = {2022-06-01},
  abstract = {Usability is not a quality that exists in any real or absolute sense. Perhaps it can be best summed up as being a general quality of the appropriateness to a purpose of any particular artefact. This notion is neatly summed up by Terry Pratchett in his novel Moving Pictures: In just the same way, the usability of any tool or system has to be viewed in terms of the context in which it is used, and its appropriateness to that context. With particular reference to information systems, this view of usability is reflected in the current draft international standard ISO 9241-11 and in the European Community ESPRIT project MUSiC (Measuring Usability of Systems in Context) (e.g. Bevan et al., 1991). In general, it is impossible to specify the usability of a system (i.e. its fitness for purpose) without first defining who are the intended users of the system, the tasks those users will perform with it, and the characteristics of the physical, organizational and social environment in which it will be used.},
  isbn = {978-0-429-15701-1},
  langid = {english},
  keywords = {human computer interaction,internship-project}
}

@incollection{brownCaseStudyUse2007,
  title = {A {{Case Study}} on the {{Use}} of {{Workflow Technologies}} for {{Scientific Analysis}}: {{Gravitational Wave Data Analysis}}},
  shorttitle = {A {{Case Study}} on the {{Use}} of {{Workflow Technologies}} for {{Scientific Analysis}}},
  booktitle = {Workflows for E-{{Science}}: {{Scientific Workflows}} for {{Grids}}},
  author = {Brown, Duncan A. and Brady, Patrick R. and Dietz, Alexander and Cao, Junwei and Johnson, Ben and McNabb, John},
  editor = {Taylor, Ian J. and Deelman, Ewa and Gannon, Dennis B. and Shields, Matthew},
  date = {2007},
  pages = {39--59},
  publisher = {Springer},
  location = {London},
  doi = {10.1007/978-1-84628-757-2_4},
  url = {https://doi.org/10.1007/978-1-84628-757-2_4},
  urldate = {2023-01-31},
  abstract = {Modern scientific experiments acquire large amounts of data that must be analyzed in subtle and complicated ways to extract the best results. The Laser Interferometer Gravitational Wave Observatory (LIGO) is an ambitious effort to detect gravitational waves produced by violent events in the universe, such as the collision of two black holes or the explosion of supernovae [37,258]. The experiment records approximately 1 TB of data per day, which is analyzed by scientists in a collaboration that spans four continents. LIGO and distributed computing have grown up side by side over the past decade, and the analysis strategies adopted by LIGO scientists have been strongly influenced by the increasing power of tools to manage distributed computing resources and the workflows to run on them. In this chapter, we use LIGO as an application case study in workflow design and implementation. The software architecture outlined here has been used with great efficacy to analyze LIGO data [2–5] using dedicated computing facilities operated by the LIGO Scientific Collaboration, the LIGO Data Grid. It is just the first step, however. Workflow design and implementation lies at the interface between computing and traditional scientific activities. In the conclusion, we outline a few directions for future development and provide some long-term vision for applications related to gravitational wave data analysis.},
  isbn = {978-1-84628-757-2},
  langid = {english},
  keywords = {project-acm-rep,workflow managers}
}

@article{bryanEnzoAdaptiveMesh2014,
  title = {Enzo: {{An Adaptive Mesh Refinement Code}} for {{Astrophysics}}},
  shorttitle = {{{ENZO}}},
  author = {Bryan, Greg L. and Norman, Michael L. and O'Shea, Brian W. and Abel, Tom and Wise, John H. and Turk, Matthew J. and Reynolds, Daniel R. and Collins, David C. and Wang, Peng and Skillman, Samuel W. and Smith, Britton and Harkness, Robert P. and Bordner, James and Kim, Ji-hoon and Kuhlen, Michael and Xu, Hao and Goldbaum, Nathan and Hummels, Cameron and Kritsuk, Alexei G. and Tasker, Elizabeth and Skory, Stephen and Simpson, Christine M. and Hahn, Oliver and Oishi, Jeffrey S. and So, Geoffrey C. and Zhao, Fen and Cen, Renyue and {and}, Yuan Li},
  date = {2014-03},
  journaltitle = {The Astrophysical Journal Supplement Series},
  shortjournal = {ApJS},
  volume = {211},
  number = {2},
  pages = {19},
  publisher = {American Astronomical Society},
  issn = {0067-0049},
  doi = {10.1088/0067-0049/211/2/19},
  url = {https://doi.org/10.1088/0067-0049/211/2/19},
  urldate = {2022-04-11},
  abstract = {This paper describes the open-source code Enzo, which uses block-structured adaptive mesh refinement to provide high spatial and temporal resolution for modeling astrophysical fluid flows. The code is Cartesian, can be run in one, two, and three dimensions, and supports a wide variety of physics including hydrodynamics, ideal and non-ideal magnetohydrodynamics, N-body dynamics (and, more broadly, self-gravity of fluids and particles), primordial gas chemistry, optically thin radiative cooling of primordial and metal-enriched plasmas (as well as some optically-thick cooling models), radiation transport, cosmological expansion, and models for star formation and feedback in a cosmological context. In addition to explaining the algorithms implemented, we present solutions for a wide range of test problems, demonstrate the code's parallel performance, and discuss the Enzo collaboration's code development methodology.},
  langid = {english},
  keywords = {astrophysics,cosmological simulation,numerical methods,project-astrophysics},
  file = {/home/sam/Zotero/storage/N2WMUNUQ/Bryan et al. - 2014 - ENZO AN ADAPTIVE MESH REFINEMENT CODE FOR ASTROPH.pdf}
}

@article{bryanPiecewiseParabolicMethod1995,
  title = {A Piecewise Parabolic Method for Cosmological Hydrodynamics},
  author = {Bryan, Greg L. and Norman, Michael L. and Stone, James M. and Cen, Renyue and Ostriker, Jeremiah P.},
  date = {1995-08-01},
  journaltitle = {Computer Physics Communications},
  volume = {89},
  pages = {149--168},
  issn = {0010-4655},
  doi = {10.1016/0010-4655(94)00191-4},
  url = {https://ui.adsabs.harvard.edu/abs/1995CoPhC..89..149B},
  urldate = {2022-04-11},
  abstract = {We describe a hybrid scheme for cosmological simulations that incorporates a Lagrangean particle-mesh (PM) algorithm to follow the collisionless matter with the higher order accurate piecewise parabolic method (PPM) to solve the equations of gas dynamics. Both components interact through the gravitational potential, which requires the solution of Poisson's equation, here done by Fourier transforms. Due to the vast range of conditions that occur in cosmological flows (pressure differences of up to fourteen orders of magnitude), a number of additions and modifications to PPM were required to produce accurate results. These are described, as are a suite of cosmological tests.},
  keywords = {astrophysics,computational fluid dynamics,numerical methods,project-astrophysics},
  annotation = {ADS Bibcode: 1995CoPhC..89..149B},
  file = {/home/sam/Zotero/storage/6PAJFSAI/1-s2.0-0010465594001914-main.pdf}
}

@incollection{buckheitWaveLabReproducibleResearch1995,
  title = {{{WaveLab}} and {{Reproducible Research}}},
  booktitle = {Wavelets and {{Statistics}}},
  author = {Buckheit, Jonathan B. and Donoho, David L.},
  editor = {Antoniadis, Anestis and Oppenheim, Georges},
  editora = {Bickel, P. and Diggle, P. and Fienberg, S. and Krickeberg, K. and Olkin, I. and Wermuth, N. and Zeger, S.},
  editoratype = {redactor},
  date = {1995},
  volume = {103},
  pages = {55--81},
  publisher = {Springer New York},
  location = {New York, NY},
  doi = {10.1007/978-1-4612-2544-7_5},
  url = {http://link.springer.com/10.1007/978-1-4612-2544-7_5},
  urldate = {2023-01-19},
  abstract = {WaveLab is a library of Matlab routines for wavelet analysis, wavelet-packet analysis, cosine-packet analysis and matching pursuit. The library is available free of charge over the Internet. Versions are provided for Macintosh, UNIX and Windows machines.},
  isbn = {978-0-387-94564-4 978-1-4612-2544-7},
  langid = {english},
  keywords = {reproducibility engineering},
  file = {/home/sam/Zotero/storage/X9BUVX63/Buckheit and Donoho - 1995 - WaveLab and Reproducible Research.pdf}
}

@online{bugayenkoAcademicWritingLATEX,
  title = {Academic {{Writing}} in {{LATEX}}: {{Best}} and {{Worst Practices}}},
  author = {Bugayenko, Yegor},
  abstract = {This is a humble attempt to summarize most typical mistakes we make while writing academic papers in LATEX and most important recommendations. Each suggestion or a mistake takes a short paragraph of description right here and also may suggest looking into a more detailed explanation in some other online resource. We recommend, before submitting your paper to a conference or a journal, go through this list of mistakes and make sure none of them are present in your paper},
  langid = {english},
  keywords = {academic writing},
  file = {/home/sam/Zotero/storage/LT6PX3GV/Bugayenko - Academic Writing in LATEX Best and Worst Practice.pdf}
}

@article{buranyiHitechWarScience2017,
  entrysubtype = {newspaper},
  title = {The Hi-Tech War on Science Fraud},
  author = {Buranyi, Stephen},
  date = {2017-02-01T06:00:08},
  journaltitle = {The Guardian},
  issn = {0261-3077},
  url = {https://www.theguardian.com/science/2017/feb/01/high-tech-war-on-science},
  urldate = {2022-08-30},
  abstract = {The Long Read: The problem of fake data may go far deeper than scientists admit. Now a team of researchers has a controversial plan to root out the perpetrators},
  journalsubtitle = {Science},
  langid = {british},
  keywords = {metascience,scientific misconduct},
  file = {/home/sam/Zotero/storage/NAEVYPWG/high-tech-war-on-science.html}
}

@article{buranyiStaggeringlyProfitableBusiness2017,
  entrysubtype = {newspaper},
  title = {Is the Staggeringly Profitable Business of Scientific Publishing Bad for Science?},
  author = {Buranyi, Stephen},
  date = {2017-06-27T05:00:21},
  journaltitle = {The Guardian},
  issn = {0261-3077},
  url = {https://www.theguardian.com/science/2017/jun/27/profitable-business-scientific-publishing-bad-for-science},
  urldate = {2022-08-30},
  abstract = {The long read: It is an industry like no other, with profit margins to rival Google – and it was created by one of Britain’s most notorious tycoons: Robert Maxwell},
  journalsubtitle = {Science},
  langid = {british},
  keywords = {academic publishing},
  file = {/home/sam/Zotero/storage/W2F7C7QG/profitable-business-scientific-publishing-bad-for-science.html}
}

@inproceedings{burkatServerlessContainersRising2021,
  title = {Serverless {{Containers}} – {{Rising Viable Approach}} to {{Scientific Workflows}}},
  booktitle = {2021 {{IEEE}} 17th {{International Conference}} on {{eScience}} ({{eScience}})},
  author = {Burkat, Krzysztof and Pawlik, Maciej and Balis, Bartosz and Malawski, Maciej and Vahi, Karan and Rynge, Mats and Ferreira da Silva, Rafael and Deelman, Ewa},
  date = {2021-09},
  pages = {40--49},
  doi = {10.1109/eScience51609.2021.00014},
  abstract = {The increasing popularity of the serverless computing approach has led to the emergence of new cloud infrastructures working in Container-as-a-Service (CaaS) model like AWS Fargate, Google Cloud Run, or Azure Container Instances. New infrastructures facilitate an innovative approach to running cloud containers where developers are freed from managing underlying resources. In this paper, we focus on evaluating the capabilities of elastic containers and their usefulness for scientific computing in the scientific workflow paradigm using AWS Fargate and Google Cloud Run infrastructures. For the experimental evaluation of our approach, we extended the HyperFlow engine to support these CaaS platforms, together with adapting four scientific workflows composed of several dozen to hundreds of tasks organized into a dependency graph. Studied applications are used to create cost-performance benchmarks and flow execution plots, delay, elasticity, and scalability measurements. Results show that serverless containers can be successfully utilized for running scientific workflows. Moreover, the results allow for gaining insight into the specific advantages and limits of the studied platforms.},
  eventtitle = {2021 {{IEEE}} 17th {{International Conference}} on {{eScience}} ({{eScience}})},
  keywords = {cloud computing,workflow managers},
  annotation = {interest: 90},
  file = {/home/sam/Zotero/storage/CI7Y856A/Burkat et al. - 2021 - Serverless Containers – Rising Viable Approach to .pdf}
}

@inproceedings{BURRITOWrappingYour2012,
  title = {\{\vphantom\}{{BURRITO}}\vphantom\{\}: {{Wrapping Your Lab Notebook}} in {{Computational Infrastructure}}},
  shorttitle = {\{\vphantom\}{{BURRITO}}\vphantom\{\}},
  date = {2012},
  url = {https://www.usenix.org/conference/tapp12/workshop-program/presentation/guo},
  urldate = {2023-07-07},
  eventtitle = {4th {{USENIX Workshop}} on the {{Theory}} and {{Practice}} of {{Provenance}} ({{TaPP}} 12)},
  langid = {english},
  keywords = {provenance},
  annotation = {interest: 98},
  file = {/home/sam/Zotero/storage/6FPPDENV/2012 - BURRITO Wrapping Your Lab Notebook in Computati.pdf}
}

@inproceedings{burtonWorkloadCharacterizationUsing1998,
  title = {Workload Characterization Using Lightweight System Call Tracing and Reexecution},
  booktitle = {1998 {{IEEE International Performance}}, {{Computing}} and {{Communications Conference}}. {{Proceedings}} ({{Cat}}. {{No}}.{{98CH36191}})},
  author = {Burton, A.N. and Kelly, P.H.J.},
  date = {1998-02},
  pages = {260--266},
  issn = {1097-2641},
  doi = {10.1109/PCCC.1998.659975},
  url = {https://ieeexplore.ieee.org/abstract/document/659975?casa_token=HKVbz10sLZIAAAAA:smrnozHPF8pQo71mmNw8svhVLFy0aaKiIpNKcdiy9BFIju5IIw0KjE0be84S9TrdIgB9BdK8},
  urldate = {2024-02-13},
  abstract = {This paper shows how system call traces can be obtained with minimal interference to the system being characterized, and used as realistic, repeatable workloads for experiments to evaluate operating system and file system designs and configuration alternatives. Our system call trace mechanism, called ULTra, captures a complete trace of each UNIX process's calls to the operating system. The performance impact is normally small, and it runs in user mode without special privileges. We show how the resulting traces can be used to drive full, repeatable reexecution of the captured behaviour, and present a case study which shows the usefulness and accuracy of the tool for predicting the impact of file system caching on a WWW server's performance.},
  eventtitle = {1998 {{IEEE International Performance}}, {{Computing}} and {{Communications Conference}}. {{Proceedings}} ({{Cat}}. {{No}}.{{98CH36191}})},
  keywords = {operating systems,project-provenance-pp},
  file = {/home/sam/Zotero/storage/VNTWDTX7/Burton and Kelly - 1998 - Workload characterization using lightweight system.pdf}
}

@article{butlerInvestigatingJournalsDark2013,
  title = {Investigating Journals: {{The}} Dark Side of Publishing},
  shorttitle = {Investigating Journals},
  author = {Butler, Declan},
  date = {2013-03-01},
  journaltitle = {Nature},
  volume = {495},
  number = {7442},
  pages = {433--435},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/495433a},
  url = {https://www.nature.com/articles/495433a},
  urldate = {2022-08-30},
  abstract = {The explosion in open-access publishing has fuelled the rise of questionable operators.},
  issue = {7442},
  langid = {english},
  keywords = {academic publishing,metascience,predatory journals},
  file = {/home/sam/Zotero/storage/ERJKZMUE/Butler - 2013 - Investigating journals The dark side of publishin.pdf;/home/sam/Zotero/storage/LBFKA9SG/495433a.html}
}

@inproceedings{buttProvONEProvenanceModel2020,
  title = {{{ProvONE}}+: {{A Provenance Model}} for {{Scientific Workflows}}},
  shorttitle = {{{ProvONE}}+},
  booktitle = {Web {{Information Systems Engineering}} – {{WISE}} 2020},
  author = {Butt, Anila Sahar and Fitch, Peter},
  editor = {Huang, Zhisheng and Beek, Wouter and Wang, Hua and Zhou, Rui and Zhang, Yanchun},
  date = {2020},
  series = {Web {{Information Systems Engineering}} – {{WISE}} 2020},
  pages = {431--444},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-62008-0_30},
  abstract = {The provenance of workflows is essential, both for the data they derive and for their specification, to allow for the reproducibility, sharing and reuse of information in the scientific community. Although the formal modelling of scientific workflow provenance was of interest and studied, in many fields like semantic web, yet no provenance model has existed, we are aware of, to model control-flow driven scientific workflows. The provenance models proposed by the semantic web community for data-driven scientific workflows may capture the provenance of control-flow driven workflows execution traces (i.e., retrospective provenance) but underspecify the workflow structure (i.e., workflow provenance). An underspecified or incomplete structure of a workflow results in the misinterpretation of a scientific experiment and precludes conformance checking of the workflow, thereby restricting the gains of provenance. To overcome the limitation, we present a formal, lightweight and general-purpose specification model for the control-flows involved scientific workflows. The proposed model can be combined with the existing provenance models and easy to extend to specify the common control-flow patterns. In this article, we inspire the need for control-flow driven scientific workflow provenance model, provide an overview of its key classes and properties, and briefly discuss its integration with the ProvONE provenance model as well as its compatibility to PROV-DM. We will also focus on the sample modelling using the proposed model and present a comprehensive implementation scenario from the agricultural domain for validating the model.},
  isbn = {978-3-030-62008-0},
  langid = {english},
  keywords = {project-acm-rep,project-provenance-pp,provenance,semantic web},
  file = {/home/sam/Zotero/storage/XIV45RJ9/Butt and Fitch - 2020 - ProvONE+ A Provenance Model for Scientific Workfl.pdf}
}

@inproceedings{bzeznikNixHPCPackage2017,
  title = {Nix as {{HPC}} Package Management System},
  booktitle = {Proceedings of the {{Fourth International Workshop}} on {{HPC User Support Tools}}},
  author = {Bzeznik, Bruno and Henriot, Oliver and Reis, Valentin and Richard, Olivier and Tavard, Laure},
  date = {2017-11-12},
  series = {{{HUST}}'17},
  pages = {1--6},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3152493.3152556},
  url = {https://dl.acm.org/doi/10.1145/3152493.3152556},
  urldate = {2023-05-05},
  abstract = {Modern High Performance Computing systems are becoming larger and more heterogeneous. The proper management of software for the users of such systems poses a significant challenge. These users run very diverse applications that may be compiled with proprietary tools for specialized hardware. Moreover, the application life-cycle of these software may exceed the lifetime of the HPC systems themselves. These difficulties motivate the use of specialized package management systems. In this paper, we outline an approach to HPC package development, deployment, management, sharing, and reuse based on the Nix functional package manager. We report our experience with this approach inside the GRICAD HPC center[GRICAD 2017a] in Grenoble over a 12 month period and compare it to other existing approaches.},
  isbn = {978-1-4503-5130-0},
  keywords = {project-acm-rep,project-provenance-pp,research software engineering},
  annotation = {interest: 99},
  file = {/home/sam/Zotero/storage/Z788J5GN/Bzeznik et al. - 2017 - Nix as HPC package management system.pdf}
}

@report{caralliIntroducingOCTAVEAllegro2007,
  type = {Technical Report},
  title = {Introducing {{OCTAVE Allegro}}: {{Improving}} the {{Information Security Risk Assessment Process}}},
  author = {Caralli, Richard A. and Stevens, James F. and Young, Lisa R. and Wilson, William R.},
  date = {2007-05-01},
  number = {CMU/SEI-2007-TR-012},
  pages = {154},
  institution = {Carnegie Mellon University},
  url = {https://apps.dtic.mil/sti/citations/ADA470450},
  abstract = {This technical report introduces the next generation of the Operationally Critical Threat, Asset, and Vulnerability Evaluation OCTAVE methodology, OCTAVE Allegro. OCTAVE Allegro is a methodology to streamline and optimize the process of assessing information security risks so that an organization can obtain sufficient results with a small investment in time, people, and other limited resources. It leads the organization to consider people, technology, and facilities in the context of their relationship to information and the business processes and services they support. This report highlights the design considerations and requirements for OCTAVE Allegro based on field experience with existing OCTAVE methods and provides guidance, worksheets, and examples that an organization can use to begin performing OCTAVE Allegro-based risk assessments.},
  keywords = {industry practices,security}
}

@article{carataPrimerProvenance2014,
  title = {A Primer on Provenance},
  author = {Carata, Lucian and Akoush, Sherif and Balakrishnan, Nikilesh and Bytheway, Thomas and Sohan, Ripduman and Seltzer, Margo and Hopper, Andy},
  date = {2014-05-01},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {57},
  number = {5},
  pages = {52--60},
  issn = {0001-0782},
  doi = {10.1145/2596628},
  url = {https://dl.acm.org/doi/10.1145/2596628},
  urldate = {2023-08-23},
  abstract = {Better understanding data requires tracking its history and context.},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/K2EIEAY9/Carata et al. - 2014 - A primer on provenance.pdf}
}

@article{carataPrimerProvenanceBetter2014,
  title = {A {{Primer}} on {{Provenance}}: {{Better}} Understanding of Data Requires Tracking Its History and Context.},
  shorttitle = {A {{Primer}} on {{Provenance}}},
  author = {Carata, Lucian and Akoush, Sherif and Balakrishnan, Nikilesh and Bytheway, Thomas and Sohan, Ripduman and Seltzer, Margo and Hopper, Andy},
  date = {2014-03-01},
  journaltitle = {Queue},
  shortjournal = {Queue},
  volume = {12},
  number = {3},
  pages = {10--23},
  issn = {1542-7730},
  doi = {10.1145/2602649.2602651},
  url = {https://dl.acm.org/doi/10.1145/2602649.2602651},
  urldate = {2023-07-18},
  abstract = {Assessing the quality or validity of a piece of data is not usually done in isolation. You typically examine the context in which the data appears and try to determine its original sources or review the process through which it was created. This is not so straightforward when dealing with digital data, however: the result of a computation might have been derived from numerous sources and by applying complex successive transformations, possibly over long periods of time.},
  file = {/home/sam/Zotero/storage/6CXZPWDR/Carata et al. - 2014 - A Primer on Provenance Better understanding of da.pdf}
}

@online{carloUnderstandLegacyCode,
  title = {Understand {{Legacy Code}}},
  author = {Carlo, Nicolas},
  url = {https://understandlegacycode.com},
  urldate = {2022-09-06},
  abstract = {We all have to deal with Legacy Code. But it's damn hard to! Here you'll find answers to your questions. I'm sharing useful tips and concrete advice that will help you tame the legacy codebase you've inherited.},
  langid = {english},
  organization = {Understanding Legacy Code},
  keywords = {industry practices,software engineering},
  annotation = {interest: 81},
  file = {/home/sam/Zotero/storage/FFM9K3Z7/understandlegacycode.com.html}
}

@online{carruthGoogleHereBe2011,
  title = {C++ at {{Google}}: {{Here Be Dragons}}},
  author = {Carruth, Chandler},
  date = {2011-05-23},
  url = {http://google-engtools.blogspot.com/2011/05/c-at-google-here-be-dragons.html},
  organization = {Google Engineering Tools},
  keywords = {industry practices,software engineering},
  file = {/home/sam/Zotero/storage/UKDBAIWS/c-at-google-here-be-dragons.html}
}

@incollection{cartaxoRapidReviewsSoftware2020,
  title = {Rapid {{Reviews}} in {{Software Engineering}}},
  booktitle = {Contemporary {{Empirical Methods}} in {{Software Engineering}}},
  author = {Cartaxo, Bruno and Pinto, Gustavo and Soares, Sergio},
  editor = {Felderer, Michael and Travassos, Guilherme Horta},
  date = {2020},
  pages = {357--384},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-32489-6_13},
  url = {https://doi.org/10.1007/978-3-030-32489-6_13},
  urldate = {2023-10-27},
  abstract = {Integrating research evidence into practice is one of the main goals of evidence-based software engineering (EBSE). Secondary studies, one of the main EBSE products, are intended to summarize the “best” research evidence and make them easily consumable by practitioners. However, recent studies show that some secondary studies lack connections with software engineering practice. In this chapter, we present the concept of Rapid Reviews, which are lightweight secondary studies focused on delivering evidence to practitioners in a timely manner. Rapid reviews support practitioners in their decision-making, and should be conducted bounded to a practical problem, inserted into a practical context. Thus, Rapid Reviews can be easily integrated in a knowledge/technology transfer initiative. After describing the basic concepts, we present the results and experiences of conducting two Rapid Reviews. We also provide guidelines to help researchers and practitioners who want to conduct Rapid Reviews, and we finally discuss topics that may concern the research community about the feasibility of Rapid Reviews as an evidence-based method. In conclusion, we believe Rapid Reviews might be of interest to researchers and practitioners working on the intersection of software engineering research and practice.},
  isbn = {978-3-030-32489-6},
  langid = {english},
  keywords = {project-provenance-pp,rapid reviews},
  file = {/home/sam/Zotero/storage/4XQIHKDN/Cartaxo et al. - 2020 - Rapid Reviews in Software Engineering.pdf}
}

@inproceedings{cartaxoRoleRapidReviews2018,
  title = {The {{Role}} of {{Rapid Reviews}} in {{Supporting Decision-Making}} in {{Software Engineering Practice}}},
  booktitle = {Proceedings of the 22nd {{International Conference}} on {{Evaluation}} and {{Assessment}} in {{Software Engineering}} 2018},
  author = {Cartaxo, Bruno and Pinto, Gustavo and Soares, Sergio},
  date = {2018-06-28},
  series = {{{EASE}} '18},
  pages = {24--34},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3210459.3210462},
  url = {https://dl.acm.org/doi/10.1145/3210459.3210462},
  urldate = {2023-10-26},
  abstract = {Context: Recent work on Evidence Based Software Engineering (EBSE) suggests that systematic reviews lack connection with Software Engineering (SE) practice. In Evidence Based Medicine there is a growing initiative to address this kind of problem, in particular through what has been named as Rapid Reviews (RRs). They are adaptations of regular systematic reviews made to fit practitioners constraints. Goal: Evaluate the perceptions from SE practitioners on the use of Rapid Reviews to support decision-making in SE practice. Method: We conducted an Action Research to evaluate RRs insertion in a real-world software development project. Results: Our results show that practitioners are rater positive about Rapid Reviews. They reported to have learned new concepts, reduced time and cost of decision-making, improved their understanding about the problem their facing, among other benefits. Additionally, two months after the introduction of the Rapid Review, in a follow up visit, we perceived that the practitioners have indeed adopted the evidence provided. Conclusions: Based on the positive results we obtained with this study, and the experiences reported in medicine, we believe RRs could play an important role towards knowledge transfer and decision-making support in SE practice.},
  isbn = {978-1-4503-6403-4},
  keywords = {project-provenance-pp,rapid reviews},
  file = {/home/sam/Zotero/storage/QDQC8QUF/Cartaxo et al. - 2018 - The Role of Rapid Reviews in Supporting Decision-M.pdf}
}

@article{carverSurveyStatePractice2022,
  title = {A Survey of the State of the Practice for Research Software in the {{United States}}},
  author = {Carver, Jeffrey C. and Weber, Nic and Ram, Karthik and Gesing, Sandra and Katz, Daniel S.},
  date = {2022-05-05},
  journaltitle = {PeerJ Computer Science},
  shortjournal = {PeerJ Comput. Sci.},
  volume = {8},
  pages = {e963},
  publisher = {PeerJ Inc.},
  issn = {2376-5992},
  doi = {10.7717/peerj-cs.963},
  url = {https://peerj.com/articles/cs-963},
  urldate = {2022-05-12},
  abstract = {Research software is a critical component of contemporary scholarship. Yet, most research software is developed and managed in ways that are at odds with its long-term sustainability. This paper presents findings from a survey of 1,149 researchers, primarily from the United States, about sustainability challenges they face in developing and using research software. Some of our key findings include a repeated need for more opportunities and time for developers of research software to receive training. These training needs cross the software lifecycle and various types of tools. We also identified the recurring need for better models of funding research software and for providing credit to those who develop the software so they can advance in their careers. The results of this survey will help inform future infrastructure and service support for software developers and users, as well as national research policy aimed at increasing the sustainability of research software.},
  langid = {english},
  keywords = {research software engineering},
  file = {/home/sam/Zotero/storage/SV2J3GL8/Carver et al. - 2022 - A survey of the state of the practice for research.pdf;/home/sam/Zotero/storage/7JS3GYVR/cs-963.html}
}

@article{casasPSODSSchedulingEngine2017,
  title = {{{PSO-DS}}: A Scheduling Engine for Scientific Workflow Managers},
  shorttitle = {{{PSO-DS}}},
  author = {Casas, Israel and Taheri, Javid and Ranjan, Rajiv and Zomaya, Albert Y.},
  date = {2017-09},
  journaltitle = {The Journal of Supercomputing},
  shortjournal = {J Supercomput},
  volume = {73},
  number = {9},
  pages = {3924--3947},
  issn = {0920-8542, 1573-0484},
  doi = {10.1007/s11227-017-1992-z},
  url = {http://link.springer.com/10.1007/s11227-017-1992-z},
  urldate = {2022-07-07},
  langid = {english}
}

@article{cashenImpactEconomicSanctions,
  entrysubtype = {newspaper},
  title = {The Impact of Economic Sanctions},
  author = {Cashen, Emily},
  url = {https://www.worldfinance.com/special-reports/the-impact-of-economic-sanctions},
  urldate = {2022-09-06},
  abstract = {Since the early 1990s, economic sanctions have emerged as a favoured foreign policy tool. With the US ramping up measures against North Korea and Russia, it seems sanctions are here to stay – despite their many flaws},
  langid = {american},
  keywords = {current events,geopolitics},
  annotation = {interest: 62},
  file = {/home/sam/Zotero/storage/REFLJXMR/the-impact-of-economic-sanctions.html}
}

@article{castagnaGradualTypingNew2019,
  title = {Gradual Typing: A New Perspective},
  shorttitle = {Gradual Typing},
  author = {Castagna, Giuseppe and Lanvin, Victor and Petrucciani, Tommaso and Siek, Jeremy G.},
  date = {2019-01-02},
  journaltitle = {Proceedings of the ACM on Programming Languages},
  shortjournal = {Proc. ACM Program. Lang.},
  volume = {3},
  pages = {16:1--16:32},
  doi = {10.1145/3290329},
  url = {https://doi.org/10.1145/3290329},
  urldate = {2022-09-06},
  abstract = {We define a new, more semantic interpretation of gradual types and use it to ``gradualize'' two forms of polymorphism: subtyping polymorphism and implicit parametric polymorphism. In particular, we use the new interpretation to define three gradual type systems ---Hindley-Milner, with subtyping, and with union and intersection types--- in terms of two preorders, subtyping and materialization. We define these systems both declaratively ---by adding two subsumption-like rules--- which yields clearer, more intelligible, and streamlined definitions, and algorithmically by reusing existing techniques such as unification and tallying.},
  issue = {POPL},
  keywords = {programming languages},
  annotation = {interest: 84},
  file = {/home/sam/Zotero/storage/CYRWWGIL/Castagna et al. - 2019 - Gradual typing a new perspective.pdf}
}

@software{cespedesLtrace,
  title = {Ltrace},
  author = {Cespedes, Juan},
  url = {http://ltrace.org/},
  abstract = {ltrace intercepts and records dynamic library calls which are called by an executed process and the signals received by that process. It can also intercept and print the system calls executed by the program.},
  keywords = {project-provenance-pp}
}

@online{chalmersAdventuresTechnophilosophyReality2022,
  title = {Adventures in {{Technophilosophy}}: {{On}} the {{Reality}} of {{Virtual Worlds}}},
  shorttitle = {Adventures in {{Technophilosophy}}},
  author = {Chalmers, David J.},
  date = {2022-01-28T09:50:51+00:00},
  url = {https://lithub.com/adventures-in-technophilosophy-on-the-reality-of-virtual-worlds/},
  urldate = {2022-09-06},
  abstract = {When I was ten years old, I discovered computers. My first machine was a PDP-\hspace{0pt}10 mainframe system at the medical center where my father worked. I taught myself to write simple programs in the BASIC…},
  langid = {american},
  organization = {Literary  Hub},
  keywords = {philosophy},
  annotation = {interest: 71},
  file = {/home/sam/Zotero/storage/ZPKSHI6J/adventures-in-technophilosophy-on-the-reality-of-virtual-worlds.html}
}

@inproceedings{chanExpressivenessBenchmarkingSystemLevel2017,
  title = {Expressiveness {{Benchmarking}} for \{\vphantom\}{{System-Level}}\vphantom\{\} {{Provenance}}},
  author = {Chan, Sheung Chi and Gehani, Ashish and Cheney, James and Sohan, Ripduman and Irshad, Hassaan},
  date = {2017},
  url = {https://www.usenix.org/conference/tapp17/workshop-program/presentation/chan},
  urldate = {2023-08-23},
  eventtitle = {9th {{USENIX Workshop}} on the {{Theory}} and {{Practice}} of {{Provenance}} ({{TaPP}} 2017)},
  langid = {english},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/YWH8KUWW/Chan et al. - 2017 - Expressiveness Benchmarking for System-Level Pro.pdf}
}

@article{changRetraction2006,
  title = {Retraction},
  author = {Chang, Geoffrey and Roth, Christopher B. and Reyes, Christopher L. and Pornillos, Owen and Chen, Yen-Ju and Chen, Andy P.},
  date = {2006-12-22},
  journaltitle = {Science},
  shortjournal = {Science},
  volume = {314},
  number = {5807},
  pages = {1875--1875},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.314.5807.1875b},
  url = {https://www.science.org/doi/10.1126/science.314.5807.1875b},
  urldate = {2022-05-26},
  langid = {english},
  keywords = {internship-project,research software engineering},
  file = {/home/sam/Zotero/storage/7KWRJANX/chang2006.pdf}
}

@inproceedings{chanProvMarkProvenanceExpressiveness2019a,
  title = {{{ProvMark}}: {{A Provenance Expressiveness Benchmarking System}}},
  shorttitle = {{{ProvMark}}},
  booktitle = {Proceedings of the 20th {{International Middleware Conference}}},
  author = {Chan, Sheung Chi and Cheney, James and Bhatotia, Pramod and Pasquier, Thomas and Gehani, Ashish and Irshad, Hassaan and Carata, Lucian and Seltzer, Margo},
  date = {2019-12-09},
  series = {Middleware '19},
  pages = {268--279},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3361525.3361552},
  url = {https://dl.acm.org/doi/10.1145/3361525.3361552},
  urldate = {2023-08-21},
  abstract = {System level provenance is of widespread interest for applications such as security enforcement and information protection. However, testing the correctness or completeness of provenance capture tools is challenging and currently done manually. In some cases there is not even a clear consensus about what behavior is correct. We present an automated tool, ProvMark, that uses an existing provenance system as a black box and reliably identifies the provenance graph structure recorded for a given activity, by a reduction to subgraph isomorphism problems handled by an external solver. ProvMark is a beginning step in the much needed area of testing and comparing the expressiveness of provenance systems. We demonstrate ProvMark's usefuless in comparing three capture systems with different architectures and distinct design philosophies.},
  isbn = {978-1-4503-7009-7},
  keywords = {project-provenance-pp,provenance},
  file = {/home/sam/Zotero/storage/Q4ITJ896/Chan et al. - 2019 - ProvMark A Provenance Expressiveness Benchmarking.pdf}
}

@book{chanslerArchitectureOpenSource,
  title = {The {{Architecture}} of {{Open Source Applications}}},
  author = {Chansler, Robert and Bryant, Russell and Bryant, Roy and Canino-Koening, Rosangela and Cesarini, Francesco and Allman, Eric and Bostic, Keith and Brown, Titus},
  editor = {Brown, Amy and Wilson, Greg},
  url = {http://aosabook.org/en/index.html},
  langid = {english},
  pagetotal = {434}
}

@inproceedings{chardApplicationBagItSerializedResearch2019,
  title = {Application of {{BagIt-Serialized Research Object Bundles}} for {{Packaging}} and {{Re-Execution}} of {{Computational Analyses}}},
  booktitle = {2019 15th {{International Conference}} on {{eScience}} ({{eScience}})},
  author = {Chard, Kyle and Gaffney, Niall and Jones, Matthew B. and Kowalik, Kacper and Ludäscher, Bertram and McPhillips, Timothy and Nabrzyski, Jarek and Stodden, Victoria and Taylor, Ian and Thelen, Thomas and Turk, Matthew J. and Willis, Craig},
  date = {2019-09},
  pages = {514--521},
  doi = {10.1109/eScience.2019.00068},
  abstract = {In this paper we describe our experience adopting the Research Object Bundle (RO-Bundle) format with BagIt serialization (BagIt-RO) for the design and implementation of "tales" in the Whole Tale platform. A tale is an executable research object intended for the dissemination of computational scientific findings that captures information needed to facilitate understanding, transparency, and re-execution for review and computational reproducibility at the time of publication. We describe the Whole Tale platform and requirements that led to our adoption of BagIt-RO, specifics of our implementation, and discuss migrating to the emerging Research Object Crate (RO-Crate) standard.},
  eventtitle = {2019 15th {{International Conference}} on {{eScience}} ({{eScience}})},
  keywords = {academic publishing,provenance,reproducibility,reproducibility engineering},
  annotation = {interest: 90},
  file = {/home/sam/Zotero/storage/2SZ6QBKW/Chard et al. - 2019 - Application of BagIt-Serialized Research Object Bu.pdf;/home/sam/Zotero/storage/MXGGTWY2/Chard et al. - 2019 - Application of BagIt-Serialized Research Object Bu.pdf;/home/sam/Zotero/storage/UMJMGTPX/9041738.html}
}

@inproceedings{chardFuncXFederatedFunction2020,
  title = {{{funcX}}: {{A Federated Function Serving Fabric}} for {{Science}}},
  shorttitle = {{{funcX}}},
  booktitle = {Proceedings of the 29th {{International Symposium}} on {{High-Performance Parallel}} and {{Distributed Computing}}},
  author = {Chard, Ryan and Babuji, Yadu and Li, Zhuozhao and Skluzacek, Tyler and Woodard, Anna and Blaiszik, Ben and Foster, Ian and Chard, Kyle},
  date = {2020-06-23},
  series = {{{HPDC}} '20},
  pages = {65--76},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3369583.3392683},
  url = {https://doi.org/10.1145/3369583.3392683},
  urldate = {2022-11-14},
  abstract = {Exploding data volumes and velocities, new computational methods and platforms, and ubiquitous connectivity demand new approaches to computation in the sciences. These new approaches must enable computation to be mobile, so that, for example, it can occur near data, be triggered by events (e.g., arrival of new data), be offloaded to specialized accelerators, or run remotely where resources are available. They also require new design approaches in which monolithic applications can be decomposed into smaller components, that may in turn be executed separately and on the most suitable resources. To address these needs we present funcX---a distributed function as a service (FaaS) platform that enables flexible, scalable, and high performance remote function execution. funcX's endpoint software can transform existing clouds, clusters, and supercomputers into function serving systems, while funcX's cloud-hosted service provides transparent, secure, and reliable function execution across a federated ecosystem of endpoints. We motivate the need for funcX with several scientific case studies, present our prototype design and implementation, show optimizations that deliver throughput in excess of 1 million functions per second, and demonstrate, via experiments on two supercomputers, that funcX can scale to more than more than 130 000 concurrent workers.},
  isbn = {978-1-4503-7052-3},
  keywords = {workflow managers},
  annotation = {interest: 95},
  file = {/home/sam/Zotero/storage/DW8LM2L3/Chard et al. - 2020 - funcX A Federated Function Serving Fabric for Sci.pdf}
}

@article{chazetteExplainabilityNonfunctionalRequirement2020,
  title = {Explainability as a Non-Functional Requirement: Challenges and Recommendations},
  shorttitle = {Explainability as a Non-Functional Requirement},
  author = {Chazette, Larissa and Schneider, Kurt},
  date = {2020-12},
  journaltitle = {Requirements Engineering},
  shortjournal = {Requirements Eng},
  volume = {25},
  number = {4},
  pages = {493--514},
  issn = {0947-3602, 1432-010X},
  doi = {10.1007/s00766-020-00333-1},
  url = {https://link.springer.com/10.1007/s00766-020-00333-1},
  urldate = {2022-06-30},
  abstract = {Software systems are becoming increasingly complex. Their ubiquitous presence makes users more dependent on their correctness in many aspects of daily life. As a result, there is a growing need to make software systems and their decisions more comprehensible, with more transparency in software-based decision making. Transparency is therefore becoming increasingly important as a non-functional requirement. However, the abstract quality aspect of transparency needs to be better understood and related to mechanisms that can foster it. The integration of explanations into software has often been discussed as a solution to mitigate system opacity. Yet, an important first step is to understand user requirements in terms of explainable software behavior: Are users really interested in software transparency and are explanations considered an appropriate way to achieve it? We conducted a survey with 107 end users to assess their opinion on the current level of transparency in software systems and what they consider to be the main advantages and disadvantages of embedded explanations. We assess the relationship between explanations and transparency and analyze its potential impact on software quality. As explainability has become an important issue, researchers and professionals have been discussing how to deal with it in practice. While there are differences of opinion on the need for built-in explanations, understanding this concept and its impact on software is a key step for requirements engineering. Based on our research results and on the study of existing literature, we offer recommendations for the elicitation and analysis of explainability and discuss strategies for the practice.},
  langid = {english},
  keywords = {requirements gathering},
  file = {/home/sam/Zotero/storage/GIPKXPSW/Chazette-Schneider2020_Article_ExplainabilityAsANon-functiona.pdf}
}

@inproceedings{chenDynamicSlicingPython2014,
  title = {Dynamic {{Slicing}} of {{Python Programs}}},
  booktitle = {2014 {{IEEE}} 38th {{Annual Computer Software}} and {{Applications Conference}}},
  author = {Chen, Zhifei and Chen, Lin and Zhou, Yuming and Xu, Zhaogui and Chu, William C. and Xu, Baowen},
  date = {2014-07},
  pages = {219--228},
  issn = {0730-3157},
  doi = {10.1109/COMPSAC.2014.30},
  abstract = {Python is widely used for web programming and GUI development. Due to the dynamic features of Python, Python programs may contain various unlimited errors. Dynamic slicing extracts those statements from a program which affect the variables in a slicing criterion with a particular input. Dynamic slicing of Python programs is essential for program debugging and fault location. In this paper, we propose an approach of dynamic slicing for Python programs which combines static analysis and dynamic tracing of the Python byte code. It precisely handles the dynamic features of Python, such as dynamic typing of variables, heavy usage of first-class objects, and dynamic modifications of classes and instances. Finally, we evaluate our approach on several Python programs. Experimental results show that the whole dynamic slicing for each subject program spends at most about 13 seconds on the average and costs at most 7.58 mb memory space overhead. Furthermore, the average slice ratio of Python source code ranges from 9.26\% to 59.42\%. According to it, our dynamic slicing approach can be effectively and efficiently performed. To the best of our knowledge, it is the first one of dynamic slicing for Python programs.},
  eventtitle = {2014 {{IEEE}} 38th {{Annual Computer Software}} and {{Applications Conference}}},
  keywords = {software analysis,software engineering},
  annotation = {interest: 85},
  file = {/home/sam/Zotero/storage/HKNXHGVA/Chen et al. - 2014 - Dynamic Slicing of Python Programs.pdf}
}

@article{chengCompressionLowRank2005,
  title = {On the {{Compression}} of {{Low Rank Matrices}}},
  author = {Cheng, H. and Gimbutas, Z. and Martinsson, P. G. and Rokhlin, V.},
  date = {2005-01},
  journaltitle = {SIAM Journal on Scientific Computing},
  shortjournal = {SIAM J. Sci. Comput.},
  volume = {26},
  number = {4},
  pages = {1389--1404},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {1064-8275},
  doi = {10.1137/030602678},
  url = {https://epubs.siam.org/doi/10.1137/030602678},
  urldate = {2024-02-05},
  abstract = {Randomized sampling has recently been proven a highly efficient technique for computing approximate factorizations of matrices that have low numerical rank. This paper describes an extension of such techniques to a wider class of matrices that are not themselves rank-deficient but have off-diagonal blocks that are; specifically, the class of so-called hierarchically semiseparable (HSS) matrices. HSS matrices arise frequently in numerical analysis and signal processing, particularly in the construction of fast methods for solving differential and integral equations numerically. The HSS structure admits algebraic operations (matrix-vector multiplications, matrix factorizations, matrix inversion, etc.) to be performed very rapidly, but only once the HSS representation of the matrix has been constructed. How to rapidly compute this representation in the first place is much less well understood. The present paper demonstrates that if an \$N\textbackslash times N\$ matrix can be applied to a vector in \$O(N)\$ time, and if individual entries of the matrix can be computed rapidly, then provided that an HSS representation of the matrix exists, it can be constructed in \$O(N\textbackslash,k\textasciicircum\{2\})\$ operations, where k is an upper bound for the numerical rank of the off-diagonal blocks. The point is that when legacy codes (based on, e.g., the fast multipole method) can be used for the fast matrix-vector multiply, the proposed algorithm can be used to obtain the HSS representation of the matrix, and then well-established techniques for HSS matrices can be used to invert or factor the matrix.},
  keywords = {linear algebra,project-provenance-pp},
  file = {/home/sam/Zotero/storage/AP4Z4PNZ/Cheng et al. - 2005 - On the Compression of Low Rank Matrices.pdf}
}

@inproceedings{chengWhatImprovesDeveloper2022,
  title = {What Improves Developer Productivity at Google? Code Quality},
  shorttitle = {What Improves Developer Productivity at Google?},
  booktitle = {Proceedings of the 30th {{ACM Joint European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Cheng, Lan and Murphy-Hill, Emerson and Canning, Mark and Jaspan, Ciera and Green, Collin and Knight, Andrea and Zhang, Nan and Kammer, Elizabeth},
  date = {2022-11-09},
  series = {{{ESEC}}/{{FSE}} 2022},
  pages = {1302--1313},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3540250.3558940},
  url = {https://doi.org/10.1145/3540250.3558940},
  urldate = {2022-11-14},
  abstract = {Understanding what affects software developer productivity can help organizations choose wise investments in their technical and social environment. But the research literature either focuses on what correlates with developer productivity in ecologically valid settings or focuses on what causes developer productivity in highly constrained settings. In this paper, we bridge the gap by studying software developers at Google through two analyses. In the first analysis, we use panel data with 39 productivity factors, finding that code quality, technical debt, infrastructure tools and support, team communication, goals and priorities, and organizational change and process are all causally linked to self-reported developer productivity. In the second analysis, we use a lagged panel analysis to strengthen our causal claims. We find that increases in perceived code quality tend to be followed by increased perceived developer productivity, but not vice versa, providing the strongest evidence to date that code quality affects individual developer productivity.},
  isbn = {978-1-4503-9413-0},
  keywords = {industry practices,software engineering},
  annotation = {interest: 70},
  file = {/home/sam/Zotero/storage/D69YDC8J/Cheng et al. - 2022 - What improves developer productivity at google co.pdf}
}

@inproceedings{chenHybridInformationFlow2014,
  title = {Hybrid {{Information Flow Analysis}} for {{Python Bytecode}}},
  booktitle = {2014 11th {{Web Information System}} and {{Application Conference}}},
  author = {Chen, Zhifei and Chen, Lin and Xu, Baowen},
  date = {2014-09},
  pages = {95--100},
  doi = {10.1109/WISA.2014.26},
  abstract = {Python is widely used to create and manage complex, database-driven websites. However, due to dynamic features such as dynamic typing of variables, Python programs pose a serious security risk to web applications. Most security vulnerabilities result from the fact that unsafe data input reaches security-sensitive operations. To address this problem, information flow analysis for Python programs is proposed to enforce this property. Information flow can capture the fact that a particular value affects another value in the program. In this paper, we present a novel approach for analyzing information flow in Python byte code which is a low-level language and is more widely broadcast. Our approach performs a hybrid of static and dynamic control/data flow analysis. Static analysis is used to study implicit flow, while dynamic analysis efficiently tracks execution information and determines definition-use pair. To the best of our knowledge, it is the first one for Python byte code.},
  eventtitle = {2014 11th {{Web Information System}} and {{Application Conference}}},
  keywords = {software analysis},
  annotation = {interest: 85},
  file = {/home/sam/Zotero/storage/9CHBGDHJ/Chen et al. - 2014 - Hybrid Information Flow Analysis for Python Byteco.pdf}
}

@inproceedings{chenImplementationEvaluationProtocol2008,
  title = {Implementation and {{Evaluation}} of a {{Protocol}} for {{Recording Process Documentation}} in the {{Presence}} of {{Failures}}},
  booktitle = {Provenance and {{Annotation}} of {{Data}} and {{Processes}}},
  author = {Chen, Zheng and Moreau, Luc},
  editor = {Freire, Juliana and Koop, David and Moreau, Luc},
  date = {2008},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {92--105},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-89965-5_11},
  abstract = {The provenance of a particular data item is the process that led to that piece of data. Previous work has enabled the creation of detailed representation of past executions for determining provenance, termed process documentation. However, current solutions to recording process documentation assume a failure free environment. Failures result in process documentation not being recorded, thereby causing the loss of evidence that a process occurred. We have designed F-PReP, a protocol to guarantee the recording of process documentation in the presence of failures. This paper discusses its implementation and evaluates its performance. The result reveals that it introduces acceptable overhead.},
  isbn = {978-3-540-89965-5},
  langid = {english},
  keywords = {provenance},
  annotation = {interest: 95},
  file = {/home/sam/Zotero/storage/3VTD3YGM/Chen and Moreau - 2008 - Implementation and Evaluation of a Protocol for Re.pdf}
}

@online{chenMetamorphicTestingNew2020,
  title = {Metamorphic {{Testing}}: {{A New Approach}} for {{Generating Next Test Cases}}},
  shorttitle = {Metamorphic {{Testing}}},
  author = {Chen, T. Y. and Cheung, S. C. and Yiu, S. M.},
  date = {2020-02-27},
  eprint = {2002.12543},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2002.12543},
  url = {http://arxiv.org/abs/2002.12543},
  urldate = {2022-10-11},
  abstract = {In software testing, a set of test cases is constructed according to some predefined selection criteria. The software is then examined against these test cases. Three interesting observations have been made on the current artifacts of software testing. Firstly, an error-revealing test case is considered useful while a successful test case which does not reveal software errors is usually not further investigated. Whether these successful test cases still contain useful information for revealing software errors has not been properly studied. Secondly, no matter how extensive the testing has been conducted in the development phase, errors may still exist in the software [5]. These errors, if left undetected, may eventually cause damage to the production system. The study of techniques for uncovering software errors in the production phase is seldom addressed in the literature. Thirdly, as indicated by Weyuker in [6], the availability of test oracles is pragmatically unattainable in most situations. However, the availability of test oracles is generally assumed in conventional software testing techniques. In this paper, we propose a novel test case selection technique that derives new test cases from the successful ones. The selection aims at revealing software errors that are possibly left undetected in successful test cases which may be generated using some existing strategies. As such, the proposed technique augments the effectiveness of existing test selection strategies. The technique also helps uncover software errors in the production phase and can be used in the absence of test oracles.},
  pubstate = {prepublished},
  keywords = {software testing},
  annotation = {interest: 85},
  file = {/home/sam/Zotero/storage/UB559S4D/Chen et al. - 2020 - Metamorphic Testing A New Approach for Generating.pdf;/home/sam/Zotero/storage/GCA49AFY/2002.html}
}

@article{chenOpenNotEnough2019,
  title = {Open Is Not Enough},
  author = {Chen, Xiaoli and Dallmeier-Tiessen, Sünje and Dasler, Robin and Feger, Sebastian and Fokianos, Pamfilos and Gonzalez, Jose Benito and Hirvonsalo, Harri and Kousidis, Dinos and Lavasa, Artemis and Mele, Salvatore and Rodriguez, Diego Rodriguez and Šimko, Tibor and Smith, Tim and Trisovic, Ana and Trzcinska, Anna and Tsanaktsidis, Ioannis and Zimmermann, Markus and Cranmer, Kyle and Heinrich, Lukas and Watts, Gordon and Hildreth, Michael and Lloret Iglesias, Lara and Lassila-Perini, Kati and Neubert, Sebastian},
  date = {2019-02},
  journaltitle = {Nature Physics},
  shortjournal = {Nature Phys},
  volume = {15},
  number = {2},
  pages = {113--119},
  publisher = {Nature Publishing Group},
  issn = {1745-2481},
  doi = {10.1038/s41567-018-0342-2},
  url = {https://www.nature.com/articles/s41567-018-0342-2},
  urldate = {2022-09-27},
  abstract = {The solutions adopted by the high-energy physics community to foster reproducible research are examples of best practices that could be embraced more widely. This first experience suggests that reproducibility requires going beyond openness.},
  issue = {2},
  langid = {english},
  keywords = {metascience,reproducibility engineering},
  annotation = {interest: 84},
  file = {/home/sam/Zotero/storage/Q9CKJH5S/Chen et al. - 2019 - Open is not enough.pdf;/home/sam/Zotero/storage/IBYAKJLF/s41567-018-0342-2.html}
}

@unpublished{chenRobustBenchmarkingNoisy2016,
  title = {Robust Benchmarking in Noisy Environments},
  author = {Chen, Jiahao and Revels, Jarrett},
  date = {2016-08-15},
  eprint = {1608.04295},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1608.04295},
  urldate = {2022-04-11},
  abstract = {We propose a benchmarking strategy that is robust in the presence of timer error, OS jitter and other environmental fluctuations, and is insensitive to the highly nonideal statistics produced by timing measurements. We construct a model that explains how these strongly nonideal statistics can arise from environmental fluctuations, and also justifies our proposed strategy. We implement this strategy in the BenchmarkTools Julia package, where it is used in production continuous integration (CI) pipelines for developing the Julia language and its ecosystem.},
  keywords = {software benchmarking,software engineering},
  annotation = {interest: 90},
  file = {/home/sam/Zotero/storage/RTW5NPP5/Chen and Revels - 2016 - Robust benchmarking in noisy environments.pdf}
}

@inproceedings{chirigatiReproZipComputationalReproducibility2016,
  title = {{{ReproZip}}: {{Computational Reproducibility With Ease}}},
  shorttitle = {{{ReproZip}}},
  booktitle = {Proceedings of the 2016 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Chirigati, Fernando and Rampin, Rémi and Shasha, Dennis and Freire, Juliana},
  date = {2016-06-26},
  series = {{{SIGMOD}} '16},
  pages = {2085--2088},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2882903.2899401},
  url = {https://dl.acm.org/doi/10.1145/2882903.2899401},
  urldate = {2023-10-16},
  abstract = {We present ReproZip, the recommended packaging tool for the SIGMOD Reproducibility Review. ReproZip was designed to simplify the process of making an existing computational experiment reproducible across platforms, even when the experiment was put together without reproducibility in mind. The tool creates a self-contained package for an experiment by automatically tracking and identifying all its required dependencies. The researcher can share the package with others, who can then use ReproZip to unpack the experiment, reproduce the findings on their favorite operating system, as well as modify the original experiment for reuse in new research, all with little effort. The demo will consist of examples of non-trivial experiments, showing how these can be packed in a Linux machine and reproduced on different machines and operating systems. Demo visitors will also be able to pack and reproduce their own experiments.},
  isbn = {978-1-4503-3531-7},
  keywords = {project-provenance-pp,record-replay,reproducibility engineering},
  file = {/home/sam/Zotero/storage/LL2ND3RF/Chirigati et al. - 2016 - ReproZip Computational Reproducibility With Ease.pdf}
}

@online{christensenEraseYourDarlings,
  title = {Erase Your Darlings: Immutable Infrastructure for Mutable Systems - {{Graham Christensen}}},
  author = {Christensen, Graham},
  url = {https://grahamc.com/blog/erase-your-darlings},
  urldate = {2022-08-22},
  keywords = {industry practices,infrastructure management},
  file = {/home/sam/Zotero/storage/YNIVAI6E/erase-your-darlings.html}
}

@online{christensenZFSDatasetsNixOS,
  title = {{{ZFS Datasets}} for {{NixOS}} - {{Graham Christensen}}},
  author = {Christensen, Graham},
  url = {https://grahamc.com/blog/nixos-on-zfs},
  urldate = {2022-08-22},
  keywords = {industry practices,infrastructure management},
  file = {/home/sam/Zotero/storage/GAD8CHVV/nixos-on-zfs.html}
}

@online{churchyardSingularTheirJane,
  title = {Singular "Their" in {{Jane Austen}} and Elsewhere: {{Anti-pedantry}} Page},
  author = {Churchyard, Harry},
  url = {https://www.pemberley.com/janeinfo/austheir.html#X1aii},
  urldate = {2024-03-30},
  file = {/home/sam/Zotero/storage/DNR8CSQD/austheir.html}
}

@incollection{ciancariniEvaluatingCitationFunctions2014,
  title = {Evaluating {{Citation Functions}} in {{CiTO}}: {{Cognitive Issues}}},
  shorttitle = {Evaluating {{Citation Functions}} in {{CiTO}}},
  booktitle = {The {{Semantic Web}}: {{Trends}} and {{Challenges}}},
  author = {Ciancarini, Paolo and Di Iorio, Angelo and Nuzzolese, Andrea Giovanni and Peroni, Silvio and Vitali, Fabio},
  editor = {Presutti, Valentina and family=Amato, given=Claudia, prefix=d’, useprefix=true and Gandon, Fabien and family=Aquin, given=Mathieu, prefix=d’, useprefix=true and Staab, Steffen and Tordai, Anna},
  editora = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Kobsa, Alfred and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Terzopoulos, Demetri and Tygar, Doug and Weikum, Gerhard},
  editoratype = {redactor},
  date = {2014},
  volume = {8465},
  pages = {580--594},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-07443-6_39},
  url = {http://link.springer.com/10.1007/978-3-319-07443-6_39},
  urldate = {2022-05-25},
  abstract = {Networks of citations are a key tool for referencing, disseminating and evaluating research results. The task of characterising the functional role of citations in scientific literature is very difficult, not only for software agents but for humans, too. The main problem is that the mental models of different annotators hardly ever converge to a single shared opinion. The goal of this paper is to investigate how an existing reference model for classifying citations, namely CiTO (Citation Typing Ontology), is interpreted and used by annotators of scientific literature. We present an experiment capturing the cognitive processes behind subjects’ decisions in annotating papers with CiTO, and we provide initial ideas to refine future releases of CiTO.},
  isbn = {978-3-319-07442-9 978-3-319-07443-6},
  langid = {english},
  keywords = {knowledge engineering,semantic web},
  file = {/home/sam/Zotero/storage/MMBS6V37/ciancarini-2014-evaluating-citation-functions.pdf}
}

@online{ciechanowskiCamerasLensesBartosz,
  title = {Cameras and {{Lenses}} – {{Bartosz Ciechanowski}}},
  author = {Ciechanowski, Bartosz},
  url = {https://ciechanow.ski/cameras-and-lenses/},
  urldate = {2022-09-06},
  abstract = {Interactive article explaining how cameras and lenses work.},
  langid = {english},
  keywords = {optics},
  annotation = {interest: 95},
  file = {/home/sam/Zotero/storage/IABV577S/cameras-and-lenses.html}
}

@incollection{ciepielaGridSpace2VirtualLaboratory2012,
  title = {{{GridSpace2 Virtual Laboratory Case Study}}: {{Implementation}} of {{Algorithms}} for {{Quantitative Analysis}} of {{Grain Morphology}} in {{Self-assembled Hexagonal Lattices According}} to the {{Hillebrand Method}}},
  shorttitle = {{{GridSpace2 Virtual Laboratory Case Study}}},
  booktitle = {Building a {{National Distributed}} E-{{Infrastructure}}–{{PL-Grid}}},
  author = {Ciepiela, Eryk and Zaraska, Leszek and Sulka, Grzegorz D.},
  editor = {Bubak, Marian and Szepieniec, Tomasz and Wiatr, Kazimierz},
  date = {2012},
  volume = {7136},
  pages = {240--251},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-28267-6_19},
  url = {http://link.springer.com/10.1007/978-3-642-28267-6_19},
  urldate = {2024-10-04},
  isbn = {978-3-642-28266-9 978-3-642-28267-6}
}

@article{cimpanuSupercomputersHackedEurope2020,
  entrysubtype = {newspaper},
  title = {Supercomputers Hacked across {{Europe}} to Mine Cryptocurrency},
  author = {Cimpanu, Catalin},
  date = {2020-05-16},
  journaltitle = {ZDNet},
  url = {https://www.zdnet.com/article/supercomputers-hacked-across-europe-to-mine-cryptocurrency/},
  urldate = {2022-05-23},
  abstract = {Confirmed infections have been reported in the UK, Germany, and Switzerland. Another suspected infection was reported in Spain.},
  langid = {english},
  keywords = {cybersecurity,internship-project,project-devsecops},
  file = {/home/sam/Zotero/storage/28X22WYP/supercomputers-hacked-across-europe-to-mine-cryptocurrency.html}
}

@online{cirilloDeclineViolentConflicts2016,
  type = {SSRN Scholarly Paper},
  title = {The {{Decline}} of {{Violent Conflicts}}: {{What Do}} the {{Data Really Say}}?},
  shorttitle = {The {{Decline}} of {{Violent Conflicts}}},
  author = {Cirillo, Pasquale and Taleb, Nassim Nicholas},
  date = {2016-11-27},
  number = {2876315},
  location = {Rochester, NY},
  doi = {10.2139/ssrn.2876315},
  url = {https://papers.ssrn.com/abstract=2876315},
  urldate = {2022-09-09},
  abstract = {We propose a methodology to look at violence in particular, and other aspects of quantitative historiography in general, in a way compatible with statistical inference, which needs to accommodate the fat-tailedness of the data and the unreliability of the reports of conflicts. We investigate the theses of “long peace” and drop in violence and find that these are statistically invalid and resulting from flawed and naive methodologies, incompatible with fat tails and non-robust to minor changes in data formatting and methodologies. There is no statistical basis to claim that “times are different” owing to the long inter-arrival times between conflicts; there is no basis to discuss any “trend”, and no scientific basis for narratives about change in risk. We describe naive empiricism under fat tails. We also establish that violence has a “true mean” that is underestimated in the track record. This is a historiographical adaptation of the results in Cirillo and Taleb (2016).},
  langid = {english},
  pubstate = {prepublished},
  keywords = {sociology},
  file = {/home/sam/Zotero/storage/5PDRU2KQ/Cirillo and Taleb - 2016 - The Decline of Violent Conflicts What Do the Data.pdf;/home/sam/Zotero/storage/M2CDQT67/papers.html}
}

@inproceedings{citoUsingDockerContainers2016,
  title = {Using {{Docker Containers}} to {{Improve Reproducibility}} in {{Software}} and {{Web Engineering Research}}},
  booktitle = {Web {{Engineering}}},
  author = {Cito, Jürgen and Ferme, Vincenzo and Gall, Harald C.},
  editor = {Bozzon, Alessandro and Cudre-Maroux, Philippe and Pautasso, Cesare},
  date = {2016},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {609--612},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-38791-8_58},
  abstract = {The ability to replicate and reproduce scientific results has become an increasingly important topic for many academic disciplines. In computer science and, more specifically, software and web engineering, contributions of scientific work rely on developed algorithms, tools and prototypes, quantitative evaluations, and other computational analyses. Published code and data come with many undocumented assumptions, dependencies, and configurations that are internal knowledge and make reproducibility hard to achieve. This tutorial presents how Docker containers can overcome these issues and aid the reproducibility of research artifacts in software and web engineering and discusses their applications in the field.},
  isbn = {978-3-319-38791-8},
  langid = {english},
  keywords = {reproducibility engineering},
  annotation = {interest: 94},
  file = {/home/sam/Zotero/storage/CVWZJXEC/Cito et al. - 2016 - Using Docker Containers to Improve Reproducibility.pdf}
}

@inproceedings{claerboutElectronicDocumentsGive1992,
  title = {Electronic Documents Give Reproducible Research a New Meaning},
  booktitle = {{{SEG Technical Program Expanded Abstracts}} 1992},
  author = {Claerbout, Jon F. and Karrenbach, Martin},
  date = {1992-01},
  pages = {601--604},
  publisher = {Society of Exploration Geophysicists},
  doi = {10.1190/1.1822162},
  url = {http://library.seg.org/doi/abs/10.1190/1.1822162},
  urldate = {2022-06-01},
  eventtitle = {{{SEG Technical Program Expanded Abstracts}} 1992},
  langid = {english},
  keywords = {internship-project,reproducibility engineering,research software engineering}
}

@report{clarkAnalyzingAccessibilityWikipedia2017,
  type = {SSRN Scholarly Paper},
  title = {Analyzing {{Accessibility}} of {{Wikipedia Projects Around}} the {{World}}},
  author = {Clark, Justin and Faris, Robert and Heacock Jones, Rebekah},
  date = {2017-05-01},
  number = {2951312},
  institution = {Social Science Research Network},
  location = {Rochester, NY},
  doi = {10.2139/ssrn.2951312},
  url = {https://papers.ssrn.com/abstract=2951312},
  urldate = {2022-04-13},
  abstract = {This study, conducted by the Internet Monitor project at the Berkman Klein Center for Internet \& Society, analyzes the scope of government-sponsored censorship of Wikimedia sites around the world. The study finds that, as of June 2016, China was likely censoring the Chinese language Wikipedia project, and Thailand and Uzbekistan were likely interfering intermittently with specific language projects of Wikipedia as well. However, considering the widespread use of filtering technologies and the vast coverage of Wikipedia, our study finds that, as of June 2016, there was relatively little censorship of Wikipedia globally. In fact, our study finds there was less censorship in June 2016 than before Wikipedia’s transition to HTTPS-only content delivery in June 2015. HTTPS prevents censors from seeing which page a user is viewing, which means censors must choose between blocking the entire site and allowing access to all articles. This finding suggests that the shift to HTTPS has been a good one in terms of ensuring accessibility to knowledge. The study identifies and documents the blocking of Wikipedia content using two complementary data collection and analysis strategies: a client-side system that collects data from the perspective of users around the globe and a server-side tool to analyze traffic coming in to Wikipedia servers. Both client- and server-side methods detected events that we consider likely related to censorship, in addition to a large number of suspicious events that remain unexplained. The report features results of our data analysis and insights into the state of access to Wikipedia content in 15 select countries.},
  langid = {english},
  keywords = {internet censorship},
  file = {/home/sam/Zotero/storage/8S3TMBE5/Clark et al. - 2017 - Analyzing Accessibility of Wikipedia Projects Arou.pdf}
}

@article{clarkMicropublicationsSemanticModel2014,
  title = {Micropublications: A Semantic Model for Claims, Evidence, Arguments and Annotations in Biomedical Communications},
  shorttitle = {Micropublications},
  author = {Clark, Tim and Ciccarese, Paolo N. and Goble, Carole A.},
  date = {2014-07-04},
  journaltitle = {Journal of Biomedical Semantics},
  shortjournal = {Journal of Biomedical Semantics},
  volume = {5},
  number = {1},
  pages = {28},
  issn = {2041-1480},
  doi = {10.1186/2041-1480-5-28},
  url = {https://doi.org/10.1186/2041-1480-5-28},
  urldate = {2023-11-09},
  abstract = {Scientific publications are documentary representations of defeasible arguments, supported by data and repeatable methods. They are the essential mediating artifacts in the ecosystem of scientific communications. The institutional “goal” of science is publishing results. The linear document publication format, dating from 1665, has survived transition to the Web.},
  keywords = {academic publishing,semantic web},
  file = {/home/sam/Zotero/storage/4B2A2FPN/Clark et al. - 2014 - Micropublications a semantic model for claims, ev.pdf;/home/sam/Zotero/storage/LQ3HT8HR/2041-1480-5-28.html}
}

@software{CLI11CommandLine2022,
  title = {{{CLI11}}: {{Command}} Line Parser for {{C}}++11},
  shorttitle = {{{CLI11}}},
  date = {2022-09-06T12:21:46Z},
  origdate = {2017-01-25T22:29:34Z},
  url = {https://github.com/CLIUtils/CLI11/blob/1a26afab049bb75c0523a754d62b961439248d44/README.md},
  urldate = {2022-09-06},
  abstract = {CLI11 is a command line parser for C++11 and beyond that provides a rich feature set with a simple and intuitive interface.},
  organization = {CLIUtils},
  keywords = {software}
}

@article{cohenReproducibilityNaturalLanguage2016,
  title = {Reproducibility in {{Natural Language Processing}}: {{A Case Study}} of {{Two R Libraries}} for {{Mining PubMed}}/{{MEDLINE}}},
  shorttitle = {Reproducibility in {{Natural Language Processing}}},
  author = {Cohen, K. Bretonnel and Xia, Jingbo and Roeder, Christophe and Hunter, Lawrence E.},
  date = {2016-05},
  journaltitle = {LREC ... International Conference on Language Resources \& Evaluation : [proceedings]. International Conference on Language Resources and Evaluation},
  shortjournal = {LREC Int Conf Lang Resour Eval},
  volume = {2016},
  number = {W23},
  eprint = {29568821},
  eprinttype = {pmid},
  pages = {6--12},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5860830/},
  urldate = {2022-09-20},
  abstract = {There is currently a crisis in science related to highly publicized failures to reproduce large numbers of published studies. The current work proposes, by way of case studies, a methodology for moving the study of reproducibility in computational work to a full stage beyond that of earlier work. Specifically, it presents a case study in attempting to reproduce the reports of two R libraries for doing text mining of the PubMed/MEDLINE repository of scientific publications. The main findings are that a rational paradigm for reproduction of natural language processing papers can be established; the advertised functionality was difficult, but not impossible, to reproduce; and reproducibility studies can produce additional insights into the functioning of the published system. Additionally, the work on reproducibility lead to the production of novel user-centered documentation that has been accessed 260 times since its publication—an average of once a day per library.},
  pmcid = {PMC5860830},
  keywords = {metascience,reproducibility engineering},
  annotation = {interest: 87},
  file = {/home/sam/Zotero/storage/G7E7UF7A/Cohen et al. - 2016 - Reproducibility in Natural Language Processing A .pdf}
}

@article{colellaPiecewiseParabolicMethod1984,
  title = {The {{Piecewise Parabolic Method}} ({{PPM}}) for {{Gas-Dynamical Simulations}}},
  author = {Colella, P. and Woodward, Paul R.},
  date = {1984-09-01},
  journaltitle = {Journal of Computational Physics},
  volume = {54},
  pages = {174--201},
  issn = {0021-9991},
  doi = {10.1016/0021-9991(84)90143-8},
  url = {https://ui.adsabs.harvard.edu/abs/1984JCoPh..54..174C},
  urldate = {2022-04-11},
  abstract = {We present the piecewise parabolic method, a higher-order extension of Godunov's method. There are several new features of this method which distinguish it from other higher-order Godunov-type methods. We use a higher-order spatial interpolation than previously used, which allows for a steeper representation of discontinuities, particularly contact discontinuities. We introduce a simpler and more robust algorithm for calculating the nonlinear wave interactions used to compute fluxes. Finally, we recognize the need for additional dissipation in any higher-order Godunov method of this type, and introduce it in such a way so as not to degrade the quality of the results.},
  keywords = {astrophysics,computational fluid dynamics,numerical methods,project-astrophysics},
  annotation = {ADS Bibcode: 1984JCoPh..54..174C},
  file = {/home/sam/Zotero/storage/ZULCY6N6/Colella and Woodward - 1984 - The Piecewise Parabolic Method (PPM) for Gas-Dynam.pdf}
}

@article{colemanWfCommonsFrameworkEnabling2022,
  title = {{{WfCommons}}: {{A}} Framework for Enabling Scientific Workflow Research and Development},
  shorttitle = {{{WfCommons}}},
  author = {Coleman, Tainã and Casanova, Henri and Pottier, Loïc and Kaushik, Manav and Deelman, Ewa and Ferreira da Silva, Rafael},
  date = {2022-03-01},
  journaltitle = {Future Generation Computer Systems},
  shortjournal = {Future Generation Computer Systems},
  volume = {128},
  pages = {16--27},
  issn = {0167-739X},
  doi = {10.1016/j.future.2021.09.043},
  url = {https://www.sciencedirect.com/science/article/pii/S0167739X21003897},
  urldate = {2022-10-31},
  abstract = {Scientific workflows are a cornerstone of modern scientific computing. They are used to describe complex computational applications that require efficient and robust management of large volumes of data, which are typically stored/processed on heterogeneous, distributed resources. The workflow research and development community has employed a number of methods for the quantitative evaluation of existing and novel workflow algorithms and systems. In particular, a common approach is to simulate workflow executions. In previous works, we have presented a collection of tools that have been adopted by the community for conducting workflow research. Despite their popularity, they suffer from several shortcomings that prevent easy adoption, maintenance, and consistency with the evolving structures and computational requirements of production workflows. In this work, we present WfCommons , a framework that provides a collection of tools for analyzing workflow executions, for producing generators of synthetic workflows, and for simulating workflow executions. We demonstrate the realism of the generated synthetic workflows by comparing their simulated executions to real workflow executions. We also contrast these results with results obtained when using the previously available collection of tools. We find that the workflow generators that are automatically constructed by our framework not only generate representative same-scale workflows (i.e., with structures and task characteristics distributions that resemble those observed in real-world workflows), but also do so at scales larger than that of available real-world workflows. Finally, we conduct a case study to demonstrate the usefulness of our framework for estimating the energy consumption of large-scale workflow executions.},
  langid = {english},
  keywords = {workflow managers},
  file = {/home/sam/Zotero/storage/Q8GA8Z3U/Coleman et al. - 2022 - WfCommons A framework for enabling scientific wor.pdf;/home/sam/Zotero/storage/6GBIB86Y/S0167739X21003897.html}
}

@report{collbergRepeatabilityBenefactionComputer2015,
  title = {Repeatability and {{Benefaction}} in {{Computer Systems Research}}—{{A Study}} and a {{Modest Proposal}}},
  author = {Collberg, Christian and Proebsting, Todd and Warren, Alex M},
  date = {2015-02-27},
  number = {14--04},
  institution = {University of Arizona},
  url = {http://repeatability.cs.arizona.edu/v2/RepeatabilityTR.pdf},
  abstract = {We describe a study into the extent to which Computer Systems researchers share their code and data and the extent to which such code builds. Starting with 601 papers from ACM conferences and journals, we examine 402 papers whose results were backed by code. For 32.3\% of these papers we were able to obtain the code and build it within 30 minutes; for 48.3\% of the papers we managed to build the code, but it may have required extra effort; for 54.0\% of the papers either we managed to build the code or the authors stated the code would build with reasonable effort. We also propose a novel sharing specification scheme that requires researchers to specify the level of sharing that reviewers and readers can assume from a paper.},
  keywords = {project-provenance-pp,reproducibility engineering},
  file = {/home/sam/Zotero/storage/K9CA8GSQ/RepeatabilityTR.pdf}
}

@article{collbergRepeatabilityComputerSystems2016,
  title = {Repeatability in Computer Systems Research},
  author = {Collberg, Christian and Proebsting, Todd A.},
  date = {2016-02-25},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {59},
  number = {3},
  pages = {62--69},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/2812803},
  url = {https://dl.acm.org/doi/10.1145/2812803},
  urldate = {2022-05-27},
  abstract = {To encourage repeatable research, fund repeatability engineering and reward commitments to sharing research artifacts.},
  langid = {english},
  keywords = {internship-project,project-acm-rep,project-provenance-pp,research software engineering},
  file = {/home/sam/Zotero/storage/JGDDR733/2812803.pdf}
}

@unpublished{collbergSharingSpecificationsRepeatability2016,
  title = {Sharing {{Specifications}} or {{Repeatability}} in {{Computer Systems Research}}},
  author = {Collberg, Christian S. and Proebsting, Todd A.},
  date = {2016-10-27},
  url = {https://repository.arizona.edu/handle/10150/621552},
  urldate = {2022-06-30},
  abstract = {We describe a study into the extent to which Computer Systems researchers share their code and data. Starting with 601 papers from ACM conferences and journals, we examine the papers whose results were backed by code to see for what fraction of these we would be able to obtain and build the code. Based on the results of this study, we propose a novel sharing specification scheme that requires researchers to specify the level of sharing that reviewers and readers can assume from a paper.},
  eventtitle = {Data {{Reproducibility}}: {{Integrity}} and {{Transparency}}},
  langid = {american},
  venue = {University of Arizona},
  file = {/home/sam/Zotero/storage/9K23A8EK/christian collberg open access.pdf}
}

@article{collinsCosmologicalAdaptiveMesh2010,
  title = {Cosmological {{Adaptive Mesh Refinement Magnetohydrodynamics}} with {{Enzo}}},
  author = {Collins, David C. and Xu, Hao and Norman, Michael L. and Li, Hui and Li, Shengtai},
  date = {2010-02-01},
  journaltitle = {The Astrophysical Journal Supplement Series},
  volume = {186},
  pages = {308--333},
  issn = {0067-0049},
  doi = {10.1088/0067-0049/186/2/308},
  url = {https://ui.adsabs.harvard.edu/abs/2010ApJS..186..308C},
  urldate = {2022-04-11},
  abstract = {In this work, we present EnzoMHD, the extension of the cosmological code Enzo to include the effects of magnetic fields through the ideal magnetohydrodynamics approximation. We use a higher order Godunov method for the computation of interface fluxes. We use two constrained transport methods to compute the electric field from those interface fluxes, which simultaneously advances the induction equation and maintains the divergence of the magnetic field. A second-order divergence-free reconstruction technique is used to interpolate the magnetic fields in the block-structured adaptive mesh refinement framework already extant in Enzo. This reconstruction also preserves the divergence of the magnetic field to machine precision. We use operator splitting to include gravity and cosmological expansion. We then present a series of cosmological and non-cosmological test problems to demonstrate the quality of solution resulting from this combination of solvers.},
  keywords = {astrophysics,cosmological simulation,numerical methods,project-astrophysics},
  annotation = {ADS Bibcode: 2010ApJS..186..308C},
  file = {/home/sam/Zotero/storage/M4Q2Y938/Collins et al. - 2010 - Cosmological Adaptive Mesh Refinement Magnetohydro.pdf}
}

@book{committeeonreproducibilityandreplicabilityinscienceReproducibilityReplicabilityScience2019,
  title = {Reproducibility and {{Replicability}} in {{Science}}},
  namea = {{Committee on Reproducibility and Replicability in Science} and {Board on Behavioral, Cognitive, and Sensory Sciences} and {Committee on National Statistics} and {Division of Behavioral and Social Sciences and Education} and {Nuclear and Radiation Studies Board} and {Division on Earth and Life Studies} and {Board on Mathematical Sciences and Analytics} and {Committee on Applied and Theoretical Statistics} and {Division on Engineering and Physical Sciences} and {Board on Research Data and Information} and {Committee on Science, Engineering, Medicine, and Public Policy} and {Policy and Global Affairs} and {National Academies of Sciences, Engineering, and Medicine}},
  nameatype = {collaborator},
  date = {2019-09-20},
  publisher = {National Academies Press},
  location = {Washington, D.C.},
  doi = {10.17226/25303},
  url = {https://www.nap.edu/catalog/25303},
  urldate = {2023-01-19},
  isbn = {978-0-309-48616-3},
  keywords = {reproducibility engineering},
  annotation = {interest: 90}
}

@online{ConfirmationDepthMeasure2014,
  title = {Confirmation {{Depth}} as a Measure of Reproducible Scientific Research.},
  date = {2014-10-21T11:00:00-08:00},
  url = {http://davidsoergel.com/posts/confirmation-depth-as-a-measure-of-reproducible-scientific-research},
  urldate = {2023-02-23},
  abstract = {What does it mean to reproduce a scientific study?  Confirmation Depth provides a guiding principle.},
  langid = {english},
  organization = {David Soergel},
  keywords = {project-provenance-pp},
  file = {/home/sam/Zotero/storage/KNRLR2QW/confirmation-depth-as-a-measure-of-reproducible-scientific-research.html}
}

@article{constantinDocumentComponentsOntology2016,
  title = {The {{Document Components Ontology}} ({{DoCO}})},
  author = {Constantin, Alexandru and Peroni, Silvio and Pettifer, Steve and Shotton, David and Vitali, Fabio},
  date = {2016-01-01},
  journaltitle = {Semantic Web},
  volume = {7},
  number = {2},
  pages = {167--181},
  publisher = {IOS Press},
  issn = {1570-0844},
  doi = {10.3233/SW-150177},
  url = {https://content.iospress.com/articles/semantic-web/sw177},
  urldate = {2023-05-25},
  abstract = {The availability in machine-readable form of descriptions of the structure of documents, as well as of the document discourse (e.g. the scientific discourse within scholarly articles), is crucial for facilitating semantic publishing and the overall c},
  langid = {english},
  keywords = {project-provenance-pp,semantic web},
  file = {/home/sam/Zotero/storage/AK4CAYTZ/Constantin et al. - 2016 - The&nbsp;Document&nbsp;Components&nbsp;Ontology&nb.pdf}
}

@article{copeStrongSecurityStarts2020,
  title = {Strong Security Starts with Software Development},
  author = {Cope, Rod},
  date = {2020-07},
  journaltitle = {Network Security},
  shortjournal = {Network Security},
  volume = {2020},
  number = {7},
  pages = {6--9},
  issn = {1353-4858, 1872-9371},
  doi = {10.1016/S1353-4858(20)30078-7},
  url = {http://www.magonlinelibrary.com/doi/10.1016/S1353-4858%2820%2930078-7},
  urldate = {2022-05-23},
  abstract = {While there is – rightly – a big focus on securing software that is already deployed, the reality is that many future vulnerabilities stem from the creation of that software. Insecure applications give hackers a back door. For instance, buffer overflows and code injection attacks can lead to compromised confidentiality of data, loss of service, damage to the systems of thousands of users, even – in the case of products containing embedded software, such as medical equipment or vehicles – risk to life.  While we focus on securing software that is already deployed, the reality is that many future vulnerabilities stem from the creation of that software.  Securing development is a tough challenge due to the increasing complexity of software, the volume of code, multiple contributors, distributed teams and the pressure to deliver to tight deadlines. Plus, developers traditionally have not been focused on security. That is changing with the emergence of DevSecOps, which focuses on implementing software security practices and tools at every stage of the lifecycle, explains Rod Cope of Perforce Software.},
  langid = {english},
  keywords = {cybersecurity,project-devsecops},
  file = {/home/sam/Zotero/storage/K7FIDZCG/10.1016@S1353-48582030078-7.pdf}
}

@inproceedings{copikSeBSServerlessBenchmark2021,
  title = {{{SeBS}}: A Serverless Benchmark Suite for Function-as-a-Service Computing},
  shorttitle = {{{SeBS}}},
  booktitle = {Proceedings of the 22nd {{International Middleware Conference}}},
  author = {Copik, Marcin and Kwasniewski, Grzegorz and Besta, Maciej and Podstawski, Michal and Hoefler, Torsten},
  date = {2021-10-02},
  series = {Middleware '21},
  pages = {64--78},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3464298.3476133},
  url = {https://doi.org/10.1145/3464298.3476133},
  urldate = {2022-12-18},
  abstract = {Function-as-a-Service (FaaS) is one of the most promising directions for the future of cloud services, and serverless functions have immediately become a new middleware for building scalable and cost-efficient microservices and appli cations. However, the quickly moving technology hinders reproducibility, and the lack of a standardized benchmarking suite leads to ad-hoc solutions and microbenchmarks being used in serverless research, further complicating meta-analysis and comparison of research solutions. To address this challenge, we propose the Serverless Benchmark Suite: the first benchmark for FaaS computing that systematically covers a wide spectrum of cloud resources and applications. Our benchmark consists of the specification of representative workloads, the accompanying implementation and evaluation infrastructure, and the evaluation methodology that facilitates reproducibility and enables interpretability. We demonstrate that the abstract model of a FaaS execution environment ensures the applicability of our benchmark to multiple commercial providers such as AWS, Azure, and Google Cloud. Our work facilities experimental evaluation of serverless systems, and delivers a standardized, reliable and evolving evaluation methodology of performance, efficiency, scalability and reliability of middleware FaaS platforms.},
  isbn = {978-1-4503-8534-3},
  annotation = {interest: 94}
}

@unpublished{coplienWhyMostUnit,
  title = {Why {{Most Unit Testing}} Is {{Waste}}},
  author = {Coplien, James O.},
  file = {/home/sam/Zotero/storage/D5D6D24X/why-most-unit-testing-is-waste.pdf}
}

@inproceedings{corchoWorkflowcentricResearchObjects2012,
  title = {Workflow-Centric Research Objects: {{First}} Class Citizens in Scholarly Discourse.},
  shorttitle = {Workflow-Centric Research Objects},
  booktitle = {Proceedings of {{Workshop}} on the {{Semantic Publishing}} | 9 Th {{Extended Semantic Web Conference Hersonissos}} | 28/05/2012 - 28/05/2012 | {{Hersonissos}}, {{Creta}} ({{Grecia}})},
  author = {Corcho, Oscar and Garijo Verdejo, Daniel and Belhajjame, K. and Zhao, Jun and Missier, P. and Newman, David and Palma, R. and Bechhofer, S. and García Cuesta, Esteban and Gómez-Pérez, José Manuel and Klyne, Graham and Roos, Marco and Ruiz, José Enrique and Soiland-Reyes, Stian and Verdes-Montenegro, Lourdes and De Roure, D. and Goble, C.},
  date = {2012},
  pages = {1--12},
  publisher = {Facultad de Informática (UPM)},
  location = {Hersonissos, Creta (Grecia)},
  url = {http://sepublica.mywikipaper.org/sepublica2012.pdf},
  urldate = {2022-07-25},
  abstract = {A workflow-centric research object bundles a workflow, the provenance of the results obtained by its enactment, other digital objects that are relevant for the experiment (papers, datasets, etc.), and annotations that semantically describe all these objects. In this paper, we propose a model to specify workflow-centric research objects, and show how the model can be grounded using semantic technologies and existing vocabularies, in particular the Object Reuse and Exchange (ORE) model and the Annotation Ontology (AO).We describe the life-cycle of a research object, which resembles the life-cycle of a scienti?c experiment.},
  eventtitle = {9 Th {{Extended Semantic Web Conference Hersonissos}}},
  langid = {english},
  keywords = {provenance,workflow managers}
}

@article{cosmoIdentifiersDigitalObjects2018,
  title = {Identifiers for {{Digital Objects}}: {{The}} Case of Software Source Code Preservation.},
  shorttitle = {204.4 {{Identifiers}} for {{Digital Objects}}},
  author = {Cosmo, Roberto Di and Gruenpeter, Morane and Zacchiroli, Stefano},
  date = {2018-09-21},
  publisher = {OSF},
  doi = {10.17605/OSF.IO/KDE56},
  url = {https://osf.io/kde56/},
  urldate = {2022-09-06},
  abstract = {In the very broad scope addressed by digital preservation initiatives, a special place belongs to the scientific and technical artifacts that we need to properly archive to enable scientific reproducibility. For these artifacts we need identifiers that are not only unique and persistent, but also support integrity in an intrinsic way. They must provide strong guarantees that the object denoted by a given identifier will always be the same, without relying on third parties and external administrative processes. In this article, we report on our quest for this identifiers for digital objects (IDOs), whose properties are different from, and complementary to, those of the various digital identifiers of objects (DIOs) that are in widespread use today. We argue that both kinds of identifiers are needed and present the framework for intrinsic persistent identifiers that we have adopted in Software Heritage for preserving billions of software artifacts.      Hosted on the Open Science Framework},
  langid = {english},
  keywords = {academic publishing,research software engineering},
  annotation = {interest: 86},
  file = {/home/sam/Zotero/storage/HULRPNZU/kde56.html}
}

@inproceedings{costaCapturingQueryingWorkflow2013,
  title = {Capturing and Querying Workflow Runtime Provenance with {{PROV}}: A Practical Approach},
  shorttitle = {Capturing and Querying Workflow Runtime Provenance with {{PROV}}},
  booktitle = {Proceedings of the {{Joint EDBT}}/{{ICDT}} 2013 {{Workshops}} on - {{EDBT}} '13},
  author = {Costa, Flavio and Silva, Vítor and family=Oliveira, given=Daniel, prefix=de, useprefix=true and Ocaña, Kary and Ogasawara, Eduardo and Dias, Jonas and Mattoso, Marta},
  date = {2013},
  pages = {282},
  publisher = {ACM Press},
  location = {Genoa, Italy},
  doi = {10.1145/2457317.2457365},
  url = {http://dl.acm.org/citation.cfm?doid=2457317.2457365},
  urldate = {2022-07-26},
  abstract = {Scientific workflows are commonly used to model and execute large-scale scientific experiments. They represent key resources for scientists and are enacted and managed by Scientific Workflow Management Systems (SWfMS). Each SWfMS has its particular approach to execute workflows and to capture and manage their provenance data. Due to the large scale of experiments, it may be unviable to analyze provenance data only after the end of the execution. A single experiment may demand weeks to run, even in high performance computing environments. Thus scientists need to monitor the experiment during its execution, and this can be done through provenance data. Runtime provenance analysis allows for scientists to monitor workflow execution and to take actions before the end of it (i.e. workflow steering). This provenance data can also be used to fine-tune the parallel execution of the workflow dynamically. We use the PROV data model as a basic framework for modeling and providing runtime provenance as a database that can be queried even during the execution. This database is agnostic of SWfMS and workflow engine. We show the benefits of representing and sharing runtime provenance data for improving the experiment management as well as the analysis of the scientific data.},
  eventtitle = {The {{Joint EDBT}}/{{ICDT}} 2013 {{Workshops}}},
  isbn = {978-1-4503-1599-9},
  langid = {english},
  keywords = {provenance,semantic web}
}

@article{costakohwalterProvDIFFPlayTraces2025,
  title = {Prov-{{DIFF}}: {{Play}} Traces Analysis through Provenance Differences},
  shorttitle = {Prov-{{DIFF}}},
  author = {Costa Kohwalter, Troy and Gresta Paulino Murta, Leonardo and Walter Gonzalez Clua, Esteban},
  date = {2025-01-01},
  journaltitle = {Entertainment Computing},
  shortjournal = {Entertainment Computing},
  volume = {52},
  pages = {100777},
  issn = {1875-9521},
  doi = {10.1016/j.entcom.2024.100777},
  url = {https://www.sciencedirect.com/science/article/pii/S1875952124001459},
  urldate = {2025-01-08},
  abstract = {A game session comprises a series of user decisions, inputs, and the execution of a strategy to reach specific goals. Tracking generated data of a game session is important for game analytics for developers and players. Game session data can be used for reproducibility, analysis of game traces, understanding player behavior, and improving the outcome in future sessions by learning from mistakes. However, game telemetry can rapidly lead to large amounts of data that can overwhelm the analyst’s ability to analyze it, and it can be difficult to identify the reasons that might have caused a player to lose in that session. This paper proposes a provenance-based automatic debugging approach for game analytics. It identifies possible reasons and discrepancies that might have led a player to lose by contrasting their performance with other players. Our approach also proposes possible insights on how to improve the player’s performance to reach the goal. We integrated our solution into the existing provenance visualization tool Prov Viewer. We provided an experimental study to demonstrate that our approach can identify probable causes that led the player to lose and propose changes to make it work in the next execution.},
  keywords = {Game analytics,Play traces,Provenance,Session debugging},
  file = {/home/sam/Zotero/storage/3KH6NIZ2/S1875952124001459.html}
}

@online{coulourisBlastBenchmark2016,
  title = {Blast {{Benchmark}}},
  author = {Coulouris, George and NIH Staff},
  date = {2016},
  url = {https://fiehnlab.ucdavis.edu/staff/kind/Collector/Benchmark/blast-benchmark},
  urldate = {2023-12-04},
  organization = {Fiehn Lab},
  keywords = {project-provenance-pp},
  file = {/home/sam/Zotero/storage/N6N7WD2M/blast-benchmark.html}
}

@inproceedings{courtesFunctionalPackageManagement2013,
  title = {Functional {{Package Management}} with {{Guix}}},
  author = {Courtès, Ludovic},
  date = {2013-06-03},
  url = {https://inria.hal.science/hal-00824004},
  urldate = {2023-12-19},
  abstract = {We describe the design and implementation of GNU Guix, a purely functional package manager designed to support a complete GNU/Linux distribution. Guix supports transactional upgrades and roll-backs, unprivileged package management, per-user profiles, and garbage collection. It builds upon the low-level build and deployment layer of the Nix package manager. Guix uses Scheme as its programming interface. In particular, we devise an embedded domain-specific language (EDSL) to describe and compose packages. We demonstrate how it allows us to benefit from the host general-purpose programming language while not compromising on expressiveness. Second, we show the use of Scheme to write build programs, leading to "two-tier'' programming system.},
  eventtitle = {European {{Lisp Symposium}}},
  langid = {english},
  file = {/home/sam/Zotero/storage/RN4KPARZ/Courtès - 2013 - Functional Package Management with Guix.pdf}
}

@inproceedings{courtesReproducibleUserControlledSoftware2015,
  title = {Reproducible and {{User-Controlled Software Environments}} in {{HPC}} with {{Guix}}},
  booktitle = {Euro-{{Par}} 2015: {{Parallel Processing Workshops}}},
  author = {Courtès, Ludovic and Wurmus, Ricardo},
  editor = {Hunold, Sascha and Costan, Alexandru and Giménez, Domingo and Iosup, Alexandru and Ricci, Laura and Gómez Requena, María Engracia and Scarano, Vittorio and Varbanescu, Ana Lucia and Scott, Stephen L. and Lankes, Stefan and Weidendorfer, Josef and Alexander, Michael},
  date = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {579--591},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-27308-2_47},
  abstract = {Support teams of high-performance computing (HPC) systems often find themselves between a rock and a hard place: on one hand, they understandably administrate these large systems in a conservative way, but on the other hand, they try to satisfy their users by deploying up-to-date tool chains as well as libraries and scientific software. HPC system users often have no guarantee that they will be able to reproduce results at a later point in time, even on the same system—software may have been upgraded, removed, or recompiled under their feet, and they have little hope of being able to reproduce the same software environment elsewhere. We present GNU~Guix and the functional package management paradigm and show how it can improve reproducibility and sharing among researchers with representative use cases.},
  isbn = {978-3-319-27308-2},
  langid = {english},
  keywords = {project-acm-rep,project-provenance-pp,research software engineering},
  annotation = {interest: 99},
  file = {/home/sam/Zotero/storage/BWSF7UIR/Courtès and Wurmus - 2015 - Reproducible and User-Controlled Software Environm.pdf}
}

@online{courtesTamingStatStorm2021,
  title = {Taming the ‘Stat’ Storm with a Loader Cache},
  author = {Courtès, Ludovic},
  date = {2021-08-02},
  url = {https://guix.gnu.org/en/blog/2021/taming-the-stat-storm-with-a-loader-cache/},
  urldate = {2024-01-19},
  organization = {GNU Guix Blog},
  file = {/home/sam/Zotero/storage/ZQ75YQ3L/taming-the-stat-storm-with-a-loader-cache.html}
}

@online{coxGenericDilemma2009,
  title = {The {{Generic Dilemma}}},
  author = {Cox, Russ},
  date = {2009-12-03},
  url = {https://research.swtch.com/generic},
  abstract = {Generic data structures (vectors, queues, maps, trees, and so on) seem to be the hot topic if you are evaluating a new language. One of the most frequent questions we've had about Go is where the generics are. It seems like there are three basic approaches to generics:     (The C approach.) Leave them out.     (The C++ approach.) Compile-time specialization or macro expansion.     (The Java approach.) Box everything implicitly.},
  organization = {Thoughts and links about programming},
  keywords = {programming languages}
}

@online{cpythondevelopersStatusPythonVersions,
  title = {Status of {{Python Versions}}},
  author = {CPython developers},
  url = {https://devguide.python.org/versions/},
  urldate = {2023-07-17},
  abstract = {The main branch is currently the future Python 3.13, and is the only branch that accepts new features. The latest release for each Python version can be found on the download page. Python Release C...},
  langid = {english},
  organization = {Python Developer's Guide},
  keywords = {project-bugsinpy},
  file = {/home/sam/Zotero/storage/MTLKDQIT/versions.html}
}

@inproceedings{crawlProvenanceBasedFaultTolerance2008,
  title = {A {{Provenance-Based Fault Tolerance Mechanism}} for {{Scientific Workflows}}},
  booktitle = {Provenance and {{Annotation}} of {{Data}} and {{Processes}}},
  author = {Crawl, Daniel and Altintas, Ilkay},
  editor = {Freire, Juliana and Koop, David and Moreau, Luc},
  date = {2008},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {152--159},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-89965-5_17},
  abstract = {Capturing provenance information in scientific workflows is not only useful for determining data-dependencies, but also for a wide range of queries including fault tolerance and usage statistics. As collaborative scientific workflow environments provide users with reusable shared workflows, collection and usage of provenance data in a generic way that could serve multiple data and computational models become vital. This paper presents a method for capturing data value- and control- dependencies for provenance information collection in the Kepler scientific workflow system. It also describes how the collected information based on these dependencies could be used for a fault tolerance framework in different models of computation.},
  isbn = {978-3-540-89965-5},
  langid = {english},
  keywords = {provenance,workflow managers},
  annotation = {interest: 96},
  file = {/home/sam/Zotero/storage/HGDEPVPS/Crawl and Altintas - 2008 - A Provenance-Based Fault Tolerance Mechanism for S.pdf}
}

@article{cristeaValuesDisplayItems2018,
  title = {P Values in Display Items Are Ubiquitous and Almost Invariably Significant: {{A}} Survey of Top Science Journals},
  shorttitle = {P Values in Display Items Are Ubiquitous and Almost Invariably Significant},
  author = {Cristea, Ioana Alina and Ioannidis, John P. A.},
  date = {2018-05-15},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {13},
  number = {5},
  pages = {e0197440},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0197440},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0197440},
  urldate = {2022-11-15},
  abstract = {P values represent a widely used, but pervasively misunderstood and fiercely contested method of scientific inference. Display items, such as figures and tables, often containing the main results, are an important source of P values. We conducted a survey comparing the overall use of P values and the occurrence of significant P values in display items of a sample of articles in the three top multidisciplinary journals (Nature, Science, PNAS) in 2017 and, respectively, in 1997. We also examined the reporting of multiplicity corrections and its potential influence on the proportion of statistically significant P values. Our findings demonstrated substantial and growing reliance on P values in display items, with increases of 2.5 to 14.5 times in 2017 compared to 1997. The overwhelming majority of P values (94\%, 95\% confidence interval [CI] 92\% to 96\%) were statistically significant. Methods to adjust for multiplicity were almost non-existent in 1997, but reported in many articles relying on P values in 2017 (Nature 68\%, Science 48\%, PNAS 38\%). In their absence, almost all reported P values were statistically significant (98\%, 95\% CI 96\% to 99\%). Conversely, when any multiplicity corrections were described, 88\% (95\% CI 82\% to 93\%) of reported P values were statistically significant. Use of Bayesian methods was scant (2.5\%) and rarely (0.7\%) articles relied exclusively on Bayesian statistics. Overall, wider appreciation of the need for multiplicity corrections is a welcome evolution, but the rapid growth of reliance on P values and implausibly high rates of reported statistical significance are worrisome.},
  langid = {english},
  keywords = {metascience},
  annotation = {interest: 83},
  file = {/home/sam/Zotero/storage/DEA2C45Q/Cristea and Ioannidis - 2018 - P values in display items are ubiquitous and almos.pdf}
}

@article{cuevas-vicenttinScientificWorkflowsProvenance2012,
  title = {Scientific {{Workflows}} and {{Provenance}}: {{Introduction}} and {{Research Opportunities}}},
  shorttitle = {Scientific {{Workflows}} and {{Provenance}}},
  author = {Cuevas-Vicenttín, Víctor and Dey, Saumen and Köhler, Sven and Riddle, Sean and Ludäscher, Bertram},
  date = {2012-11},
  journaltitle = {Datenbank-Spektrum},
  shortjournal = {Datenbank Spektrum},
  volume = {12},
  number = {3},
  pages = {193--203},
  issn = {1618-2162, 1610-1995},
  doi = {10.1007/s13222-012-0100-z},
  url = {http://link.springer.com/10.1007/s13222-012-0100-z},
  urldate = {2022-07-07},
  abstract = {Scientific workflows are becoming increasingly popular for compute-intensive and data-intensive scientific applications. The vision and promise of scientific workflows includes rapid, easy workflow design, reuse, scalable execution, and other advantages, e.g., to facilitate “reproducible science” through provenance (e.g., data lineage) support. However, as described in the paper, important research challenges remain. While the database community has studied (business) workflow technologies extensively in the past, most current work in scientific workflows seems to be done outside of the database community, e.g., by practitioners and researchers in the computational sciences and eScience. We provide a brief introduction to scientific workflows and provenance, and identify areas and problems that suggest new opportunities for database research.},
  langid = {english},
  keywords = {formal semantics,provenance,workflow managers}
}

@article{curtsingerSTABILIZERStatisticallySound2013,
  title = {{{STABILIZER}}: Statistically Sound Performance Evaluation},
  shorttitle = {{{STABILIZER}}},
  author = {Curtsinger, Charlie and Berger, Emery D.},
  date = {2013-03-16},
  journaltitle = {ACM SIGARCH Computer Architecture News},
  shortjournal = {SIGARCH Comput. Archit. News},
  volume = {41},
  number = {1},
  pages = {219--228},
  issn = {0163-5964},
  doi = {10.1145/2490301.2451141},
  url = {https://doi.org/10.1145/2490301.2451141},
  urldate = {2022-04-11},
  abstract = {Researchers and software developers require effective performance evaluation. Researchers must evaluate optimizations or measure overhead. Software developers use automatic performance regression tests to discover when changes improve or degrade performance. The standard methodology is to compare execution times before and after applying changes. Unfortunately, modern architectural features make this approach unsound. Statistically sound evaluation requires multiple samples to test whether one can or cannot (with high confidence) reject the null hypothesis that results are the same before and after. However, caches and branch predictors make performance dependent on machine-specific parameters and the exact layout of code, stack frames, and heap objects. A single binary constitutes just one sample from the space of program layouts, regardless of the number of runs. Since compiler optimizations and code changes also alter layout, it is currently impossible to distinguish the impact of an optimization from that of its layout effects. This paper presents Stabilizer, a system that enables the use of the powerful statistical techniques required for sound performance evaluation on modern architectures. Stabilizer forces executions to sample the space of memory configurations by repeatedly re-randomizing layouts of code, stack, and heap objects at runtime. Stabilizer thus makes it possible to control for layout effects. Re-randomization also ensures that layout effects follow a Gaussian distribution, enabling the use of statistical tests like ANOVA. We demonstrate Stabilizer's efficiency ({$<$}7\% median overhead) and its effectiveness by evaluating the impact of LLVM's optimizations on the SPEC CPU2006 benchmark suite. We find that, while -O2 has a significant impact relative to -O1, the performance impact of -O3 over -O2 optimizations is indistinguishable from random noise.},
  keywords = {software benchmarking,software engineering},
  annotation = {interest: 60},
  file = {/home/sam/Zotero/storage/GDXJDK4A/Curtsinger and Berger - STABILIZER Statistically Sound Performance Evalua.pdf}
}

@online{cvedatabaseCVECVE2020143862020,
  title = {{{CVE}} - {{CVE-2020-14386}}},
  author = {CVE database},
  date = {2020-06-17},
  url = {https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-14386},
  urldate = {2023-02-18},
  keywords = {containers,project-acm-rep,security},
  file = {/home/sam/Zotero/storage/U2ID5CHS/cvename.html}
}

@article{cyranoskiEducationPhDFactory2011,
  title = {Education: {{The PhD}} Factory},
  shorttitle = {Education},
  author = {Cyranoski, David and Gilbert, Natasha and Ledford, Heidi and Nayar, Anjali and Yahia, Mohammed},
  date = {2011-04-01},
  journaltitle = {Nature},
  volume = {472},
  number = {7343},
  pages = {276--279},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/472276a},
  url = {https://www.nature.com/articles/472276a},
  urldate = {2022-08-30},
  abstract = {The world is producing more PhDs than ever before. Is it time to stop?},
  issue = {7343},
  langid = {english},
  keywords = {science policy},
  file = {/home/sam/Zotero/storage/Q4HPBLWV/Cyranoski et al. - 2011 - Education The PhD factory.pdf;/home/sam/Zotero/storage/VYXV7AMX/472276a.html}
}

@inproceedings{daiLightweightProvenanceService2017,
  title = {Lightweight {{Provenance Service}} for {{High-Performance Computing}}},
  booktitle = {2017 26th {{International Conference}} on {{Parallel Architectures}} and {{Compilation Techniques}} ({{PACT}})},
  author = {Dai, Dong and Chen, Yong and Carns, Philip and Jenkins, John and Ross, Robert},
  date = {2017-09},
  pages = {117--129},
  doi = {10.1109/PACT.2017.14},
  url = {https://ieeexplore.ieee.org/abstract/document/8091224},
  urldate = {2024-02-14},
  abstract = {Provenance describes detailed information about the history of a piece of data, containing the relationships among elements such as users, processes, jobs, and workflows that contribute to the existence of data. Provenance is key to supporting many data management functionalities that are increasingly important in operations such as identifying data sources, parameters, or assumptions behind a given result; auditing data usage; or understanding details about how inputs are transformed into outputs. Despite its importance, however, provenance support is largely underdeveloped in highly parallel architectures and systems. One major challenge is the demanding requirements of providing provenance service in situ. The need to remain lightweight and to be always on often conflicts with the need to be transparent and offer an accurate catalog of details regarding the applications and systems. To tackle this challenge, we introduce a lightweight provenance service, called LPS, for high-performance computing (HPC) systems. LPS leverages a kernel instrument mechanism to achieve transparency and introduces representative execution and flexible granularity to capture comprehensive provenance with controllable overhead. Extensive evaluations and use cases have confirmed its efficiency and usability. We believe that LPS can be integrated into current and future HPC systems to support a variety of data management needs.},
  eventtitle = {2017 26th {{International Conference}} on {{Parallel Architectures}} and {{Compilation Techniques}} ({{PACT}})},
  keywords = {hpc,project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/7RQBN6AC/8091224.html}
}

@online{dancoCanTwitterScience2020,
  title = {Can {{Twitter Save Science}}?},
  author = {Danco, Alex},
  date = {2020-02-15T15:07:13+00:00},
  url = {https://alexdanco.com/2020/02/15/can-twitter-save-science/},
  urldate = {2022-08-30},
  abstract = {Before I found my way to the tech world, I was a grad student in the neuroscience department at McGill University. I never took the opportunity to get my PhD, and left science to do a startup instead. But I still think about it sometimes.},
  langid = {english},
  organization = {Welcome to Dancoland},
  keywords = {academic publishing,metascience}
}

@article{dashnowTenSimpleRules2014,
  title = {Ten {{Simple Rules}} for {{Writing}} a {{PLOS Ten Simple Rules Article}}},
  author = {Dashnow, Harriet and Lonsdale, Andrew and Bourne, Philip E.},
  date = {2014-10-23},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  volume = {10},
  number = {10},
  pages = {e1003858},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003858},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003858},
  urldate = {2024-04-21},
  langid = {english},
  keywords = {academic writing,Careers,Citation analysis,Computational biology,Human learning,Neuroimaging,Research reporting guidelines,Scientists,Supervisors},
  file = {/home/sam/Zotero/storage/UVYN63AZ/Dashnow et al. - 2014 - Ten Simple Rules for Writing a PLOS Ten Simple Rul.pdf}
}

@article{dasilvaReplicationEmpiricalStudies2014,
  title = {Replication of Empirical Studies in Software Engineering Research: A Systematic Mapping Study},
  shorttitle = {Replication of Empirical Studies in Software Engineering Research},
  author = {family=Silva, given=Fabio Q. B., prefix=da, useprefix=true and Suassuna, Marcos and França, A. César C. and Grubb, Alicia M. and Gouveia, Tatiana B. and Monteiro, Cleviton V. F. and family=Santos, given=Igor Ebrahim, prefix=dos, useprefix=true},
  date = {2014-06-01},
  journaltitle = {Empirical Software Engineering},
  shortjournal = {Empir Software Eng},
  volume = {19},
  number = {3},
  pages = {501--557},
  issn = {1573-7616},
  doi = {10.1007/s10664-012-9227-7},
  url = {https://doi.org/10.1007/s10664-012-9227-7},
  urldate = {2022-09-07},
  abstract = {In this article, we present a systematic mapping study of replications in software engineering. The goal is to plot the landscape of current published replications of empirical studies in software engineering research. We applied the systematic review method to search and select published articles, and to extract and synthesize data from the selected articles that reported replications. Our search retrieved more than 16,000 articles, from which we selected 96 articles, reporting 133 replications performed between 1994 and 2010, of 72 original studies. Nearly 70~\% of the replications were published after 2004 and 70~\% of these studies were internal replications. The topics of software requirements, software construction, and software quality concentrated over 55~\% of the replications, while software design, configuration management, and software tools and methods were the topics with the smallest number of replications. We conclude that the number of replications has grown in the last few years, but the absolute number of replications is still small, in particular considering the breadth of topics in software engineering. We still need incentives to perform external replications, better standards to report empirical studies and their replications, and collaborative research agendas that could speed up development and publication of replications.},
  langid = {english},
  keywords = {software engineering},
  annotation = {interest: 66},
  file = {/home/sam/Zotero/storage/5EWNKJJJ/da Silva et al. - 2014 - Replication of empirical studies in software engin.pdf}
}

@software{dataversecommunitydevelopersDataverse2023,
  title = {{{Dataverse}}®},
  author = {Dataverse community developers and Data Science {and} Products team at the Institute for Quantitative Social Science},
  date = {2023-04-21T15:02:16Z},
  origdate = {2013-11-01T18:47:39Z},
  url = {https://github.com/IQSS/dataverse},
  urldate = {2023-04-21},
  abstract = {Dataverse is an open source software platform for sharing, finding, citing, and preserving research data (developed by the Data Science and Products team at the Institute for Quantitative Social Science and the Dataverse community). dataverse.org is our home on the web and shows a map of Dataverse installations around the world, a list of features, integrations that have been made possible through REST APIs, our development roadmap, and more. We maintain a demo site at demo.dataverse.org which you are welcome to use for testing and evaluating Dataverse. To install Dataverse, please see our Installation Guide which will prompt you to download our latest release. To discuss Dataverse with the community, please join our mailing list, participate in a community call, chat with us at chat.dataverse.org, or attend our annual Dataverse Community Meeting. We love contributors! Please see our Contributing Guide for ways you can help. Dataverse is a trademark of President and Fellows of Harvard College and is registered in the United States.},
  organization = {Institute for Quantitative Social Science}
}

@online{DataVersionControl,
  title = {Data {{Version Control}} · {{DVC}}},
  url = {https://dvc.org/},
  urldate = {2022-09-06},
  abstract = {Open-source version control system for Data Science and Machine Learning projects. Git-like experience to organize your data, models, and experiments.},
  langid = {english},
  organization = {Data Version Control · DVC},
  keywords = {software,software publishing},
  annotation = {interest: 68},
  file = {/home/sam/Zotero/storage/YA8M6ILC/dvc.org.html}
}

@online{dattaPerformanceEnhancementCustomer2020,
  type = {SSRN Scholarly Paper},
  title = {Performance {{Enhancement}} of {{Customer Segmentation Using}} a {{Distributed Python Framework}}, {{Ray}}},
  author = {Datta, Debajit and Agarwal, Rishav and David, Preetha Evangeline},
  date = {2020-11-20},
  number = {3733832},
  location = {Rochester, NY},
  url = {https://papers.ssrn.com/abstract=3733832},
  urldate = {2022-11-15},
  abstract = {Over the years, there has been a huge popularity of the recommender systems worldwide. Recommender systems have been implemented over several domains ranging from recommendations for videos and movies to that for products and applications, and many more. The algorithms, which are used for recommender systems, implement segmentation of the customer based on several attributes. These algorithms are time-consuming and require comparatively high computation power. This work deals with the parallelization of different algorithms for simple customer segmentation in the Python environment using the framework, Ray. The dataset for this work includes a huge list of purchases that are carried out by 4000 customers, over a year. The parallelization is carried out throughout the multicores of CPU and the cores of GPU. Additionally, the work also shows the speedup that is obtained after parallelization, for analyzing the overall increase in performance.},
  langid = {english},
  pubstate = {prepublished},
  annotation = {interest: 75},
  file = {/home/sam/Zotero/storage/PEWWFTGA/Datta et al. - 2020 - Performance Enhancement of Customer Segmentation U.pdf;/home/sam/Zotero/storage/SRZI56N5/papers.html}
}

@inproceedings{davidsonProvenanceScientificWorkflows2008,
  title = {Provenance and Scientific Workflows: Challenges and Opportunities},
  shorttitle = {Provenance and Scientific Workflows},
  booktitle = {Proceedings of the 2008 {{ACM SIGMOD}} International Conference on {{Management}} of Data  - {{SIGMOD}} '08},
  author = {Davidson, Susan B. and Freire, Juliana},
  date = {2008},
  pages = {1345},
  publisher = {ACM Press},
  location = {Vancouver, Canada},
  doi = {10.1145/1376616.1376772},
  url = {http://portal.acm.org/citation.cfm?doid=1376616.1376772},
  urldate = {2022-07-07},
  abstract = {Provenance in the context of workflows, both for the data they derive and for their specification, is an essential component to allow for result reproducibility, sharing, and knowledge re-use in the scientific community. Several workshops have been held on the topic, and it has been the focus of many research projects and prototype systems. This tutorial provides an overview of research issues in provenance for scientific workflows, with a focus on recent literature and technology in this area. It is aimed at a general database research audience and at people who work with scientific data and workflows. We will (1) provide a general overview of scientific workflows, (2) describe research on provenance for scientific workflows and show in detail how provenance is supported in existing systems; (3) discuss emerging applications that are enabled by provenance; and (4) outline open problems and new directions for database-related research.},
  eventtitle = {The 2008 {{ACM SIGMOD}} International Conference},
  isbn = {978-1-60558-102-6},
  langid = {english},
  keywords = {provenance,workflow managers}
}

@article{davisonAutomatedCaptureExperiment2012,
  title = {Automated {{Capture}} of {{Experiment Context}} for {{Easier Reproducibility}} in {{Computational Research}}},
  author = {Davison, Andrew},
  date = {2012-07},
  journaltitle = {Computing in Science \& Engineering},
  shortjournal = {Comput. Sci. Eng.},
  volume = {14},
  number = {4},
  pages = {48--56},
  issn = {1521-9615},
  doi = {10.1109/MCSE.2012.41},
  url = {http://ieeexplore.ieee.org/document/6180156/},
  urldate = {2022-07-08},
  abstract = {Published scientific research that relies on numerical computations is too often not reproducible. For computational research to become consistently and reliably reproducible, the process must become easier to achieve, as part of day-to-day research. A combination of best practices and automated tools can make it easier to create reproducible research.},
  keywords = {provenance,reproducibility engineering},
  file = {/home/sam/Zotero/storage/VBIVFYVD/Davison - 2012 - Automated Capture of Experiment Context for Easier.pdf}
}

@article{davisPerceivedUsefulnessPerceived1989,
  title = {Perceived {{Usefulness}}, {{Perceived Ease}} of {{Use}}, and {{User Acceptance}} of {{Information Technology}}},
  author = {Davis, Fred D.},
  date = {1989-09},
  journaltitle = {MIS Quarterly},
  shortjournal = {MIS Quarterly},
  volume = {13},
  number = {3},
  eprint = {249008},
  eprinttype = {jstor},
  pages = {319},
  issn = {02767783},
  doi = {10.2307/249008},
  url = {https://www.jstor.org/stable/249008?origin=crossref},
  urldate = {2022-05-27},
  abstract = {Valid measurement scales for predicting user acceptance of computers are in short supply. Most subjective measures used in practice are unvalidated, and their relationship to system usage is unknown. The present research develops and validates new scales for two specific variables, perceived usefulness and perceived ease of use, which are hypothesized to be fundamental determinants of user acceptance. Definitions for these two variables were used to develop scale items that were pretested for content validity and then tested for reliability and construct validity in two studies involving a total of 152 users and four application programs. The measures were refined and stream-lined, resulting in two six-item scales with reliabilities of.98 for usefulness and.94 for ease of use. The scales exhibited high convergent, discriminant, and factorial validity. Perceived usefulness was significantly correlated with both self-reported current usage (r=.63, Study 1) and self-predicted future usage (r=.85, Study 2). Perceived ease of use was also significantly correlated with current usage (r=.45, Study 1) and future usage (r=.59, Study 2). In both studies, usefulness had a significantly greater correlation with usage behavior than did ease of use. Regression analyses suggest that perceived ease of use may actually be a causal antecedent to perceived usefulness, as opposed to a parallel, direct determinant of system usage. Implications are drawn for future research on user acceptance.},
  keywords = {internship-project,technology-acceptance},
  file = {/home/sam/Zotero/storage/K54E6BJV/249008.pdf}
}

@thesis{davisTechnologyAcceptanceModel1985,
  type = {Thesis},
  title = {A Technology Acceptance Model for Empirically Testing New End-User Information Systems: Theory and Results},
  shorttitle = {A Technology Acceptance Model for Empirically Testing New End-User Information Systems},
  author = {Davis, Fred D.},
  date = {1985},
  institution = {Massachusetts Institute of Technology},
  url = {https://dspace.mit.edu/handle/1721.1/15192},
  urldate = {2022-06-03},
  abstract = {The goal of this research is to develop and test a theoretical model of the effect of system characteristics on user acceptance of computer-based information systems. The model, referred to as the technology acceptance model (TAM), is being developed with two major objectives in mind. First, it should improve our understanding of user acceptance processes, providing new theoretical insights into the successful design and implementation of information systems. Second, TAM should provide the theoretical basis for a practical "user acceptance testing" methodology that would enable system designers and implementors to evaluate proposed new systems prior to their implementation. Applying the proposed model in user acceptance testing would involve demonstrating system prototypes to potential users and measuring their motivation to use the alternative systems. Such user acceptance testing could provide useful information about the relative likelihood of success of proposed systems early in their development, where such information has greatest value. Based on these objectives, key questions guiding this research include: 1. What are the major motivational variables that mediate between system characteristics and actual use of computer-based systems by end-users in organiza ional settings? 2. How are these variables causally related to one another, to system characteristics, and to user behavior? 3. How can user motivation be measured prior to organizational implementation in order to evaluate the rebtive likelihood of user acceptance for proposed new systems? For user acceptance testing to be viable, the associated model of user motivation must be valid. The present research takes several steps toward establishing a valid motivational model of the user, and aims to provide the foundation for future research that will tend to lead toward this end. Research steps taken in the present thesis include: 1. a fairly general, well-established theoretical model of human behavior from psychology was chosen as a paradigm within which to formulate the proposed technology acceptance model; 2. several adaptations to this paradigm were introduced in order to render it applicable to the present context; 3. published literature in the Management Information Systems and Human Factors fields was reviewed to demonstrate that empirical support exists for various elements of the proposed model, while at the same time the model goes beyond existing theoretical specifications, building upon and integrating previous research in a cumulative manner; 4. measures for the model's psychological variables were developed and pre-tested; 5. a field survey of 100 organizational users was conducted in order to validate the measures of the model's variables, and to test the model's structure, and 6. a laboratory user acceptance experiment of two business graphics systems involving 40 MBA student subjects was performed to further test the proposed model's structure, to test the ability to substitute videotape presentation for hands-on interaction in user acceptance tests, to evaluate the specific graphics systems being tested, and to test several theoretical extensions and refinements to the propose},
  langid = {english},
  keywords = {internship-project,project-provenance-pp,technology-acceptance},
  file = {/home/sam/Zotero/storage/VIZ75ECA/14927137-MIT.pdf}
}

@article{deelmanFutureScientificWorkflows2018,
  title = {The Future of Scientific Workflows},
  author = {Deelman, Ewa and Peterka, Tom and Altintas, Ilkay and Carothers, Christopher D and family=Dam, given=Kerstin Kleese, prefix=van, useprefix=true and Moreland, Kenneth and Parashar, Manish and Ramakrishnan, Lavanya and Taufer, Michela and Vetter, Jeffrey},
  date = {2018-01-01},
  journaltitle = {The International Journal of High Performance Computing Applications},
  shortjournal = {The International Journal of High Performance Computing Applications},
  volume = {32},
  number = {1},
  pages = {159--175},
  publisher = {SAGE Publications Ltd STM},
  issn = {1094-3420, 1741-2846},
  doi = {10.1177/1094342017704893},
  url = {http://journals.sagepub.com/doi/10.1177/1094342017704893},
  urldate = {2022-06-28},
  abstract = {Today’s computational, experimental, and observational sciences rely on computations that involve many related tasks. The success of a scientific mission often hinges on the computer automation of these workflows. In April 2015, the US Department of Energy (DOE) invited a diverse group of domain and computer scientists from national laboratories supported by the Office of Science, the National Nuclear Security Administration, from industry, and from academia to review the workflow requirements of DOE’s science and national security missions, to assess the current state of the art in science workflows, to understand the impact of emerging extreme-scale computing systems on those workflows, and to develop requirements for automated workflow management in future and existing environments. This article is a summary of the opinions of over 50 leading researchers attending this workshop. We highlight use cases, computing systems, workflow needs and conclude by summarizing the remaining challenges this community sees that inhibit large-scale scientific workflows from becoming a mainstream tool for extreme-scale science.},
  langid = {english},
  keywords = {project-acm-rep,workflow managers},
  file = {/home/sam/Zotero/storage/UTFN4KSI/Deelman et al. - 2018 - The future of scientific workflows.pdf}
}

@online{DefmacroNatureLisp,
  title = {Defmacro - {{The Nature}} of {{Lisp}}},
  url = {https://www.defmacro.org/ramblings/lisp.html},
  urldate = {2023-09-11},
  keywords = {programming languages}
}

@online{demorlhonDockerHubImage2020,
  title = {Docker {{Hub Image Retention Policy Delayed}}, {{Subscription Updates}}},
  author = {family=Morlhon, given=Jean-Laurent, prefix=de, useprefix=true},
  date = {2020-10-22},
  url = {https://www.docker.com/blog/docker-hub-image-retention-policy-delayed-and-subscription-updates/},
  urldate = {2024-09-04},
  organization = {Docker Blog}
}

@online{demorlhonScalingDockersBusiness2020,
  title = {Scaling {{Docker}}’s {{Business}} to {{Serve Millions More Developers}}: {{Storage}}},
  author = {family=Morlhon, given=Jean-Laurent, prefix=de, useprefix=true},
  date = {2020-08-24},
  url = {https://www.docker.com/blog/scaling-dockers-business-to-serve-millions-more-developers-storage/},
  urldate = {2024-09-04},
  organization = {Docker Blog}
}

@article{derossoPurposesConceptsMisfits2016,
  title = {Purposes, Concepts, Misfits, and a Redesign of Git},
  author = {De Rosso, Santiago Perez and Jackson, Daniel},
  date = {2016-10-19},
  journaltitle = {ACM SIGPLAN Notices},
  shortjournal = {SIGPLAN Not.},
  volume = {51},
  number = {10},
  pages = {292--310},
  issn = {0362-1340},
  doi = {10.1145/3022671.2984018},
  url = {https://dl.acm.org/doi/10.1145/3022671.2984018},
  urldate = {2023-07-07},
  abstract = {Git is a widely used version control system that is powerful but complicated. Its complexity may not be an inevitable consequence of its power but rather evidence of flaws in its design. To explore this hypothesis, we analyzed the design of Git using a theory that identifies concepts, purposes, and misfits. Some well-known difficulties with Git are described, and explained as misfits in which underlying concepts fail to meet their intended purpose. Based on this analysis, we designed a reworking of Git (called Gitless) that attempts to remedy these flaws. To correlate misfits with issues reported by users, we conducted a study of Stack Overflow questions. And to determine whether users experienced fewer complications using Gitless in place of Git, we conducted a small user study. Results suggest our approach can be profitable in identifying, analyzing, and fixing design problems.},
  keywords = {software engineering},
  file = {/home/sam/Zotero/storage/P83A7M86/De Rosso and Jackson - 2016 - Purposes, concepts, misfits, and a redesign of git.pdf}
}

@inproceedings{deroureComputationalResearchObjects2013,
  title = {Towards Computational Research Objects},
  booktitle = {Proceedings of the 1st {{International Workshop}} on {{Digital Preservation}} of {{Research Methods}} and {{Artefacts}}},
  author = {De Roure, David},
  date = {2013-07-25},
  series = {{{DPRMA}} '13},
  pages = {16--19},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2499583.2499590},
  url = {https://doi.org/10.1145/2499583.2499590},
  urldate = {2023-06-28},
  abstract = {Research Objects are bundles of the digital bits and pieces that make up the reusable record of a piece of research; they are identifiable, citable and sharable. The evolution of this idea within digital research practice has led to the development of workflow-centric Research Objects with executable components. To address the evolving requirements of research we propose a further step, towards objects that are composable and executable by machine: Computational Research Objects -- a vision in which the content of our digital libraries is autonomously conducting research.},
  isbn = {978-1-4503-2185-3},
  keywords = {provenance}
}

@online{desnoyersUsingLinuxKernel,
  title = {Using the {{Linux Kernel Tracepoints}}},
  author = {Desnoyers, Matthieu},
  url = {https://www.kernel.org/doc/html/latest/trace/tracepoints.html},
  urldate = {2023-08-24},
  organization = {The Linux Kernel documentation},
  keywords = {operating systems,project-provenance-pp},
  file = {/home/sam/Zotero/storage/VCQP6JNQ/tracepoints.html}
}

@inproceedings{dicosmoCuratedArchivingResearch2020,
  title = {Curated {{Archiving}} of {{Research Software Artifacts}} : Lessons Learned from the {{French}} Open Archive ({{HAL}})},
  shorttitle = {Curated {{Archiving}} of {{Research Software Artifacts}}},
  booktitle = {{{IDCC}} 2020 - {{International Digital Curation Conference}}},
  author = {Di Cosmo, Roberto and Gruenpeter, Morane and Marmol, Bruno P and Monteil, Alain and Romary, Laurent and Sadowska, Jozefina},
  date = {2020-02},
  location = {Dublin, Ireland},
  doi = {10.2218/ijdc.v15i1.698},
  url = {https://hal.inria.fr/hal-02475835},
  urldate = {2022-09-06},
  abstract = {Software has become an indissociable support of technical and scientific knowledge. The preservation of this universal body of knowledge is as essential as preserving research articles and data sets. In the quest to make scientific results reproducible, and pass knowledge to future generations, we must preserve these three main pillars: research articles that describe the results, the data sets used or produced, and the software that embodies the logic of the data transformation. The collaboration between Software Heritage (SWH), the Center for Direct Scientific Communication (CCSD) and the scientific and technical information services (IES) of The French Institute for Research in Computer Science and Automation (Inria)  has resulted in a specified moderation and curation workflow for research software artifacts deposited in the HAL  open access repository. The curation workflow was developed to help digital librarians and archivists handle this new and peculiar artifact-software source code. While implementing the workflow, a set of guidelines has emerged from the challenges and the solutions put in place to help all actors involved in the process.},
  annotation = {interest: 84},
  file = {/home/sam/Zotero/storage/8TTTKRM3/Di Cosmo et al. - 2020 - Curated Archiving of Research Software Artifacts .pdf}
}

@inproceedings{diFastErrorBoundedLossy2016,
  title = {Fast {{Error-Bounded Lossy HPC Data Compression}} with {{SZ}}},
  booktitle = {2016 {{IEEE International Parallel}} and {{Distributed Processing Symposium}} ({{IPDPS}})},
  author = {Di, Sheng and Cappello, Franck},
  date = {2016-05},
  pages = {730--739},
  issn = {1530-2075},
  doi = {10.1109/IPDPS.2016.11},
  abstract = {Today's HPC applications are producing extremely large amounts of data, thus it is necessary to use an efficient compression before storing them to parallel file systems. In this paper, we optimize the error-bounded HPC data compression, by proposing a novel HPC data compression method that works very effectively on compressing large-scale HPC data sets. The compression method starts by linearizing multi-dimensional snapshot data. The key idea is to fit/predict the successive data points with the bestfit selection of curve fitting models. The data that can be predicted precisely will be replaced by the code of the corresponding curve-fitting model. As for the unpredictable data that cannot be approximated by curve-fitting models, we perform an optimized lossy compression via a binary representation analysis. We evaluate our proposed solution using 13 real-world HPC applications across different scientific domains, and compare it to many other state-of-the-art compression methods (including Gzip, FPC, ISABELA, NUMARCK, ZFP, FPZIP, etc.). Experiments show that the compression ratio of our compressor ranges in 3.3/1 - 436/1, which is higher than the second-best solution ZFP by as little as 2x and as much as an order of magnitude for most cases. The compression time of SZ is comparable to other solutions', while its decompression time is less than the second best one by 50\%-90\%. On an extreme-scale use case, experiments show that the compression ratio of SZ exceeds that of ZFP by 80\%.},
  eventtitle = {2016 {{IEEE International Parallel}} and {{Distributed Processing Symposium}} ({{IPDPS}})},
  keywords = {high-performance computing,hpc,numerical methods},
  annotation = {interest: 70},
  file = {/home/sam/Zotero/storage/C2HE268T/Di and Cappello - 2016 - Fast Error-Bounded Lossy HPC Data Compression with.pdf;/home/sam/Zotero/storage/VW9K3H2L/7516069.html}
}

@inproceedings{dingSameAsNetworksAnalyzing2010,
  title = {{{SameAs Networks}} and {{Beyond}}: {{Analyzing Deployment Status}} and {{Implications}} of Owl:{{sameAs}} in {{Linked Data}}},
  shorttitle = {{{SameAs Networks}} and {{Beyond}}},
  booktitle = {The {{Semantic Web}} – {{ISWC}} 2010},
  author = {Ding, Li and Shinavier, Joshua and Shangguan, Zhenning and McGuinness, Deborah L.},
  editor = {Patel-Schneider, Peter F. and Pan, Yue and Hitzler, Pascal and Mika, Peter and Zhang, Lei and Pan, Jeff Z. and Horrocks, Ian and Glimm, Birte},
  date = {2010},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {145--160},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-17746-0_10},
  abstract = {Millions of owl:sameAs statements have been published on the Web of Data. Due to its unique role and heavy usage in Linked Data integration, owl:sameAs has become a topic of increasing interest and debate. This paper provides a quantitative analysis of owl:sameAs deployment status and uses these statistics to focus discussion around its usage in Linked Data.},
  isbn = {978-3-642-17746-0},
  langid = {english},
  keywords = {semantic web},
  file = {/home/sam/Zotero/storage/4AV37RWC/Ding et al. - 2010 - SameAs Networks and Beyond Analyzing Deployment S.pdf}
}

@article{dinhStatisticalAssertionMore2014,
  title = {Statistical Assertion: {{A}} More Powerful Method for Debugging Scientific Applications},
  shorttitle = {Statistical Assertion},
  author = {Dinh, Minh Ngoc and Abramson, David and Jin, Chao},
  date = {2014-03-01},
  journaltitle = {Journal of Computational Science},
  shortjournal = {Journal of Computational Science},
  series = {Empowering {{Science}} through {{Computing}} + {{BioInspired Computing}}},
  volume = {5},
  number = {2},
  pages = {126--134},
  issn = {1877-7503},
  doi = {10.1016/j.jocs.2013.12.002},
  url = {https://www.sciencedirect.com/science/article/pii/S1877750313001415},
  urldate = {2022-10-18},
  abstract = {Traditional debuggers are of limited value for modern scientific codes that manipulate large complex data structures. Current parallel machines make this even more complicated, because the data structure may be distributed across processors, making it difficult to view/interpret and validate its contents. Therefore, many applications’ developers resort to placing validation code directly in the source program. This paper discusses a novel debug-time assertion, called a “Statistical Assertion”, that allows using extracted statistics instead of raw data to reason about large data structures, therefore help locating coding defects. In this paper, we present the design and implementation of an ‘extendable’ statistical-framework which executes the assertion in parallel by exploiting the underlying parallel system. We illustrate the debugging technique with a molecular dynamics simulation. The performance is evaluated on a 20,000 processor Cray XE6 to show that it is useful for real-time debugging.},
  langid = {english},
  keywords = {software testing},
  annotation = {interest: 98},
  file = {/home/sam/Zotero/storage/W887Q824/Dinh et al. - 2014 - Statistical assertion A more powerful method for .pdf;/home/sam/Zotero/storage/9WJYX4WN/S1877750313001415.html}
}

@article{ditommasoNextflowEnablesReproducible2017,
  title = {Nextflow Enables Reproducible Computational Workflows},
  author = {Di Tommaso, Paolo and Chatzou, Maria and Floden, Evan W. and Barja, Pablo Prieto and Palumbo, Emilio and Notredame, Cedric},
  date = {2017-04},
  journaltitle = {Nature Biotechnology},
  shortjournal = {Nat Biotechnol},
  volume = {35},
  number = {4},
  pages = {316--319},
  publisher = {Nature Publishing Group},
  issn = {1546-1696},
  doi = {10.1038/nbt.3820},
  url = {https://www.nature.com/articles/nbt.3820},
  urldate = {2023-01-30},
  issue = {4},
  langid = {english},
  keywords = {project-acm-rep,reproducibility engineering,research software engineering,workflow managers},
  file = {/home/sam/Zotero/storage/C4D95KCA/Di Tommaso et al. - 2017 - Nextflow enables reproducible computational workfl.pdf}
}

@online{doctorowMetacrap2001,
  title = {Metacrap},
  author = {Doctorow, Cory},
  date = {2001-08-26},
  url = {https://people.well.com/user/doctorow/metacrap.htm},
  urldate = {2023-06-09},
  abstract = {A world of exhaustive, reliable metadata would be a utopia. It's also a pipe-dream, founded on self-delusion, nerd hubris and hysterically inflated market opportunities.},
  keywords = {semantic web,world wide web},
  file = {/home/sam/Zotero/storage/PEU65B7S/metacrap.html}
}

@online{doctorowPluralisticIfBuying,
  title = {Pluralistic: “{{If}} Buying Isn’t Owning, Piracy Isn’t Stealing” (08 {{Dec}} 2023) – {{Pluralistic}}: {{Daily}} Links from {{Cory Doctorow}}},
  author = {Doctorow, Corey},
  url = {https://pluralistic.net/2023/12/08/playstationed/#tyler-james-hill},
  urldate = {2023-12-13},
  file = {/home/sam/Zotero/storage/2UUNDJN7/playstationed.html}
}

@thesis{dolstraPurelyFunctionalSoftware2006,
  type = {phdthesis},
  title = {The {{Purely Functional Software Deployment Model}}},
  author = {Dolstra, Eelco},
  date = {2006-01-18},
  institution = {Utrecht University},
  url = {https://dspace.library.uu.nl/handle/1874/7540},
  urldate = {2023-08-24},
  abstract = {Software deployment is the set of activities related to getting software components to work on the machines of end users. It includes activities such as installation, upgrading, uninstallation, and so on. Many tools have been developed to support deployment, but they all have serious limitations with respect to correctness. For instance, the installation of a component can lead to the failure of previously installed components; a component might require other components that are not present; and it is generally difficult to undo deployment actions. The fundamental causes of these problems are a lack of isolation between components, the difficulty in identifying the dependencies between components, and incompatibilities between versions and variants of components. This thesis describes a better approach based on a purely functional deployment model, implemented in a deployment system called Nix. Components are stored in isolation from each other in a Nix store. Each component has a name that contains a cryptographic hash of all inputs that contributed to its build process, and the content of a component never changes after it has been built. Hence the model is purely functional. This storage scheme provides several important advantages. First, it ensures isolation between components: if two components differ in any way, they will be stored in different locations and will not overwrite each other. Second, it allows us to identify component dependencies. Undeclared build time dependencies are prevented due to the absence of "global" component directories used in other deployment systems. Runtime dependencies can be found by scanning for cryptographic hashes in the binary contents of components, a technique analogous to conservative garbage collection in programming language implementation. Since dependency information is complete, complete deployment can be performed by copying closures of components under the dependency relation. Developers and users are not confronted with components' cryptographic hashes directly. Components are built automatically from Nix expressions, which describe how to build and compose arbitrary software components; hashes are computed as part of this process. Components are automatically made available to users through "user environments", which are synthesised sets of activated components. User environments enable atomic upgrades and rollbacks, as well as different sets of activated components for different users. Nix expressions provide a source-based deployment model. However, source-based deployment can be transparently optimised into binary deployment by making pre-built binaries (keyed on their cryptographic hashes) available in a shared location such as a network server. This is referred to as transparent source/binary deployment. The purely functional deployment model has been validated by applying it to the deployment of more than 278 existing Unix packages. In addition, this thesis shows that the model can be applied naturally to the related activities of continuous integration using build farms, service deployment and build management.},
  langid = {english},
  keywords = {operating systems,project-provenance-pp}
}

@inproceedings{dolstraSecureSharingUntrusted2005,
  title = {Secure Sharing between Untrusted Users in a Transparent Source/Binary Deployment Model},
  booktitle = {Proceedings of the 20th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}}},
  author = {Dolstra, Eelco},
  date = {2005-11-07},
  series = {{{ASE}} '05},
  pages = {154--163},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1101908.1101933},
  url = {https://dl.acm.org/doi/10.1145/1101908.1101933},
  urldate = {2023-08-24},
  abstract = {The Nix software deployment system is based on the paradigm of transparent source/binary deployment: distributors deploy descriptors that build components from source, while client machines can transparently optimise such source builds by downloading pre-built binaries from remote repositories. This model combines the simplicity and flexibility of source deployment with the efficiency of binary deployment. A desirable property is sharing of components: if multiple users install from the same source descriptors, ideally only one remotely built binary should be installed. The problem is that users must trust that remotely downloaded binaries were built from the sources they are claimed to have been built from, while users in general do not have a trust relation with each other or with the same remote repositories.This paper presents three models that enable sharing: the extensional model that requires that all users on a system have the same remote trust relations, the intensional model that does not have this requirement but may be suboptimal in terms of space use, and the mixed model that merges the best properties of both. The latter two models are achieved through a novel technique of hash rewriting in content-addressable component stores, and were implemented in the context of the Nix system.},
  isbn = {978-1-58113-993-8},
  annotation = {interest: 99},
  file = {/home/sam/Zotero/storage/ZCADFPP6/Dolstra - 2005 - Secure sharing between untrusted users in a transp.pdf}
}

@inproceedings{dongSRRTARegressionTesting2020,
  title = {{{SRRTA}}: Regression Testing Acceleration via State Reuse},
  shorttitle = {{{SRRTA}}},
  booktitle = {Proceedings of the 35th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}}},
  author = {Dong, Jinhao and Lou, Yiling and Hao, Dan},
  date = {2020-12-21},
  series = {{{ASE}} '20},
  pages = {1244--1248},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3324884.3418928},
  url = {https://doi.org/10.1145/3324884.3418928},
  urldate = {2022-04-07},
  abstract = {Regression testing is widely recognized as an important but time-consuming process. To alleviate this cost issue, test selection, reduction, and prioritization have been widely studied, and they share the commonality that they improve regression testing by optimizing the execution of the whole test suite. In this paper, we attempt to accelerate regression testing from a totally new perspective, i.e., skipping some execution of a new program by reusing program states of an old program. Following this intuition, we propose a state-reuse based acceleration approach SRRTA, consisting of two components: state storage and loading. With the former, SRRTA collects some program states during the execution of an old version through three heuristic-based storage strategies; with the latter, SRRTA loads the stored program states with efficiency optimization strategies. Through the preliminary study on commons-math, SRRTA reduces 82.7\% of the regression testing time.},
  isbn = {978-1-4503-6768-4},
  keywords = {software engineering,software testing},
  annotation = {score: 60\\
interest: 50},
  file = {/home/sam/Zotero/storage/3VRK73SG/Dong et al. - 2020 - SRRTA regression testing acceleration via state r.pdf}
}

@article{donohoReproducibleResearchComputational2009,
  title = {Reproducible {{Research}} in {{Computational Harmonic Analysis}}},
  author = {Donoho, David L. and Maleki, Arian and Rahman, Inam Ur and Shahram, Morteza and Stodden, Victoria},
  date = {2009-01},
  journaltitle = {Computing in Science \& Engineering},
  volume = {11},
  number = {1},
  pages = {8--18},
  issn = {1558-366X},
  doi = {10.1109/MCSE.2009.15},
  abstract = {Scientific computation is emerging as absolutely central to the scientific method. Unfortunately, it's error-prone and currently immature—traditional scientific publication is incapable of finding and rooting out errors in scientific computation—which must be recognized as a crisis. An important recent development and a necessary response to the crisis is reproducible computational research in which researchers publish the article along with the full computational environment that produces the results. The authors have practiced reproducible computational research for 15 years and have integrated it with their scientific research and with doctoral and postdoctoral education. In this article, they review their approach and how it has evolved over time, discussing the arguments for and against working reproducibly.},
  eventtitle = {Computing in {{Science}} \& {{Engineering}}},
  annotation = {interest: 93},
  file = {/home/sam/Zotero/storage/5GFM2UT3/Donoho et al. - 2009 - Reproducible Research in Computational Harmonic An.pdf;/home/sam/Zotero/storage/IJCLRR7V/4720218.html}
}

@online{douglasthainTechniquesPreservingScientific2015,
  title = {Techniques for {{Preserving Scientific Software Executions}}: {{Preserve}} the {{Mess}} or {{Encourage Cleanliness}}?},
  shorttitle = {Techniques for {{Preserving Scientific Software Executions}}},
  author = {Douglas Thain, Peter Ivie},
  date = {2015},
  eprinttype = {University of Notre Dame},
  doi = {10.7274/R0CZ353M},
  url = {https://curate.nd.edu/show/n009w091d9n},
  urldate = {2024-10-05},
  pubstate = {prepublished}
}

@online{drepperHowWriteShared2011,
  title = {How {{To Write Shared Libraries}}},
  author = {Drepper, Ulrich},
  date = {2011-12-10},
  url = {https://cs.dartmouth.edu/~sergey/cs258/ABI/UlrichDrepper-How-To-Write-Shared-Libraries.pdf},
  abstract = {Today, shared libraries are ubiquitous. Developers use them for multiple reasons and create them just as they would create application code. This is a problem, though, since on many platforms some additional techniques must be applied even to generate decent code. Even more knowledge is needed to generate optimized code. This paper introduces the required rules and techniques. In addition, it introduces the concept of ABI (Application Binary Interface) stability and shows how to manage it.},
  langid = {english},
  file = {/home/sam/Zotero/storage/X4HV334Q/Drepper - How To Write Shared Libraries.pdf}
}

@online{druskatSoftwarePublicationsRich2022,
  title = {Software Publications with Rich Metadata: State of the Art, Automated Workflows and {{HERMES}} Concept},
  shorttitle = {Software Publications with Rich Metadata},
  author = {Druskat, Stephan and Bertuch, Oliver and Juckeland, Guido and Knodel, Oliver and Schlauch, Tobias},
  date = {2022-01-22},
  eprint = {2201.09015},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2201.09015},
  url = {http://arxiv.org/abs/2201.09015},
  urldate = {2022-09-06},
  abstract = {To satisfy the principles of FAIR software, software sustainability and software citation, research software must be formally published. Publication repositories make this possible and provide published software versions with unique and persistent identifiers. However, software publication is still a tedious, mostly manual process. To streamline software publication, HERMES, a project funded by the Helmholtz Metadata Collaboration, develops automated workflows to publish research software with rich metadata. The tooling developed by the project utilizes continuous integration solutions to retrieve, collate, and process existing metadata in source repositories, and publish them on publication repositories, including checks against existing metadata requirements. To accompany the tooling and enable researchers to easily reuse it, the project also provides comprehensive documentation and templates for widely used CI solutions. In this paper, we outline the concept for these workflows, and describe how our solution advance the state of the art in research software publication.},
  pubstate = {prepublished},
  keywords = {research software engineering,software publishing},
  annotation = {interest: 74},
  file = {/home/sam/Zotero/storage/M8THID3G/Druskat et al. - 2022 - Software publications with rich metadata state of.pdf;/home/sam/Zotero/storage/NJUMTHIV/2201.html}
}

@online{DTrace,
  title = {About {{DTrace}}},
  url = {http://dtrace.org/blogs/about/},
  urldate = {2023-08-23},
  langid = {english},
  keywords = {operating systems,project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/TDNCI4W3/about.html}
}

@unpublished{duffRcPlanShell,
  title = {Rc -- {{The Plan}} 9 {{Shell}}},
  author = {Duff, Tom},
  abstract = {Rc is a command interpreter for Plan 9 that provides similar facilities to UNIX's Bourne shell, with some small additions and less idiosyncratic syntax. This paper uses numerous examples to describe rc's features, and contrasts rc with the Bourne shell, a model that many readers will be familiar wit},
  keywords = {operating systems},
  file = {/home/sam/Zotero/storage/H9HAYALK/rc.pdf}
}

@inproceedings{duttaFLEXFixingFlaky2021,
  title = {{{FLEX}}: Fixing Flaky Tests in Machine Learning Projects by Updating Assertion Bounds},
  shorttitle = {{{FLEX}}},
  booktitle = {Proceedings of the 29th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Dutta, Saikat and Shi, August and Misailovic, Sasa},
  date = {2021-08-20},
  series = {{{ESEC}}/{{FSE}} 2021},
  pages = {603--614},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3468264.3468615},
  url = {https://doi.org/10.1145/3468264.3468615},
  urldate = {2022-10-13},
  abstract = {Many machine learning (ML) algorithms are inherently random – multiple executions using the same inputs may produce slightly different results each time. Randomness impacts how developers write tests that check for end-to-end quality of their implementations of these ML algorithms. In particular, selecting the proper thresholds for comparing obtained quality metrics with the reference results is a non-intuitive task, which may lead to flaky test executions. We present FLEX, the first tool for automatically fixing flaky tests due to algorithmic randomness in ML algorithms. FLEX fixes tests that use approximate assertions to compare actual and expected values that represent the quality of the outputs of ML algorithms. We present a technique for systematically identifying the acceptable bound between the actual and expected output quality that also minimizes flakiness. Our technique is based on the Peak Over Threshold method from statistical Extreme Value Theory, which estimates the tail distribution of the output values observed from several runs. Based on the tail distribution, FLEX updates the bound used in the test, or selects the number of test re-runs, based on a desired confidence level. We evaluate FLEX on a corpus of 35 tests collected from the latest versions of 21 ML projects. Overall, FLEX identifies and proposes a fix for 28 tests. We sent 19 pull requests, each fixing one test, to the developers. So far, 9 have been accepted by the developers.},
  isbn = {978-1-4503-8562-6},
  keywords = {machine learning,reproducibility engineering,software testing},
  annotation = {interest: 97},
  file = {/home/sam/Zotero/storage/MVEFQYJY/Dutta et al. - 2021 - FLEX fixing flaky tests in machine learning proje.pdf}
}

@inproceedings{duttaSeedNotSeed2022,
  title = {To {{Seed}} or {{Not}} to {{Seed}}? {{An Empirical Analysis}} of {{Usage}} of {{Seeds}} for {{Testing}} in {{Machine Learning Projects}}},
  shorttitle = {To {{Seed}} or {{Not}} to {{Seed}}?},
  booktitle = {2022 {{IEEE Conference}} on {{Software Testing}}, {{Verification}} and {{Validation}} ({{ICST}})},
  author = {Dutta, Saikat and Arunachalam, Anshul and Misailovic, Sasa},
  date = {2022-04},
  pages = {151--161},
  issn = {2159-4848},
  doi = {10.1109/ICST53961.2022.00026},
  abstract = {Many Machine Learning (ML) algorithms are in-herently random in nature - executing them using the same inputs may lead to slightly different results across different runs. Such randomness makes it challenging for developers to write tests for their implementations of ML algorithms. A natural consequence of randomness is test flakiness - tests both pass and fail non-deterministically for same version of code. Developers often choose to alleviate test flakiness in ML projects by setting seeds in the random number generators used by the code under test. However, this approach commonly serves as a “workaround” rather than an actual solution. Instead, it may be possible to mitigate flakiness and alleviate the negative effects of setting seeds using alternative approaches. To understand the role of seeds and the feasibility of alternative solutions, we conduct the first large-scale empirical study of the usage of seeds and its implications on testing on a corpus of 114 Machine Learning projects. We identify 461 tests in these projects that fail without seeds and study their nature and root causes. We try to minimize the flakiness of a subset of 42 identified tests using alternative strategies such as tuning algorithm hyper-parameters and adjusting assertion bounds and send them to developers. So far, developers have accepted our fixes for 26 tests. We further manually analyze a subset of 56 tests and study various characteristics such as the nature of test oracles and how the seed settings evolve over time. Finally, we provide a general set of recommendations for both researchers and developers in the context of setting seeds in tests.},
  eventtitle = {2022 {{IEEE Conference}} on {{Software Testing}}, {{Verification}} and {{Validation}} ({{ICST}})},
  keywords = {machine learning,reproducibility engineering},
  annotation = {interest: 95},
  file = {/home/sam/Zotero/storage/FDTIC573/Dutta et al. - 2022 - To Seed or Not to Seed An Empirical Analysis of U.pdf;/home/sam/Zotero/storage/MDN2VNMI/9787895.html}
}

@online{dykstraApptainerSetuid2022,
  title = {Apptainer {{Without Setuid}}},
  author = {Dykstra, Dave},
  date = {2022-08-25},
  eprint = {2208.12106},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2208.12106},
  url = {http://arxiv.org/abs/2208.12106},
  urldate = {2023-02-18},
  abstract = {Apptainer (formerly known as Singularity) since its beginning implemented many of its container features with the assistance of a setuid-root program. It still supports that mode, but as of version 1.1.0 it no longer uses setuid by default. This is feasible because it now can mount squash filesystems, mount ext2/3/4 filesystems, and use overlayfs using unprivileged user namespaces and FUSE. It also now enables unprivileged users to build containers, even without requiring system administrators to configure /etc/subuid and /etc/subgid unlike other "rootless" container systems. As a result, all the unprivileged functions can be used nested inside of another container, even if the container runtime prevents any elevated privileges.},
  pubstate = {prepublished},
  keywords = {containers,operating systems,project-acm-rep,reproducibility engineering},
  file = {/home/sam/Zotero/storage/H2LSHPL3/Dykstra - 2022 - Apptainer Without Setuid.pdf;/home/sam/Zotero/storage/AME9T7TQ/2208.html}
}

@inproceedings{eilersNaginiStaticVerifier2018,
  title = {Nagini: {{A Static Verifier}} for {{Python}}},
  shorttitle = {Nagini},
  booktitle = {Computer {{Aided Verification}}},
  author = {Eilers, Marco and Müller, Peter},
  editor = {Chockler, Hana and Weissenbacher, Georg},
  date = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {596--603},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-96145-3_33},
  abstract = {We present Nagini, an automated, modular verifier for statically-typed, concurrent Python 3 programs, built on the Viper verification infrastructure. Combining established concepts with new ideas, Nagini can verify memory safety, functional properties, termination, deadlock freedom, and input/output behavior. Our experiments show that Nagini is able to verify non-trivial properties of real-world Python code.},
  isbn = {978-3-319-96145-3},
  langid = {english},
  keywords = {formal verification,programming languages},
  annotation = {interest: 92},
  file = {/home/sam/Zotero/storage/BTQF8UFN/Eilers and Müller - 2018 - Nagini A Static Verifier for Python.pdf}
}

@thesis{eistyQualityAssuranceResearch,
  type = {phdthesis},
  title = {Quality {{Assurance}} in {{Research Software}}},
  author = {Eisty, Nasir U.},
  institution = {The University of Alabama},
  location = {United States -- Alabama},
  url = {https://www.proquest.com/docview/2427487185/abstract/23F7DE57839B428APQ/1},
  urldate = {2022-10-11},
  abstract = {Breakthroughs in research increasingly depend on complex software libraries, tools, and applications aimed at supporting specific science, engineering, business, or humanities disciplines. Collectively, we call these software, libraries, tools, and applications as research software. Research software plays an important role in solving real-life problems, scientific innovations, and handling emergency situations. So the correctness and trustworthiness of research software are of absolute importance. The complexity and criticality of this software motivate the need for proper software quality assurance through different software engineering practices. Software metrics, software development process, peer code review, and software testing are four key tools for assessing, measuring, and ensuring software quality and reliability. The goal of this dissertation is to better understand how research software developers use traditional software engineering concepts of software quality to support and evaluate both the software and the software development process. One key aspect of this goal is to identify how the four quality practices relevant to research software corresponds to the practices commonly used in traditional software engineering. I used empirical software engineering research methods to study the human aspects related to using software quality practices for the development of research software. I collected information related to the four software activities through surveys, interviews, and directly working with research software developers. Research software developers appear to be interested and see value in software quality practices, but maybe encountering roadblocks when trying to use them. Through this dissertation, beside current practices, I identified challenges to use those quality practices and provided guidelines to overcome the challenges and to improve the current practices.},
  isbn = {9798662404960},
  langid = {english},
  pagetotal = {155},
  keywords = {research software engineering},
  file = {/home/sam/Zotero/storage/UN8YHHQI/Eisty - Quality Assurance in Research Software.pdf}
}

@inproceedings{eistySurveySoftwareMetric2018,
  title = {A {{Survey}} of {{Software Metric Use}} in {{Research Software Development}}},
  booktitle = {2018 {{IEEE}} 14th {{International Conference}} on E-{{Science}} (e-{{Science}})},
  author = {Eisty, Nasir U. and Thiruvathukal, George K. and Carver, Jeffrey C.},
  date = {2018-10},
  pages = {212--222},
  doi = {10.1109/eScience.2018.00036},
  url = {https://ieeexplore.ieee.org/abstract/document/8588655},
  urldate = {2024-04-17},
  abstract = {Background: Breakthroughs in research increasingly depend on complex software libraries, tools, and applications aimed at supporting specific science, engineering, business, or humanities disciplines. The complexity and criticality of this software motivate the need for ensuring quality and reliability. Software metrics are a key tool for assessing, measuring, and understanding software quality and reliability. Aims: The goal of this work is to better understand how research software developers use traditional software engineering concepts, like metrics, to support and evaluate both the software and the software development process. One key aspect of this goal is to identify how the set of metrics relevant to research software corresponds to the metrics commonly used in traditional software engineering. Method: We surveyed research software developers to gather information about their knowledge and use of code metrics and software process metrics. We also analyzed the influence of demographics (project size, development role, and development stage) on these metrics. Results: The survey results, from 129 respondents, indicate that respondents have a general knowledge of metrics. However, their knowledge of specific SE metrics is lacking, their use even more limited. The most used metrics relate to performance and testing. Even though code complexity often poses a significant challenge to research software development, respondents did not indicate much use of code metrics. Conclusions: Research software developers appear to be interested and see some value in software metrics but may be encountering roadblocks when trying to use them. Further study is needed to determine the extent to which these metrics could provide value in continuous process improvement.},
  eventtitle = {2018 {{IEEE}} 14th {{International Conference}} on E-{{Science}} (e-{{Science}})},
  keywords = {research software engineering,software engineering},
  file = {/home/sam/Zotero/storage/THGBBJ86/Eisty et al. - 2018 - A Survey of Software Metric Use in Research Softwa.pdf;/home/sam/Zotero/storage/XZHTI7GC/8588655.html}
}

@article{eklundClusterFailureWhy2016,
  title = {Cluster Failure: {{Why fMRI}} Inferences for Spatial Extent Have Inflated False-Positive Rates},
  shorttitle = {Cluster Failure},
  author = {Eklund, Anders and Nichols, Thomas E. and Knutsson, Hans},
  date = {2016-07-12},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {113},
  number = {28},
  pages = {7900--7905},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1602413113},
  url = {https://www.pnas.org/doi/full/10.1073/pnas.1602413113},
  urldate = {2023-02-23},
  abstract = {The most widely used task functional magnetic resonance imaging (fMRI) analyses use parametric statistical methods that depend on a variety of assumptions. In this work, we use real resting-state data and a total of 3 million random task group analyses to compute empirical familywise error rates for the fMRI software packages SPM, FSL, and AFNI, as well as a nonparametric permutation method. For a nominal familywise error rate of 5\%, the parametric statistical methods are shown to be conservative for voxelwise inference and invalid for clusterwise inference. Our results suggest that the principal cause of the invalid cluster inferences is spatial autocorrelation functions that do not follow the assumed Gaussian shape. By comparison, the nonparametric permutation test is found to produce nominal results for voxelwise as well as clusterwise inference. These findings speak to the need of validating the statistical methods being used in the field of neuroimaging.},
  keywords = {retraction},
  file = {/home/sam/Zotero/storage/8QTWDU5B/Eklund et al. - 2016 - Cluster failure Why fMRI inferences for spatial e.pdf}
}

@inproceedings{elbaumTechniquesImprovingRegression2014,
  title = {Techniques for Improving Regression Testing in Continuous Integration Development Environments},
  booktitle = {Proceedings of the 22nd {{ACM SIGSOFT International Symposium}} on {{Foundations}} of {{Software Engineering}}},
  author = {Elbaum, Sebastian and Rothermel, Gregg and Penix, John},
  date = {2014-11-11},
  series = {{{FSE}} 2014},
  pages = {235--245},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2635868.2635910},
  url = {https://doi.org/10.1145/2635868.2635910},
  urldate = {2022-04-10},
  abstract = {In continuous integration development environments, software engineers frequently integrate new or changed code with the mainline codebase. This can reduce the amount of code rework that is needed as systems evolve and speed up development time. While continuous integration processes traditionally require that extensive testing be performed following the actual submission of code to the codebase, it is also important to ensure that enough testing is performed prior to code submission to avoid breaking builds and delaying the fast feedback that makes continuous integration desirable. In this work, we present algorithms that make continuous integration processes more cost-effective. In an initial pre-submit phase of testing, developers specify modules to be tested, and we use regression test selection techniques to select a subset of the test suites for those modules that render that phase more cost-effective. In a subsequent post-submit phase of testing, where dependent modules as well as changed modules are tested, we use test case prioritization techniques to ensure that failures are reported more quickly. In both cases, the techniques we utilize are novel, involving algorithms that are relatively inexpensive and do not rely on code coverage information -- two requirements for conducting testing cost-effectively in this context. To evaluate our approach, we conducted an empirical study on a large data set from Google that we make publicly available. The results of our study show that our selection and prioritization techniques can each lead to cost-effectiveness improvements in the continuous integration process.},
  isbn = {978-1-4503-3056-5},
  keywords = {continuous integration,software engineering,software testing},
  file = {/home/sam/Zotero/storage/74V2844B/2635868.2635910.pdf}
}

@online{elsabbaghVortexOpenCLCompatible2020,
  title = {Vortex: {{OpenCL Compatible RISC-V GPGPU}}},
  shorttitle = {Vortex},
  author = {Elsabbagh, Fares and Tine, Blaise and Roshan, Priyadarshini and Lyons, Ethan and Kim, Euna and Shim, Da Eun and Zhu, Lingjun and Lim, Sung Kyu and {kim}, Hyesoon},
  date = {2020-02-27},
  eprint = {2002.12151},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2002.12151},
  url = {http://arxiv.org/abs/2002.12151},
  urldate = {2022-09-06},
  abstract = {The current challenges in technology scaling are pushing the semiconductor industry towards hardware specialization, creating a proliferation of heterogeneous systems-on-chip, delivering orders of magnitude performance and power benefits compared to traditional general-purpose architectures. This transition is getting a significant boost with the advent of RISC-V with its unique modular and extensible ISA, allowing a wide range of low-cost processor designs for various target applications. In addition, OpenCL is currently the most widely adopted programming framework for heterogeneous platforms available on mainstream CPUs, GPUs, as well as FPGAs and custom DSP. In this work, we present Vortex, a RISC-V General-Purpose GPU that supports OpenCL. Vortex implements a SIMT architecture with a minimal ISA extension to RISC-V that enables the execution of OpenCL programs. We also extended OpenCL runtime framework to use the new ISA. We evaluate this design using 15nm technology. We also show the performance and energy numbers of running them with a subset of benchmarks from the Rodinia Benchmark suite.},
  pubstate = {prepublished},
  keywords = {computer architecture},
  annotation = {interest: 66},
  file = {/home/sam/Zotero/storage/6LW5QR6L/Elsabbagh et al. - 2020 - Vortex OpenCL Compatible RISC-V GPGPU.pdf;/home/sam/Zotero/storage/HI2JD3I3/2002.html}
}

@inproceedings{elsnerEmpiricallyEvaluatingReadily2021,
  title = {Empirically Evaluating Readily Available Information for Regression Test Optimization in Continuous Integration},
  booktitle = {Proceedings of the 30th {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Elsner, Daniel and Hauer, Florian and Pretschner, Alexander and Reimer, Silke},
  date = {2021-07-11},
  series = {{{ISSTA}} 2021},
  pages = {491--504},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3460319.3464834},
  url = {https://doi.org/10.1145/3460319.3464834},
  urldate = {2023-01-19},
  abstract = {Regression test selection (RTS) and prioritization (RTP) techniques aim to reduce testing efforts and developer feedback time after a change to the code base. Using various information sources, including test traces, build dependencies, version control data, and test histories, they have been shown to be effective. However, not all of these sources are guaranteed to be available and accessible for arbitrary continuous integration (CI) environments. In contrast, metadata from version control systems (VCSs) and CI systems are readily available and inexpensive. Yet, corresponding RTP and RTS techniques are scattered across research and often only evaluated on synthetic faults or in a specific industrial context. It is cumbersome for practitioners to identify insights that apply to their context, let alone to calibrate associated parameters for maximum cost-effectiveness. This paper consolidates existing work on RTP and unsafe RTS into an actionable methodology to build and evaluate such approaches that exclusively rely on CI and VCS metadata. To investigate how these approaches from prior research compare in heterogeneous settings, we apply the methodology in a large-scale empirical study on a set of 23 projects covering 37,000 CI logs and 76,000 VCS commits. We find that these approaches significantly outperform established RTP baselines and, while still triggering 90\% of the failures, we show that practitioners can expect to save on average 84\% of test execution time for unsafe RTS. We also find that it can be beneficial to limit training data, features from test history work better than change-based features, and, somewhat surprisingly, simple and well-known heuristics often outperform complex machine-learned models.},
  isbn = {978-1-4503-8459-9},
  keywords = {continuous integration,project-acm-rep,project-provenance-pp,regression testing},
  annotation = {interest: 99},
  file = {/home/sam/Zotero/storage/CG4ZZ7MN/Elsner et al. - 2021 - Empirically evaluating readily available informati.pdf}
}

@online{EmpiricalSE,
  title = {Empirical {{SE}}},
  url = {https://docs.google.com/document/d/1DvCMwc6o4X5EmWeosNm8MM8VnCs3h5rC67ms5p9-jhc/edit?usp=embed_facebook},
  urldate = {2022-09-06},
  abstract = {Reading list  Easterbrook, S., Singer, J., Storey, M.-A., \& Damian, D. (2008). Selecting empirical methods for software engineering research. In Guide to advanced empirical software engineering (pp. 285-311): Springer.  Wohlin, C., Runeson, P., Höst, M., Ohlsson, M. C., Regnell, B., \& Wesslén, A....},
  langid = {english},
  organization = {Google Docs},
  annotation = {interest: 75},
  file = {/home/sam/Zotero/storage/XK93ZTMR/edit.html}
}

@article{engstromSystematicReviewRegression2010,
  title = {A Systematic Review on Regression Test Selection Techniques},
  author = {Engström, Emelie and Runeson, Per and Skoglund, Mats},
  date = {2010-01-01},
  journaltitle = {Information and Software Technology},
  shortjournal = {Information and Software Technology},
  volume = {52},
  number = {1},
  pages = {14--30},
  issn = {0950-5849},
  doi = {10.1016/j.infsof.2009.07.001},
  url = {https://www.sciencedirect.com/science/article/pii/S0950584909001219},
  urldate = {2022-09-06},
  abstract = {Regression testing is verifying that previously functioning software remains after a change. With the goal of finding a basis for further research in a joint industry-academia research project, we conducted a systematic review of empirical evaluations of regression test selection techniques. We identified 27 papers reporting 36 empirical studies, 21 experiments and 15 case studies. In total 28 techniques for regression test selection are evaluated. We present a qualitative analysis of the findings, an overview of techniques for regression test selection and related empirical evidence. No technique was found clearly superior since the results depend on many varying factors. We identified a need for empirical studies where concepts are evaluated rather than small variations in technical implementations.},
  langid = {english},
  keywords = {literature review,software testing},
  annotation = {interst: 76},
  file = {/home/sam/Zotero/storage/B7ZB72JL/Engström et al. - 2010 - A systematic review on regression test selection t.pdf;/home/sam/Zotero/storage/88VH8T5W/S0950584909001219.html}
}

@article{enserinkDutchResearchFunding2021,
  entrysubtype = {newspaper},
  title = {Dutch Research Funding Agency, Paralyzed by Ransomware Attack, Refuses to Pay Up},
  author = {Enserink, Martin},
  date = {2021-02-25},
  url = {https://www.science.org/content/article/dutch-research-funding-agency-paralyzed-ransomware-attack-refuses-pay},
  urldate = {2022-05-23},
  abstract = {Hackers seeking to extort Netherlands Organisation for Scientific Research release confidential documents.},
  langid = {english},
  keywords = {cybersecurity,internship-project,project-devsecops},
  file = {/home/sam/Zotero/storage/DCZD498H/dutch-research-funding-agency-paralyzed-ransomware-attack-refuses-pay.html}
}

@article{ernstDaikonSystemDynamic2007,
  title = {The {{Daikon}} System for Dynamic Detection of Likely Invariants},
  author = {Ernst, Michael D. and Perkins, Jeff H. and Guo, Philip J. and McCamant, Stephen and Pacheco, Carlos and Tschantz, Matthew S. and Xiao, Chen},
  date = {2007-12-01},
  journaltitle = {Science of Computer Programming},
  shortjournal = {Science of Computer Programming},
  series = {Special Issue on {{Experimental Software}} and {{Toolkits}}},
  volume = {69},
  number = {1},
  pages = {35--45},
  issn = {0167-6423},
  doi = {10.1016/j.scico.2007.01.015},
  url = {https://www.sciencedirect.com/science/article/pii/S016764230700161X},
  urldate = {2022-08-26},
  abstract = {Daikon is an implementation of dynamic detection of likely invariants; that is, the Daikon invariant detector reports likely program invariants. An invariant is a property that holds at a certain point or points in a program; these are often used in assert statements, documentation, and formal specifications. Examples include being constant (x=a), non-zero (x≠0), being in a range (a≤x≤b), linear relationships (y=ax+b), ordering (x≤y), functions from a library (x=fn(y)), containment (x∈y), sortedness (xissorted), and many more. Users can extend Daikon to check for additional invariants. Dynamic invariant detection runs a program, observes the values that the program computes, and then reports properties that were true over the observed executions. Dynamic invariant detection is a machine learning technique that can be applied to arbitrary data. Daikon can detect invariants in C, C++, Java, and Perl programs, and in record-structured data sources; it is easy to extend Daikon to other applications. Invariants can be useful in program understanding and a host of other applications. Daikon’s output has been used for generating test cases, predicting incompatibilities in component integration, automating theorem proving, repairing inconsistent data structures, and checking the validity of data streams, among other tasks. Daikon is freely available in source and binary form, along with extensive documentation, at http://pag.csail.mit.edu/daikon/.},
  langid = {english},
  annotation = {interest: 64},
  file = {/home/sam/Zotero/storage/BHNF57WS/Ernst et al. - 2007 - The Daikon system for dynamic detection of likely .pdf;/home/sam/Zotero/storage/DSE5XS5X/S016764230700161X.html}
}

@inproceedings{erxlebenIntroducingWikidataLinked2014,
  title = {Introducing {{Wikidata}} to the {{Linked Data Web}}},
  booktitle = {The {{Semantic Web}} – {{ISWC}} 2014},
  author = {Erxleben, Fredo and Günther, Michael and Krötzsch, Markus and Mendez, Julian and Vrandečić, Denny},
  editor = {Mika, Peter and Tudorache, Tania and Bernstein, Abraham and Welty, Chris and Knoblock, Craig and Vrandečić, Denny and Groth, Paul and Noy, Natasha and Janowicz, Krzysztof and Goble, Carole},
  date = {2014},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {50--65},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-11964-9_4},
  abstract = {Wikidata is the central data management platform of Wikipedia. By the efforts of thousands of volunteers, the project has produced a large, open knowledge base with many interesting applications. The data is highly interlinked and connected to many other datasets, but it is also very rich, complex, and not available in RDF. To address this issue, we introduce new RDF exports that connect Wikidata to the Linked Data Web. We explain the data model of Wikidata and discuss its encoding in RDF. Moreover, we introduce several partial exports that provide more selective or simplified views on the data. This includes a class hierarchy and several other types of ontological axioms that we extract from the site. All datasets we discuss here are freely available online and updated regularly.},
  isbn = {978-3-319-11964-9},
  langid = {english},
  keywords = {project-provenance-pp,semantic web},
  file = {/home/sam/Zotero/storage/QZACD8IN/Erxleben et al. - 2014 - Introducing Wikidata to the Linked Data Web.pdf}
}

@online{EvaluationAntiPatterns,
  title = {Evaluation {{Anti-Patterns}}},
  url = {http://evaluate.inf.usi.ch/anti-patterns},
  urldate = {2022-07-06},
  organization = {Evaluate Collaboratory}
}

@online{EventTracingWin322021,
  title = {Event {{Tracing}} - {{Win32}} Apps},
  date = {2021-01-07},
  url = {https://learn.microsoft.com/en-us/windows/win32/etw/event-tracing-portal},
  urldate = {2023-08-23},
  abstract = {This documentation is for user-mode applications that want to use ETW. For information about instrumenting device drivers that run in kernel mode, see WPP Software Tracing and Adding Event Tracing to Kernel-Mode Drivers in the Windows Driver Kit (WDK).},
  langid = {american},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/FCT2GL8J/event-tracing-portal.html}
}

@article{ewelsNfcoreFrameworkCommunitycurated2020,
  title = {The Nf-Core Framework for Community-Curated Bioinformatics Pipelines},
  author = {Ewels, Philip A. and Peltzer, Alexander and Fillinger, Sven and Patel, Harshil and Alneberg, Johannes and Wilm, Andreas and Garcia, Maxime Ulysse and Di Tommaso, Paolo and Nahnsen, Sven},
  date = {2020-03},
  journaltitle = {Nature Biotechnology},
  shortjournal = {Nat Biotechnol},
  volume = {38},
  number = {3},
  pages = {276--278},
  publisher = {Nature Publishing Group},
  issn = {1546-1696},
  doi = {10.1038/s41587-020-0439-x},
  url = {https://www.nature.com/articles/s41587-020-0439-x},
  urldate = {2022-10-31},
  issue = {3},
  langid = {english},
  keywords = {project-acm-rep,reproducibility engineering,workflow managers},
  file = {/home/sam/Zotero/storage/H4EZXAF5/Ewels et al. - 2020 - The nf-core framework for community-curated bioinf.pdf;/home/sam/Zotero/storage/LQDQZTUV/s41587-020-0439-x.html}
}

@inproceedings{fadolalkarimPANDDEProvenancebasedANomaly2016,
  title = {{{PANDDE}}: {{Provenance-based ANomaly Detection}} of {{Data Exfiltration}}},
  shorttitle = {{{PANDDE}}},
  booktitle = {Proceedings of the {{Sixth ACM Conference}} on {{Data}} and {{Application Security}} and {{Privacy}}},
  author = {Fadolalkarim, Daren and Sallam, Asmaa and Bertino, Elisa},
  date = {2016-03-09},
  series = {{{CODASPY}} '16},
  pages = {267--276},
  publisher = {Association for Computing Machinery},
  location = {New Orleans Louisiana USA},
  doi = {10.1145/2857705.2857710},
  url = {https://dl.acm.org/doi/10.1145/2857705.2857710},
  urldate = {2023-08-24},
  abstract = {Preventing data exfiltration by insiders is a challenging process since insiders are users that have access permissions to the data. Existing mechanisms focus on tracking users’ activities while they are connected to the database, and are unable to detect anomalous actions that the users perform on the data once they gain access to it. Being able to detect anomalous actions on the data is critical as these actions are often sign of attempts to misuse data. In this paper, we propose an approach to detect anomalous actions executed on data returned to the users from a database. The approach has been implemented as part of the Provenancebased ANomaly Detection of Data Exfiltration (PANDDE) tool. PANDDE leverages data provenance information captured at the operating system level. Such information is then used to create profiles of users’ actions on the data once retrieved from the database. The profiles indicate actions that are consistent with the tasks of the users. Actions recorded in the profiles include data printing, emailing, and storage. Profiles are then used at run-time to detect anomalous actions.},
  eventtitle = {{{CODASPY}}'16: {{Sixth ACM Conference}} on {{Data}} and {{Application Security}} and {{Privacy}}},
  isbn = {978-1-4503-3935-3},
  langid = {english},
  keywords = {project-provenance-pp},
  file = {/home/sam/Zotero/storage/6KESJXH2/Fadolalkarim et al. - 2016 - PANDDE Provenance-based ANomaly Detection of Data.pdf}
}

@article{fanelliHowManyScientists2009,
  title = {How {{Many Scientists Fabricate}} and {{Falsify Research}}? {{A Systematic Review}} and {{Meta-Analysis}} of {{Survey Data}}},
  shorttitle = {How {{Many Scientists Fabricate}} and {{Falsify Research}}?},
  author = {Fanelli, Daniele},
  date = {2009-05-29},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {4},
  number = {5},
  pages = {e5738},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0005738},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0005738},
  urldate = {2022-08-30},
  abstract = {The frequency with which scientists fabricate and falsify data, or commit other forms of scientific misconduct is a matter of controversy. Many surveys have asked scientists directly whether they have committed or know of a colleague who committed research misconduct, but their results appeared difficult to compare and synthesize. This is the first meta-analysis of these surveys. To standardize outcomes, the number of respondents who recalled at least one incident of misconduct was calculated for each question, and the analysis was limited to behaviours that distort scientific knowledge: fabrication, falsification, “cooking” of data, etc… Survey questions on plagiarism and other forms of professional misconduct were excluded. The final sample consisted of 21 surveys that were included in the systematic review, and 18 in the meta-analysis. A pooled weighted average of 1.97\% (N = 7, 95\%CI: 0.86–4.45) of scientists admitted to have fabricated, falsified or modified data or results at least once –a serious form of misconduct by any standard– and up to 33.7\% admitted other questionable research practices. In surveys asking about the behaviour of colleagues, admission rates were 14.12\% (N = 12, 95\% CI: 9.91–19.72) for falsification, and up to 72\% for other questionable research practices. Meta-regression showed that self reports surveys, surveys using the words “falsification” or “fabrication”, and mailed surveys yielded lower percentages of misconduct. When these factors were controlled for, misconduct was reported more frequently by medical/pharmacological researchers than others. Considering that these surveys ask sensitive questions and have other limitations, it appears likely that this is a conservative estimate of the true prevalence of scientific misconduct.},
  langid = {english},
  keywords = {metascience,scientific misconduct},
  file = {/home/sam/Zotero/storage/5Z68EEV6/Fanelli - 2009 - How Many Scientists Fabricate and Falsify Research.pdf}
}

@article{farhoodiDevelopmentScientificSoftware2013,
  title = {Development of {{Scientific Software}}: A {{Systematic Mapping}}, a {{Bibliometric Study}}, and a {{Paper Repository}}},
  shorttitle = {{{DEVELOPMENT OF SCIENTIFIC SOFTWARE}}},
  author = {Farhoodi, Roshanak and Garousi, Vahid and Pfahl, Dietmar and Sillito, Jonathan},
  date = {2013-05},
  journaltitle = {International Journal of Software Engineering and Knowledge Engineering},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  volume = {23},
  number = {04},
  pages = {463--506},
  issn = {0218-1940, 1793-6403},
  doi = {10.1142/S0218194013500137},
  url = {https://www.worldscientific.com/doi/abs/10.1142/S0218194013500137},
  urldate = {2022-06-06},
  abstract = {Scientific and engineering research is heavily dependent on effective development and use of software artifacts. Many of these artifacts are produced by the scientists themselves, rather than by trained software engineers. To address the challenges in this area, a research community often referred to as "Development of Scientific Software" has emerged in the last few decades. As this research area has matured, there has been a sharp increase in the number of papers and results made available, and it has thus become important to summarize and provide an overview about those studies. Through a systematic mapping and bibliometrics study, we have reviewed 130 papers in this area. We present the results of our study in this paper. Also we have made the mapping data available on an online repository which is planned to be updated on a regular basis. The results of our study seem to suggest that many software engineering techniques and activities are being used in the development of scientific software. However, there is still a need for further exploration of the usefulness of specific software engineering techniques (e.g., regarding software maintenance, evolution, refactoring, re(v)-engineering, process and project management) in the scientific context. It is hoped that this article will help (new) researchers get an overview of the research space and help them to understand the trends in the area.},
  langid = {english},
  keywords = {research software engineering}
}

@article{faulkScientificComputingProductivity2009,
  title = {Scientific {{Computing}}'s {{Productivity Gridlock}}: {{How Software Engineering Can Help}}},
  shorttitle = {Scientific {{Computing}}'s {{Productivity Gridlock}}},
  author = {Faulk, Stuart and Loh, Eugene and Vanter, Michael L. Van De and Squires, Susan and Votta, Lawrence G.},
  date = {2009-11},
  journaltitle = {Computing in Science \& Engineering},
  shortjournal = {Comput. Sci. Eng.},
  volume = {11},
  number = {6},
  pages = {30--39},
  issn = {1521-9615},
  doi = {10.1109/MCSE.2009.205},
  url = {http://ieeexplore.ieee.org/document/5337642/},
  urldate = {2022-06-28},
  abstract = {Hardware improvements do little to improve real productivity in scientific programming. Indeed, the dominant barriers to productivity improvement are now in the software processes. To break the gridlock, we must establish a degree of cooperation and collaboration with the software engineering community that does not yet exist.},
  keywords = {internship-project,workflow managers},
  annotation = {interest: 95}
}

@online{feamsterHowWriteWinning2020,
  title = {How to {{Write}} a {{Winning Project Proposal}}},
  author = {Feamster, Nick},
  date = {2020-07-26T17:19:26},
  url = {https://medium.com/great-research/how-to-write-a-winning-project-proposal-fe438d4dc3a9},
  urldate = {2023-01-20},
  abstract = {A project proposal needs to answer three important questions: Why important, why now, and why you?},
  langid = {english},
  organization = {Great Research},
  keywords = {academic writing},
  file = {/home/sam/Zotero/storage/7RJY4MMW/how-to-write-a-winning-project-proposal-fe438d4dc3a9.html}
}

@article{felleisenExpressivePowerProgramming1991,
  title = {On the Expressive Power of Programming Languages},
  author = {Felleisen, Matthias},
  date = {1991-12-01},
  journaltitle = {Science of Computer Programming},
  shortjournal = {Science of Computer Programming},
  volume = {17},
  number = {1},
  pages = {35--75},
  issn = {0167-6423},
  doi = {10.1016/0167-6423(91)90036-W},
  url = {https://www.sciencedirect.com/science/article/pii/016764239190036W},
  urldate = {2023-07-07},
  abstract = {The literature on programming languages contains an abundance of informal claims on the relative expressive power of programming languages, but there is no framework for formalizing such statements nor for deriving interesting consequences. As a first step in this direction, we develop a formal notion of expressiveness and investigate its properties. To validate the theory, we analyze some widely held beliefs about the expressive power of several extensions of functional languages. Based on these results, we believe that our system correctly captures many of the informal ideas on expressiveness, and that it constitutes a foundation for further research in this direction.},
  langid = {english},
  keywords = {programming languages},
  file = {/home/sam/Zotero/storage/8NSC7Q4Y/Felleisen - 1991 - On the expressive power of programming languages.pdf;/home/sam/Zotero/storage/HLACUZEZ/016764239190036W.html}
}

@article{ferreiradasilvaCharacterizationWorkflowManagement2017,
  title = {A Characterization of Workflow Management Systems for Extreme-Scale Applications},
  author = {Ferreira da Silva, Rafael and Filgueira, Rosa and Pietri, Ilia and Jiang, Ming and Sakellariou, Rizos and Deelman, Ewa},
  date = {2017-10},
  journaltitle = {Future Generation Computer Systems},
  shortjournal = {Future Generation Computer Systems},
  volume = {75},
  pages = {228--238},
  issn = {0167739X},
  doi = {10.1016/j.future.2017.02.026},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167739X17302510},
  urldate = {2022-07-07},
  langid = {english},
  keywords = {workflow managers}
}

@inproceedings{ferreiradasilvaCommunityRoadmapScientific2021,
  title = {A {{Community Roadmap}} for {{Scientific Workflows Research}} and {{Development}}},
  booktitle = {2021 {{IEEE Workshop}} on {{Workflows}} in {{Support}} of {{Large-Scale Science}} ({{WORKS}})},
  author = {Ferreira da Silva, Rafael and Casanova, Henri and Chard, Kyle and Altintas, Ilkay and Badia, Rosa M and Balis, Bartosz and Coleman, Taina and Coppens, Frederik and Di Natale, Frank and Enders, Bjoern and Fahringer, Thomas and Filgueira, Rosa and Fursin, Grigori and Garijo, Daniel and Goble, Carole and Howell, Dorran and Jha, Shantenu and Katz, Daniel S. and Laney, Daniel and Leser, Ulf and Malawski, Maciej and Mehta, Kshitij and Pottier, Loic and Ozik, Jonathan and Peterson, J. Luc and Ramakrishnan, Lavanya and Soiland-Reyes, Stian and Thain, Douglas and Wolf, Matthew},
  date = {2021-11},
  eprint = {2110.02168},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {81--90},
  publisher = {IEEE},
  location = {St. Louis, MO, USA},
  doi = {10.1109/WORKS54523.2021.00016},
  url = {https://ieeexplore.ieee.org/document/9652570/},
  urldate = {2022-06-28},
  abstract = {The landscape of workflow systems for scientific applications is notoriously convoluted with hundreds of seemingly equivalent workflow systems, many isolated research claims, and a steep learning curve. To address some of these challenges and lay the groundwork for transforming workflows research and development, the WorkflowsRI and ExaWorks projects partnered to bring the international workflows community together. This paper reports on discussions and findings from two virtual "Workflows Community Summits" (January and April, 2021). The overarching goals of these workshops were to develop a view of the state of the art, identify crucial research challenges in the workflows community, articulate a vision for potential community efforts, and discuss technical approaches for realizing this vision. To this end, participants identified six broad themes: FAIR computational workflows; AI workflows; exascale challenges; APIs, interoperability, reuse, and standards; training and education; and building a workflows community. We summarize discussions and recommendations for each of these themes.},
  eventtitle = {2021 {{IEEE Workshop}} on {{Workflows}} in {{Support}} of {{Large-Scale Science}} ({{WORKS}})},
  isbn = {978-1-6654-1136-3},
  keywords = {project-acm-rep,workflow managers},
  annotation = {interest: 90},
  file = {/home/sam/Zotero/storage/FPGAQU2S/da Silva et al. - 2021 - A Community Roadmap for Scientific Workflows Resea.pdf;/home/sam/Zotero/storage/T4DLYZB2/2110.html}
}

@inproceedings{ferreiradasilvaWorkflowHubCommunityFramework2020,
  title = {{{WorkflowHub}}: {{Community Framework}} for {{Enabling Scientific Workflow Research}} and {{Development}}},
  shorttitle = {{{WorkflowHub}}},
  booktitle = {2020 {{IEEE}}/{{ACM Workflows}} in {{Support}} of {{Large-Scale Science}} ({{WORKS}})},
  author = {Ferreira da Silva, Rafael and Pottier, Loïc and Coleman, Tainã and Deelman, Ewa and Casanova, Henri},
  date = {2020-11},
  pages = {49--56},
  publisher = {IEEE},
  location = {Georgia, USA},
  doi = {10.1109/WORKS51914.2020.00012},
  abstract = {Scientific workflows are a cornerstone of modern scientific computing. They are used to describe complex computational applications that require efficient and robust management of large volumes of data, which are typically stored/processed on heterogeneous, distributed resources. The workflow research and development community has employed a number of methods for the quantitative evaluation of existing and novel workflow algorithms and systems. In particular, a common approach is to simulate workflow executions. In previous work, we have presented a collection of tools that have been used for aiding research and development activities in the Pegasus project, and that have been adopted by others for conducting workflow research. Despite their popularity, there are several shortcomings that prevent easy adoption, maintenance, and consistency with the evolving structures and computational requirements of production workflows. In this work, we present WorkflowHub, a community framework that provides a collection of tools for analyzing workflow execution traces, producing realistic synthetic workflow traces, and simulating workflow executions. We demonstrate the realism of the generated synthetic traces by comparing simulated executions of these traces with actual workflow executions. We also contrast these results with those obtained when using the previously available collection of tools. We find that our framework not only can be used to generate representative synthetic workflow traces (i.e., with workflow structures and task characteristics distributions that resemble those in traces obtained from real-world workflow executions), but can also generate representative workflow traces at larger scales than that of available workflow traces.},
  eventtitle = {2020 {{IEEE}}/{{ACM Workflows}} in {{Support}} of {{Large-Scale Science}} ({{WORKS}})},
  keywords = {project-acm-rep,project-provenance-pp,workflow managers},
  file = {/home/sam/Zotero/storage/262NG8DJ/Silva et al. - 2020 - WorkflowHub Community Framework for Enabling Scie.pdf;/home/sam/Zotero/storage/8UFHN6M6/Silva et al. - 2020 - WorkflowHub Community Framework for Enabling Scie.pdf;/home/sam/Zotero/storage/C33VHWM8/9308170.html}
}

@online{ferreiradasilvaWorkflowsCommunityInitiative,
  title = {Workflows {{Community Initiative}}},
  author = {Ferreira da Silva, Rafael and Casanova, Henri and Chard, Kyle and Initiative, Workflows Community},
  url = {https://workflows.community/},
  urldate = {2022-06-28},
  langid = {english},
  organization = {Workflows Community Initiative}
}

@report{ferreiradasilvaWorkflowsCommunitySummit2021,
  title = {Workflows {{Community Summit}}: {{Advancing}} the {{State-of-the-art}} of {{Scientific Workflows Management Systems Research}} and {{Development}}},
  shorttitle = {Workflows {{Community Summit}}},
  author = {Ferreira da Silva, Rafael and Casanova, Henri and Chard, Kyle and Coleman, Tainã and Laney, Dan and Ahn, Dong and Jha, Shantenu and Howell, Dorran and Soiland-Reys, Stian and Altintas, Ilkay and Thain, Douglas and Filgueira, Rosa and Babuji, Yadu and Badia, Rosa M. and Balis, Bartosz and Caino-Lores, Silvina and Callaghan, Scott and Coppens, Frederik and Crusoe, Michael R. and De, Kaushik and Di Natale, Frank and Do, Tu M. A. and Enders, Bjoern and Fahringer, Thomas and Fouilloux, Anne and Fursin, Grigori and Gaignard, Alban and Ganose, Alex and Garijo, Daniel and Gesing, Sandra and Goble, Carole and Hasan, Adil and Huber, Sebastiaan and Katz, Daniel S. and Leser, Ulf and Lowe, Douglas and Ludaescher, Bertram and Maheshwari, Ketan and Malawski, Maciej and Mayani, Rajiv and Mehta, Kshitij and Merzky, Andre and Munson, Todd and Ozik, Jonathan and Pottier, Loïc and Ristov, Sashko and Roozmeh, Mehdi and Souza, Renan and Suter, Frédéric and Tovar, Benjamin and Turilli, Matteo and Vahi, Karan and Vidal-Torreira, Alvaro and Whitcup, Wendy and Wilde, Michael and Williams, Alan and Wolf, Matthew and Wozniak, Justin},
  date = {2021-06-09},
  institution = {Zenodo},
  doi = {10.5281/zenodo.4915801},
  url = {https://zenodo.org/record/4915801},
  urldate = {2022-06-28},
  abstract = {Scientific workflows are a cornerstone of modern scientific computing, and they have underpinned some of the most significant discoveries of the last decade. Many of these workflows have high computational, storage, and/or communication demands, and thus must execute on a wide range of large-scale platforms, from large clouds to upcoming exascale HPC platforms. Workflows will play a crucial role in the data-oriented and post-Moore’s computing landscape as they democratize the application of cutting-edge research techniques, computationally intensive methods, and use of new computing platforms. As workflows continue to be adopted by scientific projects and user communities, they are becoming more complex. Workflows are increasingly composed of tasks that perform computations such as short machine learning inference, multi-node simulations, long-running machine learning model training, amongst others, and thus increasingly rely on heterogeneous architectures that include CPUs but also GPUs and accelerators. The workflow management system (WMS) technology landscape is currently segmented and presents significant barriers to entry due to the hundreds of seemingly comparable, yet incompatible, systems that exist. Another fundamental problem is that there are conflicting theoretical bases and abstractions for a WMS. Systems that use the same underlying abstractions can likely be translated between, which is not the case for systems that use different abstractions. More information:~https://workflowsri.org/summits/technical},
  langid = {english},
  keywords = {internship-project,workflow managers}
}

@report{ferreiradasilvaWorkflowsCommunitySummit2021a,
  title = {Workflows {{Community Summit}}: {{Tightening}} the {{Integration}} between {{Computing Facilities}} and {{Scientific Workflows}}},
  shorttitle = {Workflows {{Community Summit}}},
  author = {Ferreira da Silva, Rafael and Chard, Kyle and Casanova, Henri and Laney, Dan and Ahn, Dong and Jha, Shantenu and Allcock, William E. and Bauer, Gregory and Duplyakin, Dmitry and Enders, Bjoern and Heer, Todd M. and Lançon, Eric and Sanielevici, Sergiu and Sayers, Kevin},
  date = {2021-11-08},
  number = {ORNL/TM-2022/1832},
  institution = {Oak Ridge National Lab. (ORNL), Oak Ridge, TN (United States)},
  doi = {10.2172/1842590},
  url = {https://www.osti.gov/biblio/1842590},
  urldate = {2022-06-28},
  abstract = {The importance of workflows is highlighted by the fact that they have underpinned some of the most significant discoveries of the past decades.~Many of these workflows have significant computational, storage, and communication demands, and thus must execute on a range of large-scale computer systems, from local clusters to public clouds and upcoming exascale HPC platforms.~Historically, infrastructures for workflow execution consisted of complex, integrated systems, developed in-house by workflow practitioners with strong dependencies on a range of legacy technologies.~Due to the increasing need to support workflows, dedicated workflow systems were developed to provide abstractions for creating, executing, and adapting workflows conveniently and efficiently while ensuring portability. ~While these efforts are all worthwhile individually, there are now hundreds of independent workflow systems.~The resulting workflow system technology landscape is fragmented, which may present significant barriers for future workflow users due to many seemingly comparable, yet usually mutually incompatible, systems that exist.~In order to tackle some of these challenges, the DOE-funded ExaWorks~and NSF-funded WorkflowsRI~projects have organized in 2021 a series of events entitled the "Workflows Community Summit". The third edition of the ``Workflows Community Summit" explored workflows challenges and opportunities from the perspective of computing centers and facilities.~The third summit brought together a small group of facilities representatives with the aim to understand how workflows are currently being used at each facility, how facilities would like to interact with workflow developers and users, how workflows fit with facility roadmaps, and what opportunities there are for tighter integration between facilities and workflows. More information at:~https://workflowsri.org/summits/facilities/},
  langid = {english},
  keywords = {internship-project,project-acm-rep,research software engineering,workflow managers},
  file = {/home/sam/Zotero/storage/4L5KHWV6/Ferreira da Silva et al. - 2021 - Workflows Community Summit Tightening the Integra.pdf}
}

@article{ferroSIGIRInitiativeImplement2018,
  title = {{{SIGIR Initiative}} to {{Implement ACM Artifact Review}} and {{Badging}}},
  author = {Ferro, Nicola and Kelly, Diane},
  date = {2018-08-31},
  journaltitle = {SIGIR Forum},
  volume = {52},
  number = {1},
  pages = {4--10},
  issn = {0163-5840},
  doi = {10.1145/3274784.3274786},
  url = {https://dl.acm.org/doi/10.1145/3274784.3274786},
  urldate = {2024-10-23},
  abstract = {Recently, the ACM created a policy on Artifact Review and Badging, which presents a framework to help SIGs recognize repeatability, replicability and reproducibility in published research. While the ACM policy established a vocabulary and definitions, it did not prescribe procedures for implementation. Rather, the ACM has left this to each SIG to define given the variety of research traditions and approaches that exist with the ACM community. SIGs are not required to implement badging, but given the growing interest in this topic in the SIGIR community, a task force has been assembled to determine how badging might be implemented. This report describes the ACM policy on Artifact Review and Badging, introduces the task force, and presents survey data describing the SIGIR community's opinions about this initiative.},
  file = {/home/sam/Zotero/storage/77Y6MPKR/Ferro and Kelly - 2018 - SIGIR Initiative to Implement ACM Artifact Review and Badging.pdf}
}

@article{fienbergWhenDidBayesian2006,
  title = {When Did {{Bayesian}} Inference Become "{{Bayesian}}"?},
  author = {Fienberg, Stephen E.},
  date = {2006-03},
  journaltitle = {Bayesian Analysis},
  volume = {1},
  number = {1},
  pages = {1--40},
  publisher = {International Society for Bayesian Analysis},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/06-BA101},
  url = {https://projecteuclid.org/journals/bayesian-analysis/volume-1/issue-1/When-did-Bayesian-inference-become-Bayesian/10.1214/06-BA101.full},
  urldate = {2022-09-09},
  abstract = {While Bayes' theorem has a 250-year history, and the method of inverse probability that flowed from it dominated statistical thinking into the twentieth century, the adjective "Bayesian" was not part of the statistical lexicon until relatively recently. This paper provides an overview of key Bayesian developments, beginning with Bayes' posthumously published 1763 paper and continuing up through approximately 1970, including the period of time when "Bayesian" emerged as the label of choice for those who advocated Bayesian methods.},
  keywords = {bayesian inference},
  annotation = {interest: 83},
  file = {/home/sam/Zotero/storage/CK673FVT/Fienberg - 2006 - When did Bayesian inference become Bayesian.pdf;/home/sam/Zotero/storage/YZXJW8HY/06-BA101.html}
}

@inproceedings{filgueiraInspect4pyKnowledgeExtraction2022,
  title = {Inspect4py: A Knowledge Extraction Framework for Python Code Repositories},
  shorttitle = {Inspect4py},
  booktitle = {Proceedings of the 19th {{International Conference}} on {{Mining Software Repositories}}},
  author = {Filgueira, Rosa and Garijo, Daniel},
  date = {2022-10-17},
  series = {{{MSR}} '22},
  pages = {232--236},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3524842.3528497},
  url = {https://doi.org/10.1145/3524842.3528497},
  urldate = {2022-12-18},
  abstract = {This work presents inspect4py, a static code analysis framework designed to automatically extract the main features, metadata and documentation of Python code repositories. Given an input folder with code, inspect4py uses abstract syntax trees and state of the art tools to find all functions, classes, tests, documentation, call graphs, module dependencies and control flows within all code files in that repository. Using these findings, inspect4py infers different ways of invoking a software component. We have evaluated our framework on 95 annotated repositories, obtaining promising results for software type classification (over 95\% F1-score). With inspect4py, we aim to ease the understandability and adoption of software repositories by other researchers and developers. Code: https://github.com/SoftwareUnderstanding/inspect4py DOI: https://doi.org/10.5281/zenodo.5907936 License: Open (BSD3-Clause)},
  isbn = {978-1-4503-9303-4},
  annotation = {interest: 90}
}

@book{fishbeinBeliefAttitudeIntention1975,
  title = {Belief, Attitude, Intention, and Behavior: An Introduction to Theory and Research},
  shorttitle = {Belief, Attitude, Intention, and Behavior},
  author = {Fishbein, Martin and Ajzen, Icek},
  date = {1975},
  series = {Addison-{{Wesley}} Series in Social Psychology},
  publisher = {Addison-Wesley Pub. Co},
  location = {Reading, Mass},
  isbn = {978-0-201-02089-2},
  pagetotal = {578},
  keywords = {internship-project,psychology,technology-acceptance}
}

@book{fogelProducingOpenSource2022,
  title = {Producing {{Open Source Software}}: {{How}} to {{Run}} a {{Successful Free Software Project}}},
  author = {Fogel, Karl},
  date = {2022-12-19},
  url = {https://producingoss.com/en/producingoss.html},
  urldate = {2023-02-24},
  keywords = {industry practices},
  annotation = {interest: 50},
  file = {/home/sam/Zotero/storage/8Q87LVQ7/producingoss.html}
}

@inproceedings{folkOverviewHDF5Technology2011,
  title = {An Overview of the {{HDF5}} Technology Suite and Its Applications},
  booktitle = {Proceedings of the {{EDBT}}/{{ICDT}} 2011 {{Workshop}} on {{Array Databases}}},
  author = {Folk, Mike and Heber, Gerd and Koziol, Quincey and Pourmal, Elena and Robinson, Dana},
  date = {2011-03-25},
  series = {{{AD}} '11},
  pages = {36--47},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1966895.1966900},
  url = {https://doi.org/10.1145/1966895.1966900},
  urldate = {2023-02-06},
  abstract = {In this paper, we give an overview of the HDF5 technology suite and some of its applications. We discuss the HDF5 data model, the HDF5 software architecture and some of its performance enhancing capabilities.},
  isbn = {978-1-4503-0614-0},
  keywords = {databases,project-acm-rep,research software engineering},
  file = {/home/sam/Zotero/storage/RPGSE2L5/Folk et al. - 2011 - An overview of the HDF5 technology suite and its a.pdf}
}

@inproceedings{fosterChimeraVirtualData2002,
  title = {Chimera: A Virtual Data System for Representing, Querying, and Automating Data Derivation},
  shorttitle = {Chimera},
  booktitle = {Proceedings 14th {{International Conference}} on {{Scientific}} and {{Statistical Database Management}}},
  author = {Foster, I. and Vockler, J. and Wilde, M. and Zhao, Yong},
  date = {2002-07},
  pages = {37--46},
  issn = {1099-3371},
  doi = {10.1109/SSDM.2002.1029704},
  url = {https://ieeexplore.ieee.org/abstract/document/1029704},
  urldate = {2024-01-21},
  abstract = {A lot of scientific data is not obtained from measurements but rather derived from other data by the application of computational procedures. We hypothesize that explicit representation of these procedures can enable documentation of data provenance, discovery of available methods, and on-demand data generation (so-called "virtual data"). To explore this idea, we have developed the Chimera virtual data system, which combines a virtual data catalog for representing data derivation procedures and derived data, with a virtual data language interpreter that translates user requests into data definition and query operations on the database. We couple the Chimera system with distributed "data grid" services to enable on-demand execution of computation schedules constructed from database queries. We have applied this system to two challenge problems, the reconstruction of simulated collision event data from a high-energy physics experiment, and searching digital sky survey data for galactic clusters, with promising results.},
  eventtitle = {Proceedings 14th {{International Conference}} on {{Scientific}} and {{Statistical Database Management}}},
  keywords = {project-provenance-pp},
  file = {/home/sam/Zotero/storage/K4GKCHY6/1029704.html}
}

@article{fouladiLaptopLambdaOutsourcing,
  title = {From {{Laptop}} to {{Lambda}}: {{Outsourcing Everyday Jobs}} to {{Thousands}} of {{Transient Functional Containers}}},
  author = {Fouladi, Sadjad and Romero, Francisco and Iter, Dan and Li, Qian and Chatterjee, Shuvo},
  abstract = {We present gg, a framework and a set of command-line tools that helps people execute everyday applications—e.g., software compilation, unit tests, video encoding, or object recognition—using thousands of parallel threads on a cloudfunctions service to achieve near-interactive completion times. In the future, instead of running these tasks on a laptop, or keeping a warm cluster running in the cloud, users might push a button that spawns 10,000 parallel cloud functions to execute a large job in a few seconds from start. gg is designed to make this practical and easy.},
  langid = {english},
  keywords = {build systems,cloud computing},
  annotation = {interest: 97},
  file = {/home/sam/Zotero/storage/SJ6EEPYW/Fouladi et al. - From Laptop to Lambda Outsourcing Everyday Jobs t.pdf}
}

@article{freireProvenanceComputationalTasks2008,
  title = {Provenance for {{Computational Tasks}}: {{A Survey}}},
  shorttitle = {Provenance for {{Computational Tasks}}},
  author = {Freire, Juliana and Koop, David and Santos, Emanuele and Silva, Cláudio T.},
  date = {2008-05},
  journaltitle = {Computing in Science \& Engineering},
  shortjournal = {Comput. Sci. Eng.},
  volume = {10},
  number = {3},
  pages = {11--21},
  issn = {1521-9615},
  doi = {10.1109/MCSE.2008.79},
  url = {http://ieeexplore.ieee.org/document/4488060/},
  urldate = {2022-07-08},
  abstract = {The problem of systematically capturing and managing provenance for computational tasks has recently received significant attention because of its relevance to a wide range of domains and applications. The authors give an overview of important concepts related to provenance management, so that potential users can make informed decisions when selecting or designing a provenance solution.},
  keywords = {project-provenance-pp,provenance},
  annotation = {interest: 97}
}

@inproceedings{frewES3DemonstrationTransparent2008,
  title = {{{ES3}}: {{A Demonstration}} of {{Transparent Provenance}} for {{Scientific Computation}}},
  shorttitle = {{{ES3}}},
  booktitle = {Provenance and {{Annotation}} of {{Data}} and {{Processes}}},
  author = {Frew, James and Slaughter, Peter},
  editor = {Freire, Juliana and Koop, David and Moreau, Luc},
  date = {2008},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {200--207},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-89965-5_21},
  abstract = {The Earth System Science Server (ES3) is a software environment for data-intensive Earth science, with unique capabilities for automatically and transparently capturing and managing the provenance of arbitrary computations. Transparent acquisition avoids the scientist having to express their computations in specific languages or schemas for provenance to be available. ES3 models provenance as relationships between processes and their input and output files. These relationships are captured by monitoring read and write accesses at various levels in the science software and asynchronously converting them to time-ordered streams of provenance events which are stored in an XML database. An ES3 provenance query returns an XML serialization of a provenance graph, forward or backwards from a specified process or file. We demonstrate ES3 provenance by generating complex data products from Earth satellite imagery.},
  isbn = {978-3-540-89965-5},
  langid = {english},
  keywords = {project-provenance-pp},
  file = {/home/sam/Zotero/storage/QREK9HHT/Frew and Slaughter - 2008 - ES3 A Demonstration of Transparent Provenance for.pdf}
}

@inproceedings{fritzschResearchSoftwareLandscape2019,
  title = {Research Software Landscape and Stakeholders},
  booktitle = {{{EPIC3GeoMünster}} 2019 "{{Earth}}! {{Past}}, {{Present}}, {{Future}}", {{Münster}}, 2019-09-22-2019-09-25},
  author = {Fritzsch, Bernadette},
  date = {2019-09-25},
  location = {Münster},
  url = {https://epic.awi.de/id/eprint/50451/},
  urldate = {2022-08-25},
  abstract = {Software plays a crucial role in the whole lifecycle of most of scientific data and must be considered in all discussions about openness of data and reproducibility of science. Consequently, some interest and working groups in RDA are dealing with software code.  The spectrum of research software ranges from packages driven by large teams of developers and used by a broad community, to small scripts that scientists almost casually write for their own work. The knowledge of modern software development practice is very different, as well as individual skills in programming and documentation. The quality levels are often correspondingly different. In order to reduce such differences in the medium term, general policies can be helpful. Policies can provide a framework for a defined way of dealing with software.  In Germany, some coordinated activities are delving into this topic. For example in the Helmholtz Association, the working group Open Science expanded its perspective from initially only data, and has now set up a Taskgroup. Research software was also anchored as an issue in the Priority Initiative Digital Information of the Alliance of Science Organisations in Germany. The panels have already published some recommendations on the development, use, and provision of research software. At present, they are working on guidelines that can serve as a basis for daily work at the institutions. The documents cover different facets, ranging from development practice and quality assurance to publication of software and licensing. The talk will give an overview about the status of these activities. It will also shed light on the players in the software development. Starting from the UK, people are increasingly organizing themselves, working at the interface between science and computer science. In Germany too, there has been an association de-RSE since 2018, which campaigns for the interests of research software engineers.},
  eventtitle = {{{GeoMünster}} 2019 "{{Earth}}! {{Past}}, {{Present}}, {{Future}}"},
  keywords = {research software engineering},
  annotation = {interest: 85},
  file = {/home/sam/Zotero/storage/RZ9XVT6G/Fritzsch - 2019 - Research software landscape and stakeholders.pdf;/home/sam/Zotero/storage/69VEP95M/50451.html}
}

@software{fsspecauthorsFilesystem_spec2023,
  title = {Filesystem\_spec},
  author = {FSspec authors},
  date = {2023-05-01T21:48:55Z},
  origdate = {2018-04-23T16:45:08Z},
  url = {https://github.com/fsspec/filesystem_spec},
  urldate = {2023-05-03},
  abstract = {A specification that python filesystems should adhere to.}
}

@online{FUSE,
  title = {{{FUSE}}},
  url = {https://www.kernel.org/doc/html/latest/filesystems/fuse.html},
  urldate = {2023-08-24},
  organization = {The Linux Kernel documentation},
  keywords = {operating systems,project-provenance-pp},
  file = {/home/sam/Zotero/storage/V8S6UDQG/fuse.html}
}

@inproceedings{gamblinSpackPackageManager2015,
  title = {The {{Spack}} Package Manager: Bringing Order to {{HPC}} Software Chaos},
  shorttitle = {The {{Spack}} Package Manager},
  booktitle = {Proceedings of the {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  author = {Gamblin, Todd and LeGendre, Matthew and Collette, Michael R. and Lee, Gregory L. and Moody, Adam and family=Supinski, given=Bronis R., prefix=de, useprefix=true and Futral, Scott},
  date = {2015-11-15},
  series = {{{SC}} '15},
  pages = {1--12},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2807591.2807623},
  url = {https://doi.org/10.1145/2807591.2807623},
  urldate = {2022-04-10},
  abstract = {Large HPC centers spend considerable time supporting software for thousands of users, but the complexity of HPC software is quickly outpacing the capabilities of existing software management tools. Scientific applications require specific versions of compilers, MPI, and other dependency libraries, so using a single, standard software stack is infeasible. However, managing many configurations is difficult because the configuration space is combinatorial in size. We introduce Spack, a tool used at Lawrence Livermore National Laboratory to manage this complexity. Spack provides a novel, recursive specification syntax to invoke parametric builds of packages and dependencies. It allows any number of builds to coexist on the same system, and it ensures that installed packages can find their dependencies, regardless of the environment. We show through real-world use cases that Spack supports diverse and demanding applications, bringing order to HPC software chaos.},
  isbn = {978-1-4503-3723-6},
  keywords = {high-performance computing,operating systems,package managers,project-acm-rep,project-astrophysics,project-provenance-pp,reproducibility engineering,research software engineering},
  annotation = {interest: 80},
  file = {/home/sam/Zotero/storage/7RMEVM2B/Gamblin et al. - 2015 - The Spack package manager bringing order to HPC s.pdf}
}

@online{gandonRDFXMLSyntax2014,
  title = {{{RDF}} 1.1 {{XML Syntax}}},
  author = {Gandon, Fabian and Shcreiber, Guus and Beckett, David},
  date = {2014-02-25},
  url = {https://www.w3.org/TR/rdf-syntax-grammar/#section-Syntax-blank-nodes},
  urldate = {2023-05-26},
  abstract = {This document defines an XML syntax for RDF called RDF/XML in terms of Namespaces in XML, the XML Information Set and XML Base.},
  organization = {W3C Standards},
  keywords = {project-provenance-pp,semantic web},
  file = {/home/sam/Zotero/storage/XP5APX3P/rdf-syntax-grammar.html}
}

@online{gannsleWhyYouShouldnt2021,
  title = {Why You Shouldn't Invoke Setup.Py Directly},
  author = {Gannsle, Paul},
  date = {2021-10-19T08:55:30-04:00},
  url = {https://blog.ganssle.io/articles/2021/10/../../../articles/2021/10/setup-py-deprecated.html},
  urldate = {2023-06-14},
  abstract = {The setuptools team no longer wants to be in the business of providing a command line interface and is actively working to become just a library for building packages. What you should do instead depends on your use case, but if you want some basic rules of thumb, there is a table in the summary section. This does not mean that setuptools itself is deprecated, or that using setup.py to configure your package builds is going to be removed. The only thing you must stop doing is directly executing the setup.py file — instead delegate that to purpose-built or standards-based tools, preferably those that work with any build backend.},
  langid = {english},
  organization = {Paul Gannsle},
  keywords = {industry practices,package managers},
  file = {/home/sam/Zotero/storage/EKBRNX7P/setup-py-deprecated.html}
}

@article{garijoAbstractLinkPublish2017,
  title = {Abstract, Link, Publish, Exploit: {{An}} End to End Framework for Workflow Sharing},
  shorttitle = {Abstract, Link, Publish, Exploit},
  author = {Garijo, Daniel and Gil, Yolanda and Corcho, Oscar},
  date = {2017-10},
  journaltitle = {Future Generation Computer Systems},
  shortjournal = {Future Generation Computer Systems},
  volume = {75},
  pages = {271--283},
  issn = {0167739X},
  doi = {10.1016/j.future.2017.01.008},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167739X17300274},
  urldate = {2022-08-02},
  langid = {english},
  keywords = {academic publishing,provenance,semantic web,workflow managers}
}

@inproceedings{garijoNewApproachPublishing2011,
  title = {A New Approach for Publishing Workflows: Abstractions, Standards, and Linked Data},
  shorttitle = {A New Approach for Publishing Workflows},
  booktitle = {Proceedings of the 6th Workshop on {{Workflows}} in Support of Large-Scale Science},
  author = {Garijo, Daniel and Gil, Yolanda},
  date = {2011-11-14},
  series = {{{WORKS}} '11},
  pages = {47--56},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2110497.2110504},
  url = {https://dl.acm.org/doi/10.1145/2110497.2110504},
  urldate = {2023-05-26},
  abstract = {In recent years, a variety of systems have been developed that export the workflows used to analyze data and make them part of published articles. We argue that the workflows that are published in current approaches are dependent on the specific codes used for execution, the specific workflow system used, and the specific workflow catalogs where they are published. In this paper, we describe a new approach that addresses these shortcomings and makes workflows more reusable through: 1) the use of abstract workflows to complement executable workflows to make them reusable when the execution environment is different, 2) the publication of both abstract and executable workflows using standards such as the Open Provenance Model that can be imported by other workflow systems, 3) the publication of workflows as Linked Data that results in open web accessible workflow repositories. We illustrate this approach using a complex workflow that we re-created from an influential publication that describes the generation of 'drugomes'.},
  isbn = {978-1-4503-1100-7},
  keywords = {project-provenance-pp,provenance,semantic web,workflow managers},
  file = {/home/sam/Zotero/storage/5FU8H8X6/Garijo and Gil - 2011 - A new approach for publishing workflows abstracti.pdf}
}

@article{garijoNineBestPractices2022,
  title = {Nine Best Practices for Research Software Registries and Repositories},
  author = {Garijo, Daniel and Ménager, Hervé and Hwang, Lorraine and Trisovic, Ana and Hucka, Michael and Morrell, Thomas and Allen, Alice},
  date = {2022-08-08},
  journaltitle = {PeerJ Computer Science},
  shortjournal = {PeerJ Comput. Sci.},
  volume = {8},
  pages = {e1023},
  publisher = {PeerJ Inc.},
  issn = {2376-5992},
  doi = {10.7717/peerj-cs.1023},
  url = {https://peerj.com/articles/cs-1023},
  urldate = {2022-09-06},
  abstract = {Scientific software registries and repositories improve software findability and research transparency, provide information for software citations, and foster preservation of computational methods in a wide range of disciplines. Registries and repositories play a critical role by supporting research reproducibility and replicability, but developing them takes effort and few guidelines are available to help prospective creators of these resources. To address this need, the FORCE11 Software Citation Implementation Working Group convened a Task Force to distill the experiences of the managers of existing resources in setting expectations for all stakeholders. In this article, we describe the resultant best practices which include defining the scope, policies, and rules that govern individual registries and repositories, along with the background, examples, and collaborative work that went into their development. We believe that establishing specific policies such as those presented here will help other scientific software registries and repositories better serve their users and their disciplines.},
  langid = {english},
  keywords = {library science},
  annotation = {interest: 65},
  file = {/home/sam/Zotero/storage/5WSLAGGX/Garijo et al. - 2022 - Nine best practices for research software registri.pdf;/home/sam/Zotero/storage/VQVI74VE/cs-1023.html}
}

@inproceedings{garijoWorkflowEcosystemsSemantic2014,
  title = {Towards {{Workflow Ecosystems}} through {{Semantic}} and {{Standard Representations}}},
  booktitle = {2014 9th {{Workshop}} on {{Workflows}} in {{Support}} of {{Large-Scale Science}}},
  author = {Garijo, Daniel and Gil, Yolanda and Corcho, Oscar},
  date = {2014-11},
  pages = {94--104},
  publisher = {IEEE},
  location = {New Orleans, LA},
  doi = {10.1109/WORKS.2014.13},
  url = {https://ieeexplore.ieee.org/document/7019866/},
  urldate = {2022-08-02},
  abstract = {Workflows are increasingly used to manage and share scientific computations and methods. Workflow tools can be used to design, validate, execute and visualize scientific workflows and their execution results. Other tools manage workflow libraries or mine their contents. There has been a lot of recent work on workflow system integration as well as common workflow interlinguas, but the interoperability among workflow systems remains a challenge. Ideally, these tools would form a workflow ecosystem such that it should be possible to create a workflow with a tool, execute it with another, visualize it with another, and use yet another tool to mine a repository of such workflows or their executions. In this paper, we describe our approach to create a workflow ecosystem through the use of standard models for provenance (OPM and W3C PROV) and extensions (P-PLAN and OPMW) to represent workflows. The ecosystem integrates different workflow tools with diverse functions (workflow generation, execution, browsing, mining, and visualization) created by a variety of research groups. This is, to our knowledge, the first time that such a variety of workflow systems and functions are integrated.},
  eventtitle = {2014 9th {{Workshop}} on {{Workflows}} in {{Support}} of {{Large-Scale Science}} ({{WORKS}})},
  isbn = {978-1-4799-7067-4},
  keywords = {provenance,semantic web}
}

@article{gavishUniversalIdentifierComputational2011,
  title = {A {{Universal Identifier}} for {{Computational Results}}},
  author = {Gavish, Matan and Donoho, David},
  date = {2011},
  journaltitle = {Procedia Computer Science},
  shortjournal = {Procedia Computer Science},
  volume = {4},
  pages = {637--647},
  issn = {18770509},
  doi = {10.1016/j.procs.2011.04.067},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050911001256},
  urldate = {2022-07-08},
  abstract = {We present a discipline for verifiable computational scienti\_c research. Our discipline revolves around three simple new concepts — verifiable computational result (VCR), VCR repository and Verifiable Result Identifier (VRI). These are web- and cloud-computing oriented concepts, which exploit today's web infrastructure to achieve standard, simple and automatic reproducibility in computational scientific research. The VCR discipline requires very slight modifications to the way researchers already conduct their computational research and authoring, and to the way publishers manage their content. In return, the discipline marks a significant step towards delivering on the long-anticipated promises of making scientific computation truly reproducible. A researcher practicing this discipline in everyday work produces computational scripts and word processor files that look very much like those they already produce today, but in which a few lines change very subtly and naturally. Those scripts produce a stream of verifiable results, which are the same tables, figures, charts and datasets the researcher traditionally would have produced, but which are watermarked for permanent identification by a VRI, and are automatically and permanently stored in a VCR repository. In a scientific community practicing Verifiable Computational Research, exchange of both ideas and data involves exchanging result identifiers—VRIs—rather than exchanging files. These identifiers are controlled, trusted and automatically generated strings that point to publicly available result as it was originally created by the computational process itself. When a verifiable result is included in a publication, its identifier can be used by any reader with a web browser to locate, browse and, where appropriate, re-execute the computation that produced the result. Journal readers can therefore scrutinize, dispute, understand and eventually trust these computational results, all to an extent impossible through textual explanations that constitute the core of scientific publications to date. In addition, the result identi\_er can be used by subsequent computations to locate and retrieve both the published result (in graphical or numerical form) and the original datasets used by its generating computation. Colleagues can thus cite and import data into their own computations, just as traditional publications allow them to cite and import ideas. We describe an existing software implementation of the Verifiable Computational Research discipline, and argue that it solves many of the crucial problems commonly facing computer-based and computer-aided research in various scientific fields. Our system is secure, naturally adapted to large-scale and cloud computations and to modern massive data analysis, yet places effectively no additional workload on either the researcher or the publisher.},
  langid = {english},
  keywords = {academic publishing,provenance},
  annotation = {interest: 71}
}

@online{gaynorModernWonUs,
  title = {Modern {{C}}++ {{Won}}'t {{Save Us}}},
  author = {Gaynor, Alex},
  url = {https://alexgaynor.net/2019/apr/21/modern-c++-wont-save-us/},
  urldate = {2022-05-10},
  file = {/home/sam/Zotero/storage/574EBJVZ/modern-c++-wont-save-us.html}
}

@online{gedamThoughtsPythonPackaging2023,
  title = {Thoughts on the {{Python}} Packaging Ecosystem},
  author = {Gedam, Pradyun},
  date = {2023-01-21T00:00:00+00:00},
  url = {https://pradyunsg.me/blog/2023/01/21/thoughts-on-python-packaging/},
  urldate = {2023-01-22},
  abstract = {My response to the discussion topic posed in Python Packaging Strategy Discussion Part 1 had become quite long, so I decided to move it to write a blog post instead. This post then started absorbing various draft posts I’ve had on this topic since this blog was started, morphing to include my broader thoughts on where we are today. Note: I’ve updated this to cover an aspect of the recent LWN article on the topic as well.},
  langid = {english},
  keywords = {industry practices,project-python-packaging,software engineering},
  annotation = {interest: 90},
  file = {/home/sam/Zotero/storage/NUVVB9PL/thoughts-on-python-packaging.html}
}

@inproceedings{gehaniSPADESupportProvenance2012,
  title = {{{SPADE}}: {{Support}} for {{Provenance Auditing}} in {{Distributed Environments}}},
  shorttitle = {{{SPADE}}},
  booktitle = {Middleware 2012},
  author = {Gehani, Ashish and Tariq, Dawood},
  editor = {Narasimhan, Priya and Triantafillou, Peter},
  date = {2012},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {101--120},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-35170-9_6},
  abstract = {SPADE is an open source software infrastructure for data provenance collection and management. The underlying data model used throughout the system is graph-based, consisting of vertices and directed edges that are modeled after the node and relationship types described in the Open Provenance Model. The system has been designed to decouple the collection, storage, and querying of provenance metadata. At its core is a novel provenance kernel that mediates between the producers and consumers of provenance information, and handles the persistent storage of records. It operates as a service, peering with remote instances to enable distributed provenance queries. The provenance kernel on each host handles the buffering, filtering, and multiplexing of incoming metadata from multiple sources, including the operating system, applications, and manual curation. Provenance elements can be located locally with queries that use wildcard, fuzzy, proximity, range, and Boolean operators. Ancestor and descendant queries are transparently propagated across hosts until a terminating expression is satisfied, while distributed path queries are accelerated with provenance sketches.},
  isbn = {978-3-642-35170-9},
  langid = {english},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/S8LZBZWF/Gehani and Tariq - 2012 - SPADE Support for Provenance Auditing in Distribu.pdf}
}

@online{gelmanConfirmationistFalsificationistParadigms,
  title = {Confirmationist and Falsificationist Paradigms of Science},
  author = {Gelman, Andrew},
  url = {https://statmodeling.stat.columbia.edu/2014/09/05/confirmationist-falsificationist-paradigms-science/},
  organization = {Statistical Modeling, Causal Inference, and Social Science},
  keywords = {bayesian inference,philosophy of science},
  annotation = {interest: 95}
}

@article{gelmanDifferenceSignificantNot2006,
  title = {The {{Difference Between}} “{{Significant}}” and “{{Not Significant}}” Is Not {{Itself Statistically Significant}}},
  author = {Gelman, Andrew and Stern, Hal},
  date = {2006-11-01},
  journaltitle = {The American Statistician},
  volume = {60},
  number = {4},
  pages = {328--331},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1198/000313006X152649},
  url = {https://doi.org/10.1198/000313006X152649},
  urldate = {2024-01-29},
  abstract = {It is common to summarize statistical comparisons by declarations of statistical significance or nonsignificance. Here we discuss one problem with such declarations, namely that changes in statistical significance are often not themselves statistically significant. By this, we are not merely making the commonplace observation that any particular threshold is arbitrary—for example, only a small change is required to move an estimate from a 5.1\% significance level to 4.9\%, thus moving it into statistical significance. Rather, we are pointing out that even large changes in significance levels can correspond to small, nonsignificant changes in the underlying quantities. The error we describe is conceptually different from other oft-cited problems—that statistical significance is not the same as practical importance, that dichotomization into significant and nonsignificant results encourages the dismissal of observed differences in favor of the usually less interesting null hypothesis of no difference, and that any particular threshold for declaring significance is arbitrary. We are troubled by all of these concerns and do not intend to minimize their importance. Rather, our goal is to bring attention to this additional error of interpretation. We illustrate with a theoretical example and two applied examples. The ubiquity of this statistical error leads us to suggest that students and practitioners be made more aware that the difference between “significant” and “not significant” is not itself statistically significant.},
  keywords = {statistics}
}

@article{genelFollowingFlowTracer2013,
  title = {Following the Flow: Tracer Particles in Astrophysical Fluid Simulations},
  shorttitle = {Following the Flow},
  author = {Genel, Shy and Vogelsberger, Mark and Nelson, Dylan and Sijacki, Debora and Springel, Volker and Hernquist, Lars},
  date = {2013-10-21},
  journaltitle = {Monthly Notices of the Royal Astronomical Society},
  shortjournal = {Monthly Notices of the Royal Astronomical Society},
  volume = {435},
  number = {2},
  pages = {1426--1442},
  issn = {0035-8711},
  doi = {10.1093/mnras/stt1383},
  url = {https://doi.org/10.1093/mnras/stt1383},
  urldate = {2022-04-11},
  abstract = {We present two numerical schemes for passive tracer particles in the hydrodynamical moving-mesh code arepo, and compare their performance for various problems, from simple set-ups to cosmological simulations. The purpose of tracer particles is to allow the flow to be followed in a Lagrangian way, tracing the evolution of the fluid with time, and allowing the thermodynamical history of individual fluid parcels to be recorded. We find that the commonly used ‘velocity field tracers’, which are advected using the fluid velocity field, do not in general follow the mass flow correctly, and explain why this is the case. This method can result in order-of-magnitude biases in simulations of driven turbulence and in cosmological simulations, rendering the velocity field tracers inappropriate for following these flows. We then discuss a novel implementation of ‘Monte Carlo tracers’, which are moved along with fluid cells and are exchanged probabilistically between them following the mass flux. This method reproduces the mass distribution of the fluid correctly. The main limitation of this approach is that it is more diffusive than the fluid itself. Nonetheless, we show that this novel approach is more reliable than that has been employed previously and demonstrate that it is appropriate for following hydrodynamical flows in mesh-based codes. The Monte Carlo tracers can also naturally be transferred between fluid cells and other types of particles, such as stellar particles, so that the mass flow in cosmological simulations can be followed in its entirety.},
  keywords = {astrophysics,cosmological simulation,numerical methods,project-astrophysics},
  file = {/home/sam/Zotero/storage/S8PZSWS4/Genel et al. - 2013 - Following the flow tracer particles in astrophysi.pdf;/home/sam/Zotero/storage/CY7YP257/1039776.html}
}

@article{genovaComputerScienceTruly2010,
  title = {Is Computer Science Truly Scientific?},
  author = {Génova, Gonzalo},
  date = {2010-07},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {53},
  number = {7},
  pages = {37--39},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/1785414.1785431},
  url = {https://dl.acm.org/doi/10.1145/1785414.1785431},
  urldate = {2023-02-12},
  abstract = {Reflections on the (experimental) scientific method in computer science.},
  langid = {english},
  keywords = {philosophy of science}
}

@article{gibneyCouldMachineLearning2022,
  title = {Could Machine Learning Fuel a Reproducibility Crisis in Science?},
  author = {Gibney, Elizabeth},
  date = {2022-07-26},
  journaltitle = {Nature},
  shortjournal = {Nature},
  pages = {d41586-022-02035-w},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/d41586-022-02035-w},
  url = {https://www.nature.com/articles/d41586-022-02035-w},
  urldate = {2022-07-28},
  abstract = {‘Data leakage’ threatens the reliability of machine-learning use across disciplines, researchers warn.},
  langid = {english},
  keywords = {machine learning,metascience,reproducibility}
}

@article{gigerenzerMindlessStatistics2004,
  title = {Mindless Statistics},
  author = {Gigerenzer, Gerd},
  date = {2004-11-01},
  journaltitle = {The Journal of Socio-Economics},
  shortjournal = {The Journal of Socio-Economics},
  series = {Statistical {{Significance}}},
  volume = {33},
  number = {5},
  pages = {587--606},
  issn = {1053-5357},
  doi = {10.1016/j.socec.2004.09.033},
  url = {https://www.sciencedirect.com/science/article/pii/S1053535704000927},
  urldate = {2024-01-28},
  abstract = {Statistical rituals largely eliminate statistical thinking in the social sciences. Rituals are indispensable for identification with social groups, but they should be the subject rather than the procedure of science. What I call the “null ritual” consists of three steps: (1) set up a statistical null hypothesis, but do not specify your own hypothesis nor any alternative hypothesis, (2) use the 5\% significance level for rejecting the null and accepting your hypothesis, and (3) always perform this procedure. I report evidence of the resulting collective confusion and fears about sanctions on the part of students and teachers, researchers and editors, as well as textbook writers.},
  keywords = {philosophy of science,psychology,statistics},
  file = {/home/sam/Zotero/storage/45U4RUIU/S1053535704000927.html}
}

@article{gilExaminingChallengesScientific2007,
  title = {Examining the {{Challenges}} of {{Scientific Workflows}}},
  author = {Gil, Yolanda and Deelman, Ewa and Ellisman, Mark and Fahringer, Thomas and Fox, Geoffrey and Gannon, Dennis and Goble, Carole and Livny, Miron and Moreau, Luc and Myers, Jim},
  date = {2007-12},
  journaltitle = {Computer},
  volume = {40},
  number = {12},
  pages = {24--32},
  issn = {1558-0814},
  doi = {10.1109/MC.2007.421},
  abstract = {Workflows have emerged as a paradigm for representing and managing complex distributed computations and are used to accelerate the pace of scientific progress. A recent National Science Foundation workshop brought together domain, computer, and social scientists to discuss requirements of future scientific applications and the challenges they present to current workflow technologies.},
  eventtitle = {Computer},
  keywords = {workflow managers},
  annotation = {interest: 91},
  file = {/home/sam/Zotero/storage/SHKAM8S5/Gil et al. - 2007 - Examining the Challenges of Scientific Workflows.pdf;/home/sam/Zotero/storage/FPVVU7W6/4404805.html}
}

@online{GiottoTDADocumentationGiottotda,
  title = {Giotto-{{TDA}} Documentation — Giotto-Tda 0.5.1 Documentation},
  url = {https://giotto-ai.github.io/gtda-docs/0.5.1/index.html},
  urldate = {2022-09-06},
  keywords = {software,topological data analysis},
  file = {/home/sam/Zotero/storage/PC4WG285/index.html}
}

@article{gligoricAutomatedMigrationBuild2014,
  title = {Automated Migration of Build Scripts Using Dynamic Analysis and Search-Based Refactoring},
  author = {Gligoric, Milos and Schulte, Wolfram and Prasad, Chandra and family=Velzen, given=Danny, prefix=van, useprefix=true and Narasamdya, Iman and Livshits, Benjamin},
  date = {2014-10-15},
  journaltitle = {ACM SIGPLAN Notices},
  shortjournal = {SIGPLAN Not.},
  volume = {49},
  number = {10},
  pages = {599--616},
  issn = {0362-1340},
  doi = {10.1145/2714064.2660239},
  url = {https://dl.acm.org/doi/10.1145/2714064.2660239},
  urldate = {2023-10-16},
  abstract = {The efficiency of a build system is an important factor for developer productivity. As a result, developer teams have been increasingly adopting new build systems that allow higher build parallelization. However, migrating the existing legacy build scripts to new build systems is a tedious and error-prone process. Unfortunately, there is insufficient support for automated migration of build scripts, making the migration more problematic. We propose the first dynamic approach for automated migration of build scripts to new build systems. Our approach works in two phases. First, from a set of execution traces, we synthesize build scripts that accurately capture the intent of the original build. The synthesized build scripts are typically long and hard to maintain. Second, we apply refactorings that raise the abstraction level of the synthesized scripts (e.g., introduce functions for similar fragments). As different refactoring sequences may lead to different build scripts, we use a search-based approach that explores various sequences to identify the best (e.g., shortest) build script. We optimize search-based refactoring with partial-order reduction to faster explore refactoring sequences. We implemented the proposed two phase migration approach in a tool called METAMORPHOSIS that has been recently used at Microsoft.},
  keywords = {build systems,provenance},
  file = {/home/sam/Zotero/storage/NSEVIAKC/Gligoric et al. - 2014 - Automated migration of build scripts using dynamic.pdf}
}

@inproceedings{gligoricCoDeSeFastDeserialization2011,
  title = {{{CoDeSe}}: Fast Deserialization via Code Generation},
  shorttitle = {{{CoDeSe}}},
  booktitle = {Proceedings of the 2011 {{International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Gligoric, Milos and Marinov, Darko and Kamin, Sam},
  date = {2011-07-17},
  series = {{{ISSTA}} '11},
  pages = {298--308},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2001420.2001456},
  url = {https://doi.org/10.1145/2001420.2001456},
  urldate = {2022-04-07},
  abstract = {Many tools for automated testing, model checking, and debugging store and restore program states multiple times. Storing/restoring a program state is commonly done with serialization/deserialization. Traditionally, the format for stored states is based on data: serialization generates the data that encodes the state, and deserialization interprets this data to restore the state. We propose a new approach, called CoDeSe, where the format for stored states is based on code: serialization generates code whose execution restores the state, and deserialization simply executes the code. We implemented CoDeSe in Java and performed a number of experiments on deserialization of states. CoDeSe provides on average more than 6X speedup over the highly optimized deserialization from the standard Java library. Our new format also allows simple parallel deserialization that can provide additional speedup on top of the sequential CoDeSe but only for larger states.},
  isbn = {978-1-4503-0562-4},
  keywords = {serialization/deserialization,software engineering},
  annotation = {score: 50},
  file = {/home/sam/Zotero/storage/M4EWUPNI/2001420.2001456.pdf}
}

@inproceedings{gligoricEmpiricalEvaluationComparison2014,
  title = {An Empirical Evaluation and Comparison of Manual and Automated Test Selection},
  booktitle = {Proceedings of the 29th {{ACM}}/{{IEEE}} International Conference on {{Automated}} Software Engineering},
  author = {Gligoric, Milos and Negara, Stas and Legunsen, Owolabi and Marinov, Darko},
  date = {2014-09-15},
  series = {{{ASE}} '14},
  pages = {361--372},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2642937.2643019},
  url = {https://doi.org/10.1145/2642937.2643019},
  urldate = {2022-08-31},
  abstract = {Regression test selection speeds up regression testing by re-running only the tests that can be affected by the most recent code changes. Much progress has been made on research in automated test selection over the last three decades, but it has not translated into practical tools that are widely adopted. Therefore, developers either re-run all tests after each change or perform manual test selection. Re-running all tests is expensive, while manual test selection is tedious and error-prone. Despite such a big trade-off, no study assessed how developers perform manual test selection and compared it to automated test selection. This paper reports on our study of manual test selection in practice and our comparison of manual and automated test selection. We are the first to conduct a study that (1) analyzes data from manual test selection, collected in real time from 14 developers during a three-month study and (2) compares manual test selection with an automated state-of-the-research test-selection tool for 450 test sessions. Almost all developers in our study performed manual test selection, and they did so in mostly ad-hoc ways. Comparing manual and automated test selection, we found the two approaches to select different tests in each and every one of the 450 test sessions investigated. Manual selection chose more tests than automated selection 73\% of the time (potentially wasting time) and chose fewer tests 27\% of the time (potentially missing bugs). These results show the need for better automated test-selection techniques that integrate well with developers' programming environments.},
  isbn = {978-1-4503-3013-8},
  keywords = {software testing},
  annotation = {interest: 60},
  file = {/home/sam/Zotero/storage/8K89SKG8/Gligoric et al. - 2014 - An empirical evaluation and comparison of manual a.pdf}
}

@inproceedings{gligoricPracticalRegressionTest2015,
  title = {Practical Regression Test Selection with Dynamic File Dependencies},
  booktitle = {Proceedings of the 2015 {{International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Gligoric, Milos and Eloussi, Lamyaa and Marinov, Darko},
  date = {2015-07-13},
  series = {{{ISSTA}} 2015},
  pages = {211--222},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2771783.2771784},
  url = {https://doi.org/10.1145/2771783.2771784},
  urldate = {2022-04-06},
  abstract = {Regression testing is important but can be time-intensive. One approach to speed it up is regression test selection (RTS), which runs only a subset of tests. RTS was proposed over three decades ago but has not been widely adopted in practice. Meanwhile, testing frameworks, such as JUnit, are widely adopted and well integrated with many popular build systems. Hence, integrating RTS in a testing framework already used by many projects would increase the likelihood that RTS is adopted. We propose a new, lightweight RTS technique, called Ekstazi, that can integrate well with testing frameworks. Ekstazi tracks dynamic dependencies of tests on files, and unlike most prior RTS techniques, Ekstazi requires no integration with version-control systems. We implemented Ekstazi for Java and JUnit, and evaluated it on 615 revisions of 32 open-source projects (totaling almost 5M LOC) with shorter- and longer-running test suites. The results show that Ekstazi reduced the end-to-end testing time 32\% on average, and 54\% for longer-running test suites, compared to executing all tests. Ekstazi also has lower end-to-end time than the existing techniques, despite the fact that it selects more tests. Ekstazi has been adopted by several popular open source projects, including Apache Camel, Apache Commons Math, and Apache CXF.},
  isbn = {978-1-4503-3620-8},
  keywords = {software engineering,software testing},
  annotation = {score: 70},
  file = {/home/sam/Zotero/storage/XVTWJCVC/2771783.2771784.pdf}
}

@thesis{glukhovaToolsEnsuringReproducible2017,
  type = {mathesis},
  title = {Tools for {{Ensuring Reproducible Builds}} for {{Open-Source Software}}},
  author = {Glukhova, Maria},
  date = {2017},
  institution = {Lappeenranta University of Technology},
  url = {https://lutpub.lut.fi/bitstream/handle/10024/135304/MariaGlukhova_ToolsForEnsuringReproducibleBuildsForOpenSourceSoftware.pdf?sequence=2},
  abstract = {The Reproducible Builds project is a collective effort of multiple open-source software projects, aiming to provide a verifiable path from human readable source code to the binary code used by computers. To achieve this goal, several tools were created, allowing for identifying common sources of unreproducibility in build process. In this work, an overview of the Reproducible Builds project and the tools designed is made. One of the tools, named diffoscope, is discussed in details; several improvements to this tool are made as part of this work.},
  langid = {english},
  pagetotal = {36},
  file = {/home/sam/Zotero/storage/NC3DPHAF/Glukhova - TOOLS FOR ENSURING REPRODUCIBLE BUILDS FOR OPEN-SO.pdf}
}

@article{gobleFAIRComputationalWorkflows2020,
  title = {{{FAIR Computational Workflows}}},
  author = {Goble, Carole and Cohen-Boulakia, Sarah and Soiland-Reyes, Stian and Garijo, Daniel and Gil, Yolanda and Crusoe, Michael R. and Peters, Kristian and Schober, Daniel},
  date = {2020-01},
  journaltitle = {Data Intelligence},
  shortjournal = {Data Intellegence},
  volume = {2},
  number = {1--2},
  pages = {108--121},
  issn = {2641-435X},
  doi = {10.1162/dint_a_00033},
  url = {https://direct.mit.edu/dint/article/2/1-2/108-121/10003},
  urldate = {2022-07-07},
  abstract = {Computational workflows describe the complex multi-step methods that are used for data collection, data preparation, analytics, predictive modelling, and simulation that lead to new data products. They can inherently contribute to the FAIR data principles: by processing data according to established metadata; by creating metadata themselves during the processing of data; and by tracking and recording data provenance. These properties aid data quality assessment and contribute to secondary data usage. Moreover, workflows are digital objects in their own right. This paper argues that FAIR principles for workflows need to address their specific nature in terms of their composition of executable software steps, their provenance, and their development.},
  langid = {english},
  keywords = {open data,workflow managers}
}

@article{gobleMyExperimentRepositorySocial2010,
  title = {{{myExperiment}}: A Repository and Social Network for the Sharing of Bioinformatics Workflows},
  shorttitle = {{{myExperiment}}},
  author = {Goble, Carole A. and Bhagat, Jiten and Aleksejevs, Sergejs and Cruickshank, Don and Michaelides, Danius and Newman, David and Borkum, Mark and Bechhofer, Sean and Roos, Marco and Li, Peter and De Roure, David},
  date = {2010-07-01},
  journaltitle = {Nucleic Acids Research},
  shortjournal = {Nucleic Acids Research},
  volume = {38},
  pages = {W677-W682},
  issn = {0305-1048},
  doi = {10.1093/nar/gkq429},
  url = {https://doi.org/10.1093/nar/gkq429},
  urldate = {2022-10-31},
  abstract = {myExperiment (http://www.myexperiment.org) is an online research environment that supports the social sharing of bioinformatics workflows. These workflows are procedures consisting of a series of computational tasks using web services, which may be performed on data from its retrieval, integration and analysis, to the visualization of the results. As a public repository of workflows, myExperiment allows anybody to discover those that are relevant to their research, which can then be reused and repurposed to their specific requirements. Conversely, developers can submit their workflows to myExperiment and enable them to be shared in a secure manner. Since its release in 2007, myExperiment currently has over 3500 registered users and contains more than 1000 workflows. The social aspect to the sharing of these workflows is facilitated by registered users forming virtual communities bound together by a common interest or research project. Contributors of workflows can build their reputation within these communities by receiving feedback and credit from individuals who reuse their work. Further documentation about myExperiment including its REST web service is available from http://wiki.myexperiment.org. Feedback and requests for support can be sent to bugs@myexperiment.org.},
  issue = {suppl\_2},
  keywords = {project-acm-rep,reproducibility engineering,workflow managers},
  file = {/home/sam/Zotero/storage/JWIDGCGY/Goble et al. - 2010 - myExperiment a repository and social network for .pdf}
}

@article{goecksGalaxyComprehensiveApproach2010,
  title = {Galaxy: A Comprehensive Approach for Supporting Accessible, Reproducible, and Transparent Computational Research in the Life Sciences},
  shorttitle = {Galaxy},
  author = {Goecks, Jeremy and Nekrutenko, Anton and Taylor, James and {The Galaxy Team}},
  date = {2010-08-25},
  journaltitle = {Genome Biology},
  shortjournal = {Genome Biol},
  volume = {11},
  number = {8},
  pages = {R86},
  issn = {1474-760X},
  doi = {10.1186/gb-2010-11-8-r86},
  url = {https://doi.org/10.1186/gb-2010-11-8-r86},
  urldate = {2023-02-21},
  abstract = {Increased reliance on computational approaches in the life sciences has revealed grave concerns about how accessible and reproducible computation-reliant results truly are. Galaxy http://usegalaxy.org, an open web-based platform for genomic research, addresses these problems. Galaxy automatically tracks and manages data provenance and provides support for capturing the context and intent of computational methods. Galaxy Pages are interactive, web-based documents that provide users with a medium to communicate a complete computational analysis.},
  langid = {english},
  keywords = {project-acm-rep,workflow managers},
  file = {/home/sam/Zotero/storage/6QUL2WLN/Goecks et al. - 2010 - Galaxy a comprehensive approach for supporting ac.pdf}
}

@report{goertzelPotentialComputationalLinguistics2005,
  title = {Potential {{Computational Linguistics Resources}}},
  author = {Goertzel, Ben},
  date = {2005-03-06},
  pages = {16},
  langid = {english},
  keywords = {artificial intelligence,conlang},
  file = {/home/sam/Zotero/storage/5AVNPCR9/Goertzel - Potential Computational Linguistics Resources.pdf}
}

@article{gomesWhyDontWe2022,
  title = {Why Don't We Share Data and Code? {{Perceived}} Barriers and Benefits to Public Archiving Practices},
  shorttitle = {Why Don't We Share Data and Code?},
  author = {Gomes, Dylan G. E. and Pottier, Patrice and Crystal-Ornelas, Robert and Hudgins, Emma J. and Foroughirad, Vivienne and Sánchez-Reyes, Luna L. and Turba, Rachel and Martinez, Paula Andrea and Moreau, David and Bertram, Michael G. and Smout, Cooper A. and Gaynor, Kaitlyn M.},
  date = {2022-11-23},
  journaltitle = {Proceedings of the Royal Society B: Biological Sciences},
  volume = {289},
  number = {1987},
  pages = {20221113},
  publisher = {Royal Society},
  doi = {10.1098/rspb.2022.1113},
  url = {https://royalsocietypublishing.org/doi/10.1098/rspb.2022.1113},
  urldate = {2024-10-23},
  abstract = {The biological sciences community is increasingly recognizing the value of open, reproducible and transparent research practices for science and society at large. Despite this recognition, many researchers fail to share their data and code publicly. This pattern may arise from knowledge barriers about how to archive data and code, concerns about its reuse, and misaligned career incentives. Here, we define, categorize and discuss barriers to data and code sharing that are relevant to many research fields. We explore how real and perceived barriers might be overcome or reframed in the light of the benefits relative to costs. By elucidating these barriers and the contexts in which they arise, we can take steps to mitigate them and align our actions with the goals of open science, both as individual scientists and as a scientific community.},
  langid = {english},
  keywords = {code reuse‌,data reuse,data science,open science,reproducibility,transparency},
  file = {/home/sam/Zotero/storage/AJXYMVCC/Gomes et al. - 2022 - Why don't we share data and code Perceived barriers and benefits to public archiving practices.pdf;/home/sam/Zotero/storage/MAFA6N37/6296319.html}
}

@article{gomez-diazEvaluationResearchSoftware2019,
  title = {On the Evaluation of Research Software: The {{CDUR}} Procedure},
  shorttitle = {On the Evaluation of Research Software},
  author = {Gomez-Diaz, Teresa and Recio, Tomas},
  date = {2019-11-26},
  number = {8:1353},
  publisher = {F1000Research},
  doi = {10.12688/f1000research.19994.2},
  url = {https://f1000research.com/articles/8-1353},
  urldate = {2022-09-06},
  abstract = {Background: Evaluation of the quality of research software is a challenging and relevant issue, still not sufficiently addressed by the scientific community. Methods: Our contribution begins by defining, precisely but widely enough, the notions of research software and of its authors followed by a study of the evaluation issues, as the basis for the proposition of a sound assessment protocol: the CDUR procedure. Results: CDUR comprises four steps introduced as follows: C itation, to deal with correct RS identification, D issemination, to measure good dissemination practices, U se, devoted to the evaluation of usability aspects, and R esearch, to assess the impact of the scientific work. Conclusions: Some conclusions and recommendations are finally included. The evaluation of research is the keystone to boost the evolution of the Open Science policies and practices. It is as well our belief that research software evaluation is a fundamental step to induce better research software practices and, thus, a step towards more efficient science.},
  langid = {english},
  keywords = {research software engineering},
  annotation = {interest: 83},
  file = {/home/sam/Zotero/storage/PQXI8G2X/Gomez-Diaz and Recio - 2019 - On the evaluation of research software the CDUR p.pdf}
}

@inproceedings{gomez-perezWhenHistoryMatters2013,
  title = {When {{History Matters}} - {{Assessing Reliability}} for the {{Reuse}} of {{Scientific Workflows}}},
  booktitle = {The {{Semantic Web}} – {{ISWC}} 2013},
  author = {Gómez-Pérez, José Manuel and García-Cuesta, Esteban and Garrido, Aleix and Ruiz, José Enrique and Zhao, Jun and Klyne, Graham},
  editor = {Alani, Harith and Kagal, Lalana and Fokoue, Achille and Groth, Paul and Biemann, Chris and Parreira, Josiane Xavier and Aroyo, Lora and Noy, Natasha and Welty, Chris and Janowicz, Krzysztof},
  date = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {81--97},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-41338-4_6},
  abstract = {Scientific workflows play an important role in computational research as essential artifacts for communicating the methods used to produce research findings. We are witnessing a growing number of efforts that treat workflows as first-class artifacts for sharing and exchanging scientific knowledge, either as part of scholarly articles or as stand-alone objects. However, workflows are not born to be reliable, which can seriously damage their reusability and trustworthiness as knowledge exchange instruments. Scientific workflows are commonly subject to decay, which consequently undermines their reliability over their lifetime. The reliability of workflows can be notably improved by advocating scientists to preserve a minimal set of information that is essential to assist the interpretations of these workflows and hence improve their potential for reproducibility and reusability. In this paper we show how, by measuring and monitoring the completeness and stability of scientific workflows over time we are able to provide scientists with a measure of their reliability, supporting the reuse of trustworthy scientific knowledge.},
  isbn = {978-3-642-41338-4},
  langid = {english},
  keywords = {project-acm-rep,project-provenance-pp,reproducibility,workflow managers},
  file = {/home/sam/Zotero/storage/5ARRBNBG/Gómez-Pérez et al. - 2013 - When History Matters - Assessing Reliability for t.pdf}
}

@online{gommersPythonPackagingWorkflows2023,
  title = {Python Packaging \& Workflows - Where to Next?},
  author = {Gommers, Ralf},
  date = {2023-01-10},
  url = {https://labs.quansight.org/blog/python-packaging-where-to-next},
  urldate = {2023-02-24},
  organization = {Quansight Labs blog},
  keywords = {package managers,project-python-packaging},
  file = {/home/sam/Zotero/storage/MFLDXA9V/python-packaging-where-to-next.html}
}

@online{gonzalezHardPartTypechecking,
  title = {The Hard Part of Type-Checking {{Nix}}},
  author = {Gonzalez, Gabriella},
  url = {https://www.haskellforall.com/2022/03/the-hard-part-of-type-checking-nix.html},
  urldate = {2024-12-02},
  abstract = {The hard part of type-checking Nix          I’ve been banging my head for a while on the challenge of building a t...}
}

@online{goochOverviewLinuxVirtual,
  title = {Overview of the {{Linux Virtual File System}}},
  author = {Gooch},
  url = {https://docs.kernel.org/filesystems/vfs.html},
  urldate = {2023-08-24},
  organization = {The Linux Kernel documentation},
  keywords = {operating systems,project-provenance-pp},
  file = {/home/sam/Zotero/storage/CLVRT56M/vfs.html}
}

@inproceedings{goodfellowGenerativeAdversarialNets2014,
  title = {Generative {{Adversarial Nets}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  date = {2014},
  volume = {27},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html},
  urldate = {2022-05-02},
  abstract = {We propose a new framework for estimating generative models via adversarial nets, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitatively evaluation of the generated samples.},
  keywords = {artificial intelligence,machine learning,project-astrophysics},
  file = {/home/sam/Zotero/storage/Z5F5WDJX/Goodfellow et al. - 2014 - Generative Adversarial Nets.pdf;/home/sam/Zotero/storage/M6XVSYUF/5ca3e9b122f61f8f06494c97b1afccf3-Reviews.html}
}

@article{goodmanPValuesBayesModest2001,
  title = {Of {{P-Values}} and {{Bayes}}: {{A Modest Proposal}}},
  shorttitle = {Of {{P-Values}} and {{Bayes}}},
  author = {Goodman, Steven N.},
  date = {2001-05},
  journaltitle = {Epidemiology},
  volume = {12},
  number = {3},
  pages = {295},
  issn = {1044-3983},
  url = {https://journals.lww.com/epidem/fulltext/2001/05000/of_p_values_and_bayes__a_modest_proposal.6.aspx},
  urldate = {2024-01-29},
  abstract = {An abstract is unavailable.},
  langid = {american},
  keywords = {statistics}
}

@article{goodmanTenSimpleRules2014,
  title = {Ten {{Simple Rules}} for the {{Care}} and {{Feeding}} of {{Scientific Data}}},
  author = {Goodman, Alyssa and Pepe, Alberto and Blocker, Alexander W. and Borgman, Christine L. and Cranmer, Kyle and Crosas, Merce and Stefano, Rosanne Di and Gil, Yolanda and Groth, Paul and Hedstrom, Margaret and Hogg, David W. and Kashyap, Vinay and Mahabal, Ashish and Siemiginowska, Aneta and Slavkovic, Aleksandra},
  date = {2014-04-24},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  volume = {10},
  number = {4},
  pages = {e1003542},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003542},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003542},
  urldate = {2023-01-19},
  langid = {english},
  file = {/home/sam/Zotero/storage/CRDI8AQW/Goodman et al. - 2014 - Ten Simple Rules for the Care and Feeding of Scien.pdf}
}

@online{gouldAcademiaPolicyDavid,
  title = {From Academia to Policy with {{David Carr}}},
  author = {Gould, Julie},
  url = {http://blogs.nature.com/naturejobs/2014/12/03/from-academia-to-policy-with-david-carr/},
  urldate = {2022-08-30},
  abstract = {An interview with David Carr about his transition from academia to science policy.},
  organization = {Naturejobs Blog},
  keywords = {academic careers},
  file = {/home/sam/Zotero/storage/J7PUJVAK/from-academia-to-policy-with-david-carr.html}
}

@article{gounarisInvestigatingDriversInternet2008,
  title = {Investigating the Drivers of Internet Banking Adoption Decision: {{A}} Comparison of Three Alternative Frameworks},
  shorttitle = {Investigating the Drivers of Internet Banking Adoption Decision},
  author = {Gounaris, Spiros and Koritos, Christos},
  date = {2008-07-25},
  journaltitle = {International Journal of Bank Marketing},
  volume = {26},
  number = {5},
  pages = {282--304},
  issn = {0265-2323},
  doi = {10.1108/02652320810894370},
  url = {https://www.emerald.com/insight/content/doi/10.1108/02652320810894370/full/html},
  urldate = {2022-06-02},
  abstract = {Purpose: The paper seeks to compare, through empirical evidence, two widely adopted models (the Technology Acceptance Model (TAM) and the Diffusion of Innovations (DoI) model) to an underutilized one (Perceived Characteristics of the Innovation) in order to examine which is better in predicting consumer adoption of internet banking (IB), while investigating innovation attributes vis‐à‐vis other important predictors of adoption of innovations, such as consumer personal characteristics. Design/methodology/approach: The data derive from both users and non‐users of IB through a web survey. The paper assesses the psychometric properties of the measures through confirmatory factor analysis and then employs logistic regression analysis in order to assess and compare the ability of the models to accurately predict consumer adoption of IB. Findings: The paper finds that PCI performed significantly better than TAM and DoI in predicting consumer adoption of IB, whereas the addition of consumer demographics and psychographics further improved the predictive ability of the overall logit model. Research limitations/implications: Limitations of the study include the non‐random nature of the IB non‐users sample, and the fact that this was a study of a single shopping context (i.e. banking). Non‐usability innovation characteristics are important predictors of consumer adoption of technologically based innovations. Bank managers should reconsider their segmentation and targeting strategies in the light of more refined as well as new segmentation criteria. Originality/value: The PCI model has never been examined within online contexts. The paper also incorporates other non‐usability types of characteristics (i.e. social, psychological) into TAM and DoI, and identifies the moderating role of shopping context, between innovation characteristics and decision to adopt.},
  langid = {english},
  keywords = {internship-project,technology-acceptance},
  file = {/home/sam/Zotero/storage/BPU5Q3CB/10-1108_02652320810894370.pdf}
}

@article{govoniQrespToolCurating2019,
  title = {Qresp, a Tool for Curating, Discovering and Exploring Reproducible Scientific Papers},
  author = {Govoni, Marco and Munakami, Milson and Tanikanti, Aditya and Skone, Jonathan H. and Runesha, Hakizumwami B. and Giberti, Federico and family=Pablo, given=Juan, prefix=de, useprefix=true and Galli, Giulia},
  date = {2019-01-29},
  journaltitle = {Scientific Data},
  shortjournal = {Sci Data},
  volume = {6},
  number = {1},
  pages = {190002},
  publisher = {Nature Publishing Group},
  issn = {2052-4463},
  doi = {10.1038/sdata.2019.2},
  url = {https://www.nature.com/articles/sdata20192},
  urldate = {2022-04-14},
  abstract = {We propose a strategy and present a simple tool to facilitate scientific data reproducibility by making available, in a distributed manner, all data and procedures presented in scientific papers, together with metadata to render them searchable and discoverable. In particular, we describe a graphical user interface (GUI), Qresp, to curate papers (i.e. generate metadata) and to explore curated papers and automatically access the data presented in scientific publications.},
  issue = {1},
  langid = {english},
  keywords = {data management,reproducibility engineering,scientific method,workflow managers},
  annotation = {interest: 30},
  file = {/home/sam/Zotero/storage/2HWN8I4R/Govoni et al. - 2019 - Qresp, a tool for curating, discovering and explor.pdf;/home/sam/Zotero/storage/36FFV7RI/2019-02-tool-tackles-crisis-science.html;/home/sam/Zotero/storage/KDQLP87L/sdata20192.html}
}

@unpublished{grahamRootsLisp2002,
  title = {The {{Roots}} of {{Lisp}}},
  author = {Graham, Paul},
  date = {2002-01-18},
  file = {/home/sam/Zotero/storage/9YUA27BN/jmc.pdf}
}

@software{grandeNfprov2023,
  title = {Nf-Prov},
  author = {Grande, Bruno and Sherman, Ben and Di Tomasso, Paolo},
  date = {2023-05-07T13:10:54Z},
  origdate = {2022-12-19T21:16:30Z},
  url = {https://github.com/Sage-Bionetworks-Workflows/nf-prov},
  urldate = {2023-05-25},
  organization = {Sage-Bionetworks-Workflows},
  keywords = {project-provenance-pp,provenance}
}

@inproceedings{grayBioschemasPotatoSalad2017,
  title = {Bioschemas: {{From Potato Salad}} to {{Protein Annotation}}},
  shorttitle = {Bioschemas},
  booktitle = {{{ISWC}} 2017 {{Posters}} \& {{Demonstrations}} and {{Industry Tracks}}: {{Proceedings}} of the {{ISWC}} 2017 {{Posters}} \& {{Demonstrations}} and {{Industry Tracks}} Co-Located with 16th {{International Semantic Web Conference}} ({{ISWC}} 2017)},
  author = {Gray, Alasdair J. G. and Goble, Carole and Jimenez, Rafael C.},
  date = {2017-10-22},
  publisher = {RWTH Aachen University},
  url = {https://research.manchester.ac.uk/en/publications/bioschemas-from-potato-salad-to-protein-annotation},
  urldate = {2023-05-26},
  eventtitle = {The 16th {{International Semantic Web Conference}} 2017},
  langid = {english},
  keywords = {project-provenance-pp,semantic web},
  file = {/home/sam/Zotero/storage/XU99KNK2/Gray et al. - 2017 - Bioschemas From Potato Salad to Protein Annotatio.pdf}
}

@unpublished{grayDataMiningSDSS2002,
  title = {Data {{Mining}} the {{SDSS SkyServer Database}}},
  author = {Gray, Jim and Szalay, Alex S. and Thakar, Ani R. and Kunszt, Peter Z. and Stoughton, Christopher and Slutz, Don and {vandenBerg}, Jan},
  date = {2002-02-12},
  eprint = {cs/0202014},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/cs/0202014},
  urldate = {2022-04-11},
  abstract = {An earlier paper (Szalay et. al. "Designing and Mining MultiTerabyte Astronomy Archives: The Sloan Digital Sky Survey," ACM SIGMOD 2000) described the Sloan Digital Sky Survey's (SDSS) data management needs by defining twenty database queries and twelve data visualization tasks that a good data management system should support. We built a database and interfaces to support both the query load and also a website for ad-hoc access. This paper reports on the database design, describes the data loading pipeline, and reports on the query implementation and performance. The queries typically translated to a single SQL statement. Most queries run in less than 20 seconds, allowing scientists to interactively explore the database. This paper is an in-depth tour of those queries. Readers should first have studied the companion overview paper Szalay et. al. "The SDSS SkyServer, Public Access to the Sloan Digital Sky Server Data" ACM SIGMOND 2002.},
  keywords = {astronomical observations,data mining,databases,project-astrophysics,research software engineering},
  annotation = {interest: 70},
  file = {/home/sam/Zotero/storage/L6I7D69E/Gray et al. - 2002 - Data Mining the SDSS SkyServer Database.pdf}
}

@inproceedings{graysonAutomaticReproductionWorkflows2023,
  title = {Automatic {{Reproduction}} of {{Workflows}} in the {{Snakemake Workflow Catalog}} and Nf-Core {{Registries}}},
  booktitle = {Proceedings of the 2023 {{ACM Conference}} on {{Reproducibility}} and {{Replicability}}},
  author = {Grayson, Samuel and Marinov, Darko and Katz, Daniel S. and Milewicz, Reed},
  date = {2023-06-28},
  series = {{{ACM REP}} '23},
  pages = {74--84},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3589806.3600037},
  url = {https://dl.acm.org/doi/10.1145/3589806.3600037},
  urldate = {2024-01-20},
  abstract = {Workflows make it easier for scientists to assemble computational experiments consisting of many disparate components. However, those disparate components also increase the probability that the computational experiment fails to be reproducible. Even if software is reproducible today, it may become irreproducible tomorrow without the software itself changing at all, because of the constantly changing software environment in which the software is run. To alleviate irreproducibility, workflow engines integrate with container engines. Additionally, communities that sprung up around workflow engines started to host registries for workflows that follow standards. These standards reduce the effort needed to make workflows automatically reproducible. In this paper, we study automatic reproduction of workflows from two registries, focusing on non-crashing executions. The experimental data lets us analyze the upper bound to which workflow engines could achieve reproducibility. We identify lessons learned in achieving reproducibility in practice.},
  isbn = {979-8-4007-0176-4},
  keywords = {project-provenance-pp,reproducibility,reproducibility engineering,research software engineering,workflow engines},
  file = {/home/sam/Zotero/storage/W6NHPEL2/Grayson et al. - 2023 - Automatic Reproduction of Workflows in the Snakema.pdf}
}

@inproceedings{graysonBenchmarkSuitePerformance2024,
  title = {A Benchmark Suite and Performance Analysis of User-Space Provenance Collectors},
  booktitle = {Proceedings of the 2nd {{ACM Conference}} on {{Reproducibility}} and {{Replicability}}},
  author = {Grayson, Samuel and Aguilar, Faustino and Milewicz, Reed and Katz, Daniel S. and Marinov, Darko},
  date = {2024-06-18},
  pages = {85--95},
  publisher = {ACM},
  location = {Rennes France},
  doi = {10.1145/3641525.3663627},
  url = {https://dl.acm.org/doi/10.1145/3641525.3663627},
  urldate = {2024-10-05},
  eventtitle = {{{ACM REP}} '24: {{ACM Conference}} on {{Reproducibility}} and {{Replicability}}},
  isbn = {979-8-4007-0530-4},
  langid = {english}
}

@report{graysonEvaluatingSystemLevelProvenance2023,
  title = {Evaluating {{System-Level Provenance Tools}} for {{Practical Use}}},
  author = {Grayson, Samuel and Reed, Milewicz},
  date = {2023},
  number = {SAND2023-13916R},
  pages = {172--181},
  institution = {Sandia National Laboratories},
  location = {Albuquerque, NM and Livermore, CA},
  url = {https://www.sandia.gov/ccr/csri-summer-programs/computer-science-research-institute-summer-proceedings-2023/},
  abstract = {Tracking provenance has many applications in computational science, especially for im- proving reproducibility, but it is not yet widely used in practice. In this report, we execute a literature rapid review to find system-level provenance tools and methods for use in practice based on the method of collection, source availability, and platform compatibility.},
  langid = {english},
  keywords = {project-provenance-pp},
  file = {/home/sam/Zotero/storage/G2VJ9G2I/Seritan and Reuter - 2023 - CSRI Summer Program The Center for Computing Resea.pdf}
}

@inproceedings{graysonWantedStandardsAutomatic2023,
  title = {Wanted: Standards for Automatic Reproducibility of Computational Experiments},
  shorttitle = {Wanted},
  booktitle = {Software {{Engineering}} for {{Research Science}} ({{SE4RS}})},
  author = {Grayson, Samuel and Milewicz, Reed and Teves, Joshua and Katz, Daniel S. and Marinov, Darko},
  date = {2023-07-21},
  eprint = {2307.11383},
  eprinttype = {arXiv},
  eprintclass = {cs},
  publisher = {arXiv},
  location = {Portland, OR, USA},
  doi = {10.48550/arXiv.2307.11383},
  url = {http://arxiv.org/abs/2307.11383},
  urldate = {2023-08-01},
  abstract = {Those seeking to reproduce a computational experiment often need to manually look at the code to see how to build necessary libraries, configure parameters, find data, and invoke the experiment; it is not automatic. Automatic reproducibility is a more stringent goal, but working towards it would benefit the community. This work discusses a machine-readable language for specifying how to execute a computational experiment. We invite interested stakeholders to discuss this language at https://github.com/charmoniumQ/execution-description .},
  file = {/home/sam/Zotero/storage/K7DKVNDE/Grayson et al. - 2023 - Wanted standards for automatic reproducibility of.pdf;/home/sam/Zotero/storage/KR435BVG/2307.html}
}

@article{greenlandStatisticalTestsValues2016,
  title = {Statistical Tests, {{P}} Values, Confidence Intervals, and Power: A Guide to Misinterpretations},
  shorttitle = {Statistical Tests, {{P}} Values, Confidence Intervals, and Power},
  author = {Greenland, Sander and Senn, Stephen J. and Rothman, Kenneth J. and Carlin, John B. and Poole, Charles and Goodman, Steven N. and Altman, Douglas G.},
  date = {2016-04},
  journaltitle = {European Journal of Epidemiology},
  shortjournal = {Eur J Epidemiol},
  volume = {31},
  number = {4},
  eprint = {27209009},
  eprinttype = {pmid},
  pages = {337--350},
  issn = {1573-7284},
  doi = {10.1007/s10654-016-0149-3},
  abstract = {Misinterpretation and abuse of statistical tests, confidence intervals, and statistical power have been decried for decades, yet remain rampant. A key problem is that there are no interpretations of these concepts that are at once simple, intuitive, correct, and foolproof. Instead, correct use and interpretation of these statistics requires an attention to detail which seems to tax the patience of working scientists. This high cognitive demand has led to an epidemic of shortcut definitions and interpretations that are simply wrong, sometimes disastrously so-and yet these misinterpretations dominate much of the scientific literature. In light of this problem, we provide definitions and a discussion of basic statistics that are more general and critical than typically found in traditional introductory expositions. Our goal is to provide a resource for instructors, researchers, and consumers of statistics whose knowledge of statistical theory and technique may be limited but who wish to avoid and spot misinterpretations. We emphasize how violation of often unstated analysis protocols (such as selecting analyses for presentation based on the P values they produce) can lead to small P values even if the declared test hypothesis is correct, and can lead to large P values even if that hypothesis is incorrect. We then provide an explanatory list of 25 misinterpretations of P values, confidence intervals, and power. We conclude with guidelines for improving statistical interpretation and reporting.},
  langid = {english},
  pmcid = {PMC4877414},
  keywords = {statistics},
  file = {/home/sam/Zotero/storage/RQ45FQ6Y/Greenland et al. - 2016 - Statistical tests, P values, confidence intervals,.pdf}
}

@online{greenTruthBayesianPriors2017,
  title = {The Truth about {{Bayesian}} Priors and Overfitting},
  author = {Green, Michael},
  date = {2017-08-02T07:29:22},
  url = {https://towardsdatascience.com/the-truth-about-bayesian-priors-and-overfitting-84e24d3a1153},
  urldate = {2022-12-18},
  abstract = {Have you ever thought about how strong a prior is compared to observed data? It’s not an entirely easy thing to conceptualize. In order to…},
  langid = {english},
  organization = {Medium},
  keywords = {statistics},
  annotation = {interest: 90},
  file = {/home/sam/Zotero/storage/5WYTDZ8G/the-truth-about-bayesian-priors-and-overfitting-84e24d3a1153.html}
}

@unpublished{gregwilsonNotShelvesWhat2016,
  title = {Not on the {{Shelves}}: {{What Nonexistent Books}}, {{Tools}}, and {{Courses Can Tell Us}} about {{Ourselves}}},
  shorttitle = {Not on the {{Shelves}}},
  author = {{Greg Wilson}},
  date = {2016-10-11},
  url = {https://www.youtube.com/watch?v=vx0DUiv1Gvw},
  urldate = {2023-02-24},
  abstract = {Hundreds of books about writing compilers are currently on the market, but there are only three about writing debuggers. Spreadsheets are used to do calculations more often than every other kind of tool combined, but thirty-five years after their invention, version control systems still can't handle them. Everyone thinks we should teach children how to program, but undergraduate courses on computing education are practically nonexistent. This talk will explore what these gaps in our books, tools, and courses tell us about the state of computing today, and about what it could look like tomorrow. Greg Wilson is the co-founder of Software Carpentry, a volunteer organization that teaches basic computing skills to researchers in a wide range of disciplines. He has worked for 30 years in both industry and academia, and is the author or editor of several books on computing (including the 2008 Jolt Award winner "Beautiful Code") and two for children. Greg received a Ph.D. in Computer Science from the University of Edinburgh in 1993.},
  eventtitle = {Software {{Carpentry}}}
}

@article{grenningPlanningPokerHow2002,
  title = {Planning Poker or How to Avoid Analysis Paralysis While Release Planning},
  author = {Grenning, James},
  date = {2002},
  journaltitle = {Hawthorn Woods: Renaissance Software Consulting},
  volume = {3},
  pages = {22--23},
  keywords = {internship-project,software engineering process},
  annotation = {interest: 30}
}

@article{grothAnatomyNanopublication2010,
  title = {The Anatomy of a Nanopublication},
  author = {Groth, Paul and Gibson, Andrew and Velterop, Jan},
  date = {2010-01-01},
  journaltitle = {Information Services \& Use},
  volume = {30},
  number = {1--2},
  pages = {51--56},
  publisher = {IOS Press},
  issn = {0167-5265},
  doi = {10.3233/ISU-2010-0613},
  url = {https://content.iospress.com/articles/information-services-and-use/isu613},
  urldate = {2023-05-26},
  abstract = {As the amount of scholarly communication increases, it is increasingly difficult for specific core scientific statements to be found, connected and curated. Additionally, the redundancy of these statements in multiple fora makes it difficult to deter},
  langid = {english},
  keywords = {academic publishing,project-provenance-pp,semantic web},
  file = {/home/sam/Zotero/storage/LLFTU3XH/Groth et al. - 2010 - The anatomy of a nanopublication.pdf}
}

@inproceedings{gruberEmpiricalStudyFlaky2021,
  title = {An {{Empirical Study}} of {{Flaky Tests}} in {{Python}}},
  booktitle = {2021 14th {{IEEE Conference}} on {{Software Testing}}, {{Verification}} and {{Validation}} ({{ICST}})},
  author = {Gruber, Martin and Lukasczyk, Stephan and Kroiß, Florian and Fraser, Gordon},
  date = {2021-04},
  pages = {148--158},
  issn = {2159-4848},
  doi = {10.1109/ICST49551.2021.00026},
  abstract = {Tests that cause spurious failures without any code changes, i.e., flaky tests, hamper regression testing, increase maintenance costs, may shadow real bugs, and decrease trust in tests. While the prevalence and importance of flakiness is well established, prior research focused on Java projects, thus raising the question of how the findings generalize. In order to provide a better understanding of the role of flakiness in software development beyond Java, we empirically study the prevalence, causes, and degree of flakiness within software written in Python, one of the currently most popular programming languages. For this, we sampled 22 352 open source projects from the popular PyPI package index, and analyzed their 876 186 test cases for flakiness. Our investigation suggests that flakiness is equally prevalent in Python as it is in Java. The reasons, however, are different: Order dependency is a much more dominant problem in Python, causing 59 \% of the 7 571 flaky tests in our dataset. Another 28 \% were caused by test infrastructure problems, which represent a previously undocumented cause of flakiness. The remaining 13 \% can mostly be attributed to the use of network and randomness APIs by the projects, which is indicative of the type of software commonly written in Python. Our data also suggests that finding flaky tests requires more runs than are often done in the literature: A 95 \% confidence that a passing test case is not flaky on average would require 170 reruns.},
  eventtitle = {2021 14th {{IEEE Conference}} on {{Software Testing}}, {{Verification}} and {{Validation}} ({{ICST}})},
  keywords = {industry practices,project-provenance-pp,software mining,software testing},
  annotation = {intersest: 98},
  file = {/home/sam/Zotero/storage/97U5SWGI/Gruber et al. - 2021 - An Empirical Study of Flaky Tests in Python.pdf;/home/sam/Zotero/storage/YCVW5QVS/9438576.html}
}

@report{gruenpeterDefiningResearchSoftware2021,
  title = {Defining {{Research Software}}: A Controversial Discussion},
  shorttitle = {Defining {{Research Software}}},
  author = {Gruenpeter, Morane and Katz, Daniel S. and Lamprecht, Anna-Lena and Honeyman, Tom and Garijo, Daniel and Struck, Alexander and Niehues, Anna and Martinez, Paula Andrea and Castro, Leyla Jael and Rabemanantsoa, Tovo and Chue Hong, Neil P. and Martinez-Ortiz, Carlos and Sesink, Laurents and Liffers, Matthias and Fouilloux, Anne Claire and Erdmann, Chris and Peroni, Silvio and Martinez Lavanchy, Paula and Todorov, Ilian and Sinha, Manodeep},
  date = {2021-09-13},
  institution = {Zenodo},
  doi = {10.5281/zenodo.5504016},
  url = {https://zenodo.org/record/5504016},
  urldate = {2022-04-12},
  abstract = {Software is essential in modern research; it plays vital roles at multiple stages of the research lifecycle. The term Research Software is widely used in the academic community but, what do we mean when we use these terms? Software and research? When you think of software, you may think of a digital object that is executed on a machine. Yet software is more than just this, it is a complex and evolving artifact. It may be a concept or a project designed to solve a puzzle by a team or a community that develops its functionalities and algorithms, which might not be digital objects. Furthermore, the software artifacts are digital objects, e.g., executables and source code files for different environments. These digital artifacts, which are used in a scholarly setting, might be important in the research process, but should all these be considered Research Software? This report is the result of a discussion examining the scope of the community definition of the FAIR principles for Research Software as part of the work in the FAIR for Research Software working group (FAIR4RS). We aim to clarify the scope of the FAIR principles by identifying which software artifacts the FAIR principles should apply to. This discussion portrayed a complex landscape of software uses in research and existing definitions that can help to better understand the complexity of different types of software in academia. Finally we determine the scope of the FAIR4RS with a short and concise definition of Research Software as a separate metaphor of software in research.},
  keywords = {research software engineering},
  file = {/home/sam/Zotero/storage/76ZKWHUM/Gruenpeter et al. - 2021 - Defining Research Software a controversial discus.pdf}
}

@inproceedings{gueretFindingAchillesHeel2010,
  title = {Finding the {{Achilles Heel}} of the {{Web}} of {{Data}}: {{Using Network Analysis}} for {{Link-Recommendation}}},
  shorttitle = {Finding the {{Achilles Heel}} of the {{Web}} of {{Data}}},
  booktitle = {The {{Semantic Web}} – {{ISWC}} 2010},
  author = {Guéret, Christophe and Groth, Paul and family=Harmelen, given=Frank, prefix=van, useprefix=true and Schlobach, Stefan},
  editor = {Patel-Schneider, Peter F. and Pan, Yue and Hitzler, Pascal and Mika, Peter and Zhang, Lei and Pan, Jeff Z. and Horrocks, Ian and Glimm, Birte},
  date = {2010},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {289--304},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-17746-0_19},
  abstract = {The Web of Data is increasingly becoming an important infrastructure for such diverse sectors as entertainment, government, e-commerce and science. As a result, the robustness of this Web of Data is now crucial. Prior studies show that the Web of Data is strongly dependent on a small number of central hubs, making it highly vulnerable to single points of failure. In this paper, we present concepts and algorithms to analyse and repair the brittleness of the Web of Data. We apply these on a substantial subset of it, the 2010 Billion Triple Challenge dataset. We first distinguish the physical structure of the Web of Data from its semantic structure. For both of these structures, we then calculate their robustness, taking betweenness centrality as a robustness-measure. To the best of our knowledge, this is the first time that such robustness-indicators have been calculated for the Web of Data. Finally, we determine which links should be added to the Web of Data in order to improve its robustness most effectively. We are able to determine such links by interpreting the question as a very large optimisation problem and deploying an evolutionary algorithm to solve this problem. We believe that with this work, we offer an effective method to analyse and improve the most important structure that the Semantic Web community has constructed to date.},
  isbn = {978-3-642-17746-0},
  langid = {english},
  keywords = {semantic web},
  file = {/home/sam/Zotero/storage/MCAQG349/Guéret et al. - 2010 - Finding the Achilles Heel of the Web of Data Usin.pdf}
}

@inproceedings{guilloteauLongevityArtifactsLeading2024,
  title = {Longevity of {{Artifacts}} in {{Leading Parallel}} and {{Distributed Systems Conferences}}: A {{Review}} of the {{State}} of the {{Practice}} in 2023},
  shorttitle = {Longevity of {{Artifacts}} in {{Leading Parallel}} and {{Distributed Systems Conferences}}},
  booktitle = {Proceedings of the 2nd {{ACM Conference}} on {{Reproducibility}} and {{Replicability}}},
  author = {Guilloteau, Quentin and Ciorba, Florina and Poquet, Millian and Goepp, Dorian and Richard, Olivier},
  date = {2024-06-18},
  pages = {121--133},
  publisher = {ACM},
  location = {Rennes France},
  doi = {10.1145/3641525.3663631},
  url = {https://dl.acm.org/doi/10.1145/3641525.3663631},
  urldate = {2024-09-04},
  eventtitle = {{{ACM REP}} '24: {{ACM Conference}} on {{Reproducibility}} and {{Replicability}}},
  isbn = {979-8-4007-0530-4},
  langid = {english},
  keywords = {reproducibility}
}

@article{gundersenStateArtReproducibility2018,
  title = {State of the {{Art}}: {{Reproducibility}} in {{Artificial Intelligence}}},
  shorttitle = {State of the {{Art}}},
  author = {Gundersen, Odd Erik and Kjensmo, Sigbjørn},
  date = {2018-04-25},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {32},
  number = {1},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v32i1.11503},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/11503},
  urldate = {2024-10-04},
  abstract = {Background: Research results in artificial intelligence (AI) are criticized for not being reproducible. Objective: To quantify the state of reproducibility of empirical AI research using six reproducibility metrics measuring three different degrees of reproducibility. Hypotheses: 1) AI research is not documented well enough to reproduce the reported results. 2) Documentation practices have improved over time. Method: The literature is reviewed and a set of variables that should be documented to enable reproducibility are grouped into three factors: Experiment, Data and Method. The metrics describe how well the factors have been documented for a paper. A total of 400 research papers from the conference series IJCAI and AAAI have been surveyed using the metrics. Findings: None of the papers document all of the variables. The metrics show that between 20\% and 30\% of the variables for each factor are documented. One of the metrics show statistically significant increase over time while the others show no change. Interpretation: The reproducibility scores decrease with in- creased documentation requirements. Improvement over time is found. Conclusion: Both hypotheses are supported.},
  file = {/home/sam/Zotero/storage/FAW65MYL/Gundersen and Kjensmo - 2018 - State of the Art Reproducibility in Artificial Intelligence.pdf}
}

@book{guntherGuerrillaCapacityPlanning2007,
  title = {Guerrilla {{Capacity Planning}}},
  author = {Gunther, Neil J.},
  date = {2007},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-31010-5},
  url = {http://link.springer.com/10.1007/978-3-540-31010-5},
  urldate = {2022-09-06},
  abstract = {Guerrilla Capacity Planning techniques are the only way to address the so-called 3-month planning horizon This book contains the first and only complete presentation of the "Universal Law of Computational Scaling" Learn how to apply the 3 themes: Guerrilla tactics, Guerrilla scalability, and Guerrilla victories Benefit from the detailed case studies showing how to do Guerrilla website and Internet planning Use the Guerrilla Manual fold-out as an authoritative source to convince your colleagues about capacity planning},
  isbn = {978-3-540-26138-4},
  langid = {english},
  keywords = {queuing theory},
  annotation = {- http://www.perfdynamics.com/Manifesto/USLscalability.html\\
interest: 76}
}

@article{guntherHadoopSuperlinearScalability2015,
  title = {Hadoop Superlinear Scalability},
  author = {Gunther, Neil J. and Puglia, Paul and Tomasette, Kristofer},
  date = {2015-03-23},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {58},
  number = {4},
  pages = {46--55},
  issn = {0001-0782},
  doi = {10.1145/2719919},
  url = {https://doi.org/10.1145/2719919},
  urldate = {2022-09-06},
  abstract = {The perpetual motion of parallel performance.},
  keywords = {high performance computing},
  annotation = {interest: 88}
}

@inproceedings{guoCDEUsingSystem2011,
  title = {{{CDE}}: {{Using System Call Interposition}} to {{Automatically Create Portable Software Packages}}},
  booktitle = {2011 {{USENIX Annual Technical Conference}}},
  author = {Guo, Philip and Engler, Dawson},
  date = {2011-06-14},
  publisher = {USENIX},
  location = {Portland, OR, USA},
  url = {https://www.usenix.org/legacy/events/atc11/tech/final_files/GuoEngler.pdf},
  abstract = {It can be painfully hard to take software that runs on one person’s machine and get it to run on another machine. Online forums and mailing lists are filled with discussions of users' troubles with compiling, installing, and configuring software and their myriad of dependencies. To eliminate this dependency problem, we created a system called CDE that uses system call interposition to monitor the execution of x86-Linux programs and package up the Code, Data, and Environment required to run them on other x86-Linux machines. Creating a CDE package is completely automatic, and running programs within a package requires no installation, configuration, or root permissions. Hundreds of people in both academia and industry have used CDE to distribute software, demo prototypes, make their scientific experiments reproducible, run software natively on older Linux distributions, and deploy experiments to compute clusters.},
  eventtitle = {{{USENIX Annual Technical Conference}}},
  keywords = {project-provenance-pp,record-replay,reproducibility engineering}
}

@inproceedings{guoUsingAutomaticPersistent2011,
  title = {Using Automatic Persistent Memoization to Facilitate Data Analysis Scripting},
  booktitle = {Proceedings of the 2011 {{International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Guo, Philip J. and Engler, Dawson},
  date = {2011-07-17},
  series = {{{ISSTA}} '11},
  pages = {287--297},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2001420.2001455},
  url = {https://doi.org/10.1145/2001420.2001455},
  urldate = {2022-05-12},
  abstract = {Programmers across a wide range of disciplines (e.g., bioinformatics, neuroscience, econometrics, finance, data mining, information retrieval, machine learning) write scripts to parse, transform, process, and extract insights from data. To speed up iteration times, they split their analyses into stages and write extra code to save the intermediate results of each stage to files so that those results do not have to be re-computed in every subsequent run. As they explore and refine hypotheses, their scripts often create and process lots of intermediate data files. They need to properly manage the myriad of dependencies between their code and data files, or else their analyses will produce incorrect results. To enable programmers to iterate quickly without needing to manage intermediate data files, we added a set of dynamic analyses to the programming language interpreter so that it automatically memoizes (caches) the results of long-running pure function calls to disk, manages dependencies between code and on-disk data, and later re-uses memoized results, rather than re-executing those functions, when guaranteed safe to do so. We created an implementation for Python and show how it enables programmers to iterate faster on their data analysis scripts while writing less code and not having to manage dependencies between their code and datasets.},
  isbn = {978-1-4503-0562-4},
  keywords = {project-charmonium.cache,software engineering},
  file = {/home/sam/Zotero/storage/VAKIAC9X/Guo and Engler - 2011 - Using automatic persistent memoization to facilita.pdf}
}

@online{guptaTestingSpeedScale2011,
  title = {Testing at the Speed and Scale of {{Google}}},
  author = {Gupta, Pooja and Ivey, Mark and Penix, John},
  date = {2011-06-07},
  url = {https://google-engtools.blogspot.com/2011/06/testing-at-speed-and-scale-of-google.html},
  file = {/home/sam/Zotero/storage/S6MY5PVM/testing-at-speed-and-scale-of-google.html}
}

@unpublished{guyl.steelejr.GrowingLanguage1998,
  title = {Growing {{A Language}}},
  author = {{Guy L. Steele Jr.}},
  date = {1998},
  location = {OOPSLA},
  url = {https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.68.7907&rep=rep1&type=pdf#page=256},
  abstract = {This talk argues that most programming languages have impoverished vocabularies that make them awkward to use. The talk also makes a case for designing facilities into programming languages to allow the programmer, not just the language designer, to add new facilities to the language, to grow the language for the benefit of other programmers. What makes the talk unusual is that it is reflexive, or metacircular, serving as an example of the phenomenon being described.},
  howpublished = {Lecture},
  keywords = {language engineering},
  file = {/home/sam/Zotero/storage/753RQPAG/Guy L. Steele Jr. - 1998 - Growing A Language.pdf}
}

@inproceedings{gyoriEvaluatingRegressionTest2018,
  title = {Evaluating {{Regression Test Selection Opportunities}} in a {{Very Large Open-Source Ecosystem}}},
  booktitle = {2018 {{IEEE}} 29th {{International Symposium}} on {{Software Reliability Engineering}} ({{ISSRE}})},
  author = {Gyori, Alex and Legunsen, Owolabi and Hariri, Farah and Marinov, Darko},
  date = {2018-10},
  pages = {112--122},
  issn = {2332-6549},
  doi = {10.1109/ISSRE.2018.00022},
  abstract = {Regression testing in very large software ecosystems is notoriously costly, requiring computational resources that even large corporations struggle to cope with. Very large ecosystems contain thousands of rapidly evolving, interconnected projects where client projects transitively depend on library projects. Regression test selection (RTS) reduces regression testing costs by rerunning only tests whose pass/fail behavior may flip after code changes. For single projects, researchers showed that class-level RTS is more effective than lower method-or statement-level RTS. Meanwhile, several very large ecosystems in industry, e.g., at Facebook, Google, and Microsoft, perform project-level RTS, rerunning tests in a changed library and in all its transitive clients. However, there was no previous study of the comparative benefits of class-level and project-level RTS in such ecosystems. We evaluate RTS opportunities in the MAVEN Central open-source ecosystem. There, some popular libraries have up to 924589 clients; in turn, clients can depend on up to 11190 libraries. We sampled 408 popular projects and found that 202 (almost half) cannot update to latest library versions without breaking compilation or tests. If developers want to detect these breakages earlier, they need to run very many tests. We compared four variants of class-level RTS with project-level RTS in MAVEN Central. The results showed that class-level RTS may be an order of magnitude less costly than project-level RTS in very large ecosystems. Specifically, various class-level RTS variants select, on average, 7.8\%-17.4\% of tests selected by project-level RTS.},
  eventtitle = {2018 {{IEEE}} 29th {{International Symposium}} on {{Software Reliability Engineering}} ({{ISSRE}})},
  keywords = {industry practices,opensource software,software engineering,software testing},
  file = {/home/sam/Zotero/storage/NAEPJ5US/Gyori et al. - 2018 - Evaluating Regression Test Selection Opportunities.pdf}
}

@inproceedings{gyoriNonDexToolDetecting2016,
  title = {{{NonDex}}: A Tool for Detecting and Debugging Wrong Assumptions on {{Java API}} Specifications},
  shorttitle = {{{NonDex}}},
  booktitle = {Proceedings of the 2016 24th {{ACM SIGSOFT International Symposium}} on {{Foundations}} of {{Software Engineering}}},
  author = {Gyori, Alex and Lambeth, Ben and Shi, August and Legunsen, Owolabi and Marinov, Darko},
  date = {2016-11-01},
  series = {{{FSE}} 2016},
  pages = {993--997},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2950290.2983932},
  url = {https://doi.org/10.1145/2950290.2983932},
  urldate = {2022-04-10},
  abstract = {We present NonDex, a tool for detecting and debugging wrong assumptions on Java APIs. Some APIs have underdetermined specifications to allow implementations to achieve different goals, e.g., to optimize performance. When clients of such APIs assume stronger-than-specified guarantees, the resulting client code can fail. For example, HashSet’s iteration order is underdetermined, and code assuming some implementation-specific iteration order can fail. NonDex helps to proactively detect and debug such wrong assumptions. NonDex performs detection by randomly exploring different behaviors of underdetermined APIs during test execution. When a test fails during exploration, NonDex searches for the invocation instance of the API that caused the failure. NonDex is open source, well-integrated with Maven, and also runs from the command line. During our experiments with the NonDex Maven plugin, we detected 21 new bugs in eight Java projects from GitHub, and, using the debugging feature of NonDex, we identified the underlying wrong assumptions for these 21 new bugs and 54 previously detected bugs. We opened 13 pull requests; developers already accepted 12, and one project changed the continuous-integration configuration to run NonDex on every push. The demo video is at: https://youtu.be/h3a9ONkC59c},
  isbn = {978-1-4503-4218-6},
  keywords = {API specification,software engineering,software testing},
  file = {/home/sam/Zotero/storage/VFP9R6KM/Gyori et al. - 2016 - NonDex a tool for detecting and debugging wrong a.pdf}
}

@article{haferAssessingOpenSource2009,
  title = {Assessing Open Source Software as a Scholarly Contribution},
  author = {Hafer, Lou and Kirkpatrick, Arthur E.},
  date = {2009-12},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {52},
  number = {12},
  pages = {126--129},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/1610252.1610285},
  url = {https://dl.acm.org/doi/10.1145/1610252.1610285},
  urldate = {2022-06-30},
  abstract = {Academic computer science has an odd relationship with software: Publishing papers about software is considered a distinctly stronger contribution than publishing the software. The historical reasons for this paradox no longer apply, but their legacy remains. This limits researchers who see the open-source software movement as an opportunity to make a scholarly contribution. Expanded definitions of scholarship acknowledge both application and discovery as important components.  One obstacle remains: evaluation. To raise software to the status of a first-class contribution, we propose "best practices" for the evaluation of the scholarly contribution of open-source software.  Typically, scholars who develop software do not include it as a primary contribution for performance reviews. Instead, they write articles about the software and present the articles as contributions. This conflation of articles and software serves neither medium well. An article describes an original intellectual contribution consisting of an idea, the argument for its importance and correctness, and supporting data. In contrast, software is more often an implementation of prior ideas in a usable form. It bridges the often considerable gap between an idea and the practical application of that idea. The original idea and its implementation represent distinct kinds of contribution.  The critical gap is the perceived incomparability of these two contributions. Lacking a concise description adapted to the traditional practices of performance review committees, software is difficult to evaluate as a scholarly contribution and is often relegated to second-class status. We propose a framework for common assessment based on widely accepted definitions of scholarship. Within this general framework, we consider the material and procedures that a performance review committee uses to evaluate a publication. We then describe how software can be summarized in a compatible form of bibliographic citation and supplementary material.},
  langid = {english},
  keywords = {metascience,research software engineering},
  annotation = {interest: 89},
  file = {/home/sam/Zotero/storage/2DAVHVJM/1610252.1610285.pdf}
}

@article{hahnMultiscaleInitialConditions2011,
  title = {Multi-Scale Initial Conditions for Cosmological Simulations},
  author = {Hahn, Oliver and Abel, Tom},
  date = {2011-08-11},
  journaltitle = {Monthly Notices of the Royal Astronomical Society},
  shortjournal = {Monthly Notices of the Royal Astronomical Society},
  volume = {415},
  number = {3},
  eprint = {1103.6031},
  eprinttype = {arXiv},
  pages = {2101--2121},
  issn = {00358711},
  doi = {10.1111/j.1365-2966.2011.18820.x},
  url = {https://doi.org/10.1111/j.1365-2966.2011.18820.x},
  urldate = {2022-04-18},
  abstract = {We discuss a new algorithm to generate multi-scale initial conditions with multiple levels of refinements for cosmological "zoom-in" simulations. The method uses an adaptive convolution of Gaussian white noise with a real space transfer function kernel together with an adaptive multi-grid Poisson solver to generate displacements and velocities following first (1LPT) or second order Lagrangian perturbation theory (2LPT). The new algorithm achieves RMS relative errors of order 10\textasciicircum (-4) for displacements and velocities in the refinement region and thus improves in terms of errors by about two orders of magnitude over previous approaches. In addition, errors are localized at coarse-fine boundaries and do not suffer from Fourier-space induced interference ringing. An optional hybrid multi-grid and Fast Fourier Transform (FFT) based scheme is introduced which has identical Fourier space behaviour as traditional approaches. Using a suite of re-simulations of a galaxy cluster halo our real space based approach is found to reproduce correlation functions, density profiles, key halo properties and subhalo abundances with per cent level accuracy. Finally, we generalize our approach for two-component baryon and dark-matter simulations and demonstrate that the power spectrum evolution is in excellent agreement with linear perturbation theory. For initial baryon density fields, it is suggested to use the local Lagrangian approximation in order to generate a density field for mesh based codes that is consistent with Lagrangian perturbation theory instead of the current practice of using the Eulerian linearly scaled densities.},
  keywords = {astrophysics,cosmological simulation,project-astrophysics},
  file = {/home/sam/Zotero/storage/QP3CADHC/Hahn and Abel - 2011 - Multi-scale initial conditions for cosmological si.pdf;/home/sam/Zotero/storage/Z558N7QW/Hahn and Abel - 2011 - Multi-scale initial conditions for cosmological si.pdf}
}

@article{haidtWhy10Years2022,
  entrysubtype = {newspaper},
  title = {Why the {{Past}} 10 {{Years}} of {{American Life Have Been Uniquely Stupid}}},
  author = {Haidt, Jonathan},
  date = {2022-04-11T10:00:00Z},
  journaltitle = {The Atlantic},
  url = {https://www.theatlantic.com/magazine/archive/2022/05/social-media-democracy-trust-babel/629369/},
  urldate = {2022-09-06},
  abstract = {It’s not just a phase.},
  journalsubtitle = {Ideas},
  langid = {english},
  keywords = {current events},
  annotation = {interest: 73},
  file = {/home/sam/Zotero/storage/9LCUW68N/629369.html}
}

@inproceedings{halpinWhenOwlSameAs2010,
  title = {When Owl:{{sameAs Isn}}’t the {{Same}}: {{An Analysis}} of {{Identity}} in {{Linked Data}}},
  shorttitle = {When Owl},
  booktitle = {The {{Semantic Web}} – {{ISWC}} 2010},
  author = {Halpin, Harry and Hayes, Patrick J. and McCusker, Jamie P. and McGuinness, Deborah L. and Thompson, Henry S.},
  editor = {Patel-Schneider, Peter F. and Pan, Yue and Hitzler, Pascal and Mika, Peter and Zhang, Lei and Pan, Jeff Z. and Horrocks, Ian and Glimm, Birte},
  date = {2010},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {305--320},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-17746-0_20},
  abstract = {In Linked Data, the use of owl:sameAs is ubiquitous in interlinking data-sets. There is however, ongoing discussion about its use, and potential misuse, particularly with regards to interactions with inference. In fact, owl:sameAs can be viewed as encoding only one point on a scale of similarity, one that is often too strong for many of its current uses. We describe how referentially opaque contexts that do not allow inference exist, and then outline some varieties of referentially-opaque alternatives to owl:sameAs. Finally, we report on an empirical experiment over randomly selected owl:sameAs statements from the Web of data. This theoretical apparatus and experiment shed light upon how owl:sameAs is being used (and misused) on the Web of data.},
  isbn = {978-3-642-17746-0},
  langid = {english},
  keywords = {semantic web},
  file = {/home/sam/Zotero/storage/MM4ADANS/Halpin et al. - 2010 - When owlsameAs Isn’t the Same An Analysis of Ide.pdf}
}

@online{hammondBewareXHTML2005a,
  title = {Beware of {{XHTML}}},
  author = {Hammond, David},
  date = {2005-12-06},
  url = {http://www.webdevout.net/articles/beware-of-xhtml},
  urldate = {2022-10-18},
  abstract = {If you're a web developer, you've probably worked a lot with XHTML, the markup language developed in 1999 to implement HTML as an XML format. Most people who use and promote XHTML do so because they think it's the “next version” of HTML, and they may have heard of some benefits here and there. But there is a lot more to it than you may realize, and if you're using it on your website, even if it validates, you are probably using it incorrectly. I believe that XHTML has many good potential applications, and I hope it continues to thrive as a standard. This is precisely why I have written this article. The state of XHTML on the Web today is more broken than the state of HTML, and most people don't realize because the major browsers are using classic HTML parsers that hide the problems. Even among the few sites that know how to trigger the XML parser, the authors tend to overlook some important issues. If you really hope for the XHTML standard to succeed, you should read this article carefully.},
  organization = {Web Devout},
  keywords = {industry practices,internet},
  file = {/home/sam/Zotero/storage/JKKHK5NM/beware-of-xhtml.html}
}

@online{haneyNPMLeftpadHave,
  title = {{{NPM}} \& Left-Pad: {{Have We Forgotten How To Program}}?},
  shorttitle = {{{NPM}} \& Left-Pad},
  author = {Haney, David},
  url = {https://www.davidhaney.io/npm-left-pad-have-we-forgotten-how-to-program/},
  urldate = {2022-04-13},
  abstract = {Intro Okay developers, time to have a serious talk. As you are probably already aware, this week React, Babel, and a bunch of other high-profile packages on NPM broke. The reason they broke is rather astounding: A simple NPM package called left-pad that was a dependency of their code. left-pad, at the time of writing this, has 11 stars on GitHub. The entire package is 11 simple lines that implement a basic left-pad string function.},
  langid = {english},
  organization = {David Haney},
  file = {/home/sam/Zotero/storage/IWZILQIZ/npm-left-pad-have-we-forgotten-how-to-program.html}
}

@article{hannartCausalCounterfactualTheory2016,
  title = {Causal {{Counterfactual Theory}} for the {{Attribution}} of {{Weather}} and {{Climate-Related Events}}},
  author = {Hannart, A. and Pearl, J. and Otto, F. E. L. and Naveau, P. and Ghil, M.},
  date = {2016-01-01},
  journaltitle = {Bulletin of the American Meteorological Society},
  volume = {97},
  number = {1},
  pages = {99--110},
  publisher = {American Meteorological Society},
  issn = {0003-0007, 1520-0477},
  doi = {10.1175/BAMS-D-14-00034.1},
  url = {https://journals.ametsoc.org/view/journals/bams/97/1/bams-d-14-00034.1.xml},
  urldate = {2022-12-18},
  abstract = {Abstract The emergence of clear semantics for causal claims and of a sound logic for causal reasoning is relatively recent, with the consolidation over the past decades of a coherent theoretical corpus of definitions, concepts, and methods of general applicability that is anchored into counterfactuals. The latter corpus has proved to be of high practical interest in numerous applied fields (e.g., epidemiology, economics, and social science). In spite of their rather consensual nature and proven efficacy, these definitions and methods are to a large extent not used in detection and attribution (D\&A). This article gives a brief overview of the main concepts underpinning the causal theory and proposes some methodological extensions for the causal attribution of weather and climate-related events that are rooted into the latter. Implications for the formulation of causal claims and their uncertainty are finally discussed.},
  langid = {english},
  keywords = {statistics},
  annotation = {interest: 95},
  file = {/home/sam/Zotero/storage/U2ML8357/Hannart et al. - 2016 - Causal Counterfactual Theory for the Attribution o.pdf}
}

@inproceedings{hannayHowScientistsDevelop2009,
  title = {How Do Scientists Develop and Use Scientific Software?},
  booktitle = {2009 {{ICSE Workshop}} on {{Software Engineering}} for {{Computational Science}} and {{Engineering}}},
  author = {Hannay, Jo Erskine and MacLeod, Carolyn and Singer, Janice and Langtangen, Hans Petter and Pfahl, Dietmar and Wilson, Greg},
  date = {2009-05},
  pages = {1--8},
  doi = {10.1109/SECSE.2009.5069155},
  abstract = {New knowledge in science and engineering relies increasingly on results produced by scientific software. Therefore, knowing how scientists develop and use software in their research is critical to assessing the necessity for improving current development practices and to making decisions about the future allocation of resources. To that end, this paper presents the results of a survey conducted online in October-December 2008 which received almost 2000 responses. Our main conclusions are that (1) the knowledge required to develop and use scientific software is primarily acquired from peers and through self-study, rather than from formal education and training; (2) the number of scientists using supercomputers is small compared to the number using desktop or intermediate computers; (3) most scientists rely primarily on software with a large user base; (4) while many scientists believe that software testing is important, a smaller number believe they have sufficient understanding about testing concepts; and (5) that there is a tendency for scientists to rank standard software engineering concepts higher if they work in large software development projects and teams, but that there is no uniform trend of association between rank of importance of software engineering concepts and project/team size.},
  eventtitle = {2009 {{ICSE Workshop}} on {{Software Engineering}} for {{Computational Science}} and {{Engineering}}},
  keywords = {research software engineering},
  annotation = {interest: 90},
  file = {/home/sam/Zotero/storage/JMDBN8JV/Hannay et al. - 2009 - How do scientists develop and use scientific softw.pdf;/home/sam/Zotero/storage/JNIU5F6E/5069155.html}
}

@article{hannaySystematicReviewTheory2007,
  title = {A {{Systematic Review}} of {{Theory Use}} in {{Software Engineering Experiments}}},
  author = {Hannay, Jo E. and Sjoberg, Dag I.K. and Dyba, Tore},
  date = {2007-02},
  journaltitle = {IEEE Transactions on Software Engineering},
  shortjournal = {IIEEE Trans. Software Eng.},
  volume = {33},
  number = {2},
  pages = {87--107},
  issn = {0098-5589, 1939-3520},
  doi = {10.1109/TSE.2007.12},
  url = {http://ieeexplore.ieee.org/document/4052585/},
  urldate = {2022-07-25},
  abstract = {Empirically based theories are generally perceived as foundational to science. However, in many disciplines, the nature, role and even the necessity of theories remain matters for debate, particularly in young or practical disciplines such as software engineering. This article reports a systematic review of the explicit use of theory in a comprehensive set of 103 articles reporting experiments, from of a total of 5,453 articles published in major software engineering journals and conferences in the decade 1993-2002. Of the 103 articles, 24 use a total of 40 theories in various ways to explain the cause-effect relationship(s) under investigation. The majority of these use theory in the experimental design to justify research questions and hypotheses, some use theory to provide post hoc explanations of their results, and a few test or modify theory. A third of the theories are proposed by authors of the reviewed articles. The interdisciplinary nature of the theories used is greater than that of research in software engineering in general. We found that theory use and awareness of theoretical issues are present, but that theory-driven research is, as yet, not a major issue in empirical software engineering. Several articles comment explicitly on the lack of relevant theory. We call for an increased awareness of the potential benefits of involving theory, when feasible. To support software engineering researchers who wish to use theory, we show which of the reviewed articles on which topics use which theories for what purposes, as well as details of the theories' characteristics.},
  eventtitle = {{{IEEE Transactions}} on {{Software Engineering}}},
  keywords = {internship-project,social science,sociology,software engineering},
  file = {/home/sam/Zotero/storage/BWPTGK66/Hannay et al. - 2007 - A Systematic Review of Theory Use in Software Engi.pdf;/home/sam/Zotero/storage/C72TY6YA/4052585.html}
}

@inproceedings{hanPROVIOOCentricProvenance2022,
  title = {{{PROV-IO}}: {{An I}}/{{O-Centric Provenance Framework}} for {{Scientific Data}} on {{HPC Systems}}},
  shorttitle = {{{PROV-IO}}},
  booktitle = {Proceedings of the 31st {{International Symposium}} on {{High-Performance Parallel}} and {{Distributed Computing}}},
  author = {Han, Runzhou and Byna, Suren and Tang, Houjun and Dong, Bin and Zheng, Mai},
  date = {2022-06-27},
  series = {{{HPDC}} '22},
  pages = {213--226},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3502181.3531477},
  url = {https://dl.acm.org/doi/10.1145/3502181.3531477},
  urldate = {2024-02-14},
  abstract = {cData provenance, or data lineage, describes the life cycle of data. In scientific workflows on HPC systems, scientists often seek diverse provenance (e.g., origins of data products, usage patterns of datasets). Unfortunately, existing provenance solutions cannot address the challenges due to their incompatible provenance models and/or system implementations. In this paper, we analyze three representative scientific workflows in collaboration with the domain scientists to identify concrete provenance needs. Based on the first-hand analysis, we propose a provenance framework called PROV-IO, which includes an I/O-centric provenance model for describing scientific data and the associated I/O operations and environments precisely. Moreover, we build a prototype of PROV-IO to enable end-to-end provenance support on real HPC systems with little manual effort. The PROV-IO framework provides flexibility in selecting various classes of provenance. Our experiments with realistic workflows show that PROV-IO can address the provenance needs of the domain scientists effectively with reasonable performance (e.g., less than 3.5\% tracking overhead for most experiments). Moreover, PROV-IO outperforms a state-of-the-art system (i.e., ProvLake) in our experiments.},
  isbn = {978-1-4503-9199-3},
  keywords = {hpc,project-provenance-pp,provenance},
  file = {/home/sam/Zotero/storage/FTWVMNMW/Han et al. - 2022 - PROV-IO An IO-Centric Provenance Framework for S.pdf}
}

@article{hassanOmegaLogHighFidelityAttack2020,
  title = {{{OmegaLog}}: {{High-Fidelity Attack Investigation}} via {{Transparent Multi-layer Log Analysis}}},
  shorttitle = {{{OmegaLog}}},
  author = {Hassan, Wajih Ul and Noureddine, Mohammad Ali and Datta, Pubali and Bates, Adam},
  date = {2020-01},
  journaltitle = {Network and Distributed System Security Symposium},
  url = {https://par.nsf.gov/biblio/10146531-omegalog-high-fidelity-attack-investigation-via-transparent-multi-layer-log-analysis},
  urldate = {2023-08-23},
  abstract = {Recent advances in causality analysis have enabled investigators to trace multi-stage attacks using whole- system provenance graphs. Based on system-layer audit logs (e.g., syscalls), these approaches omit vital sources of application context (e.g., email addresses, HTTP response codes) that can found in higher layers of the system. Although this information is often essential to understanding attack behaviors, incorporating this evidence into causal analysis engines is difficult due to the semantic gap that exists between system layers. To address this shortcoming, we propose the notion of universal provenance, which encodes all forensically-relevant causal dependencies regardless of their layer of origin. To transparently realize this vision on commodity systems, we present ωLOG (“Omega Log”), a provenance tracking mechanism that bridges the semantic gap between system and application logging contexts. ωLOG analyzes program binaries to identify and model application-layer logging behaviors, enabling application events to be accurately reconciled with system-layer accesses. ωLOG then intercepts applications’ runtime logging activities and grafts those events onto the system-layer provenance graph, allowing investigators to reason more precisely about the nature of attacks. We demonstrate that ωLOG is widely-applicable to existing software projects and can transparently facilitate execution partitioning of dependency graphs without any training or developer intervention. Evaluation on real-world attack scenarios shows that universal provenance graphs are concise and rich with semantic information as compared to the state-of-the-art, with 12\% average runtime overhead.},
  langid = {english},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/B7Y8LDR8/Hassan et al. - 2020 - OmegaLog High-Fidelity Attack Investigation via T.pdf}
}

@inproceedings{hassanScalableClusterAuditing2018,
  title = {Towards {{Scalable Cluster Auditing}} through {{Grammatical Inference}} over {{Provenance Graphs}}},
  booktitle = {Proceedings 2018 {{Network}} and {{Distributed System Security Symposium}}},
  author = {Hassan, Wajih Ul and Lemay, Mark and Aguse, Nuraini and Bates, Adam and Moyer, Thomas},
  date = {2018},
  publisher = {Internet Society},
  location = {San Diego, CA},
  doi = {10.14722/ndss.2018.23141},
  url = {https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_07B-1_Hassan_paper.pdf},
  urldate = {2023-08-23},
  abstract = {Investigating the nature of system intrusions in large distributed systems remains a notoriously difficult challenge. While monitoring tools (e.g., Firewalls, IDS) provide preliminary alerts through easy-to-use administrative interfaces, attack reconstruction still requires that administrators sift through gigabytes of system audit logs stored locally on hundreds of machines. At present, two fundamental obstacles prevent synergy between system-layer auditing and modern cluster monitoring tools: 1) the sheer volume of audit data generated in a data center is prohibitively costly to transmit to a central node, and 2) systemlayer auditing poses a “needle-in-a-haystack” problem, such that hundreds of employee hours may be required to diagnose a single intrusion.},
  eventtitle = {Network and {{Distributed System Security Symposium}}},
  isbn = {978-1-891562-49-5},
  langid = {english},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/JRBUPWS8/Hassan et al. - 2018 - Towards Scalable Cluster Auditing through Grammati.pdf}
}

@article{heffernanAcademicExodusRole2019,
  title = {The Academic Exodus: The Role of Institutional Support in Academics Leaving Universities and the Academy},
  shorttitle = {The Academic Exodus},
  author = {Heffernan, Troy A. and Heffernan, Amanda},
  date = {2019-01},
  journaltitle = {Professional Development in Education},
  shortjournal = {Professional Development in Education},
  volume = {45},
  number = {1},
  pages = {102--113},
  issn = {1941-5257, 1941-5265},
  doi = {10.1080/19415257.2018.1474491},
  url = {https://www.tandfonline.com/doi/full/10.1080/19415257.2018.1474491},
  urldate = {2022-05-31},
  abstract = {Recent studies argue that in the next 5 years, the higher education sector will see half to two-thirds of its academic workforce leave the academy due to retirement, career burnout, or job dissatisfaction. This study surveyed over 100 working academics in Australia, North America, and the United Kingdom to determine their aspirations for remaining within, or leaving, the academy. The study found that the professional development and career support available to academics played major roles in their career satisfaction. The study’s significance lies in highlighting the types of support academics most value. The paper also explores what motivates participants’ intentions to remain in, or leave, their current positions, or the academy entirely. This assessment occurs at a time when the literature indicates that a significant period of staff turnover is imminent.},
  langid = {english},
  keywords = {academia},
  file = {/home/sam/Zotero/storage/KP3W6VHG/The academic exodus the role of institutional support in academics leaving universities and the academy.pdf}
}

@online{hemsothWhatBadPOSIX2017,
  title = {What's {{So Bad About POSIX I}}/{{O}}?},
  author = {Hemsoth, Nicole},
  date = {2017-09-11T12:19:55+00:00},
  url = {https://www.nextplatform.com/2017/09/11/whats-bad-posix-io/},
  urldate = {2023-01-25},
  abstract = {POSIX I/O is almost universally agreed to be one of the most significant limitations standing in the way of I/O performance exascale system designs push},
  langid = {american},
  organization = {The Next Platform},
  keywords = {filesystems,hpc},
  file = {/home/sam/Zotero/storage/5FLR2P8F/whats-bad-posix-io.html}
}

@unpublished{hendersonSoftwareEngineeringGoogle2020,
  title = {Software {{Engineering}} at {{Google}}},
  author = {Henderson, Fergus},
  date = {2020-01-30},
  eprint = {1702.01715},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1702.01715},
  urldate = {2022-04-07},
  abstract = {We catalog and describe Google's key software engineering practices.},
  keywords = {industry practices,software engineering},
  file = {/home/sam/Zotero/storage/7G9A5CYW/Henderson - 2020 - Software Engineering at Google.pdf}
}

@inproceedings{henkelShipwrightHumanintheLoopSystem2021,
  title = {Shipwright: {{A Human-in-the-Loop System}} for {{Dockerfile Repair}}},
  shorttitle = {Shipwright},
  booktitle = {2021 {{IEEE}}/{{ACM}} 43rd {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Henkel, Jordan and Silva, Denini and Teixeira, Leopoldo and family=Amorim, given=Marcelo, prefix=d’, useprefix=true and Reps, Thomas},
  date = {2021-05},
  pages = {1148--1160},
  issn = {1558-1225},
  doi = {10.1109/ICSE43902.2021.00106},
  abstract = {Docker is a tool for lightweight OS-level virtualization. Docker images are created by performing a build, controlled by a source-level artifact called a Dockerfile. We studied Dockerfiles on GitHub, and-to our great surprise-found that over a quarter of the examined Dockerfiles failed to build (and thus to produce images). To address this problem, we propose SHIPWRIGHT, a human-in-the-loop system for finding repairs to broken Dockerfiles. SHIPWRIGHT uses a modified version of the BERT language model to embed build logs and to cluster broken Dockerfiles. Using these clusters and a search-based procedure, we were able to design 13 rules for making automated repairs to Dockerfiles. With the aid of SHIPWRIGHT, we submitted 45 pull requests (with a 42.2\% acceptance rate) to GitHub projects with broken Dockerfiles. Furthermore, in a "time-travel" analysis of broken Dockerfiles that were later fixed, we found that SHIPWRIGHT proposed repairs that were equivalent to human-authored patches in 22.77\% of the cases we studied. Finally, we compared our work with recent, state-of-the-art, static Dockerfile analyses, and found that, while static tools detected possible build-failure-inducing issues in 20.6-33.8\% of the files we examined, SHIPWRIGHT was able to detect possible issues in 73.25\% of the files and, additionally, provide automated repairs for 18.9\% of the files.},
  eventtitle = {2021 {{IEEE}}/{{ACM}} 43rd {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  keywords = {reproducibility engineering},
  file = {/home/sam/Zotero/storage/ME8QXGUQ/Henkel et al. - 2021 - Shipwright A Human-in-the-Loop System for Dockerf.pdf}
}

@article{henningSPECCPU2000Measuring2000,
  title = {{{SPEC CPU2000}}: Measuring {{CPU}} Performance in the {{New Millennium}}},
  shorttitle = {{{SPEC CPU2000}}},
  author = {Henning, J.L.},
  date = {2000-07},
  journaltitle = {Computer},
  volume = {33},
  number = {7},
  pages = {28--35},
  issn = {1558-0814},
  doi = {10.1109/2.869367},
  abstract = {As computers and software have become more powerful, it seems almost human nature to want the biggest and fastest toy you can afford. But how do you know if your toy is tops? Even if your application never does any I/O, it's not just the speed of the CPU that dictates performance. Cache, main memory, and compilers also play a role. Software applications also have differing performance requirements. So whom do you trust to provide this information? The Standard Performance Evaluation Corporation (SPEC) is a nonprofit consortium whose members include hardware vendors, software vendors, universities, customers, and consultants. SPEC's mission is to develop technically credible and objective component- and system-level benchmarks for multiple operating systems and environments, including high-performance numeric computing, Web servers, and graphical subsystems. On 30 June 2000, SPEC retired the CPU95 benchmark suite. Its replacement is CPU2000, a new CPU benchmark suite with 19 applications that have never before been in a SPEC CPU suite. The article discusses how SPEC developed this benchmark suite and what the benchmarks do.},
  eventtitle = {Computer},
  keywords = {compilers,computer architecture,software benchmarking},
  file = {/home/sam/Zotero/storage/I6USJGMN/Henning - 2000 - SPEC CPU2000 measuring CPU performance in the New.pdf}
}

@article{henningSPECCPU2006Benchmark2006,
  title = {{{SPEC CPU2006}} Benchmark Descriptions},
  author = {Henning, John L.},
  date = {2006-09},
  journaltitle = {ACM SIGARCH Computer Architecture News},
  shortjournal = {SIGARCH Comput. Archit. News},
  volume = {34},
  number = {4},
  pages = {1--17},
  issn = {0163-5964},
  doi = {10.1145/1186736.1186737},
  url = {https://dl.acm.org/doi/10.1145/1186736.1186737},
  urldate = {2024-02-08},
  abstract = {On August 24, 2006, the Standard Performance Evaluation Corporation (SPEC) announced CPU2006 [2], which replaces CPU2000. The SPEC CPU benchmarks are widely used in both industry and academia [3].},
  langid = {english},
  keywords = {benchmarking,project-provenance-pp},
  file = {/home/sam/Zotero/storage/LA6IRPY6/Henning - 2006 - SPEC CPU2006 benchmark descriptions.pdf}
}

@inproceedings{hermannCommunityExpectationsResearch2020,
  title = {Community Expectations for Research Artifacts and Evaluation Processes},
  booktitle = {Proceedings of the 28th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Hermann, Ben and Winter, Stefan and Siegmund, Janet},
  date = {2020-11-08},
  series = {{{ESEC}}/{{FSE}} 2020},
  pages = {469--480},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3368089.3409767},
  url = {https://doi.org/10.1145/3368089.3409767},
  urldate = {2023-01-31},
  abstract = {Background. Artifact evaluation has been introduced into the software engineering and programming languages research community with a pilot at ESEC/FSE 2011 and has since then enjoyed a healthy adoption throughout the conference landscape. Objective. In this qualitative study, we examine the expectations of the community toward research artifacts and their evaluation processes. Method. We conducted a survey including all members of artifact evaluation committees of major conferences in the software engineering and programming language field since the first pilot and compared the answers to expectations set by calls for artifacts and reviewing guidelines. Results. While we find that some expectations exceed the ones expressed in calls and reviewing guidelines, there is no consensus on quality thresholds for artifacts in general. We observe very specific quality expectations for specific artifact types for review and later usage, but also a lack of their communication in calls. We also find problematic inconsistencies in the terminology used to express artifact evaluation’s most important purpose – replicability. Conclusion. We derive several actionable suggestions which can help to mature artifact evaluation in the inspected community and also to aid its introduction into other communities in computer science.},
  isbn = {978-1-4503-7043-1},
  keywords = {reproducibility engineering},
  annotation = {interest: 92},
  file = {/home/sam/Zotero/storage/37D7UR6V/Hermann et al. - 2020 - Community expectations for research artifacts and .pdf}
}

@article{herndonDoesHighPublic2014,
  title = {Does High Public Debt Consistently Stifle Economic Growth? {{A}} Critique of {{Reinhart}} and {{Rogoff}}},
  shorttitle = {Does High Public Debt Consistently Stifle Economic Growth?},
  author = {Herndon, T. and Ash, M. and Pollin, R.},
  date = {2014-03-01},
  journaltitle = {Cambridge Journal of Economics},
  shortjournal = {Cambridge Journal of Economics},
  volume = {38},
  number = {2},
  pages = {257--279},
  issn = {0309-166X, 1464-3545},
  doi = {10.1093/cje/bet075},
  url = {https://academic.oup.com/cje/article-lookup/doi/10.1093/cje/bet075},
  urldate = {2022-05-26},
  abstract = {We replicate Reinhart and Rogoff (2010A and 2010B) and find that selective exclusion of available data, coding errors and inappropriate weighting of summary statistics lead to serious miscalculations that inaccurately represent the relationship between public debt and GDP growth among 20 advanced economies. Over 1946–2009, countries with public debt/GDP ratios above 90\% averaged 2.2\% real annual GDP growth, not −0.1\% as published. The published results for (i) median GDP growth rates for the 1946–2009 period and (ii) mean and median GDP growth figures over 1790–2009 are all distorted by similar methodological errors, although the magnitudes of the distortions are somewhat smaller than with the mean figures for 1946–2009. Contrary to Reinhart and Rogoff’s broader contentions, both mean and median GDP growth when public debt levels exceed 90\% of GDP are not dramatically different from when the public debt/GDP ratios are lower. The relationship between public debt and GDP growth varies significantly by period and country. Our overall evidence refutes RR’s claim that public debt/GDP ratios above 90\% consistently reduce a country’s GDP growth.},
  langid = {english},
  keywords = {internship-project,project-acm-rep,research software engineering,retraction},
  file = {/home/sam/Zotero/storage/4I3PZ5XK/bet075.pdf}
}

@report{herouxCompatibleReproducibilityTaxonomy2018,
  title = {Toward a {{Compatible Reproducibility Taxonomy}} for {{Computational}} and {{Computing Sciences}}},
  author = {Heroux, Michael and Barba, Lorena and Parashar, Manish and Stodden, Victoria and Taufer, Michela},
  date = {2018-10-01},
  number = {SAND--2018-11186, 1481626, 669580},
  pages = {SAND--2018-11186, 1481626, 669580},
  doi = {10.2172/1481626},
  url = {https://www.osti.gov/servlets/purl/1481626/},
  urldate = {2024-10-04}
}

@article{herouxEditorialACMTOMS2015,
  title = {Editorial: {{ACM TOMS Replicated Computational Results Initiative}}},
  shorttitle = {Editorial},
  author = {Heroux, Michael A.},
  date = {2015-06-01},
  journaltitle = {ACM Transactions on Mathematical Software},
  shortjournal = {ACM Trans. Math. Softw.},
  volume = {41},
  number = {3},
  pages = {13:1--13:5},
  issn = {0098-3500},
  doi = {10.1145/2743015},
  url = {https://doi.org/10.1145/2743015},
  urldate = {2023-02-23},
  abstract = {The scientific community relies on the peer review process for assuring the quality of published material, the goal of which is to build a body of work we can trust. Computational journals such as the ACM Transactions on Mathematical Software (TOMS) use this process for rigorously promoting the clarity and completeness of content, and citation of prior work. At the same time, it is unusual to independently confirm computational results. ACM TOMS has established a Replicated Computational Results (RCR) review process as part of the manuscript peer review process. The purpose is to provide independent confirmation that results contained in a manuscript are replicable. Successful completion of the RCR process awards a manuscript with the Replicated Computational Results Designation. This issue of ACM TOMS contains the first [Van Zee and van de Geijn 2015] of what we anticipate to be a growing number of articles to receive the RCR designation, and the related RCR reviewer report [Willenbring 2015]. We hope that the TOMS RCR process will serve as a model for other publications and increase the confidence in and value of computational results in TOMS articles.},
  keywords = {academic publishing,artifact evaluation},
  file = {/home/sam/Zotero/storage/L6CRNSFQ/Heroux - 2015 - Editorial ACM TOMS Replicated Computational Resul.pdf}
}

@report{herouxLightweightSoftwareProcess2019,
  title = {Lightweight {{Software Process Improvement}} Using {{Productivity}} and {{Sustainability Improvement Planning}} ({{PSIP}}).},
  author = {Heroux, Michael A. and Gonsiorowski, Elsa and Gupta, Rinku and Milewicz, Reed and Moulton, David and Watson, Gregory and Willenbring, James Michael and Zamora, Richard and Raybourn, Elaine M.},
  date = {2019-08-01},
  volume = {1190},
  number = {SAND2019-9693C},
  institution = {Sandia National Lab. (SNL-NM), Albuquerque, NM (United States); Sandia National Laboratories, Unknown, Unknown},
  issn = {1865--0929},
  doi = {10.1007/978-3-030-44728-1_6},
  url = {https://www.osti.gov/biblio/1641678},
  urldate = {2022-09-06},
  abstract = {Abstract not provided.},
  langid = {english},
  annotation = {interest: 65},
  file = {/home/sam/Zotero/storage/QXB8SNAC/Heroux et al. - 2019 - Lightweight Software Process Improvement using Pro.pdf;/home/sam/Zotero/storage/RUL32JX4/1641678.html}
}

@article{herschelSurveyProvenanceWhat2017,
  title = {A Survey on Provenance: {{What}} for? {{What}} Form? {{What}} From?},
  shorttitle = {A Survey on Provenance},
  author = {Herschel, Melanie and Diestelkämper, Ralf and Ben Lahmar, Houssem},
  date = {2017-12-01},
  journaltitle = {The VLDB Journal},
  shortjournal = {The VLDB Journal},
  volume = {26},
  number = {6},
  pages = {881--906},
  issn = {0949-877X},
  doi = {10.1007/s00778-017-0486-1},
  url = {https://doi.org/10.1007/s00778-017-0486-1},
  urldate = {2025-01-13},
  abstract = {Provenance refers to any information describing the production process of an end product, which can be anything from a piece of digital data to a physical object. While this survey focuses on the former type of end product, this definition still leaves room for many different interpretations of and approaches to provenance. These are typically motivated by different application domains for provenance (e.g., accountability, reproducibility, process debugging) and varying technical requirements such as runtime, scalability, or privacy. As a result, we observe a wide variety of provenance types and provenance-generating methods. This survey provides an overview of the research field of provenance, focusing on what provenance is used for (what for?), what types of provenance have been defined and captured for the different applications (what form?), and which resources and system requirements impact the choice of deploying a particular provenance solution (what from?). For each of these three key questions, we provide a classification and review the state of the art for each class. We conclude with a summary and possible future research challenges.},
  langid = {english},
  keywords = {Data provenance,Provenance applications,Provenance capture,Provenance requirements,Provenance types,Survey,Workflow provenance},
  file = {/home/sam/Zotero/storage/IYBVJPNB/Herschel et al. - 2017 - A survey on provenance What for What form What from.pdf}
}

@inproceedings{hettneBestPracticesWorkflow2012,
  title = {Best Practices for Workflow Design: How to Prevent Workflow Decay},
  shorttitle = {Best Practices for Workflow Design},
  booktitle = {{{SWAT4LS}}: Semantic Web Applications and Tools for Life Sciences 2012: Proceedings of the 5th International Workshop on Semantic Web Applications and Tools for Life Sciences},
  author = {Hettne, Kristina and Wolstencroft, Katy and Belhajjame, Khalid and Goble, Carole and Mina, Eleni and Dharuri, Harish and Verdes-Montenegro, Lourdes and Garrido, Julián and family=Roure, given=David, prefix=de, useprefix=false and Roos, Marco},
  date = {2012-11-28},
  pages = {23},
  publisher = {RWTH Aachen University},
  url = {https://www.research.manchester.ac.uk/portal/en/publications/best-practices-for-workflow-design(f7ded259-61de-46a6-86a0-611027afa683).html},
  urldate = {2022-06-24},
  abstract = {In this position paper we present a set of best practices for workflow design to prevent workflow decay and increase reuse and re-purposing of scientific workflows. MyExperiment provides access to a large number of scientific workflows. However, scientists find it difficult to reuse or re-purpose these workflows for mainly two reasons: workflows suffer from decay over time and lack sufficient metadata to understand their purpose. We argue that good workflow design is a prerequisite for repairing a workflow, or redesigning an equivalent workflow pattern with new components. We present a set of best practices for workflow design and the semantic tooling that is being developed in the Workflow4Ever (Wf4Ever) project to support these best practices.},
  eventtitle = {5th {{International Workshop}} on {{Semantic Web Applications}} and {{Tools}} for {{Life Sciences}}},
  langid = {english},
  annotation = {interest: 90},
  file = {/home/sam/Zotero/storage/ZXTSDJSF/paper_23.pdf}
}

@online{hettrickJourneyReproducibilityExcel2017,
  title = {A Journey of Reproducibility from {{Excel}} to {{Pandas}}},
  author = {Hettrick, Simon},
  date = {2017-09-06},
  url = {https://www.software.ac.uk/blog/2017-09-06-journey-reproducibility-excel-pandas},
  urldate = {2023-01-24},
  abstract = {This is a story about reproducibility. It’s about the first study I conducted at the Institute, the difficulties I’ve faced in reproducing analysis that was originally conducted in Excel, and it’s testament to the power of a tweet that’s haunted me for three years.},
  langid = {english},
  organization = {Software and research: the Institute's Blog},
  keywords = {data science,project-repro-py,reproducibility engineering,retraction},
  file = {/home/sam/Zotero/storage/KDMYRUV5/2017-09-06-journey-reproducibility-excel-pandas.html}
}

@software{hettrickSoftwaresavedSoftware_In_Research_Survey_2014Software2018,
  title = {Softwaresaved/{{Software}}\_{{In}}\_{{Research}}\_{{Survey}}\_2014: {{Software In Research Survey}}},
  shorttitle = {Softwaresaved/{{Software}}\_{{In}}\_{{Research}}\_{{Survey}}\_2014},
  author = {Hettrick, Simon},
  date = {2018-02-23},
  doi = {10.5281/ZENODO.1183562},
  url = {https://zenodo.org/record/1183562},
  urldate = {2022-05-26},
  abstract = {This reproducible, Python-based re-analysis of the Software Sustainability Institute's 2014 research software survey. The original analysis was conducted in Excel, so this re-analysis was performed to improve the reproducibility of the results.},
  organization = {Zenodo},
  keywords = {internship-project,project-repro-py,research software engineering}
}

@article{hicksSCOREAgileResearch2010,
  title = {{{SCORE}}: Agile Research Group Management},
  shorttitle = {{{SCORE}}},
  author = {Hicks, Michael and Foster, Jeffrey S.},
  date = {2010-10},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {53},
  number = {10},
  pages = {30--31},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/1831407.1831421},
  url = {https://dl.acm.org/doi/10.1145/1831407.1831421},
  urldate = {2022-06-01},
  abstract = {Adapting agile software development methodology toward more efficient management of academic research groups.},
  langid = {english},
  file = {/home/sam/Zotero/storage/8MNASQMA/1831407.1831421.pdf}
}

@inproceedings{hiltonUsageCostsBenefits2016,
  title = {Usage, Costs, and Benefits of Continuous Integration in Open-Source Projects},
  booktitle = {Proceedings of the 31st {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}}},
  author = {Hilton, Michael and Tunnell, Timothy and Huang, Kai and Marinov, Darko and Dig, Danny},
  date = {2016-08-25},
  series = {{{ASE}} '16},
  pages = {426--437},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2970276.2970358},
  url = {https://doi.org/10.1145/2970276.2970358},
  urldate = {2023-02-20},
  abstract = {Continuous integration (CI) systems automate the compilation, building, and testing of software. Despite CI rising as a big success story in automated software engineering, it has received almost no attention from the research community. For example, how widely is CI used in practice, and what are some costs and benefits associated with CI? Without answering such questions, developers, tool builders, and researchers make decisions based on folklore instead of data. In this paper, we use three complementary methods to study the usage of CI in open-source projects. To understand which CI systems developers use, we analyzed 34,544 open-source projects from GitHub. To understand how developers use CI, we analyzed 1,529,291 builds from the most commonly used CI system. To understand why projects use or do not use CI, we surveyed 442 developers. With this data, we answered several key questions related to the usage, costs, and benefits of CI. Among our results, we show evidence that supports the claim that CI helps projects release more often, that CI is widely adopted by the most popular projects, as well as finding that the overall percentage of projects using CI continues to grow, making it important and timely to focus more research on CI.},
  isbn = {978-1-4503-3845-5},
  keywords = {continuous integration,industry practices,project-acm-rep},
  file = {/home/sam/Zotero/storage/ZTMJ8U76/Hilton et al. - 2016 - Usage, costs, and benefits of continuous integrati.pdf}
}

@article{hinesPerformanceComparisonFilesystem2022,
  title = {Performance {{Comparison}} of the {{Filesystem}} and {{Embedded Key-Value Databases}}},
  author = {Hines, Jesse and Cunningham, Nicholas},
  date = {2022-04-21},
  journaltitle = {Campus Research Day},
  url = {https://knowledge.e.southern.edu/crd/2022/noncompetingsessionpm/3},
  keywords = {industry practices,performance engineering},
  annotation = {interest: 60},
  file = {/home/sam/Zotero/storage/Z8W74Y75/3.html}
}

@article{hinkleyRatioTwoCorrelated1969,
  title = {On the Ratio of Two Correlated Normal Random Variables},
  author = {HINKLEY, D. V.},
  date = {1969-12-01},
  journaltitle = {Biometrika},
  shortjournal = {Biometrika},
  volume = {56},
  number = {3},
  pages = {635--639},
  issn = {0006-3444},
  doi = {10.1093/biomet/56.3.635},
  url = {https://doi.org/10.1093/biomet/56.3.635},
  urldate = {2025-01-14},
  abstract = {The distribution of the ratio of two correlated normal random variables is discussed. The exact distribution and an approximation are compared. The comparison is illustrated numerically for the case of the normal least squares estimate of α|β in the linear model E(yi)=α+βυi(i=1,…,n) with uncorrelated normal error terms.},
  file = {/home/sam/Zotero/storage/PU3XMV4X/HINKLEY - 1969 - On the ratio of two correlated normal random variables.pdf;/home/sam/Zotero/storage/V95QHKW6/233807.html}
}

@article{hinsenApproximationTowerComputational2015,
  title = {The {{Approximation Tower}} in {{Computational Science}}: {{Why Testing Scientific Software Is Difficult}}},
  shorttitle = {The {{Approximation Tower}} in {{Computational Science}}},
  author = {Hinsen, Konrad},
  date = {2015-07},
  journaltitle = {Computing in Science \& Engineering},
  volume = {17},
  number = {4},
  pages = {72--77},
  issn = {1558-366X},
  doi = {10.1109/MCSE.2015.75},
  abstract = {Numerical solutions of mathematical equations in scientific models are the result of several approximation steps. Konrad Hinsen uses a simulation of the solar system as an example for illustrating these approximations and explaining their role in the difficult problem of testing scientific software.},
  eventtitle = {Computing in {{Science}} \& {{Engineering}}},
  keywords = {research software engineering},
  annotation = {interest: 75},
  file = {/home/sam/Zotero/storage/YDSNVTGJ/7131419.html}
}

@article{hinsenDealingSoftwareCollapse2019,
  title = {Dealing {{With Software Collapse}}},
  author = {Hinsen, Konrad},
  date = {2019-05},
  journaltitle = {Computing in Science \& Engineering},
  volume = {21},
  number = {3},
  pages = {104--108},
  issn = {1558-366X},
  doi = {10.1109/MCSE.2019.2900945},
  abstract = {Discusses the concept of software collapse. There is A good chance that you have never heard of software collapse before, for the simple reason that it is a term I have made up myself two years ago in a blog post. However, if you have been doing computational science for a few years, there is a good chance that you have experienced software collapse, and probably it was not a pleasant experience. In this paper, I will explain what software collapse is, what causes it, and how you can manage the risk of it happening to you. What I call software collapse is more commonly referred to as software rot: the fact that software stops working eventually if is not actively maintained. The rot metaphor has a long history, the first documented reference being the 1983 edition of the Hacker’s Dictionary. Back then, it was used jokingly by a small community of computer experts who understood the phenomenon perfectly well, and therefore a funny but technically inaccurate metaphor was not a problem. Today, it is being discussed in much wider circles, for example, in the context of reproducible research. In my opinion, it is appropriate to introduce a useful metaphor in place of the traditional humorous one, because good metaphors contribute to a better understanding of what is actually going on. The main issue with the rot metaphor is that it puts the blame on the wrong piece of the puzzle. If software becomes unusable over time, it is not because of any alteration to that software that needs to be reversed. Rather, it is the foundation on which the software has been built, ranging from the actual hardware via the operating system to programming languages and libraries, that has changed so much that the software is no longer compatible with it. Since unstable foundations resemble how a house is destroyed by an earthquake rather than how spoiling food is transformed by fungi, I consider collapse an appropriate metaphor.},
  eventtitle = {Computing in {{Science}} \& {{Engineering}}},
  keywords = {project-acm-rep,research software engineering,software collapse},
  file = {/home/sam/Zotero/storage/J9AJJ73B/Hinsen - 2019 - Dealing With Software Collapse.pdf;/home/sam/Zotero/storage/6FX4X7BB/8701540.html}
}

@article{hinsenEssentialToolsVersion2009,
  title = {Essential {{Tools}}: {{Version Control Systems}}},
  shorttitle = {Essential {{Tools}}},
  author = {Hinsen, Konrad and Läufer, Konstantin and Thiruvathukal, George K.},
  date = {2009-11},
  journaltitle = {Computing in Science \& Engineering},
  volume = {11},
  number = {6},
  pages = {84--91},
  issn = {1558-366X},
  doi = {10.1109/MCSE.2009.194},
  abstract = {Did you ever wish you'd made a backup copy of a file before changing it? Or before applying a collaborator's modifications? Version control systems make this easier, and do a lot more.},
  eventtitle = {Computing in {{Science}} \& {{Engineering}}},
  keywords = {research software engineering},
  annotation = {interest: 72},
  file = {/home/sam/Zotero/storage/QBQ8YXZT/Hinsen et al. - 2009 - Essential Tools Version Control Systems.pdf;/home/sam/Zotero/storage/3A5ANLA9/5337649.html}
}

@article{hinsenMagicContentAddressableStorage2020,
  title = {The {{Magic}} of {{Content-Addressable Storage}}},
  author = {Hinsen, Konrad},
  date = {2020-05},
  journaltitle = {Computing in Science \& Engineering},
  volume = {22},
  number = {3},
  pages = {113--119},
  issn = {1558-366X},
  doi = {10.1109/MCSE.2019.2949441},
  abstract = {The term “content-addressable storage” does not sound exciting, but superficial explanations can make it look like magic, to the point of raising suspicion. In this article, I will show that content-addressable storage is a technology that works, is already in widespread use, and holds many promises for the future of both scientific programming and the management of scientific data. I will start by outlining the theory, and then illustrate how it works in practice, using IPFS, the Inter-Planetary FileSystem.},
  eventtitle = {Computing in {{Science}} \& {{Engineering}}},
  keywords = {research software engineering},
  annotation = {interest: 83},
  file = {/home/sam/Zotero/storage/N9JDQ6HC/Hinsen - 2020 - The Magic of Content-Addressable Storage.pdf;/home/sam/Zotero/storage/LXL93A3X/8887277.html}
}

@article{hinsenVerifiabilityComputeraidedResearch2018,
  title = {Verifiability in Computer-Aided Research: The Role of Digital Scientific Notations at the Human-Computer Interface},
  shorttitle = {Verifiability in Computer-Aided Research},
  author = {Hinsen, Konrad},
  date = {2018-07-23},
  journaltitle = {PeerJ Computer Science},
  shortjournal = {PeerJ Comput. Sci.},
  volume = {4},
  pages = {e158},
  publisher = {PeerJ Inc.},
  issn = {2376-5992},
  doi = {10.7717/peerj-cs.158},
  url = {https://peerj.com/articles/cs-158},
  urldate = {2023-02-23},
  abstract = {Most of today’s scientific research relies on computers and software for processing scientific information. Examples of such computer-aided research are the analysis of experimental data or the simulation of phenomena based on theoretical models. With the rapid increase of computational power, scientific software has integrated more and more complex scientific knowledge in a black-box fashion. As a consequence, its users do not know, and do not even have a chance of finding out, which assumptions and approximations their computations are based on. This black-box nature of scientific software has made the verification of much computer-aided research close to impossible. The present work starts with an analysis of this situation from the point of view of human-computer interaction in scientific research. It identifies the key role of digital scientific notations at the human-computer interface, reviews the most popular ones in use today, and describes a proof-of-concept implementation of Leibniz, a language designed as a verifiable digital scientific notation for models formulated as mathematical equations.},
  langid = {english},
  keywords = {academic publishing},
  file = {/home/sam/Zotero/storage/4J3P6H7V/Hinsen - 2018 - Verifiability in computer-aided research the role.pdf}
}

@article{hinshawNineyearWilkinsonMicrowave2013,
  title = {Nine-Year {{Wilkinson Microwave Anisotropy Probe}} ({{WMAP}}) {{Observations}}: {{Cosmological Parameter Results}}},
  shorttitle = {Nine-Year {{Wilkinson Microwave Anisotropy Probe}} ({{WMAP}}) {{Observations}}},
  author = {Hinshaw, G. and Larson, D. and Komatsu, E. and Spergel, D. N. and Bennett, C. L. and Dunkley, J. and Nolta, M. R. and Halpern, M. and Hill, R. S. and Odegard, N. and Page, L. and Smith, K. M. and Weiland, J. L. and Gold, B. and Jarosik, N. and Kogut, A. and Limon, M. and Meyer, S. S. and Tucker, G. S. and Wollack, E. and Wright, E. L.},
  date = {2013-10-01},
  journaltitle = {The Astrophysical Journal Supplement Series},
  volume = {208},
  pages = {19},
  issn = {0067-0049},
  doi = {10.1088/0067-0049/208/2/19},
  url = {https://ui.adsabs.harvard.edu/abs/2013ApJS..208...19H},
  urldate = {2022-04-11},
  abstract = {We present cosmological parameter constraints based on the final nine-year Wilkinson Microwave Anisotropy Probe (WMAP) data, in conjunction with a number of additional cosmological data sets. The WMAP data alone, and in combination, continue to be remarkably well fit by a six-parameter ΛCDM model. When WMAP data are combined with measurements of the high-l cosmic microwave background anisotropy, the baryon acoustic oscillation scale, and the Hubble constant, the matter and energy densities, Ω b h 2, Ω c h 2, and ΩΛ, are each determined to a precision of \textasciitilde 1.5\%. The amplitude of the primordial spectrum is measured to within 3\%, and there is now evidence for a tilt in the primordial spectrum at the 5σ level, confirming the first detection of tilt based on the five-year WMAP data. At the end of the WMAP mission, the nine-year data decrease the allowable volume of the six-dimensional ΛCDM parameter space by a factor of 68,000 relative to pre-WMAP measurements. We investigate a number of data combinations and show that their ΛCDM parameter fits are consistent. New limits on deviations from the six-parameter model are presented, for example: the fractional contribution of tensor modes is limited to r {$<$} 0.13 (95\% CL); the spatial curvature parameter is limited to \textbackslash Omega \_k = -0.0027\textasciicircum\{+ 0.0039\}\_\{- 0.0038\}; the summed mass of neutrinos is limited to ∑m ν {$<$} 0.44 eV (95\% CL); and the number of relativistic species is found to lie within N eff = 3.84 ± 0.40, when the full data are analyzed. The joint constraint on N eff and the primordial helium abundance, Y He, agrees with the prediction of standard big bang nucleosynthesis. We compare recent Planck measurements of the Sunyaev-Zel'dovich effect with our seven-year measurements, and show their mutual agreement. Our analysis of the polarization pattern around temperature extrema is updated. This confirms a fundamental prediction of the standard cosmological model and provides a striking illustration of acoustic oscillations and adiabatic initial conditions in the early universe.},
  keywords = {astronomical observations,cosmic microwave background,project-astrophysics},
  annotation = {ADS Bibcode: 2013ApJS..208...19H},
  file = {/home/sam/Zotero/storage/MSSK9PFU/Hinshaw_2013_ApJS_208_19.pdf}
}

@online{hitchcockEffectOpenAccess2004,
  type = {Monograph},
  title = {The Effect of Open Access and Downloads ('hits') on Citation Impact: A Bibliography of Studies},
  shorttitle = {The Effect of Open Access and Downloads ('hits') on Citation Impact},
  author = {Hitchcock, Steve},
  date = {2004-09-15},
  publisher = {University of Southampton},
  url = {http://opcit.eprints.org/oacitation-biblio.html},
  urldate = {2024-10-23},
  abstract = {Since 1998 studies have shown that open access increases impact as measured by the number of citations. This chronological and comprehensive bibliography of those reports provides a way of investigating the meaning of that statement, to understand its effect, to recognise the significant factors involved, and identify the causal relationships between the respective factors. Reviews in 2010 suggest a ratio of between 5.5:1 and 7:1 of studies showing an open access citation advantage against those that do not. The bibliography also lists the Web tools available to measure impact. It is a focused bibliography, on the relationship between impact and access. It does not attempt to cover citation impact, or other related topics such as open access, more generally, although some key papers in these areas are listed as jump-off points for wider study. Other important effects can be found in the area of research assessment. There is an occasionally updated section on the correlation between research access, impact and assessment. This is a snapshot of the bibliography taken towards the end June 2013. The research on this topic continues, as does the updated version of the bibliography.},
  langid = {english},
  file = {/home/sam/Zotero/storage/4XHMLZYK/354006.html}
}

@article{hocquetEpistemicIssuesComputational2021,
  title = {Epistemic Issues in Computational Reproducibility: Software as the Elephant in the Room},
  shorttitle = {Epistemic Issues in Computational Reproducibility},
  author = {Hocquet, Alexandre and Wieber, Frédéric},
  date = {2021-04-17},
  journaltitle = {European Journal for Philosophy of Science},
  shortjournal = {Euro Jnl Phil Sci},
  volume = {11},
  number = {2},
  pages = {38},
  issn = {1879-4920},
  doi = {10.1007/s13194-021-00362-9},
  url = {https://doi.org/10.1007/s13194-021-00362-9},
  urldate = {2025-01-14},
  abstract = {Computational reproducibility (i.e. issues of reproducibility stemming from the computer as a scientific tool) possesses its own dynamics and narratives of crisis. Alongside the difficulties of computing as an ubiquitous yet complex scientific activity, computational reproducibility suffers from a naive expectancy of total reproducibility and a moral imperative to embrace the principles of free software as a non-negotiable epistemic virtue. We argue that the epistemic issues at stake in actual practices of computational reproducibility are best unveiled by focusing on software as a pivotal concept, one that is surprisingly often overlooked in accounts of reproducibility issues. Software is not only about designing and coding but also about maintaining, supporting, distributing, licensing, and governance; it is not only about developers but also about users. We focus on openness debates among computational chemists involved in molecular modeling software packages as empirical grounding for our argument. We then identify and analyse four epistemic characteristics (transparency, consistency, sustainability and inclusivity) as key to the role of software in computational reproducibility.},
  langid = {english},
  keywords = {Computational chemistry,Computational reproducibility,Consistency,Inclusivity,Software,Sustainability,Transparency},
  file = {/home/sam/Zotero/storage/7FW8I34X/Hocquet and Wieber - 2021 - Epistemic issues in computational reproducibility software as the elephant in the room.pdf}
}

@article{hoeflerBenchmarkingDataScience2022,
  title = {Benchmarking {{Data Science}}: 12 {{Ways}} to {{Lie With Statistics}} and {{Performance}} on {{Parallel Computers}}},
  shorttitle = {Benchmarking {{Data Science}}},
  author = {Hoefler, Torsten},
  date = {2022-08},
  journaltitle = {Computer},
  volume = {55},
  number = {8},
  pages = {49--56},
  issn = {1558-0814},
  doi = {10.1109/MC.2022.3152681},
  abstract = {We humorously discuss 12 fallacies when focusing on compute performance that we have frequently observed in practice. We follow each with a recommendation to mitigate the danger and hope to contribute to good benchmarking etiquette for data science.},
  eventtitle = {Computer},
  keywords = {software benchmarking,statistics},
  annotation = {interest: 86}
}

@incollection{hoekstraPROVOVizUnderstandingRole2015,
  title = {{{PROV-O-Viz}} - {{Understanding}} the {{Role}} of {{Activities}} in {{Provenance}}},
  booktitle = {Provenance and {{Annotation}} of {{Data}} and {{Processes}}},
  author = {Hoekstra, Rinke and Groth, Paul},
  editor = {Ludäscher, Bertram and Plale, Beth},
  date = {2015},
  volume = {8628},
  pages = {215--220},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-16462-5_18},
  url = {http://link.springer.com/10.1007/978-3-319-16462-5_18},
  urldate = {2022-08-02},
  isbn = {978-3-319-16461-8 978-3-319-16462-5}
}

@report{hoffmanSandiaAnalysisWorkbench2015,
  title = {Sandia {{Analysis Workbench Enabling Advanced Modeling}} \& {{Simulation Technologies}}.},
  author = {Hoffman, Edward L. and Friedman-Hill, Ernest J. and Gibson, Marcus J. and Clay, Robert L.},
  date = {2015-04-01},
  number = {SAND2015-3400C},
  institution = {Sandia National Lab. (SNL-CA), Livermore, CA (United States); Sandia National Laboratories.,},
  url = {https://www.osti.gov/biblio/1251360},
  urldate = {2022-06-14},
  abstract = {Abstract not provided.},
  langid = {english},
  keywords = {internship-project,workflow managers},
  file = {/home/sam/Zotero/storage/UQNHB2M8/document.pdf}
}

@unpublished{hoggDataAnalysisRecipes2010,
  title = {Data Analysis Recipes: {{Fitting}} a Model to Data},
  shorttitle = {Data Analysis Recipes},
  author = {Hogg, David W. and Bovy, Jo and Lang, Dustin},
  date = {2010-08-27},
  eprint = {1008.4686},
  eprinttype = {arXiv},
  eprintclass = {astro-ph, physics:physics},
  url = {http://arxiv.org/abs/1008.4686},
  urldate = {2022-04-18},
  abstract = {We go through the many considerations involved in fitting a model to data, using as an example the fit of a straight line to a set of points in a two-dimensional plane. Standard weighted least-squares fitting is only appropriate when there is a dimension along which the data points have negligible uncertainties, and another along which all the uncertainties can be described by Gaussians of known variance; these conditions are rarely met in practice. We consider cases of general, heterogeneous, and arbitrarily covariant two-dimensional uncertainties, and situations in which there are bad data (large outliers), unknown uncertainties, and unknown but expected intrinsic scatter in the linear relationship being fit. Above all we emphasize the importance of having a "generative model" for the data, even an approximate one. Once there is a generative model, the subsequent fitting is non-arbitrary because the model permits direct computation of the likelihood of the parameters or the posterior probability distribution. Construction of a posterior probability distribution is indispensible if there are "nuisance parameters" to marginalize away.},
  keywords = {data analysis,scientific method,statistics},
  annotation = {interest: 80},
  file = {/home/sam/Zotero/storage/6C3R66IV/Hogg et al. - 2010 - Data analysis recipes Fitting a model to data.pdf;/home/sam/Zotero/storage/JDKSETB4/1008.html}
}

@misc{holdenIncreasingAccessResults2013,
  title = {Increasing {{Access}} to the {{Results}} of {{Federally Funded Scientific Research}}},
  author = {Holden, John P.},
  date = {2013-02-22},
  organization = {Executive Office of the President, Office of Science and Technology Policy},
  keywords = {open data,project-acm-rep},
  file = {/home/sam/Zotero/storage/MBP57769/Holden - 2013 - Increasing Access to the Results of Federally Fund.pdf}
}

@article{hollandPASSingProvenanceChallenge2008,
  title = {{{PASSing}} the Provenance Challenge},
  author = {Holland, David A. and Seltzer, Margo I. and Braun, Uri and Muniswamy-Reddy, Kiran-Kumar},
  date = {2008},
  journaltitle = {Concurrency and Computation: Practice and Experience},
  volume = {20},
  number = {5},
  pages = {531--540},
  issn = {1532-0634},
  doi = {10.1002/cpe.1227},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.1227},
  urldate = {2023-08-23},
  abstract = {Provenance-aware storage systems (PASS) are a new class of storage system treating provenance as a first-class object, providing automatic collection, storage, and management of provenance as well as query capabilities. We developed the first PASS prototype between 2005 and 2006, targeting scientific end users. Prior to undertaking the provenance challenge, we had focused on provenance collection and storage, without much emphasis on a query model or language. The challenge forced us to (quickly) develop a query model and infrastructure implementing this model. We present a brief overview of the PASS prototype and a discussion of the evolution of the query model that we developed for the challenge. Copyright © 2007 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/VD4HIMW2/Holland et al. - 2008 - PASSing the provenance challenge.pdf;/home/sam/Zotero/storage/8HRDN6SV/cpe.html}
}

@article{hongFAIRPrinciplesResearch2022,
  title = {{{FAIR Principles}} for {{Research Software}} ({{FAIR4RS Principles}})},
  author = {Hong, Neil P. Chue and Katz, Daniel S. and Barker, Michelle and Lamprecht, Anna-Lena and Martinez, Carlos and Psomopoulos, Fotis E. and Harrow, Jen and Castro, Leyla Jael and Gruenpeter, Morane and Martinez, Paula Andrea and Honeyman, Tom and Struck, Alexander and Lee, Allen and Loewe, Axel and family=Werkhoven, given=Ben, prefix=van, useprefix=false and Garijo, Daniel and Plomp, Esther and Genova, Francoise and Shanahan, Hugh and Hellström, Maggie and Sandström, Malin and Sinha, Manodeep and Kuzak, Mateusz and Herterich, Patricia and Islam, Sharif and Sansone, Susanna-Assunta and Pollard, Tom and Atmojo, Udayanto Dwi and Williams, Alan and Czerniak, Andreas and Niehues, Anna and Fouilloux, Anne Claire and Desinghu, Bala and Goble, Carole and Richard, Céline and Gray, Charles and Erdmann, Chris and Nüst, Daniel and Tartarini, Daniele and Ranguelova, Elena and Anzt, Hartwig and Todorov, Ilian and McNally, James and Burnett, Jessica and Garrido-Sánchez, Julián and Belhajjame, Khalid and Sesink, Laurents and Hwang, Lorraine and Tovani-Palone, Marcos Roberto and Wilkinson, Mark D. and Servillat, Mathieu and Liffers, Matthias and Fox, Merc and Miljković, Nadica and Lynch, Nick and Lavanchy, Paula Martinez and Gesing, Sandra and Stevens, Sarah and Cuesta, Sergio Martinez and Peroni, Silvio and Soiland-Reyes, Stian and Bakker, Tom and Rabemanantsoa, Tovo and Sochat, Vanessa and Yehudi, Yo and Wg, Fair4rs},
  date = {2022-03-16},
  doi = {10.15497/RDA00065},
  url = {https://www.research.manchester.ac.uk/portal/en/publications/fair-principles-for-research-software-fair4rs-principles(751dfce3-56e5-441f-8e3f-8c1401e0a1e0).html},
  urldate = {2022-09-14},
  langid = {english},
  keywords = {reproducibility engineering,research software engineering},
  file = {/home/sam/Zotero/storage/T9QPY4VT/fair-principles-for-research-software-fair4rs-principles(751dfce3-56e5-441f-8e3f-8c1401e0a1e0).html}
}

@article{hongWhyWeNeed2016,
  title = {Why Do We Need to Compare Research Software, and How Should We Do It?},
  author = {Hong, Neil Chue},
  date = {2016-09-14},
  journaltitle = {Proceedings of the Fourth Workshop on Sustainable Software for Science: Practice and Experiences (WSSSPE4): University of Manchester, Manchester, UK, September 12-­14, 2016},
  publisher = {CEUR Workshop Proceedings (CEUR-WS.org)},
  url = {https://www.research.ed.ac.uk/en/publications/why-do-we-need-to-compare-research-software-and-how-should-we-do-},
  urldate = {2022-09-06},
  langid = {english},
  keywords = {research software engineering},
  annotation = {interest: 85},
  file = {/home/sam/Zotero/storage/YE7IR9SP/Hong - 2016 - Why do we need to compare research software, and h.pdf;/home/sam/Zotero/storage/5L276GGK/why-do-we-need-to-compare-research-software-and-how-should-we-do-.html}
}

@report{horaAtomPublishingProtocol2007,
  type = {Request for Comments},
  title = {The {{Atom Publishing Protocol}}},
  author = {family=hÓra, given=Bill, prefix=de, useprefix=false and Gregorio, Joe},
  date = {2007-10},
  number = {RFC 5023},
  institution = {Internet Engineering Task Force},
  url = {https://datatracker.ietf.org/doc/rfc5023/},
  urldate = {2022-08-05},
  abstract = {The Atom Publishing Protocol (AtomPub) is an application-level protocol for publishing and editing Web resources. The protocol is based on HTTP transfer of Atom-formatted representations. The Atom format is documented in the Atom Syndication Format.},
  keywords = {semantic web}
}

@inproceedings{hortonDockerizeMeAutomaticInference2019,
  title = {{{DockerizeMe}}: {{Automatic Inference}} of {{Environment Dependencies}} for {{Python Code Snippets}}},
  shorttitle = {{{DockerizeMe}}},
  booktitle = {2019 {{IEEE}}/{{ACM}} 41st {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Horton, Eric and Parnin, Chris},
  date = {2019-05},
  pages = {328--338},
  issn = {1558-1225},
  doi = {10.1109/ICSE.2019.00047},
  abstract = {Platforms like Stack Overflow and GitHub's gist system promote the sharing of ideas and programming techniques via the distribution of code snippets designed to illustrate particular tasks. Python, a popular and fast-growing programming language, sees heavy use on both sites, with nearly one million questions asked on Stack Overflow and 400 thousand public gists on GitHub. Unfortunately, around 75\% of the Python example code shared through these sites cannot be directly executed. When run in a clean environment, over 50\% of public Python gists fail due to an import error for a missing library. We present DockerizeMe, a technique for inferring the dependencies needed to execute a Python code snippet without import error. DockerizeMe starts with offline knowledge acquisition of the resources and dependencies for popular Python packages from the Python Package Index (PyPI). It then builds Docker specifications using a graph-based inference procedure. Our inference procedure resolves import errors in 892 out of nearly 3,000 gists from the Gistable dataset for which Gistable's baseline approach could not find and install all dependencies.},
  eventtitle = {2019 {{IEEE}}/{{ACM}} 41st {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  keywords = {automated program repair},
  annotation = {interest: 97},
  file = {/home/sam/Zotero/storage/EUEXFHR6/Horton and Parnin - 2019 - DockerizeMe Automatic Inference of Environment De.pdf}
}

@inproceedings{hortonGistableEvaluatingExecutability2018,
  title = {Gistable: {{Evaluating}} the {{Executability}} of {{Python Code Snippets}} on {{GitHub}}},
  shorttitle = {Gistable},
  booktitle = {2018 {{IEEE International Conference}} on {{Software Maintenance}} and {{Evolution}} ({{ICSME}})},
  author = {Horton, Eric and Parnin, Chris},
  date = {2018-09},
  pages = {217--227},
  issn = {2576-3148},
  doi = {10.1109/ICSME.2018.00031},
  abstract = {Software developers create and share code online to demonstrate programming language concepts and programming tasks. Code snippets can be a useful way to explain and demonstrate a programming concept, but may not always be directly executable. A code snippet can contain parse errors, or fail to execute if the environment contains unmet dependencies. This paper presents an empirical analysis of the executable status of Python code snippets shared through the GitHub gist system, and the ability of developers familiar with software configuration to correctly configure and run them. We find that 75.6\% of gists require non-trivial configuration to overcome missing dependencies, configuration files, reliance on a specific operating system, or some other environment configuration. Our study also suggests the natural assumption developers make about resource names when resolving configuration errors is correct less than half the time. We also present Gistable, a database and extensible framework built on GitHub's gist system, which provides executable code snippets to enable reproducible studies in software engineering. Gistable contains 10,259 code snippets, approximately 5,000 with a Dockerfile to configure and execute them without import error. Gistable is publicly available at https://github.com/gistable/gistable.},
  eventtitle = {2018 {{IEEE International Conference}} on {{Software Maintenance}} and {{Evolution}} ({{ICSME}})},
  keywords = {automated program repair},
  annotation = {interest: 97}
}

@inproceedings{hortonV2FastDetection2019,
  title = {V2: {{Fast Detection}} of {{Configuration Drift}} in {{Python}}},
  shorttitle = {V2},
  booktitle = {2019 34th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  author = {Horton, Eric and Parnin, Chris},
  date = {2019-11},
  pages = {477--488},
  issn = {2643-1572},
  doi = {10.1109/ASE.2019.00052},
  abstract = {Code snippets are prevalent, but are hard to reuse because they often lack an accompanying environment configuration. Most are not actively maintained, allowing for drift between the most recent possible configuration and the code snippet as the snippet becomes out-of-date over time. Recent work has identified the problem of validating and detecting out-of-date code snippets as the most important consideration for code reuse. However, determining if a snippet is correct, but simply out-of-date, is a non-trivial task. In the best case, breaking changes are well documented, allowing developers to manually determine when a code snippet contains an out-of-date API usage. In the worst case, determining if and when a breaking change was made requires an exhaustive search through previous dependency versions. We present V2, a strategy for determining if a code snippet is out-of-date by detecting discrete instances of configuration drift, where the snippet uses an API which has since undergone a breaking change. Each instance of configuration drift is classified by a failure encountered during validation and a configuration patch, consisting of dependency version changes, which fixes the underlying fault. V2 uses feedback-directed search to explore the possible configuration space for a code snippet, reducing the number of potential environment configurations that need to be validated. When run on a corpus of public Python snippets from prior research, V2 identifies 248 instances of configuration drift.},
  eventtitle = {2019 34th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  keywords = {automated program repair},
  annotation = {interest: 97},
  file = {/home/sam/Zotero/storage/P336SN7G/Horton and Parnin - 2019 - V2 Fast Detection of Configuration Drift in Pytho.pdf;/home/sam/Zotero/storage/XC9LARJJ/8952262.html}
}

@article{hosnyYourResearchReproducible2016,
  title = {Is Your Research Reproducible?},
  author = {Hosny, Abdelrahman},
  date = {2016-06-13},
  journaltitle = {XRDS: Crossroads, The ACM Magazine for Students},
  shortjournal = {XRDS},
  volume = {22},
  number = {4},
  pages = {14--15},
  issn = {1528-4972},
  doi = {10.1145/2951008},
  url = {https://doi.org/10.1145/2951008},
  urldate = {2022-09-14},
  abstract = {The XRDS blog highlights a range of topics from conference coverage, to security and privacy, to CS theory. Selected blog posts, edited for print, are featured in every issue. Please visit xrds.acm.org/blog to read each post in its entirety. If you are interested in joining as a student blogger, please contact us.},
  keywords = {reproducibility engineering},
  annotation = {interest: 92},
  file = {/home/sam/Zotero/storage/55JQIT6M/Hosny - 2016 - Is your research reproducible.pdf}
}

@unpublished{hosteHowMakePackage2018,
  title = {How {{To Make Package Managers Cry}}},
  author = {Hoste, Kenneth},
  date = {2018-02-03},
  url = {https://archive.fosdem.org/2018/schedule/event/how_to_make_package_managers_cry/},
  urldate = {2023-01-31},
  abstract = {In this talk, I will outline how (primarily scientific) software developers have found ways to complicate the job of the people who are responsible for compiling, installing and/or packaging their software, mainly in the context of multi-user high-performance computing environments. Next to an overview of the commonly used techniques, the motivations behind them, and the excuses that software developers can use to get away with it, I will showcase a couple of examples of software applications that have done a great job to make the life of package managers (in the broad sense) as miserable as possible.},
  eventtitle = {{{FOSDEM}} '18},
  langid = {english},
  venue = {Brussels, Belgium},
  keywords = {industry practices,package managers},
  file = {/home/sam/Zotero/storage/FAVEMRFE/Hoste - 2018 - How To Make Package Managers Cry.pdf;/home/sam/Zotero/storage/JP7AB7TG/how_to_make_package_managers_cry.html}
}

@article{HowEditorsEdit2019,
  title = {How Editors Edit},
  date = {2019-02},
  journaltitle = {Nature Methods},
  shortjournal = {Nat Methods},
  volume = {16},
  number = {2},
  pages = {135--135},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/s41592-019-0324-z},
  url = {https://www.nature.com/articles/s41592-019-0324-z},
  urldate = {2022-09-06},
  abstract = {We shed some light on how the Nature Methods editorial team evaluates papers submitted to the journal.},
  issue = {2},
  langid = {english},
  keywords = {academic publishing,metascience},
  annotation = {interest: 89},
  file = {/home/sam/Zotero/storage/V2ITCBWD/2019 - How editors edit.pdf;/home/sam/Zotero/storage/AVGFVAYD/s41592-019-0324-z.html}
}

@online{HowEffectiveASLR2021,
  title = {How {{Effective}} Is {{ASLR}} on {{Linux Systems}}?},
  shorttitle = {How {{Effective}} Is {{ASLR}} on {{Linux Systems}}?},
  date = {2021-01-18},
  url = {https://web.archive.org/web/20210118035736/http://securityetalii.es/2013/02/03/how-effective-is-aslr-on-linux-systems/},
  urldate = {2022-11-15},
  keywords = {operating systems,security},
  file = {/home/sam/Zotero/storage/HP3IM7K2/how-effective-is-aslr-on-linux-systems.html}
}

@inproceedings{howisonIncentivesIntegrationScientific2013,
  title = {Incentives and Integration in Scientific Software Production},
  booktitle = {Proceedings of the 2013 Conference on {{Computer}} Supported Cooperative Work},
  author = {Howison, James and Herbsleb, James D.},
  date = {2013-02-23},
  series = {{{CSCW}} '13},
  pages = {459--470},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2441776.2441828},
  url = {https://doi.org/10.1145/2441776.2441828},
  urldate = {2022-08-25},
  abstract = {Science policy makers are looking for approaches to increase the extent of collaboration in the production of scientific software, looking to open collaborations in open source software for inspiration. We examine the software ecosystem surrounding BLAST, a key bioinformatics tool, identifying outside improvements and interviewing their authors. We find that academic credit is a powerful motivator for the production and revealing of improvements. Yet surprisingly, we also find that improvements motivated by academic credit are less likely to be integrated than those with other motivations, including financial gain. We argue that this is because integration makes it harder to see who has contributed what and thereby undermines the ability of reputation to function as a reward for collaboration. We consider how open source avoids these issues and conclude with policy approaches to promoting wider collaboration by addressing incentives for integration.},
  isbn = {978-1-4503-1331-5},
  keywords = {research software engineering},
  annotation = {interest: 87},
  file = {/home/sam/Zotero/storage/V2RE4LGK/Howison and Herbsleb - 2013 - Incentives and integration in scientific software .pdf}
}

@article{howisonRetractBitrottenPublications2014,
  title = {Retract Bit-Rotten Publications: {{Aligning}} Incentives for Sustaining Scientific Software},
  shorttitle = {Retract Bit-Rotten Publications},
  author = {Howison, James},
  date = {2014-07-20},
  publisher = {figshare},
  doi = {10.6084/m9.figshare.1111632.v1},
  url = {https://figshare.com/articles/journal_contribution/Retract_bit_rotten_publications_Aligning_incentives_for_sustaining_scientific_software/1111632/1},
  urldate = {2023-02-23},
  abstract = {A provocation for the WSSSPE2 workshop},
  langid = {english},
  keywords = {continuous integration,project-provenance-pp,reproducibility engineering},
  file = {/home/sam/Zotero/storage/7VJNSRVB/1111632.html}
}

@inproceedings{howisonScientificSoftwareProduction2011,
  title = {Scientific Software Production: Incentives and Collaboration},
  shorttitle = {Scientific Software Production},
  booktitle = {Proceedings of the {{ACM}} 2011 Conference on {{Computer}} Supported Cooperative Work},
  author = {Howison, James and Herbsleb, James D.},
  date = {2011-03-19},
  series = {{{CSCW}} '11},
  pages = {513--522},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1958824.1958904},
  url = {https://doi.org/10.1145/1958824.1958904},
  urldate = {2022-08-25},
  abstract = {Software plays an increasingly critical role in science, including data analysis, simulations, and managing workflows. Unlike other technologies supporting science, software can be copied and distributed at essentially no cost, potentially opening the door to unprecedented levels of sharing and collaborative innovation. Yet we do not have a clear picture of how software development for science fits into the day-to-day practice of science, or how well the methods and incentives of its production facilitate realization of this potential. We report the results of a multiple-case study of software development in three fields: high energy physics, structural biology, and microbiology. In each case, we identify a typical publication, and use qualitative methods to explore the production of the software used in the science represented by the publication. We identify several different production systems, characterized primarily by differences in incentive structures. We identify ways in which incentives are matched and mismatched with the needs of the science fields, especially with respect to collaboration.},
  isbn = {978-1-4503-0556-3},
  keywords = {research software engineering},
  annotation = {interest: 87}
}

@article{howisonUnderstandingScientificSoftware2015,
  title = {Understanding the Scientific Software Ecosystem and Its Impact: {{Current}} and Future Measures},
  shorttitle = {Understanding the Scientific Software Ecosystem and Its Impact},
  author = {Howison, James and Deelman, Ewa and McLennan, Michael J. and Ferreira da Silva, Rafael and Herbsleb, James D.},
  date = {2015-10-01},
  journaltitle = {Research Evaluation},
  shortjournal = {Research Evaluation},
  volume = {24},
  number = {4},
  pages = {454--470},
  issn = {0958-2029},
  doi = {10.1093/reseval/rvv014},
  url = {https://doi.org/10.1093/reseval/rvv014},
  urldate = {2022-08-25},
  abstract = {Software is increasingly important to the scientific enterprise, and science-funding agencies are increasingly funding software work. Accordingly, many different participants need insight into how to understand the relationship between software, its development, its use, and its scientific impact. In this article, we draw on interviews and participant observation to describe the information needs of domain scientists, software component producers, infrastructure providers, and ecosystem stewards, including science funders. We provide a framework by which to categorize different types of measures and their relationships as they reach around from funding, development, scientific use, and through to scientific impact. We use this framework to organize a presentation of existing measures and techniques, and to identify areas in which techniques are either not widespread, or are entirely missing. We conclude with policy recommendations designed to improve insight into the scientific software ecosystem, make it more understandable, and thereby contribute to the progress of science.},
  keywords = {research software engineering},
  annotation = {interest: 85},
  file = {/home/sam/Zotero/storage/Y5XA3U4I/Howison et al. - 2015 - Understanding the scientific software ecosystem an.pdf;/home/sam/Zotero/storage/S4H8K6WL/1518466.html}
}

@article{hullTavernaToolBuilding2006,
  title = {Taverna: A Tool for Building and Running Workflows of Services},
  shorttitle = {Taverna},
  author = {Hull, Duncan and Wolstencroft, Katy and Stevens, Robert and Goble, Carole and Pocock, Mathew R. and Li, Peter and Oinn, Tom},
  date = {2006-07-01},
  journaltitle = {Nucleic Acids Research},
  shortjournal = {Nucleic Acids Research},
  volume = {34},
  pages = {W729-W732},
  issn = {0305-1048},
  doi = {10.1093/nar/gkl320},
  url = {https://doi.org/10.1093/nar/gkl320},
  urldate = {2022-09-06},
  abstract = {Taverna is an application that eases the use and integration of the growing number of molecular biology tools and databases available on the web, especially web services. It allows bioinformaticians to construct workflows or pipelines of services to perform a range of different analyses, such as sequence analysis and genome annotation. These high-level workflows can integrate many different resources into a single analysis. Taverna is available freely under the terms of the GNU Lesser General Public License (LGPL) from http://taverna.sourceforge.net/ .},
  issue = {suppl\_2},
  keywords = {project-acm-rep,reproducibility engineering,workflow managers},
  annotation = {interest: 89},
  file = {/home/sam/Zotero/storage/5YI4EXBV/Hull et al. - 2006 - Taverna a tool for building and running workflows.pdf}
}

@article{hunter-zinckTenSimpleRules2021,
  title = {Ten Simple Rules on Writing Clean and Reliable Open-Source Scientific Software},
  author = {Hunter-Zinck, Haley and family=Siqueira, given=Alexandre Fioravante, prefix=de, useprefix=false and Vásquez, Váleri N. and Barnes, Richard and Martinez, Ciera C.},
  date = {2021-11-11},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  volume = {17},
  number = {11},
  pages = {e1009481},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1009481},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009481},
  urldate = {2023-04-18},
  abstract = {Functional, usable, and maintainable open-source software is increasingly essential to scientific research, but there is a large variation in formal training for software development and maintainability. Here, we propose 10 “rules” centered on 2 best practice components: clean code and testing. These 2 areas are relatively straightforward and provide substantial utility relative to the learning investment. Adopting clean code practices helps to standardize and organize software code in order to enhance readability and reduce cognitive load for both the initial developer and subsequent contributors; this allows developers to concentrate on core functionality and reduce errors. Clean coding styles make software code more amenable to testing, including unit tests that work best with modular and consistent software code. Unit tests interrogate specific and isolated coding behavior to reduce coding errors and ensure intended functionality, especially as code increases in complexity; unit tests also implicitly provide example usages of code. Other forms of testing are geared to discover erroneous behavior arising from unexpected inputs or emerging from the interaction of complex codebases. Although conforming to coding styles and designing tests can add time to the software development project in the short term, these foundational tools can help to improve the correctness, quality, usability, and maintainability of open-source scientific software code. They also advance the principal point of scientific research: producing accurate results in a reproducible way. In addition to suggesting several tips for getting started with clean code and testing practices, we recommend numerous tools for the popular open-source scientific software languages Python, R, and Julia.},
  langid = {english},
  keywords = {research software engineering},
  file = {/home/sam/Zotero/storage/P36CISCP/Hunter-Zinck et al. - 2021 - Ten simple rules on writing clean and reliable ope.pdf}
}

@inproceedings{hvatumWhatThinkConway,
  title = {What Do {{I}} Think about {{Conway}}'s {{Law}} Now?},
  author = {Hvatum, Lise and Kelly, Allan and EuroPLoP focus group},
  eventtitle = {{{EuroPLoP}} 2005},
  keywords = {human factors,software engineering},
  file = {/home/sam/Zotero/storage/WY7DNYQ9/document.pdf}
}

@article{igarashiPolymorphicGradualTyping2017,
  title = {On Polymorphic Gradual Typing},
  author = {Igarashi, Yuu and Sekiyama, Taro and Igarashi, Atsushi},
  date = {2017-08-29},
  journaltitle = {Proceedings of the ACM on Programming Languages},
  shortjournal = {Proc. ACM Program. Lang.},
  volume = {1},
  pages = {40:1--40:29},
  doi = {10.1145/3110284},
  url = {https://doi.org/10.1145/3110284},
  urldate = {2022-09-06},
  abstract = {We study an extension of gradual typing—a method to integrate dynamic typing and static typing smoothly in a single language—to parametric polymorphism and its theoretical properties, including conservativity of typing and semantics over both statically and dynamically typed languages, type safety, blame-subtyping theorem, and the gradual guarantee—the so-called refined criteria, advocated by Siek et al. We develop System FG, which is a gradually typed extension of System F with the dynamic type and a new type consistency relation, and translation to a new polymorphic blame calculus System FC, which is based on previous polymorphic blame calculi by Ahmed et al. The design of System FG and System FC, geared to the criteria, is influenced by the distinction between static and gradual type variables, first observed by Garcia and Cimini. This distinction is also useful to execute statically typed code without incurring additional overhead to manage type names as in the prior calculi. We prove that System FG satisfies most of the criteria: all but the hardest property of the gradual guarantee on semantics. We show that a key conjecture to prove the gradual guarantee leads to the Jack-of-All-Trades property, conjectured as an important property of the polymorphic blame calculus by Ahmed et al.},
  issue = {ICFP},
  keywords = {programming languages},
  annotation = {interest: 84},
  file = {/home/sam/Zotero/storage/8T5XHS57/Igarashi et al. - 2017 - On polymorphic gradual typing.pdf}
}

@book{ike-nwosuPythonVirtualMachine2015,
  title = {Inside {{The Python Virtual Machine}}},
  author = {Ike-Nwosu, Obi},
  date = {2015-08-25T11:00:43},
  publisher = {Leanpub},
  url = {https://leanpub.com/insidethepythonvirtualmachine},
  urldate = {2022-08-31},
  abstract = {You know how to program in Python but are interested in what goes on under the covers of the interpreter? Well, fasten your seat-belts as this book will take you on a tour of ~the virtual machine that runs your Python code. It will describe how Python code is compiled and run, how the language itself can be modified and will demystify the myster...},
  langid = {english},
  keywords = {programming languages},
  file = {/home/sam/Zotero/storage/LYA73UU3/insidethepythonvirtualmachine.html}
}

@standard{InternationalVocabularyMetrology2012,
  title = {International {{Vocabulary}} of {{Metrology}} – {{Basic}} and General Concepts and Associated Terms ({{VIM}})},
  date = {2012},
  number = {200:2012},
  publisher = {JCGM},
  doi = {10.59161/JCGM200-2012},
  url = {https://www.bipm.org/doi/10.59161/JCGM200-2012},
  urldate = {2024-10-25},
  langid = {english},
  version = {3rd Ed},
  file = {/home/sam/Zotero/storage/9V7QZ9G2/2012 - International Vocabulary of Metrology – Basic and general concepts and associated terms (VIM), 3rd e.pdf}
}

@article{ioannidisDiscussionWhyEstimate2014,
  title = {Discussion: {{Why}} “{{An}} Estimate of the Science-Wise False Discovery Rate and Application to the Top Medical Literature” Is False},
  shorttitle = {Discussion},
  author = {Ioannidis, John P. A.},
  date = {2014-01-01},
  journaltitle = {Biostatistics},
  shortjournal = {Biostatistics},
  volume = {15},
  number = {1},
  pages = {28--36},
  issn = {1465-4644},
  doi = {10.1093/biostatistics/kxt036},
  url = {https://doi.org/10.1093/biostatistics/kxt036},
  urldate = {2022-11-15},
  abstract = {Jager and Leek have tried to estimate a false-discovery rate (FDR) in abstracts of articles published in five medical journals during 2000–2010. Their approach is flawed in sampling, calculations, and conclusions. It uses a tiny portion of select papers in highly select journals. Randomized controlled trials and systematic reviews (designs with the lowest anticipated false-positive rates) are 52\% of the analyzed papers, while these designs account for only 4\% in PubMed in the same period. The FDR calculations consider the entire published literature as equivalent to a single genomic experiment where all performed analyses are reported without selection or distortion. However, the data used are the P-values reported in the abstracts of published papers; these P-values are a highly distorted, highly select sample. Besides selective reporting biases, all other biases, in particular confounding in observational studies, are also ignored, while these are often the main drivers for high false-positive rates in the biomedical literature. A reproducibility check of the raw data shows that much of the data Jager and Leek used are either wrong or make no sense: most of the usable data were missed by their script, 94\% of the abstracts that reported ≥2 P-values had high correlation/overlap between reported outcomes, and only a minority of P-values corresponded to relevant primary outcomes. The Jager and Leek paper exemplifies the dreadful combination of using automated scripts with wrong methods and unreliable data. Sadly, this combination is common in the medical literature.},
  keywords = {metascience},
  annotation = {interest: 83},
  file = {/home/sam/Zotero/storage/4PJV977B/Ioannidis - 2014 - Discussion Why “An estimate of the science-wise f.pdf;/home/sam/Zotero/storage/E8YRU35D/245654.html}
}

@article{ioannidisWhyMostPublished2005,
  title = {Why {{Most Published Research Findings Are False}}},
  author = {Ioannidis, John P. A.},
  date = {2005-08-30},
  journaltitle = {PLOS Medicine},
  shortjournal = {PLOS Medicine},
  volume = {2},
  number = {8},
  pages = {e124},
  publisher = {Public Library of Science},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.0020124},
  url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124},
  urldate = {2022-09-14},
  abstract = {Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
  langid = {english},
  file = {/home/sam/Zotero/storage/YNN44PSX/Ioannidis - 2005 - Why Most Published Research Findings Are False.pdf;/home/sam/Zotero/storage/BKJXJ6QD/article.html}
}

@online{iryBriefIncompleteMostly2009,
  title = {A {{Brief}}, {{Incomplete}}, and {{Mostly Wrong History}} of {{Programming Languages}}},
  author = {Iry, James},
  date = {2009-05-07},
  url = {http://james-iry.blogspot.com/2009/05/brief-incomplete-and-mostly-wrong.html},
  urldate = {2022-08-31},
  organization = {One Div Zero},
  keywords = {programming languages},
  file = {/home/sam/Zotero/storage/9X3PN93S/brief-incomplete-and-mostly-wrong.html}
}

@report{iso/iec25010:2011SystemsSoftwareEngineering2011,
  title = {Systems and Software Engineering — {{Systems}} and Software {{Quality Requirements}} and {{Evaluation}} ({{SQuaRE}}) — {{System}} and Software Quality Models},
  shorttitle = {{{ISO}}/{{IEC}} 25010},
  author = {ISO/IEC 25010:2011},
  date = {2011-03},
  institution = {ISO/IEC},
  url = {https://www.iso.org/cms/render/live/en/sites/isoorg/contents/data/standard/03/57/35733.html},
  urldate = {2022-06-01},
  abstract = {Systems and software engineering — Systems and software Quality Requirements and Evaluation (SQuaRE) — System and software quality models},
  langid = {english},
  keywords = {internship-project,software engineering},
  file = {/home/sam/Zotero/storage/XXFX4J8W/claerbout1992.pdf}
}

@article{jacobToolDesigningReplicable2021,
  title = {[{{Tool}}] {{Designing Replicable Networking Experiments With Triscale}}},
  author = {Jacob, Romain and Zimmerling, Marco and Boano, Carlo Alberto and Vanbever, Laurent and Thiele, Lothar},
  date = {2021},
  journaltitle = {Journal of Systems Research},
  volume = {1},
  number = {1},
  doi = {10.5070/SR31155408},
  url = {https://escholarship.org/uc/item/63n4s9w2},
  urldate = {2022-11-15},
  abstract = {When designing their performance evaluations, networking researchers often encounter questions such as: How long should a run be? How many runs to perform? How to account for the variability across multiple runs? What statistical methods should be used to analyze the data? Despite their best intentions, researchers often answer these questions differently, thus impairing the replicability of their evaluations and the confidence in their results. In this paper, we propose a concrete methodology for the design and analysis of performance evaluations. Our approach hierarchically partitions the performance evaluation into three timescales, following the principle of separation of concerns. The idea is to understand, for each timescale, the temporal characteristics of variability sources, and then to apply rigorous statistical methods to derive performance results with quantifiable confidence in spite of the inherent variability. We implement this methodology in a software framework called TriScale. For each performance metric, TriScale computes a variability score that estimates, with a given confidence, how similar the results would be if the evaluation were replicated; in other words, TriScale quantifies the replicability of evaluations. We showcase the practicality and usefulness of TriScale on four different case studies demonstrating that TriScale helps to generalize and strengthen published results. Improving the standards of replicability in networking is a complex challenge. This paper is an important contribution to this endeavor; it provides networking researchers with a rational and concrete experimental methodology rooted in sound statistical foundations. The first of its kind.},
  langid = {english},
  keywords = {networking,reproducibility engineering},
  file = {/home/sam/Zotero/storage/P6EIBREB/Jacob et al. - 2021 - [Tool] Designing Replicable Networking Experiments.pdf;/home/sam/Zotero/storage/SXCTEL8F/63n4s9w2.html}
}

@article{jagerEstimateSciencewiseFalse2014,
  title = {An Estimate of the Science-Wise False Discovery Rate and Application to the Top Medical Literature},
  author = {Jager, Leah R. and Leek, Jeffrey T.},
  date = {2014-01-01},
  journaltitle = {Biostatistics},
  shortjournal = {Biostatistics},
  volume = {15},
  number = {1},
  pages = {1--12},
  issn = {1465-4644},
  doi = {10.1093/biostatistics/kxt007},
  url = {https://doi.org/10.1093/biostatistics/kxt007},
  urldate = {2022-11-15},
  abstract = {The accuracy of published medical research is critical for scientists, physicians and patients who rely on these results. However, the fundamental belief in the medical literature was called into serious question by a paper suggesting that most published medical research is false. Here we adapt estimation methods from the genomics community to the problem of estimating the rate of false discoveries in the medical literature using reported \$P\$-values as the data. We then collect \$P\$-values from the abstracts of all 77\,430 papers published in The Lancet, The Journal of the American Medical Association, The New England Journal of Medicine, The British Medical Journal, and The American Journal of Epidemiology between 2000 and 2010. Among these papers, we found 5322 reported \$P\$-values. We estimate that the overall rate of false discoveries among reported results is 14\% (s.d. 1\%), contrary to previous claims. We also found that there is no a significant increase in the estimated rate of reported false discovery results over time (0.5\% more false positives (FP) per year, \$P = 0.18\$) or with respect to journal submissions (0.5\% more FP per 100 submissions, \$P = 0.12\$). Statistical analysis must allow for false discoveries in order to make claims on the basis of noisy data. But our analysis suggests that the medical literature remains a reliable record of scientific progress.},
  keywords = {metascience},
  annotation = {interest: 83},
  file = {/home/sam/Zotero/storage/5VLASBNL/Jager and Leek - 2014 - An estimate of the science-wise false discovery ra.pdf;/home/sam/Zotero/storage/2QQXSR5Y/244509.html}
}

@inproceedings{janinCAREComprehensiveArchiver2014,
  title = {{{CARE}}, the Comprehensive Archiver for Reproducible Execution},
  booktitle = {Proceedings of the 1st {{ACM SIGPLAN Workshop}} on {{Reproducible Research Methodologies}} and {{New Publication Models}} in {{Computer Engineering}}},
  author = {Janin, Yves and Vincent, Cédric and Duraffort, Rémi},
  date = {2014-06-09},
  series = {{{TRUST}} '14},
  pages = {1--7},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2618137.2618138},
  url = {https://dl.acm.org/doi/10.1145/2618137.2618138},
  urldate = {2024-02-14},
  abstract = {We present CARE, the Comprehensive Archiver for Reproducible Execution on Linux. CARE runs in userland, requires no setup and performs a single task: building an archive that contains selected executables and files accessed by a given application during an observation run. To reproduce computational results from this initial run, it is then enough to unpack the archive that comes equipped with all necessary tools for re-execution in a confined environment. Technically, CARE leverages on PRoot, a generic system call interposition engine that relies on the ptrace mechanism to monitor (and if needed to modify) system calls emitted by applications under scrutiny. PRoot is extensible and CARE is properly speaking an extension of PRoot. CARE is available on x86\_64, x86 and ARM processors, and benefits from a new history-based algorithm that automatically selects files to be stored in a CARE archive.},
  isbn = {978-1-4503-2951-4},
  keywords = {operating systems,project-provenance-pp,provenance},
  file = {/home/sam/Zotero/storage/L8GLA9K4/Janin et al. - 2014 - CARE, the comprehensive archiver for reproducible .pdf}
}

@article{jiangGeneratingMergerTrees2014,
  title = {Generating Merger Trees for Dark Matter Haloes: A Comparison of Methods},
  shorttitle = {Generating Merger Trees for Dark Matter Haloes},
  author = {Jiang, Fangzhou and family=Bosch, given=Frank C., prefix=van den, useprefix=true},
  date = {2014-05-01},
  journaltitle = {Monthly Notices of the Royal Astronomical Society},
  volume = {440},
  number = {1},
  pages = {193--207},
  issn = {1365-2966, 0035-8711},
  doi = {10.1093/mnras/stu280},
  url = {http://academic.oup.com/mnras/article/440/1/193/1747580/Generating-merger-trees-for-dark-matter-haloes-a},
  urldate = {2022-07-22},
  abstract = {Halo merger trees describe the hierarchical assembly of dark matter haloes, and are the backbone for modelling galaxy formation and evolution. Merger trees constructed using Monte Carlo algorithms based on the extended Press–Schechter (EPS) formalism are complementary to using N-body simulations and have the advantage that they are not trammelled by limited numerical resolution and uncertainties in identifying and linking (sub)haloes. This paper compares multiple EPS-based merger tree algorithms to simulation results using four diagnostics: progenitor mass function, mass assembly history (MAH), merger rate per descendant halo and the unevolved subhalo mass function. Spherical collapse-based methods typically overpredict major-merger rates, whereas ellipsoidal collapse dramatically overpredicts the minor-merger rate for massive haloes. The only algorithm in our comparison that yields results in good agreement with simulations is that by Parkinson et al. (P08). We emphasize, though, that the simulation results used as benchmarks in testing the merger trees are hampered by significant uncertainties themselves: MAHs and merger rates from different studies easily disagree by 50 per\,cent, even when based on the same simulation. Given this status quo, the P08 merger trees can be considered as accurate as those extracted from simulations.},
  langid = {english},
  keywords = {dark matter halos,project-astrophysics}
}

@inproceedings{jiEnablingRefinableCrossHost2018,
  title = {Enabling {{Refinable}} \{\vphantom\}{{Cross-Host}}\vphantom\{\} {{Attack Investigation}} with {{Efficient Data Flow Tagging}} and {{Tracking}}},
  author = {Ji, Yang and Lee, Sangho and Fazzini, Mattia and Allen, Joey and Downing, Evan and Kim, Taesoo and Orso, Alessandro and Lee, Wenke},
  date = {2018},
  pages = {1705--1722},
  url = {https://www.usenix.org/conference/usenixsecurity18/presentation/jia-yang},
  urldate = {2023-08-23},
  abstract = {Investigating attacks across multiple hosts is challenging. The true dependencies between security-sensitive files, network endpoints, or memory objects from different hosts can be easily concealed by dependency explosion or undefined program behavior (e.g., memory corruption). Dynamic information flow tracking (DIFT) is a potential solution to this problem, but, existing DIFT techniques only track information flow within a single host and lack an efficient mechanism to maintain and synchronize the data flow tags globally across multiple hosts.  In this paper, we propose RTAG, an efficient data flow tagging and tracking mechanism that enables practical cross-host attack investigations. RTAG is based on three novel techniques. First, by using a record-and-replay technique, it decouples the dependencies between different data flow tags from the analysis, enabling lazy synchronization between independent and parallel DIFT instances of different hosts. Second, it takes advantage of systemcall-level provenance information to calculate and allocate the optimal tag map in terms of memory consumption. Third, it embeds tag information into network packets to track cross-host data flows with less than 0.05\% network bandwidth overhead. Evaluation results show that RTAG is able to recover the true data flows of realistic cross-host attack scenarios. Performance wise, RTAG reduces the memory consumption of DIFT-based analysis by up to 90\% and decreases the overall analysis time by 60\%–90\% compared with previous investigation systems.},
  eventtitle = {27th {{USENIX Security Symposium}} ({{USENIX Security}} 18)},
  isbn = {978-1-939133-04-5},
  langid = {english},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/Y526VM3S/Ji et al. - 2018 - Enabling Refinable Cross-Host Attack Investigati.pdf}
}

@inproceedings{jindalMagpiePythonSpeed2021,
  title = {Magpie: {{Python}} at {{Speed}} and {{Scale}} Using {{Cloud Backends}}},
  shorttitle = {Magpie},
  booktitle = {11th {{Conference}} on {{Innovative Data Systems Research}}, {{CIDR}} 2021, {{Virtual Event}}, {{January}} 11-15, 2021, {{Online Proceedings}}},
  author = {Jindal, Alekh and Emani, K. Venkatesh and Daum, Maureen and Poppe, Olga and Haynes, Brandon and Pavlenko, Anna and Gupta, Ayushi and Ramachandra, Karthik and Curino, Carlo and Müller, Andreas and Wu, Wentao and Patel, Hiren},
  date = {2021-02},
  publisher = {www.cidrdb.org},
  location = {Virtual event},
  url = {https://www.microsoft.com/en-us/research/publication/magpie-python-at-speed-and-scale-using-cloud-backends/},
  urldate = {2022-10-18},
  abstract = {Python has become overwhelmingly popular for ad-hoc data analysis, and Pandas dataframes have quickly become the de facto standard~ API for data science. However, performance and scaling to large datasets remain significant challenges. This is in stark contrast with the~ world of databases, where decades of investments have led to both sub-millisecond latencies for small queries and many orders of~ magnitude better scalability for large analytical queries. Furthermore, databases offer enterprise-grade features (e.g., transactions, fine-grained access control, tamper-proof logging, encryption) as well as a mature ecosystem of tools in modern clouds. In this paper, we bring together the ease of use and versatility of Python environments with the enterprise-grade, high-performance query processing of cloud database systems. We describe a system we are building, coined Magpie, which exposes the popular Pandas API while lazily pushing large chunks of computation into scalable, efficient, and secured database engines. Magpie assists the data scientist by automatically selecting the most efficient engine (e.g., SQL DW, SCOPE, Spark) in cloud environments that offer multiple engines atop a data lake. Magpie’s common data layer virtually eliminates data transfer costs across potentially many such engines. We describe experiments pushing Python dataframe programs into the SQL DW, Spark, and SCOPE query engines. An initial analysis of our production workloads suggest that over a quarter of the computations in our internal analytics clusters could be optimized through Magpie by picking the optimal backend.},
  eventtitle = {Conference on {{Innovative Data Systems Research}} ({{CIDR}})},
  keywords = {industry practices,performance engineering},
  annotation = {interest: 75}
}

@inproceedings{jiRAINRefinableAttack2017,
  title = {{{RAIN}}: {{Refinable Attack Investigation}} with {{On-demand Inter-Process Information Flow Tracking}}},
  shorttitle = {{{RAIN}}},
  booktitle = {Proceedings of the 2017 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Ji, Yang and Lee, Sangho and Downing, Evan and Wang, Weiren and Fazzini, Mattia and Kim, Taesoo and Orso, Alessandro and Lee, Wenke},
  date = {2017-10-30},
  series = {{{CCS}} '17},
  pages = {377--390},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3133956.3134045},
  url = {https://dl.acm.org/doi/10.1145/3133956.3134045},
  urldate = {2023-08-23},
  abstract = {As modern attacks become more stealthy and persistent, detecting or preventing them at their early stages becomes virtually impossible. Instead, an attack investigation or provenance system aims to continuously monitor and log interesting system events with minimal overhead. Later, if the system observes any anomalous behavior, it analyzes the log to identify who initiated the attack and which resources were affected by the attack and then assess and recover from any damage incurred. However, because of a fundamental tradeoff between log granularity and system performance, existing systems typically record system-call events without detailed program-level activities (e.g., memory operation) required for accurately reconstructing attack causality or demand that every monitored program be instrumented to provide program-level information. To address this issue, we propose RAIN, a Refinable Attack INvestigation system based on a record-replay technology that records system-call events during runtime and performs instruction-level dynamic information flow tracking (DIFT) during on-demand process replay. Instead of replaying every process with DIFT, RAIN conducts system-call-level reachability analysis to filter out unrelated processes and to minimize the number of processes to be replayed, making inter-process DIFT feasible. Evaluation results show that RAIN effectively prunes out unrelated processes and determines attack causality with negligible false positive rates. In addition, the runtime overhead of RAIN is similar to existing system-call level provenance systems and its analysis overhead is much smaller than full-system DIFT.},
  isbn = {978-1-4503-4946-8},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/T2R2UAKP/Ji et al. - 2017 - RAIN Refinable Attack Investigation with On-deman.pdf}
}

@inproceedings{jiRecProvProvenanceAwareUser2016,
  title = {{{RecProv}}: {{Towards Provenance-Aware User Space Record}} and {{Replay}}},
  shorttitle = {{{RecProv}}},
  booktitle = {Provenance and {{Annotation}} of {{Data}} and {{Processes}}},
  author = {Ji, Yang and Lee, Sangho and Lee, Wenke},
  editor = {Mattoso, Marta and Glavic, Boris},
  date = {2016},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {3--15},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-40593-3_1},
  abstract = {Deterministic record and replay systems have widely been used in software debugging, failure diagnosis, and intrusion detection. In order to detect the Advanced Persistent Threat (APT), online execution needs to be recorded with acceptable runtime overhead; then, investigators can analyze the replayed execution with heavy dynamic instrumentation. While most record and replay systems rely on kernel module or OS virtualization, those running at user space are favoured for being lighter weight and more portable without any of the changes needed for OS/Kernel virtualization. On the other hand, higher level provenance data at a higher level provides dynamic analysis with system causalities and hugely increases its efficiency. Considering both benefits, we propose a provenance-aware user space record and replay system, called RecProv. RecProv is designed to provide high provenance fidelity; specifically, with versioning files from the recorded trace logs and integrity protection to provenance data through real-time trace isolation. The collected provenance provides the high-level system dependency that helps pinpoint suspicious activities where further analysis can be applied. We show that RecProv is able to output accurate provenance in both visualized graph and W3C standardized PROV-JSON formats.},
  isbn = {978-3-319-40593-3},
  langid = {english},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/R4637A8D/Ji et al. - 2016 - RecProv Towards Provenance-Aware User Space Recor.pdf}
}

@online{johnsonMoreDotsSyntactic,
  title = {More {{Dots}}: {{Syntactic Loop Fusion}} in {{Julia}}},
  shorttitle = {More {{Dots}}},
  author = {Johnson, Steven G.},
  url = {https://julialang.org/blog/2017/01/moredots/},
  urldate = {2022-09-06},
  abstract = {More Dots: Syntactic Loop Fusion in Julia | After a lengthy design process (https://github.com/JuliaLang/julia/issues/8450) and preliminary foundations in Julia 0.5 (/blog/2016-10-11-julia-0.5-highlights\#vectorized\_function\_calls), Julia 0.6 includes new facilities for writing code in the},
  langid = {english},
  keywords = {programming languages},
  annotation = {interest: 72},
  file = {/home/sam/Zotero/storage/JW898XIL/moredots.html}
}

@article{joppaTroublingTrendsScientific2013,
  title = {Troubling {{Trends}} in {{Scientific Software Use}}},
  author = {Joppa, Lucas N. and McInerny, Greg and Harper, Richard and Salido, Lara and Takeda, Kenji and O'Hara, Kenton and Gavaghan, David and Emmott, Stephen},
  date = {2013-05-17},
  journaltitle = {Science},
  shortjournal = {Science},
  volume = {340},
  number = {6134},
  pages = {814--815},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1231535},
  url = {https://www.science.org/doi/10.1126/science.1231535},
  urldate = {2022-06-06},
  abstract = {"Blind trust" is dangerous when choosing software to support research.            ,                               Software pervades every domain of science (                                  1                                –                                  3                                ), perhaps nowhere more decisively than in modeling. In key scientific areas of great societal importance, models and the software that implement them define both how science is done and what science is done (                                  4                                ,                                  5                                ). Across all science, this dependence has led to concerns around the need for open access to software (                                  6                                ,                                  7                                ), centered on the reproducibility of research (                                  1                                ,                                  8                                –                                  10                                ). From fields such as high-performance computing, we learn key insights and best practices for how to develop, standardize, and implement software (                                  11                                ). Open and systematic approaches to the development of software are essential for all sciences. But for many scientists this is not sufficient. We describe problems with the adoption and use of scientific software.},
  langid = {english},
  keywords = {research software engineering},
  annotation = {interest: 69},
  file = {/home/sam/Zotero/storage/STM4P92Z/joppa2013.pdf}
}

@report{josephCreatingEconomicModels2013,
  title = {Creating {{Economic Models Showing}} the {{Relationship Between Investments}} in {{HPC}} and the {{Resulting Financial ROI}} and {{Innovation}} — and {{How It Can Impact}} a {{Nation}}'s {{Competitiveness}} and {{Innovation}}},
  author = {Joseph, Earl C. and Dekate, Chirag and Conway, Steve},
  date = {2013-09},
  number = {243296},
  institution = {IDC},
  url = {https://www.hpcuserforum.com/ROI/},
  urldate = {2022-05-23},
  abstract = {This pilot study investigates how high-performance computing (HPC) investments can improve economic success and increase scientific innovation. This research is focused on the common good and should be useful to DOE, other government agencies, industry, and academia. The study has created two unique economic models and an innovation index: - A macroeconomic model that depicts the way HPC investments result in economic advancements in the form of ROI in revenue (GDP), profits (and cost savings), and jobs. - A macroeconomic model that depicts the way HPC investments result in basic and applied innovations, looking at variations by sector, industry, country, and organization size. - A new innovation index that provides a means of measuring and comparing innovation levels. Key findings of the pilot study include: - IDC is able to collect the required data across a broad set of organizations, with enough detail to create the economic models and the innovation index. - Early results indicate very substantial returns for investments in HPC: - \$356.5 on average in revenue per dollar of HPC invested - \$38.7 on average of profits (or cost savings) per dollar of HPC invested - The average number of years before returns started was 1.9 years. - The average HPC investment per innovation was \$3.1 million.},
  keywords = {hpc,project-devsecops},
  file = {/home/sam/Zotero/storage/NNEJDBPB/243296 IDC HPC and ROI Pilot Study 10.30.2013.pdf}
}

@inproceedings{jupyterBinder20Reproducible2018,
  title = {Binder 2.0 - {{Reproducible}}, Interactive, Sharable Environments for Science at Scale},
  author = {Jupyter, Project and Bussonnier, Matthias and Forde, Jessica and Freeman, Jeremy and Granger, Brian and Head, Tim and Holdgraf, Chris and Kelley, Kyle and Nalvarte, Gladys and Osheroff, Andrew and Pacer, M and Panda, Yuvi and Perez, Fernando and Ragan-Kelley, Benjamin and Willing, Carol},
  date = {2018},
  pages = {113--120},
  location = {Austin, Texas},
  doi = {10.25080/Majora-4af1f417-011},
  url = {https://doi.curvenote.com/10.25080/Majora-4af1f417-011},
  urldate = {2024-10-04},
  eventtitle = {Python in {{Science Conference}}},
  file = {/home/sam/Zotero/storage/E4RC3GSH/Jupyter et al. - 2018 - Binder 2.0 - Reproducible, interactive, sharable environments for science at scale.pdf}
}

@incollection{juristoReplicationSoftwareEngineering2012,
  title = {Replication of {{Software Engineering Experiments}}},
  booktitle = {Empirical {{Software Engineering}} and {{Verification}}},
  author = {Juristo, Natalia and Gómez, Omar S.},
  editor = {Meyer, Bertrand and Nordio, Martin},
  date = {2012},
  volume = {7007},
  pages = {60--88},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-25231-0_2},
  url = {http://link.springer.com/10.1007/978-3-642-25231-0_2},
  urldate = {2022-06-30},
  isbn = {978-3-642-25230-3 978-3-642-25231-0},
  keywords = {metascience,reproducibility engineering,software benchmarking}
}

@online{kaliberaQuantifyingPerformanceChanges2020,
  title = {Quantifying {{Performance Changes}} with {{Effect Size Confidence Intervals}}},
  author = {Kalibera, Tomas and Jones, Richard},
  date = {2020-07-21},
  eprint = {2007.10899},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2007.10899},
  url = {http://arxiv.org/abs/2007.10899},
  urldate = {2023-08-22},
  abstract = {Measuring performance \& quantifying a performance change are core evaluation techniques in programming language and systems research. Of 122 recent scientific papers, as many as 65 included experimental evaluation that quantified a performance change using a ratio of execution times. Few of these papers evaluated their results with the level of rigour that has come to be expected in other experimental sciences. The uncertainty of measured results was largely ignored. Scarcely any of the papers mentioned uncertainty in the ratio of the mean execution times, and most did not even mention uncertainty in the two means themselves. Most of the papers failed to address the non-deterministic execution of computer programs (caused by factors such as memory placement, for example), and none addressed non-deterministic compilation. It turns out that the statistical methods presented in the computer systems performance evaluation literature for the design and summary of experiments do not readily allow this either. This poses a hazard to the repeatability, reproducibility and even validity of quantitative results. Inspired by statistical methods used in other fields of science, and building on results in statistics that did not make it to introductory textbooks, we present a statistical model that allows us both to quantify uncertainty in the ratio of (execution time) means and to design experiments with a rigorous treatment of those multiple sources of non-determinism that might impact measured performance. Better still, under our framework summaries can be as simple as "system A is faster than system B by 5.5\% \$\textbackslash pm\$ 2.5\%, with 95\% confidence", a more natural statement than those derived from typical current practice, which are often misinterpreted. November 2013},
  pubstate = {prepublished},
  keywords = {project-provenance-pp,software benchmarking},
  file = {/home/sam/Zotero/storage/U4JKMEN2/Kalibera and Jones - 2020 - Quantifying Performance Changes with Effect Size C.pdf;/home/sam/Zotero/storage/U9LM4IQU/2007.html}
}

@article{kanewalaTestingScientificSoftware2014,
  title = {Testing Scientific Software: {{A}} Systematic Literature Review},
  shorttitle = {Testing Scientific Software},
  author = {Kanewala, Upulee and Bieman, James M.},
  date = {2014-10-01},
  journaltitle = {Information and Software Technology},
  shortjournal = {Inf. Softw. Technol.},
  volume = {56},
  number = {10},
  pages = {1219--1232},
  issn = {0950-5849},
  doi = {10.1016/j.infsof.2014.05.006},
  url = {https://doi.org/10.1016/j.infsof.2014.05.006},
  urldate = {2023-05-06},
  abstract = {Context: Scientific software plays an important role in critical decision making, for example making weather predictions based on climate models, and computation of evidence for research publications. Recently, scientists have had to retract publications due to errors caused by software faults. Systematic testing can identify such faults in code. Objective: This study aims to identify specific challenges, proposed solutions, and unsolved problems faced when testing scientific software. Method: We conducted a systematic literature survey to identify and analyze relevant literature. We identified 62 studies that provided relevant information about testing scientific software. Results: We found that challenges faced when testing scientific software fall into two main categories: (1) testing challenges that occur due to characteristics of scientific software such as oracle problems and (2) testing challenges that occur due to cultural differences between scientists and the software engineering community such as viewing the code and the model that it implements as inseparable entities. In addition, we identified methods to potentially overcome these challenges and their limitations. Finally we describe unsolved challenges and how software engineering researchers and practitioners can help to overcome them. Conclusions: Scientific software presents special challenges for testing. Specifically, cultural differences between scientist developers and software engineers, along with the characteristics of the scientific software make testing more difficult. Existing techniques such as code clone detection can help to improve the testing process. Software engineers should consider special challenges posed by scientific software such as oracle problems when developing testing techniques.},
  keywords = {research software engineering},
  annotation = {interest: 95},
  file = {/home/sam/Zotero/storage/2ZWXRPZP/Kanewala and Bieman - 2014 - Testing scientific software A systematic literatu.pdf}
}

@online{kaptur500LinesLess,
  title = {500 {{Lines}} or {{Less}} | {{A Python Interpreter Written}} in {{Python}}},
  author = {Kaptur, Allison},
  url = {http://www.aosabook.org/en/500L/a-python-interpreter-written-in-python.html},
  urldate = {2022-08-31},
  abstract = {Byterun is a Python interpreter implemented in Python. Through my work on Byterun, I was surprised and delighted to discover that the fundamental structure of the Python interpreter fits easily into the 500-line size restriction. This chapter will walk through the structure of the interpreter and give you enough context to explore it further. The goal is not to explain everything there is to know about interpreters—like so many interesting areas of programming and computer science, you could devote years to developing a deep understanding of the topic. Byterun was written by Ned Batchelder and myself, building on the work of Paul Swartz. Its structure is similar to the primary implementation of Python, CPython, so understanding Byterun will help you understand interpreters in general and the CPython interpreter in particular. (If you don't know which Python you're using, it's probably CPython.) Despite its short length, Byterun is capable of running most simple Python programs.},
  keywords = {programming languages},
  file = {/home/sam/Zotero/storage/K273MYCK/a-python-interpreter-written-in-python.html}
}

@online{karpinskiPutThisYour,
  title = {Put {{This In Your Pipe}}},
  author = {Karpinski, Stefan},
  url = {https://julialang.org/blog/2013/04/put-this-in-your-pipe/},
  urldate = {2022-09-06},
  langid = {english},
  keywords = {programming languages},
  annotation = {interest: 63},
  file = {/home/sam/Zotero/storage/Q5U5TU4V/put-this-in-your-pipe.html}
}

@online{karpinskiShellingOutSucks,
  title = {Shelling {{Out Sucks}}},
  author = {Karpinski, Stefan},
  url = {https://julialang.org/blog/2012/03/shelling-out-sucks/},
  urldate = {2022-09-06},
  abstract = {Shelling Out Sucks},
  langid = {english},
  keywords = {programming languages},
  annotation = {interest: 81},
  file = {/home/sam/Zotero/storage/MZLL43J9/shelling-out-sucks.html}
}

@report{katcherPostMarkNewFile2005,
  title = {{{PostMark}}: {{A New File System Benchmark}}},
  shorttitle = {Network {{Appliance}} - {{PostMark}}},
  author = {Katcher, Jeffrey},
  date = {2005-09-01},
  number = {TR3022},
  url = {https://web.archive.org/web/20050901112245/https://www.netapp.com/tech_library/3022.html},
  urldate = {2024-01-22},
  abstract = {Existing file system benchmarks are deficient in portraying performance in the ephemeral small-file regime used by Internet software, especially:     electronic mail;     netnews; and     web-based commerce.  PostMark is a new benchmark to measure performance for this class of application. In this paper, PostMark test results are presented and analyzed for both UNIX and Windows NT application servers. Network Appliance Filers (file server appliances) are shown to provide superior performance (via NFS or CIFS) compared to local disk alternatives, especially at higher loads. Such results are consistent with reports from ISPs (Internet Service Providers) who have deployed NetApp filers to support such applications on a large scale.},
  keywords = {benchmarking,project-provenance-pp},
  file = {/home/sam/Zotero/storage/FVZZJ693/3022.html}
}

@article{katsaggelosSuperResolutionImages2007,
  title = {Super {{Resolution}} of {{Images}} and {{Video}}},
  author = {Katsaggelos, Aggelos K. and Molina, Rafael and Mateos, Javier},
  date = {2007-01},
  journaltitle = {Synthesis Lectures on Image, Video, and Multimedia Processing},
  volume = {3},
  number = {1},
  pages = {1--134},
  publisher = {Morgan \& Claypool Publishers},
  issn = {1559-8136},
  doi = {10.2200/S00036ED1V01Y200606IVM007},
  url = {https://www.morganclaypool.com/doi/abs/10.2200/S00036ED1V01Y200606IVM007},
  urldate = {2022-05-02},
  keywords = {image processing,project-astrophysics},
  file = {/home/sam/Zotero/storage/977W7EUM/Katsaggelos et al. - 2007 - Super Resolution of Images and Video.pdf}
}

@article{katzApplicationSkeletonsConstruction2016,
  title = {Application Skeletons: {{Construction}} and Use in {{eScience}}},
  shorttitle = {Application Skeletons},
  author = {Katz, Daniel S. and Merzky, Andre and Zhang, Zhao and Jha, Shantenu},
  date = {2016-06-01},
  journaltitle = {Future Generation Computer Systems},
  shortjournal = {Future Generation Computer Systems},
  volume = {59},
  pages = {114--124},
  issn = {0167-739X},
  doi = {10.1016/j.future.2015.10.001},
  url = {https://www.sciencedirect.com/science/article/pii/S0167739X15003143},
  urldate = {2022-09-06},
  abstract = {Computer scientists who work on tools and systems to support eScience (a variety of parallel and distributed) applications usually use actual applications to prove that their systems will benefit science and engineering (e.g., improve application performance). Accessing and building the applications and necessary data sets can be difficult because of policy or technical issues, and it can be difficult to modify the characteristics of the applications to understand corner cases in the system design. In this paper, we present the Application Skeleton, a simple yet powerful tool to build synthetic applications that represent real applications, with runtime and I/O close to those of the real applications. This allows computer scientists to focus on the system they are building; they can work with the simpler skeleton applications and be sure that their work will also be applicable to the real applications. In addition, skeleton applications support simple reproducible system experiments since they are represented by a compact set of parameters. Our Application Skeleton tool (available as open source at https://github.com/applicationskeleton/Skeleton) currently can create easy-to-access, easy-to-build, and easy-to-run bag-of-task, (iterative) map-reduce, and (iterative) multistage workflow applications. The tasks can be serial, parallel, or a mix of both. The parameters to represent the tasks can either be discovered through a manual profiling of the applications or through an automated method. We select three representative applications (Montage, BLAST, CyberShake Postprocessing), then describe and generate skeleton applications for each. We show that the skeleton applications have identical (or close) performance to that of the real applications. We then show examples of using skeleton applications to verify system optimizations such as data caching, I/O tuning, and task scheduling, as well as the system resilience mechanism, in some cases modifying the skeleton applications to emphasize some characteristic, and thus show that using skeleton applications simplifies the process of designing, implementing, and testing these optimizations.},
  langid = {english},
  keywords = {workflow managers},
  annotation = {interest: 90},
  file = {/home/sam/Zotero/storage/9FPNLXXZ/Katz et al. - 2016 - Application skeletons Construction and use in eSc.pdf;/home/sam/Zotero/storage/GB3NYK9N/S0167739X15003143.html}
}

@unpublished{katzRSEExplainer2022,
  title = {{{RSE Explainer}}},
  shorttitle = {Day 1, {{Session}} 2},
  author = {Katz, Dan},
  date = {2022-03-23},
  url = {https://www.youtube.com/watch?v=L-KCAzFr8no},
  urldate = {2022-04-20},
  abstract = {RSE explainer - Daniel S. Katz, National Center for Supercomputing Applications (NCSA), USA},
  keywords = {research software engineering}
}

@online{katzSoftwareReproducibilityPossible2017,
  title = {Is Software Reproducibility Possible and Practical?},
  author = {Katz, Dan},
  date = {2017-02-07T19:39:20+00:00},
  url = {https://danielskatzblog.wordpress.com/2017/02/07/is-software-reproducibility-possible-and-practical/},
  urldate = {2023-01-24},
  langid = {english},
  organization = {Daniel S. Katz's blog},
  keywords = {reproducibility engineering},
  file = {/home/sam/Zotero/storage/56F9CZV5/is-software-reproducibility-possible-and-practical.html}
}

@article{katzStateSustainableResearch2019,
  title = {The {{State}} of {{Sustainable Research Software}}: {{Learning}} from the {{Workshop}} on {{Sustainable Software}} for {{Science}}: {{Practice}} and {{Experiences}} ({{WSSSPE5}}.1)},
  shorttitle = {The {{State}} of {{Sustainable Research Software}}},
  author = {Katz, Daniel S. and Druskat, Stephan and Haines, Robert and Jay, Caroline and Struck, Alexander},
  date = {2019-04-02},
  journaltitle = {Journal of Open Research Software},
  volume = {7},
  number = {1},
  pages = {11},
  publisher = {Ubiquity Press},
  issn = {2049-9647},
  doi = {10.5334/jors.242},
  url = {http://openresearchsoftware.metajnl.com/article/10.5334/jors.242/},
  urldate = {2022-08-25},
  abstract = {Article: The State of Sustainable Research Software: Learning from the Workshop on Sustainable Software for Science: Practice and Experiences (WSSSPE5.1)},
  issue = {1},
  langid = {english},
  keywords = {research software engineering},
  annotation = {interest: 85},
  file = {/home/sam/Zotero/storage/EAZS67AX/Katz et al. - 2019 - The State of Sustainable Research Software Learni.pdf;/home/sam/Zotero/storage/9DKQ8WXH/jors.242.html}
}

@article{katzTakingFreshLook2021,
  title = {Taking a Fresh Look at {{FAIR}} for Research Software},
  author = {Katz, Daniel S. and Gruenpeter, Morane and Honeyman, Tom},
  date = {2021-03-12},
  journaltitle = {Patterns},
  shortjournal = {PATTER},
  volume = {2},
  number = {3},
  eprint = {33748799},
  eprinttype = {pmid},
  publisher = {Elsevier},
  issn = {2666-3899},
  doi = {10.1016/j.patter.2021.100222},
  url = {https://www.cell.com/patterns/abstract/S2666-3899(21)00036-2},
  urldate = {2022-09-06},
  langid = {english},
  keywords = {reproducibility engineering,research software engineering},
  annotation = {interest: 74},
  file = {/home/sam/Zotero/storage/9P8WY8EF/Katz et al. - 2021 - Taking a fresh look at FAIR for research software.pdf;/home/sam/Zotero/storage/ANI6E9WF/S2666-3899(21)00036-2.html}
}

@article{katzTransitiveCreditJSONLD2015,
  title = {Transitive {{Credit}} and {{JSON-LD}}},
  author = {Katz, Daniel S. and Smith, Arfon M.},
  date = {2015-11-05},
  journaltitle = {Journal of Open Research Software},
  volume = {3},
  number = {1},
  pages = {e7},
  publisher = {Ubiquity Press},
  issn = {2049-9647},
  doi = {10.5334/jors.by},
  url = {http://openresearchsoftware.metajnl.com/article/10.5334/jors.by/},
  urldate = {2023-02-20},
  abstract = {Article: Transitive Credit and JSON-LD},
  issue = {1},
  langid = {english},
  keywords = {academic publishing,project-acm-rep},
  file = {/home/sam/Zotero/storage/VADKNANZ/Katz and Smith - 2015 - Transitive Credit and JSON-LD.pdf}
}

@online{katzUsingWorkflowsExpressed2019,
  title = {Using Workflows Expressed as Code and Workflows Expressed as Data Together},
  author = {Katz, Daniel S.},
  date = {2019-02-05T11:06:36+00:00},
  url = {https://danielskatzblog.wordpress.com/2019/02/05/using-workflows-expressed-as-code-and-workflows-expressed-as-data-together/},
  urldate = {2022-09-06},
  abstract = {I’ve~previously written about the concept of workflows, sets of independent tasks connected by data dependencies, being expressed either as data, for example, a Pegasus DAG~or CWL document, o…},
  langid = {english},
  organization = {Daniel S. Katz's blog},
  keywords = {workflow managers},
  annotation = {interest: 91},
  file = {/home/sam/Zotero/storage/PH6Q2ELC/using-workflows-expressed-as-code-and-workflows-expressed-as-data-together.html}
}

@report{kaufmanCarbonTaxVs2016,
  title = {Carbon {{Tax}} vs. {{Cap-and-Trade}}: {{What}}’s a {{Better Policy}} to {{Cut Emissions}}?},
  shorttitle = {Carbon {{Tax}} vs. {{Cap-and-Trade}}},
  author = {Kaufman, Noah},
  date = {2016-03-01},
  url = {https://www.wri.org/insights/carbon-tax-vs-cap-and-trade-whats-better-policy-cut-emissions},
  urldate = {2022-04-13},
  abstract = {Experts often debate the pros and cons of a carbon tax versus a cap-and-trade system. But WRI research finds that if well-designed, both policies can effectively reduce emissions in the United States.},
  langid = {english},
  keywords = {carbon pricing,economics},
  file = {/home/sam/Zotero/storage/NX8V5G5F/carbon-tax-vs-cap-and-trade-whats-better-policy-cut-emissions.html}
}

@inproceedings{kauhanenRegressionTestSelection2021,
  title = {Regression {{Test Selection Tool}} for {{Python}} in {{Continuous Integration Process}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Software Analysis}}, {{Evolution}} and {{Reengineering}} ({{SANER}})},
  author = {Kauhanen, Eero and Nurminen, Jukka K. and Mikkonen, Tommi and Pashkovskiy, Matvei},
  date = {2021-03},
  pages = {618--621},
  issn = {1534-5351},
  doi = {10.1109/SANER50967.2021.00077},
  abstract = {In this paper, we present a coverage-based regression test selection (RTS) approach and a developed tool for Python. The tool can be used either on a developer's machine or on build servers. A special characteristic of the tool is the attention to easy integration to continuous integration and deployment. To evaluate the performance of the proposed approach, mutation testing is applied to three open-source projects, and the results of the execution of full test suites are compared to the execution of a set of tests selected by the tool. The missed fault rate of the test selection varies between 0-2\% at file-level granularity and 16-24\% at line-level granularity. The high missed fault rate at the line-level granularity is related to the selected basic mutation approach and the result could be improved with advanced mutation techniques. Depending on the target optimization metric (time or precision) in DevOps/MLOps process the error rate could be acceptable or further improved by using file-level granularity based test selection.},
  eventtitle = {2021 {{IEEE International Conference}} on {{Software Analysis}}, {{Evolution}} and {{Reengineering}} ({{SANER}})},
  keywords = {continuous integration,project-charmonium.cache,software engineering,software testing},
  annotation = {interest: 94},
  file = {/home/sam/Zotero/storage/DM5UNLKJ/Kauhanen et al. - 2021 - Regression Test Selection Tool for Python in Conti.pdf}
}

@inproceedings{keaheyLessonsLearnedChameleon2020,
  title = {Lessons {{Learned}} from the {{Chameleon Testbed}}},
  booktitle = {2020 {{USENIX Anuual Technical Conference}}},
  author = {Keahey, Kate and Anderson, Jason and Zhen, Zhuo and Riteau, Pierre and Ruth, Paul and Stanzione, Dan and Cevik, Mert and Colleran, Jacob and Gunawi, Haryadi and Hammock, Cody and Mambretti, Joe and Barnes, Alexander and Halbah, Francois and Rocha, Alex and Stubbs, joe},
  date = {2020-07-15},
  publisher = {USENIX},
  url = {https://www.usenix.org/conference/atc20/presentation/keahey},
  abstract = {The Chameleon testbed is a case study in adapting the cloud paradigm for computer science research. In this paper, we explain how this adaptation was achieved, evaluate it from the perspective of supporting the most experiments for the most users, and make a case that utilizing mainstream technology in research testbeds can increase efficiency without compromising on functionality. We also highlight the opportunity inherent in the shared digital artifacts generated by testbeds and give an overview of the efforts we’ve made to develop it to foster reproducibility.},
  eventtitle = {{{USENIX ATC}}},
  langid = {english},
  file = {/home/sam/Zotero/storage/R7JWWN7C/atc20-paper922-slides-keahey.pdf;/home/sam/Zotero/storage/VLHHKQR4/Keahey - Lessons Learned from the Chameleon Testbed.pdf}
}

@article{kelleyFrameworkCreatingKnowledge2021,
  title = {A Framework for Creating Knowledge Graphs of Scientific Software Metadata},
  author = {Kelley, Aidan and Garijo, Daniel},
  date = {2021-12-01},
  journaltitle = {Quantitative Science Studies},
  shortjournal = {Quantitative Science Studies},
  volume = {2},
  number = {4},
  pages = {1423--1446},
  issn = {2641-3337},
  doi = {10.1162/qss_a_00167},
  url = {https://doi.org/10.1162/qss_a_00167},
  urldate = {2022-12-18},
  abstract = {An increasing number of researchers rely on computational methods to generate or manipulate the results described in their scientific publications. Software created to this end—scientific software—is key to understanding, reproducing, and reusing existing work in many disciplines, ranging from Geosciences to Astronomy or Artificial Intelligence. However, scientific software is usually challenging to find, set up, and compare to similar software due to its disconnected documentation (dispersed in manuals, readme files, websites, and code comments) and the lack of structured metadata to describe it. As a result, researchers have to manually inspect existing tools to understand their differences and incorporate them into their work. This approach scales poorly with the number of publications and tools made available every year. In this paper we address these issues by introducing a framework for automatically extracting scientific software metadata from its documentation (in particular, their readme files); a methodology for structuring the extracted metadata in a Knowledge Graph (KG) of scientific software; and an exploitation framework for browsing and comparing the contents of the generated KG. We demonstrate our approach by creating a KG with metadata from over 10,000 scientific software entries from public code repositories.},
  annotation = {interest: 70},
  file = {/home/sam/Zotero/storage/ZWLQVX3L/Kelley and Garijo - 2021 - A framework for creating knowledge graphs of scien.pdf}
}

@inproceedings{kemerlisLibdftPracticalDynamic2012,
  title = {Libdft: Practical Dynamic Data Flow Tracking for Commodity Systems},
  shorttitle = {Libdft},
  booktitle = {Proceedings of the 8th {{ACM SIGPLAN}}/{{SIGOPS}} Conference on {{Virtual Execution Environments}}},
  author = {Kemerlis, Vasileios P. and Portokalidis, Georgios and Jee, Kangkook and Keromytis, Angelos D.},
  date = {2012-03-03},
  series = {{{VEE}} '12},
  pages = {121--132},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2151024.2151042},
  url = {https://dl.acm.org/doi/10.1145/2151024.2151042},
  urldate = {2023-08-23},
  abstract = {Dynamic data flow tracking (DFT) deals with tagging and tracking data of interest as they propagate during program execution. DFT has been repeatedly implemented by a variety of tools for numerous purposes, including protection from zero-day and cross-site scripting attacks, detection and prevention of information leaks, and for the analysis of legitimate and malicious software. We present libdft, a dynamic DFT framework that unlike previous work is at once fast, reusable, and works with commodity software and hardware. libdft provides an API for building DFT-enabled tools that work on unmodified binaries, running on common operating systems and hardware, thus facilitating research and rapid prototyping. We explore different approaches for implementing the low-level aspects of instruction-level data tracking, introduce a more efficient and 64-bit capable shadow memory, and identify (and avoid) the common pitfalls responsible for the excessive performance overhead of previous studies. We evaluate libdft using real applications with large codebases like the Apache and MySQL servers, and the Firefox web browser. We also use a series of benchmarks and utilities to compare libdft with similar systems. Our results indicate that it performs at least as fast, if not faster, than previous solutions, and to the best of our knowledge, we are the first to evaluate the performance overhead of a fast dynamic DFT implementation in such depth. Finally, libdft is freely available as open source software.},
  isbn = {978-1-4503-1176-2},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/R4P4ZSXE/Kemerlis et al. - 2012 - libdft practical dynamic data flow tracking for c.pdf}
}

@online{kemperBuildCloudHow2011,
  title = {Build in the {{Cloud}}: {{How}} the {{Build System}} Works},
  author = {Kemper, Christian},
  date = {2011-08-18},
  url = {https://google-engtools.blogspot.com/2011/08/build-in-cloud-how-build-system-works.html},
  organization = {Google Engineering Tools},
  keywords = {build systems,continuous integration,industry practices,software engineering},
  file = {/home/sam/Zotero/storage/MGU2Y4ZI/build-in-cloud-how-build-system-works.html}
}

@online{kenistonKernelProbesKprobes,
  title = {Kernel {{Probes}} ({{Kprobes}})},
  shorttitle = {Kernel {{Probes}} ({{Kprobes}})},
  author = {Keniston, Jim and Panchamukhi, Prasanna S and Hiramatsu, Masami},
  url = {https://www.kernel.org/doc/html/latest/trace/kprobes.html},
  urldate = {2023-08-24},
  langid = {american},
  organization = {The Linux Kernel documentation},
  keywords = {operating systems,project-provenance-pp},
  file = {/home/sam/Zotero/storage/PALWK2QM/kprobes.html}
}

@online{kerriskNamespacesOperation2013,
  type = {News blog},
  title = {Namespaces in Operation},
  author = {Kerrisk, Michael},
  date = {2013-01-04},
  url = {https://lwn.net/Articles/531114/},
  urldate = {2023-12-11},
  organization = {Linux Web News},
  file = {/home/sam/Zotero/storage/PXG6WI2R/531114.html}
}

@article{khanGuideConvolutionalNeural2018,
  title = {A {{Guide}} to {{Convolutional Neural Networks}} for {{Computer Vision}}},
  author = {Khan, Salman and Rahmani, Hossein and Shah, Syed Afaq Ali and Bennamoun, Mohammed},
  date = {2018-02-13},
  journaltitle = {Synthesis Lectures on Computer Vision},
  volume = {8},
  number = {1},
  pages = {1--207},
  publisher = {Morgan \& Claypool Publishers},
  issn = {2153-1056},
  doi = {10.2200/S00822ED1V01Y201712COV015},
  url = {https://www.morganclaypool.com/doi/10.2200/S00822ED1V01Y201712COV015},
  urldate = {2022-05-02},
  keywords = {artificial intelligence,machine learning}
}

@article{khanSharingInteroperableWorkflow2019,
  title = {Sharing Interoperable Workflow Provenance: {{A}} Review of Best Practices and Their Practical Application in {{CWLProv}}},
  shorttitle = {Sharing Interoperable Workflow Provenance},
  author = {Khan, Farah Zaib and Soiland-Reyes, Stian and Sinnott, Richard O and Lonie, Andrew and Goble, Carole and Crusoe, Michael R},
  date = {2019-11-01},
  journaltitle = {GigaScience},
  shortjournal = {GigaScience},
  volume = {8},
  number = {11},
  pages = {giz095},
  issn = {2047-217X},
  doi = {10.1093/gigascience/giz095},
  url = {https://doi.org/10.1093/gigascience/giz095},
  urldate = {2022-08-02},
  abstract = {The automation of data analysis in the form of scientific workflows has become a widely adopted practice in many fields of research. Computationally driven data-intensive experiments using workflows enable automation, scaling, adaptation, and provenance support. However, there are still several challenges associated with the effective sharing, publication, and reproducibility of such workflows due to the incomplete capture of provenance and lack of interoperability between different technical (software) platforms.Based on best-practice recommendations identified from the literature on workflow design, sharing, and publishing, we define a hierarchical provenance framework to achieve uniformity in provenance and support comprehensive and fully re-executable workflows equipped with domain-specific information. To realize this framework, we present CWLProv, a standard-based format to represent any workflow-based computational analysis to produce workflow output artefacts that satisfy the various levels of provenance. We use open source community-driven standards, interoperable workflow definitions in Common Workflow Language (CWL), structured provenance representation using the W3C PROV model, and resource aggregation and sharing as workflow-centric research objects generated along with the final outputs of a given workflow enactment. We demonstrate the utility of this approach through a practical implementation of CWLProv and evaluation using real-life genomic workflows developed by independent groups.The underlying principles of the standards utilized by CWLProv enable semantically rich and executable research objects that capture computational workflows with retrospective provenance such that any platform supporting CWL will be able to understand the analysis, reuse the methods for partial reruns, or reproduce the analysis to validate the published findings.},
  langid = {english},
  keywords = {academic publishing,project-acm-rep,provenance,reproducibility,semantic web},
  annotation = {interest: 99},
  file = {/home/sam/Zotero/storage/LQPEQUD2/Khan et al. - 2019 - Sharing interoperable workflow provenance A revie.pdf;/home/sam/Zotero/storage/Q4VAR9JV/Khan et al. - 2019 - Sharing interoperable workflow provenance A revie.pdf;/home/sam/Zotero/storage/S7NLQ2QK/5611001.html}
}

@article{kidwellBadgesAcknowledgeOpen2016,
  title = {Badges to {{Acknowledge Open Practices}}: {{A Simple}}, {{Low-Cost}}, {{Effective Method}} for {{Increasing Transparency}}},
  shorttitle = {Badges to {{Acknowledge Open Practices}}},
  author = {Kidwell, Mallory C. and Lazarević, Ljiljana B. and Baranski, Erica and Hardwicke, Tom E. and Piechowski, Sarah and Falkenberg, Lina-Sophia and Kennett, Curtis and Slowik, Agnieszka and Sonnleitner, Carina and Hess-Holden, Chelsey and Errington, Timothy M. and Fiedler, Susann and Nosek, Brian A.},
  date = {2016-05-12},
  journaltitle = {PLOS Biology},
  shortjournal = {PLOS Biology},
  volume = {14},
  number = {5},
  pages = {e1002456},
  publisher = {Public Library of Science},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.1002456},
  url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002456},
  urldate = {2022-09-06},
  abstract = {Beginning January 2014, Psychological Science gave authors the opportunity to signal open data and materials if they qualified for badges that accompanied published articles. Before badges, less than 3\% of Psychological Science articles reported open data. After badges, 23\% reported open data, with an accelerating trend; 39\% reported open data in the first half of 2015, an increase of more than an order of magnitude from baseline. There was no change over time in the low rates of data sharing among comparison journals. Moreover, reporting openness does not guarantee openness. When badges were earned, reportedly available data were more likely to be actually available, correct, usable, and complete than when badges were not earned. Open materials also increased to a weaker degree, and there was more variability among comparison journals. Badges are simple, effective signals to promote open practices and improve preservation of data and materials by using independent repositories.},
  langid = {english},
  keywords = {academic publishing,research software engineering},
  annotation = {interest: 83\\
pubpeer: yes},
  file = {/home/sam/Zotero/storage/Q8ISV3SR/Kidwell et al. - 2016 - Badges to Acknowledge Open Practices A Simple, Lo.pdf;/home/sam/Zotero/storage/BTCGAKEE/article.html}
}

@inproceedings{kimOptimizingUnitTest2013,
  title = {Optimizing Unit Test Execution in Large Software Programs Using Dependency Analysis},
  booktitle = {Proceedings of the 4th {{Asia-Pacific Workshop}} on {{Systems}}},
  author = {Kim, Taesoo and Chandra, Ramesh and Zeldovich, Nickolai},
  date = {2013-07-29},
  series = {{{APSys}} '13},
  pages = {1--6},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2500727.2500748},
  url = {https://doi.org/10.1145/2500727.2500748},
  urldate = {2022-04-06},
  abstract = {Tao is a system that optimizes the execution of unit tests in large software programs and reduces the programmer wait time from minutes to seconds. Tao is based on two key ideas: First, Tao focuses on efficiency, unlike past work that focused on avoiding false negatives. Tao implements simple and fast function-level dependency tracking that identifies tests to run on a code change; any false negatives missed by this dependency tracking are caught by running the entire test suite on a test server once the code change is committed. Second, to make it easy for programmers to adopt Tao, it incorporates the dependency information into the source code repository. This paper describes an early prototype of Tao and demonstrates that Tao can reduce unit test execution time in two large Python software projects by over 96\% while incurring few false negatives.},
  isbn = {978-1-4503-2316-1},
  keywords = {project-charmonium.cache,software engineering,software testing},
  annotation = {interest: 95},
  file = {/home/sam/Zotero/storage/8ZD9I66P/Kim et al. - 2013 - Optimizing unit test execution in large software p.pdf}
}

@article{kingMetaanalysisTechnologyAcceptance2006,
  title = {A Meta-Analysis of the Technology Acceptance Model},
  author = {King, William R. and He, Jun},
  date = {2006-09},
  journaltitle = {Information \& Management},
  shortjournal = {Information \& Management},
  volume = {43},
  number = {6},
  pages = {740--755},
  issn = {03787206},
  doi = {10.1016/j.im.2006.05.003},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0378720606000528},
  urldate = {2022-06-01},
  abstract = {A statistical meta-analysis of the technology acceptance model (TAM) as applied in various fields was conducted using 88 published studies that provided sufficient data to be credible. The results show TAM to be a valid and robust model that has been widely used, but which potentially has wider applicability. A moderator analysis involving user types and usage types was performed to investigate conditions under which TAM may have different effects. The study confirmed the value of using students as surrogates for professionals in some TAM studies, and perhaps more generally. It also revealed the power of meta-analysis as a rigorous alternative to qualitative and narrative literature review methods.},
  langid = {english},
  keywords = {internship-project,technology-acceptance},
  file = {/home/sam/Zotero/storage/RXGW44DS/1-s2.0-S0378720606000528-main.pdf}
}

@online{kingParseDonValidate2019,
  title = {Parse, Don’t Validate},
  author = {King, Alexis},
  date = {2019-11-05},
  url = {https://lexi-lambda.github.io/blog/2019/11/05/parse-don-t-validate/},
  urldate = {2024-01-11},
  abstract = {About a month ago, I was reflecting on Twitter about the differences I experienced parsing JSON in statically- and dynamically-typed languages, and finally, I realized what I was looking for. Now I have a single, snappy slogan that encapsulates what type-driven design means to me, and better yet, it’s only three words long: Parse, don’t validate.},
  keywords = {industry practices,language engineering},
  file = {/home/sam/Zotero/storage/9GP989PP/parse-don-t-validate.html}
}

@report{kitchenhamGuidelinesPerformingSystematic2007,
  type = {EBSE Technical Report},
  title = {Guidelines for Performing {{Systematic Literature Reviews}} in {{Software Engineering}}},
  author = {Kitchenham, Barbara and Charters, Stuart},
  date = {2007-07-09},
  number = {EBSE-2007-01},
  institution = {{Software Engineering Group, School of Computer Science and Mathematics, Keele University}},
  url = {https://www.elsevier.com/__data/promis_misc/525444systematicreviewsguide.pdf},
  keywords = {software engineering}
}

@online{kladovWhyNotRust2020,
  title = {Why {{Not Rust}}?},
  author = {Kladov, Alex},
  date = {2020-09-20},
  url = {https://matklad.github.io/2020/09/20/why-not-rust.html},
  urldate = {2023-03-09},
  abstract = {I’ve recently read an article criticizing Rust, and, while it made a bunch of good points, I didn’t enjoy it — it was an easy to argue with piece. In general, I feel that I can’t recommend an article criticizing Rust. This is a shame — confronting drawbacks is important, and debunking low effort/miss informed attempts at critique sadly inoculates against actually good arguments. So, here’s my attempt to argue against Rust:},
  organization = {matklad},
  keywords = {programming languages}
}

@unpublished{kleenIntelProcessorTrace2015,
  title = {{{Intel}}® {{Processor Trace}} on {{Linux}}},
  author = {Kleen, Andi and Strong, Beeman},
  date = {2015-08-10},
  eventtitle = {Tracing {{Summit}} 2015},
  langid = {english},
  venue = {Seattle, Washington, USA},
  keywords = {computer architecture,project-provenance-pp},
  file = {/home/sam/Zotero/storage/LLWIKPGQ/Kleen and Strong - Intel® Processor Trace on Linux.pdf}
}

@article{kleinRunYourResearch2012,
  title = {Run Your Research: On the Effectiveness of Lightweight Mechanization},
  shorttitle = {Run Your Research},
  author = {Klein, Casey and Clements, John and Dimoulas, Christos and Eastlund, Carl and Felleisen, Matthias and Flatt, Matthew and McCarthy, Jay A. and Rafkind, Jon and Tobin-Hochstadt, Sam and Findler, Robert Bruce},
  date = {2012-01-18},
  journaltitle = {ACM SIGPLAN Notices},
  shortjournal = {SIGPLAN Not.},
  volume = {47},
  number = {1},
  pages = {285--296},
  issn = {0362-1340, 1558-1160},
  doi = {10.1145/2103621.2103691},
  url = {https://dl.acm.org/doi/10.1145/2103621.2103691},
  urldate = {2022-06-01},
  abstract = {Formal models serve in many roles in the programming language community. In its primary role, a model communicates the idea of a language design; the architecture of a language tool; or the essence of a program analysis. No matter which role it plays, however, a faulty model doesn't serve its purpose.  One way to eliminate flaws from a model is to write it down in a mechanized formal language. It is then possible to state theorems about the model, to prove them, and to check the proofs. Over the past nine years, PLT has developed and explored a lightweight version of this approach, dubbed Redex. In a nutshell, Redex is a domain-specific language for semantic models that is embedded in the Racket programming language. The effort of creating a model in Redex is often no more burdensome than typesetting it with LaTeX; the difference is that Redex comes with tools for the semantics engineering life cycle.},
  langid = {english},
  keywords = {formal verification},
  annotation = {interest: 60},
  file = {/home/sam/Zotero/storage/A5ZMSP9Z/2103621.2103691.pdf}
}

@article{knebeGalaxiesGoingMAD2013,
  title = {Galaxies Going {{MAD}}: The {{Galaxy-Finder Comparison Project}}},
  shorttitle = {Galaxies Going {{MAD}}},
  author = {Knebe, Alexander and Libeskind, Noam I. and Pearce, Frazer and Behroozi, Peter and Casado, Javier and Dolag, Klaus and Dominguez-Tenreiro, Rosa and Elahi, Pascal and Lux, Hanni and Muldrew, Stuart I. and Onions, Julian},
  date = {2013-01-21},
  journaltitle = {Monthly Notices of the Royal Astronomical Society},
  volume = {428},
  number = {3},
  pages = {2039--2052},
  issn = {1365-2966, 0035-8711},
  doi = {10.1093/mnras/sts173},
  url = {http://academic.oup.com/mnras/article/428/3/2039/1062526/Galaxies-going-MAD-the-GalaxyFinder-Comparison},
  urldate = {2022-07-22},
  abstract = {With the ever-increasing size and complexity of fully self-consistent simulations of galaxy formation within the framework of the cosmic web, the demands upon object finders for these simulations have simultaneously grown. To this extent we initiated the Halo-Finder Comparison Project that gathered together all the experts in the field and has so far led to two comparison papers, one for dark matter field haloes, and one for dark matter subhaloes. However, as state-of-the-art simulation codes are perfectly capable of not only following the formation and evolution of dark matter but also accounting for baryonic physics, i.e. gas hydrodynamics, star formation, stellar feedback, etc., object finders should also be capable of taking these additional physical processes into consideration. Here we report – for the first time – on a comparison of codes as applied to the Constrained Local UniversE Simulation (CLUES) of the formation of the Local Group which incorporates much of the physics relevant for galaxy formation. We compare both the properties of the three main galaxies in the simulation (representing the Milky Way, Andromeda and M33) and their satellite populations for a variety of halo finders ranging from phase space to velocity space to spherical overdensity based codes, including also a mere baryonic object finder. We obtain agreement amongst codes comparable to (if not better than) our previous comparisons – at least for the total, dark and stellar components of the objects. However, the diffuse gas content of the haloes shows great disparity, especially for low-mass satellite galaxies. This is primarily due to differences in the treatment of the thermal energy during the unbinding procedure. We acknowledge that the handling of gas in halo finders is something that needs to be dealt with carefully, and the precise treatment may depend sensitively upon the scientific problem being studied.},
  langid = {english},
  keywords = {astrophysics,dark matter halos}
}

@article{knebeHaloesGoneMAD2011,
  title = {Haloes Gone {{MAD}}★: {{The Halo-Finder Comparison Project}}: {{The Halo-Finder Comparison Project}}},
  shorttitle = {Haloes Gone {{MAD}}★},
  author = {Knebe, Alexander and Knollmann, Steffen R. and Muldrew, Stuart I. and Pearce, Frazer R. and Aragon-Calvo, Miguel Angel and Ascasibar, Yago and Behroozi, Peter S. and Ceverino, Daniel and Colombi, Stephane and Diemand, Juerg and Dolag, Klaus and Falck, Bridget L. and Fasel, Patricia and Gardner, Jeff and Gottlöber, Stefan and Hsu, Chung-Hsing and Iannuzzi, Francesca and Klypin, Anatoly and Lukić, Zarija and Maciejewski, Michal and McBride, Cameron and Neyrinck, Mark C. and Planelles, Susana and Potter, Doug and Quilis, Vicent and Rasera, Yann and Read, Justin I. and Ricker, Paul M. and Roy, Fabrice and Springel, Volker and Stadel, Joachim and Stinson, Greg and Sutter, P. M. and Turchaninov, Victor and Tweed, Dylan and Yepes, Gustavo and Zemp, Marcel},
  date = {2011-08-11},
  journaltitle = {Monthly Notices of the Royal Astronomical Society},
  volume = {415},
  number = {3},
  pages = {2293--2318},
  issn = {00358711},
  doi = {10.1111/j.1365-2966.2011.18858.x},
  url = {https://academic.oup.com/mnras/article-lookup/doi/10.1111/j.1365-2966.2011.18858.x},
  urldate = {2022-07-22},
  abstract = {We present a detailed comparison of fundamental dark matter halo properties retrieved by a substantial number of different halo finders. These codes span a wide range of techniques including friends-of-friends, spherical-overdensity and phase-space-based algorithms. We further introduce a robust (and publicly available) suite of test scenarios that allow halo finder developers to compare the performance of their codes against those presented here. This set includes mock haloes containing various levels and distributions of substructure at a range of resolutions as well as a cosmological simulation of the large-scale structure of the universe. All the halo-finding codes tested could successfully recover the spatial location of our mock haloes. They further returned lists of particles (potentially) belonging to the object that led to coinciding values for the maximum of the circular velocity profile and the radius where it is reached. All the finders based in configuration space struggled to recover substructure that was located close to the centre of the host halo, and the radial dependence of the mass recovered varies from finder to finder. Those finders based in phase space could resolve central substructure although they found difficulties in accurately recovering its properties. Through a resolution study we found that most of the finders could not reliably recover substructure containing fewer than 30–40 particles. However, also here the phase-space finders excelled by resolving substructure down to 10–20 particles. By comparing the halo finders using a high-resolution cosmological volume, we found that they agree remarkably well on fundamental properties of astrophysical significance (e.g. mass, position, velocity and peak of the rotation curve). We further suggest to utilize the peak of the rotation curve, vmax, as a proxy for mass, given the arbitrariness in defining a proper halo edge.},
  langid = {english},
  keywords = {astrophysics,dark matter halos}
}

@article{knebeStructureFindingCosmological2013,
  title = {Structure Finding in Cosmological Simulations: The State of Affairs},
  shorttitle = {Structure Finding in Cosmological Simulations},
  author = {Knebe, Alexander and Pearce, Frazer R. and Lux, Hanni and Ascasibar, Yago and Behroozi, Peter and Casado, Javier and Moran, Christine Corbett and Diemand, Juerg and Dolag, Klaus and Dominguez-Tenreiro, Rosa and Elahi, Pascal and Falck, Bridget and Gottlöber, Stefan and Han, Jiaxin and Klypin, Anatoly and Lukić, Zarija and Maciejewski, Michal and McBride, Cameron K. and Merchán, Manuel E. and Muldrew, Stuart I. and Neyrinck, Mark and Onions, Julian and Planelles, Susana and Potter, Doug and Quilis, Vicent and Rasera, Yann and Ricker, Paul M. and Roy, Fabrice and Ruiz, Andrés N. and Sgró, Mario A. and Springel, Volker and Stadel, Joachim and Sutter, P. M. and Tweed, Dylan and Zemp, Marcel},
  date = {2013-10-21},
  journaltitle = {Monthly Notices of the Royal Astronomical Society},
  volume = {435},
  number = {2},
  pages = {1618--1658},
  issn = {1365-2966, 0035-8711},
  doi = {10.1093/mnras/stt1403},
  url = {http://academic.oup.com/mnras/article/435/2/1618/1043579/Structure-finding-in-cosmological-simulations-the},
  urldate = {2022-07-22},
  abstract = {The ever increasing size and complexity of data coming from simulations of cosmic structure formation demand equally sophisticated tools for their analysis. During the past decade, the art of object finding in these simulations has hence developed into an important discipline itself. A multitude of codes based upon a huge variety of methods and techniques have been spawned yet the question remained as to whether or not they will provide the same (physical) information about the structures of interest. Here we summarize and extent previous work of the ‘halo finder comparison project’: we investigate in detail the (possible) origin of any deviations across finders. To this extent, we decipher and discuss differences in halo-finding methods, clearly separating them from the disparity in definitions of halo properties. We observe that different codes not only find different numbers of objects leading to a scatter of up to 20 per\,cent in the halo mass and Vmax function, but also that the particulars of those objects that are identified by all finders differ. The strength of the variation, however, depends on the property studied, e.g. the scatter in position, bulk velocity, mass and the peak value of the rotation curve is practically below a few per cent, whereas derived quantities such as spin and shape show larger deviations. Our study indicates that the prime contribution to differences in halo properties across codes stems from the distinct particle collection methods and – to a minor extent – the particular aspects of how the procedure for removing unbound particles is implemented. We close with a discussion of the relevance and implications of the scatter across different codes for other fields such as semi-analytical galaxy formation models, gravitational lensing and observables in general.},
  langid = {english},
  keywords = {astrophysics,dark matter halos}
}

@article{knuthLiterateProgramming1984,
  title = {Literate {{Programming}}},
  author = {Knuth, D. E.},
  date = {1984-02-01},
  journaltitle = {The Computer Journal},
  shortjournal = {The Computer Journal},
  volume = {27},
  number = {2},
  pages = {97--111},
  issn = {0010-4620, 1460-2067},
  doi = {10.1093/comjnl/27.2.97},
  url = {https://academic.oup.com/comjnl/article-lookup/doi/10.1093/comjnl/27.2.97},
  urldate = {2024-10-04},
  langid = {english},
  file = {/home/sam/Zotero/storage/HU2PNARP/Knuth - 1984 - Literate Programming.pdf}
}

@online{koeblerTemplateEngines2013,
  title = {Template {{Engines}}},
  author = {Koebler, Roland},
  date = {2013-04-03},
  url = {https://www.simple-is-better.org/template/},
  urldate = {2022-07-11},
  abstract = {Template-Engines: Concepts, thoughts, comparisons and benchmarks.},
  organization = {simple is better}
}

@inproceedings{kohlerImprovingWorkflowFault2011,
  title = {Improving {{Workflow Fault Tolerance}} through {{Provenance-Based Recovery}}},
  booktitle = {Scientific and {{Statistical Database Management}}},
  author = {Köhler, Sven and Riddle, Sean and Zinn, Daniel and McPhillips, Timothy and Ludäscher, Bertram},
  editor = {Bayard Cushing, Judith and French, James and Bowers, Shawn},
  date = {2011},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {207--224},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-22351-8_12},
  abstract = {Scientific workflow systems frequently are used to execute a variety of long-running computational pipelines prone to premature termination due to network failures, server outages, and other faults. Researchers have presented approaches for providing fault tolerance for portions of specific workflows, but no solution handles faults that terminate the workflow engine itself when executing a mix of stateless and stateful workflow components. Here we present a general framework for efficiently resuming workflow execution using information commonly captured by workflow systems to record data provenance. Our approach facilitates fast workflow replay using only such commonly recorded provenance data. We also propose a checkpoint extension to standard provenance models to significantly reduce the computation needed to reset the workflow to a consistent state, thus resulting in much shorter re-execution times. Our work generalizes the rescue-DAG approach used by DAGMan to richer workflow models that may contain stateless and stateful multi-invocation actors as well as workflow loops.},
  isbn = {978-3-642-22351-8},
  langid = {english},
  keywords = {provenance,workflow managers},
  annotation = {interest: 91}
}

@online{konvesWhatIfWe2019,
  title = {What If We Could Verify Npm Packages?},
  author = {Konves, Steve},
  date = {2019-02-14T01:38:50},
  url = {https://medium.com/hackernoon/what-if-we-could-verify-npm-packages-c2a319cff758},
  urldate = {2022-09-06},
  abstract = {Reproducible steps for identifying unwanted and malicious code},
  langid = {english},
  organization = {HackerNoon.com},
  keywords = {industry practices,package managers},
  annotation = {interest: 50},
  file = {/home/sam/Zotero/storage/4PTJ2ZP7/what-if-we-could-verify-npm-packages-c2a319cff758.html}
}

@article{koopProvenanceBasedInfrastructureSupport2011,
  title = {A {{Provenance-Based Infrastructure}} to {{Support}} the {{Life Cycle}} of {{Executable Papers}}},
  author = {Koop, David and Santos, Emanuele and Mates, Phillip and Vo, Huy T. and Bonnet, Philippe and Bauer, Bela and Surer, Brigitte and Troyer, Matthias and Williams, Dean N. and Tohline, Joel E. and Freire, Juliana and Silva, Cláudio T.},
  date = {2011},
  journaltitle = {Procedia Computer Science},
  shortjournal = {Procedia Computer Science},
  volume = {4},
  pages = {648--657},
  issn = {18770509},
  doi = {10.1016/j.procs.2011.04.068},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050911001268},
  urldate = {2022-07-08},
  abstract = {As publishers establish a greater online presence as well as infrastructure to support the distribution of more varied information, the idea of an executable paper that enables greater interaction has developed. An executable paper provides more information for computational experiments and results than the text, tables, and figures of standard papers. Executable papers can bundle computational content that allow readers and reviewers to interact, validate, and explore experiments. By including such content, authors facilitate future discoveries by lowering the barrier to reproducing and extending results. We present an infrastructure for creating, disseminating, and maintaining executable papers. Our approach is rooted in provenance, the documentation of exactly how data, experiments, and results were generated. We seek to improve the experience for everyone involved in the life cycle of an executable paper. The automated capture of provenance information allows authors to easily integrate and update results into papers as they write, and also helps reviewers better evaluate approaches by enabling them to explore experimental results by varying parameters or data. With a provenance-based system, readers are able to examine exactly how a result was developed to better understand and extend published findings.},
  langid = {english},
  keywords = {academic publishing,provenance,reproducibility engineering},
  annotation = {interest: 79}
}

@online{korchaginSpeedingLinuxDisk2020,
  title = {Speeding up {{Linux}} Disk Encryption},
  author = {Korchagin, Ignat},
  date = {2020-03-25T12:00:00.000+00:00},
  url = {https://blog.cloudflare.com/speeding-up-linux-disk-encryption},
  urldate = {2023-12-15},
  abstract = {In this post, we will investigate the performance of disk encryption on Linux and explain how we made it at least two times faster for ourselves and our customers!},
  langid = {english},
  organization = {The Cloudflare Blog},
  keywords = {industry practices,operating systems,usable security},
  file = {/home/sam/Zotero/storage/9EGS6CKI/speeding-up-linux-disk-encryption.html}
}

@article{kosterSnakemakeScalableBioinformatics2012,
  title = {Snakemake—a Scalable Bioinformatics Workflow Engine},
  author = {Köster, Johannes and Rahmann, Sven},
  date = {2012-10-01},
  journaltitle = {Bioinformatics},
  shortjournal = {Bioinformatics},
  volume = {28},
  number = {19},
  pages = {2520--2522},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/bts480},
  url = {https://doi.org/10.1093/bioinformatics/bts480},
  urldate = {2023-01-29},
  abstract = {Summary: Snakemake is a workflow engine that provides a readable Python-based workflow definition language and a powerful execution environment that scales from single-core workstations to compute clusters without modifying the workflow. It is the first system to support the use of automatically inferred multiple named wildcards (or variables) in input and output filenames.Availability: ~http://snakemake.googlecode.com.Contact: ~johannes.koester@uni-due.de},
  keywords = {project-acm-rep,reproducibility engineering,research software engineering,workflow managers},
  file = {/home/sam/Zotero/storage/C7B6FH3N/Köster and Rahmann - 2012 - Snakemake—a scalable bioinformatics workflow engin.pdf}
}

@article{kowalewskiSustainablePackagingQuantum2022,
  title = {Sustainable Packaging of Quantum Chemistry Software with the {{Nix}} Package Manager},
  author = {Kowalewski, Markus and Seeber, Phillip},
  date = {2022},
  journaltitle = {International Journal of Quantum Chemistry},
  volume = {122},
  number = {9},
  pages = {e26872},
  issn = {1097-461X},
  doi = {10.1002/qua.26872},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/qua.26872},
  urldate = {2025-01-14},
  abstract = {The installation of quantum chemistry software packages is commonly done manually and can be a time-consuming and complicated process. An update of the underlying Linux system requires a reinstallation in many cases and can quietly break software installed on the system. In this paper, we present an approach that allows for an easy installation of quantum chemistry software packages, which is also independent of operating system updates. The use of the Nix package manager allows building software in a reproducible manner, which allows for a reconstruction of the software for later reproduction of scientific results. The build recipes that are provided can be readily used by anyone to avoid complex installation procedures.},
  langid = {english},
  keywords = {high-performance computing,Nix package manager,quantum chemistry,reproducible environments,software development},
  file = {/home/sam/Zotero/storage/7DMZVS23/Kowalewski and Seeber - 2022 - Sustainable packaging of quantum chemistry software with the Nix package manager.pdf;/home/sam/Zotero/storage/P4662E2Q/qua.html}
}

@article{krafczykLearningReproducingComputational2021,
  title = {Learning from Reproducing Computational Results: Introducing Three Principles and the {{Reproduction Package}}},
  shorttitle = {Learning from Reproducing Computational Results},
  author = {Krafczyk, M. S. and Shi, A. and Bhaskar, A. and Marinov, D. and Stodden, V.},
  date = {2021-03-29},
  journaltitle = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {379},
  number = {2197},
  pages = {20200069},
  publisher = {Royal Society},
  doi = {10.1098/rsta.2020.0069},
  url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2020.0069},
  urldate = {2023-01-31},
  abstract = {We carry out efforts to reproduce computational results for seven published articles and identify barriers to computational reproducibility. We then derive three principles to guide the practice and dissemination of reproducible computational research: (i) Provide transparency regarding how computational results are produced; (ii) When writing and releasing research software, aim for ease of (re-)executability; (iii) Make any code upon which the results rely as deterministic as possible. We then exemplify these three principles with 12 specific guidelines for their implementation in practice. We illustrate the three principles of reproducible research with a series of vignettes from our experimental reproducibility work. We define a novel Reproduction Package, a formalism that specifies a structured way to share computational research artifacts that implements the guidelines generated from our reproduction efforts to allow others to build, reproduce and extend computational science. We make our reproduction efforts in this paper publicly available as exemplar Reproduction Packages. This article is part of the theme issue ‘Reliability and reproducibility in computational science: implementing verification, validation and uncertainty quantification in silico’.},
  keywords = {project-acm-rep,project-provenance-pp,reproducibility engineering},
  file = {/home/sam/Zotero/storage/P3RYMMMA/Krafczyk et al. - 2021 - Learning from reproducing computational results i.pdf}
}

@inproceedings{krafczykScientificTestsContinuous2019,
  title = {Scientific {{Tests}} and {{Continuous Integration Strategies}} to {{Enhance Reproducibility}} in the {{Scientific Software Context}}},
  booktitle = {Proceedings of the 2nd {{International Workshop}} on {{Practical Reproducible Evaluation}} of {{Computer Systems}}},
  author = {Krafczyk, Matthew and Shi, August and Bhaskar, Adhithya and Marinov, Darko and Stodden, Victoria},
  date = {2019-06-17},
  series = {P-{{RECS}} '19},
  pages = {23--28},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3322790.3330595},
  url = {https://doi.org/10.1145/3322790.3330595},
  urldate = {2023-02-23},
  abstract = {Continuous integration (CI) is a well-established technique in commercial and open-source software projects, although not routinely used in scientific publishing. In the scientific software context, CI can serve two functions to increase reproducibility of scientific results: providing an established platform for testing the reproducibility of these results, and demonstrating to other scientists how the code and data generate the published results. We explore scientific software testing and CI strategies using two articles published in the areas of applied mathematics and computational physics. We discuss lessons learned from reproducing these articles as well as examine and discuss existing tests. We introduce the notion of a "scientific test" as one that produces computational results from a published article. We then consider full result reproduction within a CI environment. If authors find their work too time or resource intensive to easily adapt to a CI context, we recommend the inclusion of results from reduced versions of their work (e.g., run at lower resolution, with shorter time scales, with smaller data sets) alongside their primary results within their article. While these smaller versions may be less interesting scientifically, they can serve to verify that published code and data are working properly. We demonstrate such reduction tests on the two articles studied.},
  isbn = {978-1-4503-6756-1},
  keywords = {continuous integration,reproducibility engineering},
  annotation = {interest: 98},
  file = {/home/sam/Zotero/storage/C4FKPNL2/Krafczyk et al. - 2019 - Scientific Tests and Continuous Integration Strate.pdf}
}

@article{krawczykHowOpenAccess2021,
  title = {How Is Open Access Accused of Being Predatory? {{The}} Impact of {{Beall}}'s Lists of Predatory Journals on Academic Publishing},
  shorttitle = {How Is Open Access Accused of Being Predatory?},
  author = {Krawczyk, Franciszek and Kulczycki, Emanuel},
  date = {2021-03-01},
  journaltitle = {The Journal of Academic Librarianship},
  shortjournal = {The Journal of Academic Librarianship},
  volume = {47},
  number = {2},
  pages = {102271},
  issn = {0099-1333},
  doi = {10.1016/j.acalib.2020.102271},
  url = {https://www.sciencedirect.com/science/article/pii/S0099133320301622},
  urldate = {2022-08-30},
  abstract = {The aim of this paper is to investigate how predatory journals are characterized by authors who write about such journals. We emphasize the ways in which predatory journals have been conflated with—or distinguished from—open access journals. We created a list of relevant publications on predatory publishing using four databases: Web of Science, Scopus, Dimensions, and Microsoft Academic. We included 280 English-language publications in the review according to their contributions to the discussions on predatory publishing. Then, we coded and qualitatively analyzed these publications. The findings show the profound influence of Jeffrey Beall, who composed and maintained himself lists of predatory publishers and journals, on the whole discussion on predatory publishing. The major themes by which Beall has characterized predatory journals are widely present in non-Beall publications. Moreover, 122 papers we reviewed combined predatory publishing with open access using similar strategies as Beall. The overgeneralization of the flaws of some open access journals to the entire open access movement has led to unjustified prejudices among the academic community toward open access. This is the first large-scale study that systematically examines how predatory publishing is defined in the literature.},
  langid = {english},
  file = {/home/sam/Zotero/storage/NRJ8I4F6/Krawczyk and Kulczycki - 2021 - How is open access accused of being predatory The.pdf;/home/sam/Zotero/storage/IXHDKJBU/S0099133320301622.html}
}

@article{krewinkelFormattingOpenScience2017,
  title = {Formatting {{Open Science}}: Agilely Creating Multiple Document Formats for Academic Manuscripts with {{Pandoc Scholar}}},
  shorttitle = {Formatting {{Open Science}}},
  author = {Krewinkel, Albert and Winkler, Robert},
  editor = {Ventura, Sebastian},
  date = {2017-05-08},
  journaltitle = {PeerJ Computer Science},
  volume = {3},
  pages = {e112},
  issn = {2376-5992},
  doi = {10.7717/peerj-cs.112},
  url = {https://peerj.com/articles/cs-112},
  urldate = {2022-05-25},
  abstract = {The timely publication of scientific results is essential for dynamic advances in science. The ubiquitous availability of computers which are connected to a global network made the rapid and low-cost distribution of information through electronic channels possible. New concepts, such as Open Access publishing and preprint servers are currently changing the traditional print media business towards a community-driven peer production. However, the cost of scientific literature generation, which is either charged to readers, authors or sponsors, is still high. The main active participants in the authoring and evaluation of scientific manuscripts are volunteers, and the cost for online publishing infrastructure is close to negligible. A major time and cost factor is the formatting of manuscripts in the production stage. In this article we demonstrate the feasibility of writing scientific manuscripts in plain markdown (MD) text files, which can be easily converted into common publication formats, such as PDF, HTML or EPUB, using Pandoc. The simple syntax of Markdown assures the long-term readability of raw files and the development of software and workflows. We show the implementation of typical elements of scientific manuscripts—formulas, tables, code blocks and citations—and present tools for editing, collaborative writing and version control. We give an example on how to prepare a manuscript with distinct output formats, a DOCX file for submission to a journal, and a LATEX/PDF version for deposition as a PeerJ preprint. Further, we implemented new features for supporting ‘semantic web’ applications, such as the ‘journal article tag suite’—JATS, and the ‘citation typing ontology’—CiTO standard. Reducing the work spent on manuscript formatting translates directly to time and cost savings for writers, publishers, readers and sponsors. Therefore, the adoption of the MD format contributes to the agile production of open science literature. Pandoc Scholar is freely available from                https://github.com/pandoc-scholar                .},
  langid = {english},
  file = {/home/sam/Zotero/storage/BMA9NQNG/peerj-cs-112.pdf}
}

@article{krishnamurthiRealSoftwareCrisis2015,
  title = {The Real Software Crisis: Repeatability as a Core Value},
  shorttitle = {The Real Software Crisis},
  author = {Krishnamurthi, Shriram and Vitek, Jan},
  date = {2015-02-23},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {58},
  number = {3},
  pages = {34--36},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/2658987},
  url = {https://dl.acm.org/doi/10.1145/2658987},
  urldate = {2022-06-30},
  abstract = {Sharing experiences running artifact evaluation committees for five major conferences.},
  langid = {english},
  keywords = {metascience,reproducibility engineering},
  annotation = {interest: 74}
}

@online{krishnaswamiSemanticDomainGolden2022,
  title = {Semantic {{Domain}}: {{The Golden Age}} of {{PL Research}}},
  shorttitle = {Semantic {{Domain}}},
  author = {Krishnaswami, Neel},
  date = {2022-09-13},
  url = {https://semantic-domain.blogspot.com/2022/09/the-golden-age-of-pl-research.html},
  urldate = {2022-11-14},
  organization = {Semantic Domain},
  keywords = {programming languages},
  annotation = {interest: 85},
  file = {/home/sam/Zotero/storage/TQ5TGT93/the-golden-age-of-pl-research.html}
}

@online{krollWeHaveTalk,
  title = {We Have to Talk about This {{Python}}, {{Gunicorn}}, {{Gevent}} Thing},
  author = {Kroll, Rachel},
  url = {https://rachelbythebay.com/w/2020/03/07/costly/},
  urldate = {2022-09-06},
  keywords = {industry practices,operating systems},
  annotation = {interest: 91},
  file = {/home/sam/Zotero/storage/DKLJATEU/costly.html}
}

@article{kruchtenPresentFutureSoftware2006,
  title = {The {{Past}}, {{Present}}, and {{Future}} for {{Software Architecture}}},
  author = {Kruchten, P. and Obbink, H. and Stafford, J.},
  date = {2006-03},
  journaltitle = {IEEE Software},
  volume = {23},
  number = {2},
  pages = {22--30},
  issn = {1937-4194},
  doi = {10.1109/MS.2006.59},
  abstract = {It's been 10 years since David Garlan and Mary Shaw wrote their seminal book Software Architecture Perspective on an Emerging Discipline, since Maarten Boasson edited a special issue of IEEE Software on software architecture, and since the first International Software Architecture Workshop took place. What has happened over these 10 years? What have we learned? Where do we look for information? What's the community around this discipline? And where are we going from here?This article is part of a focus section on software architecture.},
  eventtitle = {{{IEEE Software}}},
  annotation = {interest: 78}
}

@inproceedings{kuhnJULEAFlexibleStorage2017,
  title = {{{JULEA}}: {{A Flexible Storage Framework}} for {{HPC}}},
  shorttitle = {{{JULEA}}},
  booktitle = {High {{Performance Computing}}},
  author = {Kuhn, Michael},
  editor = {Kunkel, Julian M. and Yokota, Rio and Taufer, Michela and Shalf, John},
  date = {2017},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {712--723},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-67630-2_51},
  abstract = {JULEA is a flexible storage framework that allows offering arbitrary client interfaces to applications. To be able to rapidly prototype new approaches, it offers data and metadata backends that can either be client-side or server-side; backends for popular storage technologies such as POSIX, LevelDB and MongoDB have already been implemented. Additionally, JULEA allows dynamically adapting the I/O operations’ semantics and can thus be adjusted to different use-cases. It runs completely in user space, which eases development and debugging. Its goal is to provide a solid foundation for storage research and teaching.},
  isbn = {978-3-319-67630-2},
  langid = {english},
  keywords = {distributed systems,high performance computing,storage systems},
  file = {/home/sam/Zotero/storage/V64LRA49/Kuhn - 2017 - JULEA A Flexible Storage Framework for HPC.pdf}
}

@article{kungClassFirewallTest1995,
  title = {Class Firewall, Test Order, and Regression Testing of Object-Oriented Programs},
  author = {Kung, David Chenho and Gao, Jerry and Hsia, Pei and Lin, Jeremy and Toyoshima, Yasufumi},
  date = {1995},
  journaltitle = {JOOP},
  volume = {8.2},
  pages = {51--65},
  keywords = {object-oriented programming,software engineering,software testing},
  file = {/home/sam/Zotero/storage/VBYZXZRC/Kung et al. - Class Firewall, Test Order, and Regression Testing.pdf}
}

@article{kurtzerSingularityScientificContainers2017,
  title = {Singularity: {{Scientific}} Containers for Mobility of Compute},
  shorttitle = {Singularity},
  author = {Kurtzer, Gregory M. and Sochat, Vanessa and Bauer, Michael W.},
  date = {2017-05-11},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {12},
  number = {5},
  pages = {e0177459},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0177459},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0177459},
  urldate = {2023-01-29},
  abstract = {Here we present Singularity, software developed to bring containers and reproducibility to scientific computing. Using Singularity containers, developers can work in reproducible environments of their choosing and design, and these complete environments can easily be copied and executed on other platforms. Singularity is an open source initiative that harnesses the expertise of system and software engineers and researchers alike, and integrates seamlessly into common workflows for both of these groups. As its primary use case, Singularity brings mobility of computing to both users and HPC centers, providing a secure means to capture and distribute software and compute environments. This ability to create and deploy reproducible environments across these centers, a previously unmet need, makes Singularity a game changing development for computational science.},
  langid = {english},
  keywords = {project-acm-rep,project-provenance-pp,reproducibility engineering,research software engineering},
  file = {/home/sam/Zotero/storage/MYW2MIFY/Kurtzer et al. - 2017 - Singularity Scientific containers for mobility of.pdf}
}

@inproceedings{kwonLDXCausalityInference2016,
  title = {{{LDX}}: {{Causality Inference}} by {{Lightweight Dual Execution}}},
  shorttitle = {{{LDX}}},
  booktitle = {Proceedings of the {{Twenty-First International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}},
  author = {Kwon, Yonghwi and Kim, Dohyeong and Sumner, William Nick and Kim, Kyungtae and Saltaformaggio, Brendan and Zhang, Xiangyu and Xu, Dongyan},
  date = {2016-03-25},
  series = {{{ASPLOS}} '16},
  pages = {503--515},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2872362.2872395},
  url = {https://dl.acm.org/doi/10.1145/2872362.2872395},
  urldate = {2023-08-23},
  abstract = {Causality inference, such as dynamic taint anslysis, has many applications (e.g., information leak detection). It determines whether an event e is causally dependent on a preceding event c during execution. We develop a new causality inference engine LDX. Given an execution, it spawns a slave execution, in which it mutates c and observes whether any change is induced at e. To preclude non-determinism, LDX couples the executions by sharing syscall outcomes. To handle path differences induced by the perturbation, we develop a novel on-the-fly execution alignment scheme that maintains a counter to reflect the progress of execution. The scheme relies on program analysis and compiler transformation. LDX can effectively detect information leak and security attacks with an average overhead of 6.08\% while running the master and the slave concurrently on separate CPUs, much lower than existing systems that require instruction level monitoring. Furthermore, it has much better accuracy in causality inference.},
  isbn = {978-1-4503-4091-5},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/LJ8B4UR9/Kwon et al. - 2016 - LDX Causality Inference by Lightweight Dual Execu.pdf}
}

@inproceedings{kwonMCIModelingbasedCausality2018,
  title = {{{MCI}} : {{Modeling-based Causality Inference}} in {{Audit Logging}} for {{Attack Investigation}}},
  shorttitle = {{{MCI}}},
  booktitle = {Proceedings 2018 {{Network}} and {{Distributed System Security Symposium}}},
  author = {Kwon, Yonghwi and Wang, Fei and Wang, Weihang and Lee, Kyu Hyung and Lee, Wen-Chuan and Ma, Shiqing and Zhang, Xiangyu and Xu, Dongyan and Jha, Somesh and Ciocarlie, Gabriela and Gehani, Ashish and Yegneswaran, Vinod},
  date = {2018},
  publisher = {Internet Society},
  location = {San Diego, CA},
  doi = {10.14722/ndss.2018.23306},
  url = {https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_07B-2_Kwon_paper.pdf},
  urldate = {2023-08-23},
  abstract = {In this paper, we develop a model based causality inference technique for audit logging that does not require any application instrumentation or kernel modification. It leverages a recent dynamic analysis, dual execution (LDX), that can infer precise causality between system calls but unfortunately requires doubling the resource consumption such as CPU time and memory consumption. For each application, we use LDX to acquire precise causal models for a set of primitive operations. Each model is a sequence of system calls that have inter-dependences, some of them caused by memory operations and hence implicit at the system call level. These models are described by a language that supports various complexity such as regular, context-free, and even context-sensitive. In production run, a novel parser is deployed to parse audit logs (without any enhancement) to model instances and hence derive causality. Our evaluation on a set of real-world programs shows that the technique is highly effective. The generated models can recover causality with 0\% false-positives (FP) and false-negatives (FN) for most programs and only 8.3\% FP and 5.2\% FN in the worst cases. The models also feature excellent composibility, meaning that the models derived from primitive operations can be composed together to describe causality for large and complex real world missions. Applying our technique to attack investigation shows that the system-wide attack causal graphs are highly precise and concise, having better quality than the state-of-the-art.},
  eventtitle = {Network and {{Distributed System Security Symposium}}},
  isbn = {978-1-891562-49-5},
  langid = {english},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/9I4EY8H5/Kwon et al. - 2018 - MCI  Modeling-based Causality Inference in Audit .pdf}
}

@article{lakensEquivalenceTests2017,
  title = {Equivalence {{Tests}}},
  author = {Lakens, Daniël},
  date = {2017-05},
  journaltitle = {Social Psychological and Personality Science},
  shortjournal = {Soc Psychol Personal Sci},
  volume = {8},
  number = {4},
  eprint = {28736600},
  eprinttype = {pmid},
  pages = {355--362},
  issn = {1948-5506},
  doi = {10.1177/1948550617697177},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5502906/},
  urldate = {2024-01-29},
  abstract = {Scientists should be able to provide support for the absence of a meaningful effect. Currently, researchers often incorrectly conclude an effect is absent based a nonsignificant result. A widely recommended approach within a frequentist framework is to test for equivalence. In equivalence tests, such as the two one-sided tests (TOST) procedure discussed in this article, an upper and lower equivalence bound is specified based on the smallest effect size of interest. The TOST procedure can be used to statistically reject the presence of effects large enough to be considered worthwhile. This practical primer with accompanying spreadsheet and R package enables psychologists to easily perform equivalence tests (and power analyses) by setting equivalence bounds based on standardized effect sizes and provides recommendations to prespecify equivalence bounds. Extending your statistical tool kit with equivalence tests is an easy way to improve your statistical and theoretical inferences.},
  pmcid = {PMC5502906},
  keywords = {statistics},
  file = {/home/sam/Zotero/storage/YR5IM8ED/Lakens - 2017 - Equivalence Tests.pdf}
}

@inproceedings{lamIDFlakiesFrameworkDetecting2019,
  title = {{{iDFlakies}}: {{A Framework}} for {{Detecting}} and {{Partially Classifying Flaky Tests}}},
  shorttitle = {{{iDFlakies}}},
  booktitle = {2019 12th {{IEEE Conference}} on {{Software Testing}}, {{Validation}} and {{Verification}} ({{ICST}})},
  author = {Lam, Wing and Oei, Reed and Shi, August and Marinov, Darko and Xie, Tao},
  date = {2019-04},
  pages = {312--322},
  issn = {2159-4848},
  doi = {10.1109/ICST.2019.00038},
  abstract = {Regression testing is increasingly important with the wide use of continuous integration. A desirable requirement for regression testing is that a test failure reliably indicates a problem in the code under test and not a false alarm from the test code or the testing infrastructure. However, some test failures are unreliable, stemming from flaky tests that can nondeterministically pass or fail for the same code under test. There are many types of flaky tests, with order-dependent tests being a prominent type. To help advance research on flaky tests, we present (1) a framework, iDFlakies, to detect and partially classify flaky tests; (2) a dataset of flaky tests in open-source projects; and (3) a study with our dataset. iDFlakies automates experimentation with our tool for Maven-based Java projects. Using iDFlakies, we build a dataset of 422 flaky tests, with 50.5\% order-dependent and 49.5\% not. Our study of these flaky tests finds the prevalence of two types of flaky tests, probability of a test-suite run to have at least one failure due to flaky tests, and how different test reorderings affect the number of detected flaky tests. We envision that our work can spur research to alleviate the problem of flaky tests.},
  eventtitle = {2019 12th {{IEEE Conference}} on {{Software Testing}}, {{Validation}} and {{Verification}} ({{ICST}})},
  keywords = {opensource software,software engineering,software testing},
  file = {/home/sam/Zotero/storage/KEV3ES68/Lam et al. - 2019 - iDFlakies A Framework for Detecting and Partially.pdf}
}

@article{larsonTooManyPhD2014,
  title = {Too {{Many PhD Graduates}} or {{Too Few Academic Job Openings}}: {{The Basic Reproductive Number R0}} in {{Academia}}},
  shorttitle = {Too {{Many PhD Graduates}} or {{Too Few Academic Job Openings}}},
  author = {Larson, Richard C. and Ghaffarzadegan, Navid and Xue, Yi},
  date = {2014},
  journaltitle = {Systems research and behavioral science},
  shortjournal = {Syst Res Behav Sci},
  volume = {31},
  number = {6},
  eprint = {25642132},
  eprinttype = {pmid},
  pages = {745--750},
  issn = {1092-7026},
  doi = {10.1002/sres.2210},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4309283/},
  urldate = {2022-08-30},
  abstract = {The academic job market has become increasingly competitive for PhD graduates. In this note, we ask the basic question of ‘Are we producing more PhDs than needed?’ We take a systems approach and offer a ‘birth rate’ perspective: professors graduate PhDs who later become professors themselves, an analogue to how a population grows. We show that the reproduction rate in academia is very high. For example, in engineering, a professor in the US graduates 7.8 new PhDs during his/her whole career on average, and only one of these graduates can replace the professor’s position. This implies that in a steady state, only 12.8\% of PhD graduates can attain academic positions in the USA. The key insight is that the system in many places is saturated, far beyond capacity to absorb new PhDs in academia at the rates that they are being produced. Based on the analysis, we discuss policy implications.},
  pmcid = {PMC4309283},
  keywords = {academic careers,metascience},
  file = {/home/sam/Zotero/storage/E9CXTDV4/Larson et al. - 2014 - Too Many PhD Graduates or Too Few Academic Job Ope.pdf}
}

@inproceedings{lattnerLLVMCompilationFramework2004,
  title = {{{LLVM}}: A Compilation Framework for Lifelong Program Analysis \& Transformation},
  shorttitle = {{{LLVM}}},
  booktitle = {International {{Symposium}} on {{Code Generation}} and {{Optimization}}, 2004. {{CGO}} 2004.},
  author = {Lattner, C. and Adve, V.},
  date = {2004-03},
  pages = {75--86},
  doi = {10.1109/CGO.2004.1281665},
  abstract = {We describe LLVM (low level virtual machine), a compiler framework designed to support transparent, lifelong program analysis and transformation for arbitrary programs, by providing high-level information to compiler transformations at compile-time, link-time, run-time, and in idle time between runs. LLVM defines a common, low-level code representation in static single assignment (SSA) form, with several novel features: a simple, language-independent type-system that exposes the primitives commonly used to implement high-level language features; an instruction for typed address arithmetic; and a simple mechanism that can be used to implement the exception handling features of high-level languages (and setjmp/longjmp in C) uniformly and efficiently. The LLVM compiler framework and code representation together provide a combination of key capabilities that are important for practical, lifelong analysis and transformation of programs. To our knowledge, no existing compilation approach provides all these capabilities. We describe the design of the LLVM representation and compiler framework, and evaluate the design in three ways: (a) the size and effectiveness of the representation, including the type information it provides; (b) compiler performance for several interprocedural problems; and (c) illustrative examples of the benefits LLVM provides for several challenging compiler problems.},
  eventtitle = {International {{Symposium}} on {{Code Generation}} and {{Optimization}}, 2004. {{CGO}} 2004.},
  keywords = {compilers,project-provenance-pp},
  file = {/home/sam/Zotero/storage/PEAVHRLJ/Lattner and Adve - 2004 - LLVM a compilation framework for lifelong program.pdf}
}

@online{lattnerWhatEveryProgrammer2011,
  title = {What {{Every C Programmer Should Know About Undefined Behavior}} \#2/3},
  author = {Lattner, Chris},
  date = {2011-05-14T12:33:00+00:00},
  url = {https://blog.llvm.org/2011/05/what-every-c-programmer-should-know_14.html},
  urldate = {2023-03-09},
  abstract = {In Part 1 of our series, we discussed what undefined behavior is, and how it allows C and C++ compilers to produce higher performance applications than "safe" languages. This post talks about how "unsafe" C really is, explaining some of the highly surprising effects that undefined behavior can cause. In Part \#3, we talk about what friendly compilers can do to mitigate some of the surprise, even if they aren't required to.},
  langid = {english},
  organization = {The LLVM Project Blog},
  keywords = {compilers},
  file = {/home/sam/Zotero/storage/L85TXTXB/what-every-c-programmer-should-know_14.html}
}

@inproceedings{leeAdaptiveWorkflowProcessing2008,
  title = {Adaptive {{Workflow Processing}} and {{Execution}} in {{Pegasus}}},
  booktitle = {Proceedings of the 2008 {{The}} 3rd {{International Conference}} on {{Grid}} and {{Pervasive Computing}} - {{Workshops}}},
  author = {Lee, Kevin and Paton, Norman W. and Sakellariou, Rizos and Deelman, Ewa and Fernandes, Alvaro A. A. and Mehta, Gaurang},
  date = {2008-05-25},
  series = {{{GPC-WORKSHOPS}} '08},
  pages = {99--106},
  publisher = {IEEE Computer Society},
  location = {USA},
  doi = {10.1109/GPC.WORKSHOPS.2008.30},
  url = {https://doi.org/10.1109/GPC.WORKSHOPS.2008.30},
  urldate = {2022-09-06},
  abstract = {Workflows are widely used in applications that require coordinated use of computational resources. Workflow definition languages typically abstract over some aspects of the way in which a workflow is to be executed, such as the level of parallelism to be used or the physical resources to be deployed. As a result, a workflow management system has responsibility for establishing how best to execute a workflow given the available resources. The Pegasus workflow management system compiles abstract workflows into concrete execution plans, and has been widely used in large-scale e-Science applications. This paper describes an extension to Pegasus whereby resource allocation decisions are revised during workflow evaluation, in the light of feedback on the performance of jobs at runtime. The contributions of this paper include: (i) a description of how adaptive processing has been retrofitted to an existing workflow management system; (ii) a scheduling algorithm that allocates resources based on runtime performance; and (iii) an experimental evaluation of the resulting infrastructure using grid middleware over clusters.},
  isbn = {978-0-7695-3177-9},
  keywords = {workflow managers},
  annotation = {interest: 64},
  file = {/home/sam/Zotero/storage/JIY4R7WG/Lee et al. - 2008 - Adaptive Workflow Processing and Execution in Pega.pdf}
}

@inproceedings{leeHighAccuracyAttack2017,
  title = {High {{Accuracy Attack Provenance}} via {{Binary-based Execution Partition}}},
  booktitle = {Proceedings of the 2017 {{Network}} and {{Distributed System Security}} ({{NDSS}}) {{Symposium}}},
  author = {Lee, Kyu Hyung and Zhang, Xiangyu and Xu, Dongyan},
  date = {2017},
  abstract = {An important aspect of cyber attack forensics is to understand the provenance of suspicious events, as it discloses the root cause and ramifications of cyber attacks. Traditionally, this is done by analyzing audit log. However, the presence of long running programs makes a live process receiving a large volume of inputs and produce many outputs and each output may be causally related to all the preceding inputs, leading to dependence explosion and making attack investigations almost infeasible. We observe that a long running execution can be partitioned into individual units by monitoring the execution of the program’s event-handling loops, with each iteration corresponding to the processing of an independent input/request. We reverse engineer such loops from application binaries. We also reverse engineer instructions that could cause workflows between units. Detecting such a workflow is critical to disclosing causality between units. We then perform selective logging for unit boundaries and unit dependences. Our experiments show that our technique, called BEEP, has negligible runtime overhead ({$<$} 1.4\%) and low space overhead (12.28\% on average). It is effective in capturing the minimal causal graph for every attack case we have studied, without any dependence explosion.},
  eventtitle = {Network and {{Distributed System Security}} ({{NDSS}}) 2017},
  langid = {english},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/ABDE2U7Q/Lee et al. - High Accuracy Attack Provenance via Binary-based E.pdf}
}

@article{leeHowUseTwitter2019,
  title = {How to Use {{Twitter}} to Further Your Research Career},
  author = {Lee, Jet-Sing M.},
  date = {2019-02-08},
  journaltitle = {Nature},
  publisher = {Nature Publishing Group},
  doi = {10.1038/d41586-019-00535-w},
  url = {https://www.nature.com/articles/d41586-019-00535-w},
  urldate = {2022-08-31},
  abstract = {The social-media platform is often a tool for procrastination, says Jet-Sing M. Lee. But what else can it be?},
  langid = {english},
  keywords = {academic publishing,metascience},
  annotation = {Bandiera\_abtest: a\\
Cg\_type: Career Column\\
Subject\_term: Communication, Lab life, Careers\\
interest: 87},
  file = {/home/sam/Zotero/storage/FFKMW4DR/d41586-019-00535-w.html}
}

@inproceedings{leeLogGCGarbageCollecting2013,
  title = {{{LogGC}}: Garbage Collecting Audit Log},
  shorttitle = {{{LogGC}}},
  booktitle = {Proceedings of the 2013 {{ACM SIGSAC}} Conference on {{Computer}} \& Communications Security},
  author = {Lee, Kyu Hyung and Zhang, Xiangyu and Xu, Dongyan},
  date = {2013-11-04},
  series = {{{CCS}} '13},
  pages = {1005--1016},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2508859.2516731},
  url = {https://dl.acm.org/doi/10.1145/2508859.2516731},
  urldate = {2024-01-21},
  abstract = {System-level audit logs capture the interactions between applications and the runtime environment. They are highly valuable for forensic analysis that aims to identify the root cause of an attack, which may occur long ago, or to determine the ramifications of an attack for recovery from it. A key challenge of audit log-based forensics in practice is the sheer size of the log files generated, which could grow at a rate of Gigabytes per day. In this paper, we propose LogGC, an audit logging system with garbage collection (GC) capability. We identify and overcome the unique challenges of garbage collection in the context of computer forensic analysis, which makes LogGC different from traditional memory GC techniques. We also develop techniques that instrument user applications at a small number of selected places to emit additional system events so that we can substantially reduce the false dependences between system events to improve GC effectiveness. Our results show that LogGC can reduce audit log size by 14 times for regular user systems and 37 times for server systems, without affecting the accuracy of forensic analysis.},
  isbn = {978-1-4503-2477-9},
  keywords = {project-provenance-pp},
  file = {/home/sam/Zotero/storage/E9P3C9QY/Lee et al. - 2013 - LogGC garbage collecting audit log.pdf}
}

@inproceedings{leeSecureProvenanceCloud2015,
  title = {Towards {{Secure Provenance}} in the {{Cloud}}: {{A Survey}}},
  shorttitle = {Towards {{Secure Provenance}} in the {{Cloud}}},
  booktitle = {2015 {{IEEE}}/{{ACM}} 8th {{International Conference}} on {{Utility}} and {{Cloud Computing}} ({{UCC}})},
  author = {Lee, Brian and Awad, Abir and Awad, Mirna},
  date = {2015-12},
  pages = {577--582},
  doi = {10.1109/UCC.2015.102},
  abstract = {Provenance information are meta-data that summarize the history of the creation and the actions performed on an artefact e.g. data, process etc. Secure provenance is essential to improve data forensics, ensure accountability and increase the trust in the cloud. In this paper, we survey the existing cloud provenance management schemes and proposed security solutions. We investigate the current related security challenges resulting from the nature of the provenance model and the characteristics of the cloud and we finally identify potential research directions which we feel necessary t should be covered in order to build a secure cloud provenance for the next generation.},
  eventtitle = {2015 {{IEEE}}/{{ACM}} 8th {{International Conference}} on {{Utility}} and {{Cloud Computing}} ({{UCC}})},
  keywords = {project-provenance-pp},
  file = {/home/sam/Zotero/storage/AN3W669M/Lee et al. - 2015 - Towards Secure Provenance in the Cloud A Survey.pdf}
}

@article{leeTechnologyAcceptanceModel2003,
  title = {The {{Technology Acceptance Model}}: {{Past}}, {{Present}}, and {{Future}}},
  shorttitle = {The {{Technology Acceptance Model}}},
  author = {Lee, Younghwa and Kozar, Kenneth A. and Larsen, Kai R.T.},
  date = {2003},
  journaltitle = {Communications of the Association for Information Systems},
  shortjournal = {CAIS},
  volume = {12},
  issn = {15293181},
  doi = {10.17705/1CAIS.01250},
  url = {https://aisel.aisnet.org/cais/vol12/iss1/50},
  urldate = {2022-06-01},
  abstract = {While the technology acceptance model (TAM), introduced in 1986, continues to be the most widely applied theoretical model in the IS field, few previous efforts examined its accomplishments and limitations. This study traces TAM’s history, investigates its findings, and cautiously predicts its future trajectory. One hundred and one articles published by leading IS journals and conferences in the past eighteen years are examined and summarized. An open-ended survey of thirty-two leadi ng IS researchers assisted in critically examining TAM and specifying future directions.},
  langid = {english},
  keywords = {human computer interaction,internship-project,technology-acceptance},
  file = {/home/sam/Zotero/storage/6KDW5MI8/The Technology Acceptance Model Past Present and Future.pdf;/home/sam/Zotero/storage/JVRXFTAX/Screenshot from 2022-06-01 15-07-55.png;/home/sam/Zotero/storage/RZ7KFEZ3/Screenshot from 2022-06-01 15-07-23.png}
}

@online{leeToxicCultureRejection,
  title = {The {{Toxic Culture}} of {{Rejection}} in {{Computer Science}}},
  author = {Lee, Edward},
  url = {https://sigbed.org/2022/08/22/the-toxic-culture-of-rejection-in-computer-science/},
  urldate = {2022-08-29},
  langid = {american},
  organization = {ACM SIGBED},
  keywords = {academic publishing,metascience},
  file = {/home/sam/Zotero/storage/FAZFUX54/the-toxic-culture-of-rejection-in-computer-science.html}
}

@article{leisersonTherePlentyRoom2020,
  title = {There’s Plenty of Room at the {{Top}}: {{What}} Will Drive Computer Performance after {{Moore}}’s Law?},
  shorttitle = {There’s Plenty of Room at the {{Top}}},
  author = {Leiserson, Charles E. and Thompson, Neil C. and Emer, Joel S. and Kuszmaul, Bradley C. and Lampson, Butler W. and Sanchez, Daniel and Schardl, Tao B.},
  date = {2020-06-05},
  journaltitle = {Science},
  volume = {368},
  number = {6495},
  pages = {eaam9744},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.aam9744},
  url = {https://www.science.org/doi/full/10.1126/science.aam9744},
  urldate = {2022-10-18},
  keywords = {hardware,performance engineering},
  annotation = {interest: 87},
  file = {/home/sam/Zotero/storage/7J3YFQ3X/Leiserson et al. - 2020 - There’s plenty of room at the Top What will drive.pdf}
}

@unpublished{lemsonHaloGalaxyFormation2006,
  title = {Halo and {{Galaxy Formation Histories}} from the {{Millennium Simulation}}: {{Public}} Release of a {{VO-oriented}} and {{SQL-queryable}} Database for Studying the Evolution of Galaxies in the {{LambdaCDM}} Cosmogony},
  shorttitle = {Halo and {{Galaxy Formation Histories}} from the {{Millennium Simulation}}},
  author = {Lemson, G. and Consortium, the Virgo},
  date = {2006-08-03},
  eprint = {astro-ph/0608019},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/astro-ph/0608019},
  urldate = {2022-04-12},
  abstract = {The Millennium Run is the largest simulation of the formation of structure within the \$\textbackslash Lambda\$CDM cosmogony so far carried out. It uses \$10\textasciicircum\{10\}\$ particles to follow the dark matter distribution in a cubic region 500\$h\textasciicircum\{-1\}\$Mpc on a side, and has a spatial resolution of 5 \$h\textasciicircum\{-1\}\$kpc. Application of simplified modelling techniques to the stored output of this calculation allows the formation and evolution of the \$\textbackslash sim 10\textasciicircum 7\$ galaxies more luminous than the Small Magellanic Cloud to be simulated for a variety of assumptions about the detailed physics involved. As part of the activities of the German Astrophysical Virtual Observatory we have used a relational database to store the detailed assembly histories both of all the haloes and subhaloes resolved by the simulation, and of all the galaxies that form within these structures for two independent models of the galaxy formation physics. We have created web applications that allow users to query these databases remotely using the standard Structured Query Language (SQL). This allows easy access to all properties of the galaxies and halos, as well as to the spatial and temporal relations between them and their environment. Information is output in table format compatible with standard Virtual Observatory tools and protocols. With this announcement we are making these structures fully accessible to all users. Interested scientists can learn SQL, gain familiarity with the database design and test queries on a small, openly accessible version of the Millennium Run (with volume 1/512 that of the full simulation). They can then request accounts to run similar queries on the databases for the full simulations.},
  keywords = {astrophysics,data mining,project-astrophysics,research software engineering},
  annotation = {interest: 70},
  file = {/home/sam/Zotero/storage/FGQXIWDH/Lemson and Consortium - 2006 - Halo and Galaxy Formation Histories from the Mille.pdf;/home/sam/Zotero/storage/XXFMKAQZ/0608019.html}
}

@article{lenbergBehavioralSoftwareEngineering2015,
  title = {Behavioral Software Engineering: {{A}} Definition and Systematic Literature Review},
  shorttitle = {Behavioral Software Engineering},
  author = {Lenberg, Per and Feldt, Robert and Wallgren, Lars Göran},
  date = {2015-09-01},
  journaltitle = {Journal of Systems and Software},
  shortjournal = {Journal of Systems and Software},
  volume = {107},
  pages = {15--37},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2015.04.084},
  url = {https://www.sciencedirect.com/science/article/pii/S0164121215000989},
  urldate = {2022-08-26},
  abstract = {Throughout the history of software engineering, the human aspects have repeatedly been recognized as important. Even though research that investigates them has been growing in the past decade, these aspects should be more generally considered. The main objective of this study is to clarify the research area concerned with human aspects of software engineering and to create a common platform for future research. In order to meet the objective, we propose a definition of the research area behavioral software engineering (BSE) and present results from a systematic literature review based on the definition. The result indicates that there are knowledge gaps in the research area of behavioral software engineering and that earlier research has been focused on a few concepts, which have been applied to a limited number of software engineering areas. The individual studies have typically had a narrow perspective focusing on few concepts from a single unit of analysis. Further, the research has rarely been conducted in collaboration by researchers from both software engineering and social science. Altogether, this review can help put a broader set of human aspects higher on the agenda for future software engineering research and practice.},
  langid = {english},
  keywords = {internship-project,social science,software engineering},
  file = {/home/sam/Zotero/storage/B34LX3Q9/S0164121215000989.html}
}

@unpublished{lerNoobTestSpack2022,
  title = {A Noob Test: {{Spack}} "vs" {{EasyBuild}}},
  author = {Ler, Jan-Patrick},
  date = {2022-01-26},
  url = {https://easybuild.io/eum22/009_eum22_spack_vs_easybuild.pdf},
  eventtitle = {7th {{EasyBuild User Meeting}}},
  venue = {Virtual event},
  keywords = {hpc,package managers},
  file = {/home/sam/Zotero/storage/XLJH6PHC/009_eum22_spack_vs_easybuild.pdf}
}

@report{leveilleWildKobalosAppears,
  title = {A {{Wild Kobalos Appears}}: {{Tricksy Linux}} Malware Goes after {{HPCs}}},
  author = {Léveillé, Marc-Etienne M. and Sanmillan, Ignacio},
  institution = {ESET},
  url = {https://www.welivesecurity.com/wp-content/uploads/2021/01/ESET_Kobalos.pdf},
  abstract = {ESET Research has analyzed Kobalos, previously unknown and complex multiplatform malware targeting Linux, FreeBSD and Solaris systems. Given that the victims of this threat are mostly high-profile organizations, it seems almost certain this malware is deployed against chosen targets rather than opportunistically. When deployed, this malware gives access to the file system of the compromised host and enables access to a remote terminal, giving the attackers the ability to run arbitrary commands. The network capabilities of Kobalos make this malware quite distinctive. It supports acting both as a passive implant and as a bot actively connecting to its C\&C server. Interestingly, these C\&C servers are themselves compromised with Kobalos; the code for running such servers is present in all Kobalos samples. By performing an internet-wide scan, ESET Research was able to identify and notify victims of this threat. It is unclear how old this malware is, but the first known activity was confirmed by a victim who was compromised in late 2019. The group operating Kobalos remained active throughout 2020. The Linux threat landscape continues to evolve, and at times, malware authors invest a considerable amount of resources into their tradecraft. Kobalos is one of these cases.},
  keywords = {cybersecurity,hpc,internship-project,project-devsecops},
  file = {/home/sam/Zotero/storage/MU8CVWA5/ESET_Kobalos.pdf}
}

@article{levequeReproducibleResearchScientific2012,
  title = {Reproducible Research for Scientific Computing: {{Tools}} and Strategies for Changing the Culture},
  shorttitle = {Reproducible Research for Scientific Computing},
  author = {LeVeque, Randall J. and Mitchell, Ian M. and Stodden, Victoria},
  date = {2012-07},
  journaltitle = {Computing in Science \& Engineering},
  volume = {14},
  number = {4},
  pages = {13--17},
  issn = {1558-366X},
  doi = {10.1109/MCSE.2012.38},
  abstract = {This article considers the obstacles involved in creating reproducible computational research as well as some efforts and approaches to overcome them.},
  eventtitle = {Computing in {{Science}} \& {{Engineering}}},
  file = {/home/sam/Zotero/storage/EFXEJXVP/LeVeque et al. - 2012 - Reproducible research for scientific computing To.pdf;/home/sam/Zotero/storage/X2BBADQT/6171147.html}
}

@article{lewisAutocorrectErrorsExcel2021,
  title = {Autocorrect Errors in {{Excel}} Still Creating Genomics Headache},
  author = {Lewis, Dyani},
  date = {2021-08-13},
  journaltitle = {Nature},
  publisher = {Nature Publishing Group},
  doi = {10.1038/d41586-021-02211-4},
  url = {https://www.nature.com/articles/d41586-021-02211-4},
  urldate = {2022-08-25},
  abstract = {Despite geneticists being warned about spreadsheet problems, 30\% of published papers contain mangled gene names in supplementary data.},
  langid = {english},
  keywords = {genomics,metascience,research software engineering},
  annotation = {Bandiera\_abtest: a\\
Cg\_type: News\\
Subject\_term: Genomics, Software, Bioinformatics, Genetics},
  file = {/home/sam/Zotero/storage/N93LFGSZ/d41586-021-02211-4.html}
}

@article{lewisSystemUsabilityScale2018,
  title = {The {{System Usability Scale}}: {{Past}}, {{Present}}, and {{Future}}},
  shorttitle = {The {{System Usability Scale}}},
  author = {Lewis, James R.},
  date = {2018-07-03},
  journaltitle = {International Journal of Human–Computer Interaction},
  shortjournal = {International Journal of Human–Computer Interaction},
  volume = {34},
  number = {7},
  pages = {577--590},
  issn = {1044-7318, 1532-7590},
  doi = {10.1080/10447318.2018.1455307},
  url = {https://www.tandfonline.com/doi/full/10.1080/10447318.2018.1455307},
  urldate = {2022-06-01},
  abstract = {The System Usability Scale (SUS) is the most widely used standardized questionnaire for the assessment of perceived usability. This review of the SUS covers its early history from inception in the 1980s through recent research and its future prospects. From relatively inauspicious beginnings, when its originator described it as a “quick and dirty usability scale,” it has proven to be quick but not “dirty.” It is likely that the SUS will continue to be a popular measurement of perceived usability for the foreseeable future. When researchers and practitioners need a measure of perceived usability, they should strongly consider using the SUS.},
  langid = {english},
  keywords = {human computer interaction,internship-project},
  file = {/home/sam/Zotero/storage/6M8M58CI/The System Usability Scale Past Present and Future.pdf}
}

@online{lewsBugPredictionGoogle2011,
  title = {Bug {{Prediction}} at {{Google}}},
  author = {Lews, Chris and Ou, Rong},
  date = {2011-12-14},
  url = {https://google-engtools.blogspot.com/2011/12/bug-prediction-at-google.html},
  organization = {Google Engineering Tools},
  keywords = {debugging,industry practices,software engineering},
  file = {/home/sam/Zotero/storage/9H5WM9CZ/bug-prediction-at-google.html}
}

@article{liAIassistedSuperresolutionCosmological2021,
  title = {{{AI-assisted}} Superresolution Cosmological Simulations},
  author = {Li, Yin and Ni, Yueying and Croft, Rupert A. C. and Di Matteo, Tiziana and Bird, Simeon and Feng, Yu},
  date = {2021-05-11},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {118},
  number = {19},
  pages = {e2022038118},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2022038118},
  url = {https://www.pnas.org/doi/10.1073/pnas.2022038118},
  urldate = {2022-05-04},
  abstract = {Cosmological simulations of galaxy formation are limited by finite computational resources. We draw from the ongoing rapid advances in artificial intelligence (AI; specifically deep learning) to address this problem. Neural networks have been developed to learn from high-resolution (HR) image data and then make accurate superresolution (SR) versions of different low-resolution (LR) images. We apply such techniques to LR cosmological N-body simulations, generating SR versions. Specifically, we are able to enhance the simulation resolution by generating 512 times more particles and predicting their displacements from the initial positions. Therefore, our results can be viewed as simulation realizations themselves, rather than projections, e.g., to their density fields. Furthermore, the generation process is stochastic, enabling us to sample the small-scale modes conditioning on the large-scale environment. Our model learns from only 16 pairs of small-volume LR-HR simulations and is then able to generate SR simulations that successfully reproduce the HR matter power spectrum to percent level up to 16\,h\textasciicircum −1Mpc and the HR halo mass function to within 10\% down to 1011\,M⊙. We successfully deploy the model in a box 1,000 times larger than the training simulation box, showing that high-resolution mock surveys can be generated rapidly. We conclude that AI assistance has the potential to revolutionize modeling of small-scale galaxy-formation physics in large cosmological volumes.},
  keywords = {astrophysics,cosmolgoical simulation,machine learning,project-astrophysics},
  file = {/home/sam/Zotero/storage/D9AQ58BI/Li et al. - 2021 - AI-assisted superresolution cosmological simulatio.pdf}
}

@article{libertyRandomizedAlgorithmsLowrank2007,
  title = {Randomized Algorithms for the Low-Rank Approximation of Matrices},
  author = {Liberty, Edo and Woolfe, Franco and Martinsson, Per-Gunnar and Rokhlin, Vladimir and Tygert, Mark},
  date = {2007-12-18},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {104},
  number = {51},
  pages = {20167--20172},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.0709640104},
  url = {https://www.pnas.org/doi/full/10.1073/pnas.0709640104},
  urldate = {2024-01-25},
  abstract = {We describe two recently proposed randomized algorithms for the construction of low-rank approximations to matrices, and demonstrate their application (inter alia) to the evaluation of the singular value decompositions of numerically low-rank matrices. Being probabilistic, the schemes described here have a finite probability of failure; in most cases, this probability is rather negligible (10−17 is a typical value). In many situations, the new procedures are considerably more efficient and reliable than the classical (deterministic) ones; they also parallelize naturally. We present several numerical examples to illustrate the performance of the schemes.},
  keywords = {linear algebra,project-provenance-pp},
  file = {/home/sam/Zotero/storage/95J2R7CS/Liberty et al. - 2007 - Randomized algorithms for the low-rank approximati.pdf}
}

@inproceedings{liFaaSFlowEnableEfficient2022,
  title = {{{FaaSFlow}}: Enable Efficient Workflow Execution for Function-as-a-Service},
  shorttitle = {{{FaaSFlow}}},
  booktitle = {Proceedings of the 27th {{ACM International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}},
  author = {Li, Zijun and Liu, Yushi and Guo, Linsong and Chen, Quan and Cheng, Jiagan and Zheng, Wenli and Guo, Minyi},
  date = {2022-02-28},
  series = {{{ASPLOS}} '22},
  pages = {782--796},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3503222.3507717},
  url = {https://doi.org/10.1145/3503222.3507717},
  urldate = {2022-09-06},
  abstract = {Serverless computing (Function-as-a-Service) provides fine-grain resource sharing by running functions (or Lambdas) in containers. Data-dependent functions are required to be invoked following a pre-defined logic, which is known as serverless workflows. However, our investigation shows that the traditional master-worker based workflow execution architecture performs poorly in serverless context. One significant overhead results from the master-side workflow schedule pattern, with which the functions are triggered in the master node and assigned to worker nodes for execution. Besides, the data movement between workers also reduces the throughput. To this end, we present a worker-side workflow schedule pattern for serverless workflow execution. Following the design, we implement FaaSFlow to enable efficient workflow execution in the serverless context. Besides, we propose an adaptive storage library FaaStore that enables fast data transfer between functions on the same node without through the database. Experiment results show that FaaSFlow effectively mitigates the workflow scheduling overhead by 74.6\% on average and data transmission overhead by 95\% at most. When the network bandwidth fluctuates, FaaSFlow-FaaStore reduces the throughput degradation by 23.0\%, and is able to multiply the utilization of network bandwidth by 1.5X-4X.},
  isbn = {978-1-4503-9205-1},
  keywords = {scheduling,workflow managers},
  annotation = {interest: 95}
}

@online{LifetimesCryptographicHash,
  title = {Lifetimes of Cryptographic Hash Functions},
  url = {https://valerieaurora.org/hash.html},
  urldate = {2023-09-07}
}

@online{linuxdevelopersUser_namespacesLinuxManual2021,
  title = {User\_namespaces(7) - {{Linux}} Manual Page},
  author = {Linux Developers},
  date = {2021-08-27},
  url = {https://www.man7.org/linux/man-pages/man7/user_namespaces.7.html},
  urldate = {2023-02-18},
  keywords = {containers,operating systems,project-acm-rep},
  file = {/home/sam/Zotero/storage/NJTIHU8G/user_namespaces.7.html}
}

@article{liThreatDetectionInvestigation2021,
  title = {Threat Detection and Investigation with System-Level Provenance Graphs: {{A}} Survey},
  shorttitle = {Threat Detection and Investigation with System-Level Provenance Graphs},
  author = {Li, Zhenyuan and Chen, Qi Alfred and Yang, Runqing and Chen, Yan and Ruan, Wei},
  date = {2021-07-01},
  journaltitle = {Computers \& Security},
  shortjournal = {Computers \& Security},
  volume = {106},
  pages = {102282},
  issn = {0167-4048},
  doi = {10.1016/j.cose.2021.102282},
  url = {https://www.sciencedirect.com/science/article/pii/S0167404821001061},
  urldate = {2023-08-23},
  abstract = {With the development of information technology, the border of the cyberspace gets much broader and thus also exposes increasingly more vulnerabilities to attackers. Traditional mitigation-based defence strategies are challenging to cope with the current complicated situation. Security practitioners urgently need better tools to describe and modelling attacks for defense. The provenance graph seems like an ideal method for threat modelling with powerful semantic expression ability and attacks historic correlation ability. In this paper, we firstly introduce the basic concepts about system-level provenance graph and present a typical system architecture for provenance graph-based threat detection and investigation. A comprehensive provenance graph-based threat detection system can be divided into three modules: data collection module, data management module, and threat detection modules. Each module contains several components and involves different research problems. We systematically taxonomize and compare the existing algorithms and designs involved in them. Based on these comparisons, we identify the strategy of technology selection for real-world deployment. We also provide insights and challenges about the existing work to guide future research in this area.},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/EXE45PJ5/Li et al. - 2021 - Threat detection and investigation with system-lev.pdf;/home/sam/Zotero/storage/4NB2FLWF/S0167404821001061.html}
}

@article{liuSurveyDataIntensiveScientific2015,
  title = {A {{Survey}} of {{Data-Intensive Scientific Workflow Management}}},
  author = {Liu, Ji and Pacitti, Esther and Valduriez, Patrick and Mattoso, Marta},
  date = {2015-12-01},
  journaltitle = {Journal of Grid Computing},
  shortjournal = {J. Grid Comput.},
  volume = {13},
  number = {4},
  pages = {457--493},
  issn = {1570-7873},
  doi = {10.1007/s10723-015-9329-8},
  url = {https://doi.org/10.1007/s10723-015-9329-8},
  urldate = {2022-09-06},
  abstract = {Nowadays, more and more computer-based scientific experiments need to handle massive amounts of data. Their data processing consists of multiple computational steps and dependencies within them. A data-intensive scientific workflow is useful for modeling such process. Since the sequential execution of data-intensive scientific workflows may take much time, Scientific Workflow Management Systems (SWfMSs) should enable the parallel execution of data-intensive scientific workflows and exploit the resources distributed in different infrastructures such as grid and cloud. This paper provides a survey of data-intensive scientific workflow management in SWfMSs and their parallelization techniques. Based on a SWfMS functional architecture, we give a comparative analysis of the existing solutions. Finally, we identify research issues for improving the execution of data-intensive scientific workflows in a multisite cloud.},
  keywords = {workflow managers},
  annotation = {interest: 94},
  file = {/home/sam/Zotero/storage/FXK8SYKW/Liu et al. - 2015 - A Survey of Data-Intensive Scientific Workflow Man.pdf;/home/sam/Zotero/storage/J4WDXAMH/Liu et al. - 2015 - A Survey of Data-Intensive Scientific Workflow Management.pdf}
}

@online{lopezVetoBattle30,
  title = {Veto {{Battle}} 30 {{Years Ago Set Freedom}} of {{Information Norms}}},
  author = {Lopez, Dan and Blanton, Thomas and Fuchs, Meredith and Elias, Barbara},
  url = {https://nsarchive2.gwu.edu/NSAEBB/NSAEBB142/},
  urldate = {2022-09-06},
  keywords = {current events,US history},
  annotation = {interest: 91},
  file = {/home/sam/Zotero/storage/5FD78INQ/NSAEBB142.html}
}

@online{lorenaBarbagroupReproducibilitySyllabus2016,
  title = {Barba-Group Reproducibility Syllabus},
  author = {Lorena, Barbara A.},
  date = {2016-10-31},
  url = {https://hackernoon.com/barba-group-reproducibility-syllabus-e3757ee635cf},
  urldate = {2023-01-24},
  abstract = {After my short piece, {$<$}a href="http://science.sciencemag.org/content/354/6308/142" target="\_blank"{$>$}“A hard road to reproducibility,”{$<$}/a{$>$} appeared in {$<$}em{$>$}Science{$<$}/em{$>$}, I received several emails and Twitter mentions asking for more specific tips{$\mkern1mu$}—{$\mkern1mu$}both about tools and documents we use in the group to train the team about reproducibility.},
  langid = {english},
  organization = {HackerNoon},
  file = {/home/sam/Zotero/storage/GVA653JR/barba-group-reproducibility-syllabus-e3757ee635cf.html}
}

@article{lukPinBuildingCustomized2005,
  title = {Pin: Building Customized Program Analysis Tools with Dynamic Instrumentation},
  shorttitle = {Pin},
  author = {Luk, Chi-Keung and Cohn, Robert and Muth, Robert and Patil, Harish and Klauser, Artur and Lowney, Geoff and Wallace, Steven and Reddi, Vijay Janapa and Hazelwood, Kim},
  date = {2005-06-12},
  journaltitle = {ACM SIGPLAN Notices},
  shortjournal = {SIGPLAN Not.},
  volume = {40},
  number = {6},
  pages = {190--200},
  issn = {0362-1340},
  doi = {10.1145/1064978.1065034},
  url = {https://dl.acm.org/doi/10.1145/1064978.1065034},
  urldate = {2023-08-24},
  abstract = {Robust and powerful software instrumentation tools are essential for program analysis tasks such as profiling, performance evaluation, and bug detection. To meet this need, we have developed a new instrumentation system called Pin. Our goals are to provide easy-to-use, portable, transparent, and efficient instrumentation. Instrumentation tools (called Pintools) are written in C/C++ using Pin's rich API. Pin follows the model of ATOM, allowing the tool writer to analyze an application at the instruction level without the need for detailed knowledge of the underlying instruction set. The API is designed to be architecture independent whenever possible, making Pintools source compatible across different architectures. However, a Pintool can access architecture-specific details when necessary. Instrumentation with Pin is mostly transparent as the application and Pintool observe the application's original, uninstrumented behavior. Pin uses dynamic compilation to instrument executables while they are running. For efficiency, Pin uses several techniques, including inlining, register re-allocation, liveness analysis, and instruction scheduling to optimize instrumentation. This fully automated approach delivers significantly better instrumentation performance than similar tools. For example, Pin is 3.3x faster than Valgrind and 2x faster than DynamoRIO for basic-block counting. To illustrate Pin's versatility, we describe two Pintools in daily use to analyze production software. Pin is publicly available for Linux platforms on four architectures: IA32 (32-bit x86), EM64T (64-bit x86), Itanium®, and ARM. In the ten months since Pin 2 was released in July 2004, there have been over 3000 downloads from its website.},
  keywords = {computer architecture,project-provenance-pp},
  file = {/home/sam/Zotero/storage/LQ986PRH/Luk et al. - 2005 - Pin building customized program analysis tools wi.pdf}
}

@online{luzHowScientistsCaptured,
  title = {How {{Scientists Captured}} the {{First Image}} of a {{Black Hole}}},
  author = {Luz, Ota},
  url = {https://www.jpl.nasa.gov/edu/news/2019/4/19/how-scientists-captured-the-first-image-of-a-black-hole/},
  urldate = {2022-09-06},
  abstract = {Find out how scientists created a virtual telescope as large as Earth itself to capture the first image of a black hole's silhouette.},
  organization = {NASA/JPL Edu},
  keywords = {research software engineering},
  annotation = {interest: 75}
}

@inproceedings{maAccurateLowCost2015,
  title = {Accurate, {{Low Cost}} and {{Instrumentation-Free Security Audit Logging}} for {{Windows}}},
  booktitle = {Proceedings of the 31st {{Annual Computer Security Applications Conference}}},
  author = {Ma, Shiqing and Lee, Kyu Hyung and Kim, Chung Hwan and Rhee, Junghwan and Zhang, Xiangyu and Xu, Dongyan},
  date = {2015-12-07},
  series = {{{ACSAC}} '15},
  pages = {401--410},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2818000.2818039},
  url = {https://dl.acm.org/doi/10.1145/2818000.2818039},
  urldate = {2023-08-23},
  abstract = {Audit logging is an important approach to cyber attack investigation. However, traditional audit logging either lacks accuracy or requires expensive and complex binary instrumentation. In this paper, we propose a Windows based audit logging technique that features accuracy and low cost. More importantly, it does not require instrumenting the applications, which is critical for commercial software with IP protection. The technique is build on Event Tracing for Windows (ETW). By analyzing ETW log and critical parts of application executables, a model can be constructed to parse ETW log to units representing independent sub-executions in a process. Causality inferred at the unit level renders much higher accuracy, allowing us to perform accurate attack investigation and highly effective log reduction.},
  isbn = {978-1-4503-3682-6},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/8Q2P53GV/Ma et al. - 2015 - Accurate, Low Cost and Instrumentation-Free Securi.pdf}
}

@inproceedings{mackoCollectingProvenanceXen2011,
  title = {Collecting {{Provenance}} via the {{Xen Hypervisor}}},
  booktitle = {Proceedings of the {{Theory}} and {{Practice}} of {{Provenance}} ({{TaPP}})},
  author = {Macko, Peter and Chiarini, Marc and Seltzer, Margo},
  date = {2011},
  publisher = {USENIX},
  url = {https://www.usenix.org/legacy/events/tapp11/tech/final_files/MackoChiariniSeltzer.pdf},
  abstract = {The Provenance Aware Storage Systems project (PASS) currently collects system-level provenance by intercept- ing system calls in the Linux kernel and storing the provenance in a stackable filesystem. While this ap- proach is reasonably efficient, it suffers from two sig- nificant drawbacks: each new revision of the kernel re- quires reintegration of PASS changes, the stability of which must be continually tested; also, the use of a stack- able filesystem makes it difficult to collect provenance on root volumes, especially during early boot. In this pa- per we describe an approach to collecting system-level provenance from virtual guest machines running under the Xen hypervisor. We make the case that our approach alleviates the aforementioned difficulties and promotes adoption of provenance collection within cloud comput- ing platforms.},
  eventtitle = {Theory and {{Practice}} of {{Provenance}} ({{TaPP}})},
  keywords = {project-provenance-pp,provenance-tool}
}

@inproceedings{macqueenMethodsClassificationAnalysis1965,
  title = {Some Methods for Classification and Analysis of Multivariate Observatiosn},
  author = {Macqueen, J},
  date = {1965-06-21},
  volume = {1},
  pages = {281},
  publisher = {University of California Press},
  location = {Los Angeles, CA},
  url = {https://www.cs.cmu.edu/~bhiksha/courses/mlsp.fall2010/class14/macqueen.pdf},
  eventtitle = {Proceedings of the {{Fifth Berkeley Symposium}} on {{Mathematical Statistics}} and {{Probability}}},
  langid = {english},
  keywords = {machine learning,project-provenance-pp},
  file = {/home/sam/Zotero/storage/VT8M33UC/Macqueen - SOME METHODS FOR CLASSIFICATION AND ANALYSIS OF MU.pdf}
}

@online{madabhushanaConfigureLinuxSystem2021,
  title = {Configure {{Linux}} System Auditing with Auditd},
  author = {Madabhushana, Ashish Bharadwaj},
  date = {2021-10-26T09:00:01+0000},
  publisher = {Red Hat, Inc.},
  url = {https://www.redhat.com/sysadmin/configure-linux-auditing-auditd},
  urldate = {2024-01-21},
  abstract = {Learn how to install, configure, and manage the audit daemon to track security-related information on your Linux systems.},
  langid = {english},
  keywords = {project-provenance-pp},
  file = {/home/sam/Zotero/storage/VX3ZS8EH/configure-linux-auditing-auditd.html}
}

@inproceedings{mahmoudMinotaurAdaptingSoftware2019,
  title = {Minotaur: {{Adapting Software Testing Techniques}} for {{Hardware Errors}}},
  shorttitle = {Minotaur},
  booktitle = {Proceedings of the {{Twenty-Fourth International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}},
  author = {Mahmoud, Abdulrahman and Venkatagiri, Radha and Ahmed, Khalique and Misailovic, Sasa and Marinov, Darko and Fletcher, Christopher W. and Adve, Sarita V.},
  date = {2019-04-04},
  series = {{{ASPLOS}} '19},
  pages = {1087--1103},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3297858.3304050},
  url = {https://doi.org/10.1145/3297858.3304050},
  urldate = {2022-04-10},
  abstract = {With the end of conventional CMOS scaling, efficient resiliency solutions are needed to address the increased likelihood of hardware errors. Silent data corruptions (SDCs) are especially harmful because they can create unacceptable output without the user's knowledge. Several resiliency analysis techniques have been proposed to identify SDC-causing instructions, but they remain too slow for practical use and/or sacrifice accuracy to improve analysis speed. We develop Minotaur, a novel toolkit to improve the speed and accuracy of resiliency analysis. The key insight behind Minotaur is that modern resiliency analysis has many conceptual similarities to software testing; therefore, adapting techniques from the rich software testing literature can lead to principled and significant improvements in resiliency analysis. Minotaur identifies and adapts four concepts from software testing: 1) it introduces the concept of input quality criteria for resiliency analysis and identifies PC coverage as a simple but effective criterion; 2) it creates (fast) minimized inputs from (slow) standard benchmark inputs, using the input quality criteria to assess the goodness of the created input; 3) it adapts the concept of test case prioritization to prioritize error injections and invoke early termination for a given instruction to speed up error-injection campaigns; and 4) it further adapts test case or input prioritization to accelerate SDC discovery across multiple inputs. We evaluate Minotaur by applying it to Approxilyzer, a state-of-the-art resiliency analysis tool. Minotaur's first three techniques speed up Approxilyzer's resiliency analysis by 10.3X (on average) for the workloads studied. Moreover, they identify 96\% (on average) of all SDC-causing instructions explored, compared to 64\% identified by Approxilyzer alone. Minotaur's fourth technique (input prioritization) enables identifying all SDC-causing instructions explored across multiple inputs at a speed 2.3X faster (on average) than analyzing each input independently for our workloads.},
  isbn = {978-1-4503-6240-5},
  keywords = {computer architecture,hardware reliability,software engineering,software testing},
  file = {/home/sam/Zotero/storage/XGCDBCNI/Mahmoud et al. - 2019 - Minotaur Adapting Software Testing Techniques for.pdf}
}

@article{mahnicUsingPlanningPoker2012,
  title = {On Using Planning Poker for Estimating User Stories},
  author = {Mahnič, Viljan and Hovelja, Tomaž},
  date = {2012-09},
  journaltitle = {Journal of Systems and Software},
  shortjournal = {Journal of Systems and Software},
  volume = {85},
  number = {9},
  pages = {2086--2095},
  issn = {01641212},
  doi = {10.1016/j.jss.2012.04.005},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0164121212001021},
  urldate = {2022-06-13},
  langid = {english},
  keywords = {internship-project,software engineering process},
  annotation = {interest: 30}
}

@inproceedings{maMPIMultiplePerspective2017,
  title = {\{\vphantom\}{{MPI}}\vphantom\{\}: {{Multiple Perspective Attack Investigation}} with {{Semantic Aware Execution Partitioning}}},
  shorttitle = {\{\vphantom\}{{MPI}}\vphantom\{\}},
  author = {Ma, Shiqing and Zhai, Juan and Wang, Fei and Lee, Kyu Hyung and Zhang, Xiangyu and Xu, Dongyan},
  date = {2017},
  pages = {1111--1128},
  url = {https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/ma},
  urldate = {2023-08-23},
  eventtitle = {26th {{USENIX Security Symposium}} ({{USENIX Security}} 17)},
  isbn = {978-1-931971-40-9},
  langid = {english},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/UXCKL2W3/Ma et al. - 2017 - MPI Multiple Perspective Attack Investigation w.pdf}
}

@inproceedings{mancoMyVMLighter2017,
  title = {My {{VM}} Is {{Lighter}} (and {{Safer}}) than Your {{Container}}},
  booktitle = {Proceedings of the 26th {{Symposium}} on {{Operating Systems Principles}}},
  author = {Manco, Filipe and Lupu, Costin and Schmidt, Florian and Mendes, Jose and Kuenzer, Simon and Sati, Sumit and Yasukata, Kenichi and Raiciu, Costin and Huici, Felipe},
  date = {2017-10-14},
  series = {{{SOSP}} '17},
  pages = {218--233},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3132747.3132763},
  url = {https://doi.org/10.1145/3132747.3132763},
  urldate = {2022-09-12},
  abstract = {Containers are in great demand because they are lightweight when compared to virtual machines. On the downside, containers offer weaker isolation than VMs, to the point where people run containers in virtual machines to achieve proper isolation. In this paper, we examine whether there is indeed a strict tradeoff between isolation (VMs) and efficiency (containers). We find that VMs can be as nimble as containers, as long as they are small and the toolstack is fast enough. We achieve lightweight VMs by using unikernels for specialized applications and with Tinyx, a tool that enables creating tailor-made, trimmed-down Linux virtual machines. By themselves, lightweight virtual machines are not enough to ensure good performance since the virtualization control plane (the toolstack) becomes the performance bottleneck. We present LightVM, a new virtualization solution based on Xen that is optimized to offer fast boot-times regardless of the number of active VMs. LightVM features a complete redesign of Xen's control plane, transforming its centralized operation to a distributed one where interactions with the hypervisor are reduced to a minimum. LightVM can boot a VM in 2.3ms, comparable to fork/exec on Linux (1ms), and two orders of magnitude faster than Docker. LightVM can pack thousands of LightVM guests on modest hardware with memory and CPU usage comparable to that of processes.},
  isbn = {978-1-4503-5085-3},
  keywords = {industry practices},
  annotation = {interest: 90},
  file = {/home/sam/Zotero/storage/KAZY8KQQ/Manco et al. - 2017 - My VM is Lighter (and Safer) than your Container.pdf}
}

@online{manianOurPointlessPursuit0000,
  title = {Our {{Pointless Pursuit Of Semantic Value}}},
  author = {Manian, Divya},
  year = {12:09:02 +0000 UTC},
  url = {https://www.smashingmagazine.com/2011/11/our-pointless-pursuit-of-semantic-value/},
  urldate = {2024-02-28},
  langid = {english},
  organization = {Smashing Magazine}
}

@inproceedings{maProTracerPracticalProvenance2016,
  title = {{{ProTracer}}: {{Towards Practical Provenance Tracing}} by {{Alternating Between Logging}} and {{Tainting}}},
  shorttitle = {{{ProTracer}}},
  booktitle = {Proceedings 2016 {{Network}} and {{Distributed System Security Symposium}}},
  author = {Ma, Shiqing and Zhang, Xiangyu and Xu, Dongyan},
  date = {2016},
  publisher = {Internet Society},
  location = {San Diego, CA},
  doi = {10.14722/ndss.2016.23350},
  url = {https://www.ndss-symposium.org/wp-content/uploads/2017/09/protracer-towards-practical-provenance-tracing-alternating-logging-tainting.pdf},
  urldate = {2023-08-23},
  abstract = {Provenance tracing is a very important approach to Advanced Persistent Threat (APT) attack detection and investigation. Existing techniques either suffer from the dependence explosion problem or have non-trivial space and runtime overhead, which hinder their application in practice. We propose ProTracer, a lightweight provenance tracing system that alternates between system event logging and unit level taint propagation. The technique is built on an on-the-fly system event processing infrastructure that features a very lightweight kernel module and a sophisticated user space daemon that performs concurrent and out-of-order event processing. The evaluation with different realistic system workloads and a number of attack cases show that ProTracer only produces 13MB log data per day, and 0.84GB(Server)/2.32GB(Client) in 3 months without losing any important information. The space consumption is only {$<$} 1.28\% of the state-of-the-art, 7 times smaller than an off-line garbage collection technique. The run-time overhead averages {$<$}7\% for servers and {$<$}5\% for regular applications. The generated attack causal graphs are a few times smaller than those by existing techniques while they are equally informative.},
  eventtitle = {Network and {{Distributed System Security Symposium}}},
  isbn = {978-1-891562-41-9},
  langid = {english},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/JW5VPETI/Ma et al. - 2016 - ProTracer Towards Practical Provenance Tracing by.pdf}
}

@article{marangunicTechnologyAcceptanceModel2015,
  title = {Technology Acceptance Model: A Literature Review from 1986 to 2013},
  shorttitle = {Technology Acceptance Model},
  author = {Marangunić, Nikola and Granić, Andrina},
  date = {2015-03},
  journaltitle = {Universal Access in the Information Society},
  shortjournal = {Univ Access Inf Soc},
  volume = {14},
  number = {1},
  pages = {81--95},
  issn = {1615-5289, 1615-5297},
  doi = {10.1007/s10209-014-0348-1},
  url = {http://link.springer.com/10.1007/s10209-014-0348-1},
  urldate = {2022-06-01},
  abstract = {With the ever-increasing development of technology and its integration into users’ private and professional life, a decision regarding its acceptance or rejection still remains an open question. A respectable amount of work dealing with the technology acceptance model (TAM), from its first appearance more than a quarter of a century ago, clearly indicates a popularity of the model in the field of technology acceptance. Originated in the psychological theory of reasoned action and theory of planned behavior, TAM has evolved to become a key model in understanding predictors of human behavior toward potential acceptance or rejection of the technology. The main aim of the paper is to provide an up-to-date, well-researched resource of past and current references to TAM-related literature and to identify possible directions for future TAM research. The paper presents a comprehensive concept-centric literature review of the TAM, from 1986 onwards. According to a designed methodology, 85 scientific publications have been selected and classified according to their aim and content into three categories such as (i) TAM literature reviews, (ii) development and extension of TAM, and (iii) modification and application of TAM. Despite a continuous progress in revealing new factors with significant influence on TAM’s core variables, there are still many unexplored areas of model potential application that could contribute to its predictive validity. Consequently, four possible future directions for TAM research based on the conducted literature review and analysis are identified and presented.},
  langid = {english},
  keywords = {internship-project,technology-acceptance},
  file = {/home/sam/Zotero/storage/4VXICM58/Screenshot from 2022-06-01 15-22-47.png;/home/sam/Zotero/storage/4XMQHRK5/Screenshot from 2022-06-01 15-19-23.png;/home/sam/Zotero/storage/I6APMTUF/Screenshot from 2022-06-01 15-20-05.png;/home/sam/Zotero/storage/VW5TQCW5/Marangunić-Granić2015_Article_TechnologyAcceptanceModelALite.pdf}
}

@online{markrussSysmonSysinternals2023,
  title = {Sysmon - {{Sysinternals}}},
  author = {{markruss}},
  date = {2023-04-11},
  url = {https://learn.microsoft.com/en-us/sysinternals/downloads/sysmon},
  urldate = {2023-08-23},
  abstract = {Monitors and reports key system activity via the Windows event log.},
  langid = {american},
  keywords = {operating systems,project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/VNS2CHDN/sysmon.html}
}

@article{marowkaPythonAcceleratorsHighperformance2018,
  title = {Python Accelerators for High-Performance Computing},
  author = {Marowka, Ami},
  date = {2018-04-01},
  journaltitle = {The Journal of Supercomputing},
  shortjournal = {J Supercomput},
  volume = {74},
  number = {4},
  pages = {1449--1460},
  issn = {1573-0484},
  doi = {10.1007/s11227-017-2213-5},
  url = {https://doi.org/10.1007/s11227-017-2213-5},
  urldate = {2022-10-18},
  abstract = {Python became the preferred language for teaching in academia, and it is one of the most popular programming languages for scientific computing. This wide popularity occurs despite the weak performance of the language. This weakness is the motivation that drives the efforts devoted by the Python community to improve the performance of the language. In this article, we are following these efforts while we focus on one specific promised solution that aims to provide high-performance and performance portability for Python applications.},
  langid = {english},
  keywords = {performance engineering},
  annotation = {interest: 80},
  file = {/home/sam/Zotero/storage/KAHFA9SV/Marowka - 2018 - Python accelerators for high-performance computing.pdf}
}

@online{matsakisReferencecountingLeaks2015,
  title = {On Reference-Counting and Leaks},
  author = {Matsakis, Niko},
  date = {2015-04-29},
  url = {https://smallcultfollowing.com/babysteps/blog/2015/04/29/on-reference-counting-and-leaks/},
  urldate = {2023-12-17}
}

@inproceedings{matteisHostingQueryableHighly2014,
  title = {Hosting Queryable and Highly Available Linked Data for Free},
  booktitle = {Proceedings of the 2014 {{International Conference}} on {{Developers}} - {{Volume}} 1268},
  author = {Matteis, Luca and Verborgh, Ruben},
  date = {2014-10-19},
  series = {{{ISWC-DEV}}'14},
  pages = {13--18},
  publisher = {CEUR-WS.org},
  location = {Aachen, DEU},
  abstract = {SPARQL endpoints suffer from low availability, and require to buy and configure complex servers to host them. With the advent of Linked Data Fragments, and more specifically Triple Pattern Fragments (TPFs), we can now perform complex queries on low-cost servers. Online file repositories and cloud hosting services, such as GitHub, Google Code, Google App Engine or Dropbox can be exploited to host this type of linked data for free. For this purpose we have developed two different proof-of-concept tools that can be used to publish TPFs on GitHub and Google App Engine. A generic TPF client can then be used to perform SPARQL queries on the freely hosted TPF servers.},
  keywords = {semantic web}
}

@article{matthewsFiveRetractedStructure2007,
  title = {Five Retracted Structure Reports: {{Inverted}} or Incorrect?},
  shorttitle = {Five Retracted Structure Reports},
  author = {Matthews, Brian W.},
  date = {2007},
  journaltitle = {Protein Science},
  volume = {16},
  number = {6},
  pages = {1013--1016},
  issn = {1469-896X},
  doi = {10.1110/ps.072888607},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1110/ps.072888607},
  urldate = {2023-02-23},
  langid = {english},
  keywords = {retraction},
  file = {/home/sam/Zotero/storage/E6B747MM/Matthews - 2007 - Five retracted structure reports Inverted or inco.pdf;/home/sam/Zotero/storage/IW6FEBWL/ps.html}
}

@article{matthewsFrameworkSoftwarePreservation2010,
  title = {A {{Framework}} for {{Software Preservation}}},
  author = {Matthews, Brian and Shaon, Arif and Bicarregui, Juan and Jones, Catherine},
  date = {2010-07-21},
  journaltitle = {International Journal of Digital Curation},
  volume = {5},
  number = {1},
  pages = {91--105},
  issn = {1746-8256},
  doi = {10.2218/ijdc.v5i1.145},
  url = {http://ijdc.net/index.php/ijdc/article/view/148},
  urldate = {2023-02-03},
  abstract = {Software preservation has not had detailed consideration as a research topic or in practical application. In this paper, we present a conceptual framework to capture and organise the main notions of software preservation, which are required for a coherent and comprehensive approach.~ This framework has three main aspects. Firstly a discussion of what it means to preserve software via a performance model which considers how a software artefact can be rebuilt from preserved components and can then be seen to be representative of the original software product. Secondly the development of a model of software artefacts, describing the basic components of all software, loosely based on the FRBR model for representing digital artefacts and their history within a library context. Finally, the definition and categorisation of the properties of software artefacts which are required to ensure that the software product has been adequately preserved. These are broken down into a number of categories and related to the concepts defined in the OAIS standard. We also discuss our experience of recording these preservation properties for a number of BADC software products, which arose from a series of case studies conducted to evaluate the software preservation framework, and also briefly describe the SPEQS toolkit, a tool to capture software preservation properties within a software development.},
  issue = {1},
  langid = {english},
  keywords = {reproducibility engineering},
  file = {/home/sam/Zotero/storage/VIJ7V2MD/Matthews et al. - 2010 - A Framework for Software Preservation.pdf}
}

@article{mcdowellShapingFutureResearch2015,
  title = {Shaping the {{Future}} of {{Research}}: A Perspective from Junior Scientists},
  shorttitle = {Shaping the {{Future}} of {{Research}}},
  author = {McDowell, Gary S. and Gunsalus, Kearney T. W. and MacKellar, Drew C. and Mazzilli, Sarah A. and Pai, Vaibhav P. and Goodwin, Patricia R. and Walsh, Erica M. and Robinson-Mosher, Avi and Bowman, Thomas A. and Kraemer, James and Erb, Marcella L. and Schoenfeld, Eldi and Shokri, Leila and Jackson, Jonathan D. and Islam, Ayesha and Mattozzi, Matthew D. and Krukenberg, Kristin A. and Polka, Jessica K.},
  date = {2015-01-09},
  number = {3:291},
  publisher = {F1000Research},
  doi = {10.12688/f1000research.5878.2},
  url = {https://f1000research.com/articles/3-291},
  urldate = {2022-08-30},
  abstract = {The landscape of scientific research and funding is in flux as a result of tight budgets, evolving models of both publishing and evaluation, and questions about training and workforce stability. As future leaders, junior scientists are uniquely poised to shape the culture and practice of science in response to these challenges. A group of postdocs in the Boston area who are invested in improving the scientific endeavor, planned a symposium held on October 2 nd and 3 rd , 2014, as a way to join the discussion about the future of US biomedical research. Here we present a report of the proceedings of participant-driven workshops and the organizers’ synthesis of the outcomes.},
  langid = {english},
  keywords = {academic careers,metascience},
  file = {/home/sam/Zotero/storage/QNH2MQ6D/McDowell et al. - 2015 - Shaping the Future of Research a perspective from.pdf}
}

@article{mcgradyDialingVideosRandom2023,
  title = {Dialing for {{Videos}}: {{A Random Sample}} of {{YouTube}}},
  shorttitle = {Dialing for {{Videos}}},
  author = {McGrady, Ryan and Zheng, Kevin and Curran, Rebecca and Baumgartner, Jason and Zuckerman, Ethan},
  date = {2023-12-20},
  journaltitle = {Journal of Quantitative Description: Digital Media},
  volume = {3},
  issn = {2673-8813},
  doi = {10.51685/jqd.2023.022},
  url = {https://journalqd.org/article/view/4066},
  urldate = {2023-12-25},
  abstract = {YouTube is one of the largest, most important communication platforms in the world, but while there is a great deal of research about the site, many of its fundamental characteristics remain unknown. To better understand YouTube as a whole, we created a random sample of videos using a new method. Through a description of the sample’s metadata, we provide answers to many essential questions about, for example, the distribution of views, comments, likes, subscribers, and categories. Our method also allows us to estimate the total number of publicly visible videos on YouTube and its growth over time. To learn more about video content, we hand-coded a subsample to answer questions like how many are primarily music, video games, or still images. Finally, we processed the videos’ audio using language detection software to determine the distribution of spoken languages. In providing basic information about YouTube as a whole, we not only learn more about an influential platform, but also provide baseline context against which samples in more focused studies can be compared.},
  langid = {english},
  keywords = {digital infrastructure,methods,random sampling,social media,YouTube},
  file = {/home/sam/Zotero/storage/A5VWWTP9/McGrady et al. - 2023 - Dialing for Videos A Random Sample of YouTube.pdf}
}

@online{mckinneyApacheArrow102017,
  title = {Apache {{Arrow}} and the “10 {{Things I Hate About}} Pandas”},
  author = {McKinney, Wes},
  date = {2017-09-21},
  url = {https://wesmckinney.com/blog/apache-arrow-pandas-internals/},
  urldate = {2024-01-15},
  langid = {english},
  organization = {Wes McKinney},
  file = {/home/sam/Zotero/storage/W36HAB9M/apache-arrow-pandas-internals.html}
}

@unpublished{mckinneyPracticalMediumData2013,
  title = {Practical {{Medium Data Analytics}} with {{Python}} (10 {{Things I Hate About}} Pandas, {{PyData NYC}} 2013)},
  author = {McKinney, Wes},
  date = {2013-11-09},
  url = {https://www.slideshare.net/wesm/practical-medium-data-analytics-with-python},
  urldate = {2024-01-15},
  eventtitle = {{{PyData}}},
  langid = {english},
  venue = {New York, New York},
  file = {/home/sam/Zotero/storage/BR92LU3P/practical-medium-data-analytics-with-python.html}
}

@article{mcphillipsScientificWorkflowDesign2009,
  title = {Scientific Workflow Design for Mere Mortals},
  author = {McPhillips, Timothy and Bowers, Shawn and Zinn, Daniel and Ludäscher, Bertram},
  date = {2009-05-01},
  journaltitle = {Future Generation Computer Systems},
  shortjournal = {Future Generation Computer Systems},
  volume = {25},
  number = {5},
  pages = {541--551},
  issn = {0167-739X},
  doi = {10.1016/j.future.2008.06.013},
  url = {https://www.sciencedirect.com/science/article/pii/S0167739X08000873},
  urldate = {2022-09-06},
  abstract = {Recent years have seen a dramatic increase in research and development of scientific workflow systems. These systems promise to make scientists more productive by automating data-driven and compute-intensive analyses. Despite many early achievements, the long-term success of scientific workflow technology critically depends on making these systems useable by “mere mortals”, i.e.,~scientists who have a very good idea of the analysis methods they wish to assemble, but who are neither software developers nor scripting-language experts. With these users in mind, we identify a set of desiderata for scientific workflow systems crucial for enabling scientists to model and design the workflows they wish to automate themselves. As a first step towards meeting these requirements, we also show how the collection-oriented modeling and design (comad) approach for scientific workflows, implemented within the Kepler system, can help provide these critical, design-oriented capabilities to scientists.},
  langid = {english},
  keywords = {workflow managers},
  annotation = {interest: 97},
  file = {/home/sam/Zotero/storage/KN2G9FQL/S0167739X08000873.html}
}

@inproceedings{mcvoyLmbenchPortableTools1996,
  title = {Lmbench: {{Portable Tools}} for {{Performance Analysis}}},
  booktitle = {Proceedings of the {{USENIX}} 1996 {{Annual Technical Conference}}},
  author = {McVoy, Larry and Staelin, Carl},
  date = {1996-01},
  publisher = {USENIX},
  location = {San Diego, CA},
  url = {https://www.usenix.org/legacy/publications/library/proceedings/sd96/full_papers/mcvoy.pdf},
  abstract = {lmbench is a micro-benchmark suite designed to focus attention on the basic building blocks of many common system applications, such as databases, simulations, software development, and networking. In almost all cases, the individual tests are the result of analysis and isolation of a customer’s actual performance problem. These tools can be, and currently are, used to compare different system implementations from different vendors. In several cases, the benchmarks have uncovered previously unknown bugs and design flaws. The results have shown a strong correlation between memory system performance and overall performance. lmbench includes an extensible database of results from systems current as of late  1995.},
  eventtitle = {{{USENIX}} 1996 {{ATC}} ({{Annual Technical Conference}})},
  langid = {english},
  keywords = {project-provenance-pp},
  file = {/home/sam/Zotero/storage/7N67DY3L/McVoy et al. - lmbench Portable Tools for Performance Analysis.pdf}
}

@article{meehlWhySummariesResearch1990,
  title = {Why {{Summaries}} of {{Research}} on {{Psychological Theories}} Are {{Often Uninterpretable}}},
  author = {Meehl, Paul E.},
  date = {1990-02-01},
  journaltitle = {Psychological Reports},
  shortjournal = {Psychol Rep},
  volume = {66},
  number = {1},
  pages = {195--244},
  publisher = {SAGE Publications Inc},
  issn = {0033-2941},
  doi = {10.2466/pr0.1990.66.1.195},
  url = {https://doi.org/10.2466/pr0.1990.66.1.195},
  urldate = {2022-09-09},
  abstract = {Null hypothesis testing of correlational predictions from weak substantive theories in soft psychology is subject to the influence of ten obfuscating factors whose effects are usually (1) sizeable, (2) opposed, (3) variable, and (4) unknown. The net epistemic effect of these ten obfuscating influences is that the usual research literature review is well-nigh uninterpretable. Major changes in graduate education, conduct of research, and editorial policy are proposed.},
  langid = {english},
  keywords = {statistics},
  annotation = {interest: 85}
}

@online{MemoizationCheckpointing,
  title = {Memoization and Checkpointing},
  url = {https://parsl.readthedocs.io/en/stable/userguide/checkpoints.html#app-equivalence},
  urldate = {2022-05-12},
  organization = {Parsl 1.2.0 documentation},
  keywords = {project-charmonium.cache},
  file = {/home/sam/Zotero/storage/KK7YQJDR/checkpoints.html}
}

@inproceedings{memonTamingGooglescaleContinuous2017,
  title = {Taming {{Google-scale}} Continuous Testing},
  booktitle = {2017 {{IEEE}}/{{ACM}} 39th {{International Conference}} on {{Software Engineering}}: {{Software Engineering}} in {{Practice Track}} ({{ICSE-SEIP}})},
  author = {Memon, Atif and Gao, Zebao and Nguyen, Bao and Dhanda, Sanjeev and Nickell, Eric and Siemborski, Rob and Micco, John},
  date = {2017-05},
  pages = {233--242},
  doi = {10.1109/ICSE-SEIP.2017.16},
  abstract = {Growth in Google's code size and feature churn rate has seen increased reliance on continuous integration (CI) and testing to maintain quality. Even with enormous resources dedicated to testing, we are unable to regression test each code change individually, resulting in increased lag time between code check-ins and test result feedback to developers. We report results of a project that aims to reduce this time by: (1) controlling test workload without compromising quality, and (2) distilling test results data to inform developers, while they write code, of the impact of their latest changes on quality. We model, empirically understand, and leverage the correlations that exist between our code, test cases, developers, programming languages, and code-change and test-execution frequencies, to improve our CI and development processes. Our findings show: very few of our tests ever fail, but those that do are generally "closer" to the code they test, certain frequently modified code and certain users/tools cause more breakages, and code recently modified by multiple developers (more than 3) breaks more often.},
  eventtitle = {2017 {{IEEE}}/{{ACM}} 39th {{International Conference}} on {{Software Engineering}}: {{Software Engineering}} in {{Practice Track}} ({{ICSE-SEIP}})},
  keywords = {industry practices,software testing},
  annotation = {interest: 85},
  file = {/home/sam/Zotero/storage/S8C4JGN8/7965447.html}
}

@article{mengFacilitatingReproducibilityScientific2017,
  title = {Facilitating the {{Reproducibility}} of {{Scientific Workflows}} with {{Execution Environment Specifications}}},
  author = {Meng, Haiyan and Thain, Douglas},
  date = {2017-01-01},
  journaltitle = {Procedia Computer Science},
  shortjournal = {Procedia Computer Science},
  series = {International {{Conference}} on {{Computational Science}}, {{ICCS}} 2017, 12-14 {{June}} 2017, {{Zurich}}, {{Switzerland}}},
  volume = {108},
  pages = {705--714},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2017.05.116},
  url = {https://www.sciencedirect.com/science/article/pii/S1877050917306816},
  urldate = {2023-02-20},
  abstract = {Scientific workflows are designed to solve complex scientific problems and accelerate scientific progress. Ideally, scientific workflows should improve the reproducibility of scientific applications by making it easier to share and reuse workflows between scientists. However, scientists often find it difficult to reuse others’ workflows, which is known as workflow decay. In this paper, we explore the challenges in reproducing scientific workflows, and propose a framework for facilitating the reproducibility of scientific workflows at the task level by giving scientists complete control over the execution environments of the tasks in their workflows and integrating execution environment specifications into scientific workflow systems. Our framework allows dependencies to be archived in basic units of OS image, software and data instead of gigantic all-in-one images. We implement a prototype of our framework by integrating Umbrella, an execution environment creator, into Makeflow, a scientific workflow system. To evaluate our framework, we use it to run two bioinformatics scientific workflows, BLAST and BWA. The execution environment of the tasks in each workflow is specified as an Umbrella specification file, and sent to execution nodes where Umbrella is used to create the specified environment for running the tasks. For each workflow we evaluate the size of the Umbrella specification file, the time and space overheads of creating execution environments using Umbrella, and the heterogeneity of execution nodes contributing to each workflow. The evaluation results show that our framework improves the utilization of heterogeneous computing resources, and improves the portability and reproducibility of scientific workflows.},
  langid = {english},
  keywords = {project-acm-rep,reproducibility engineering,workflow managers},
  file = {/home/sam/Zotero/storage/CJZ2CVK8/Meng and Thain - 2017 - Facilitating the Reproducibility of Scientific Wor.pdf;/home/sam/Zotero/storage/U866VWV9/S1877050917306816.html}
}

@book{mertonSociologyScienceTheoretical1974,
  title = {The Sociology of Science: Theoretical and Empirical Investigations},
  shorttitle = {The Sociology of Science},
  author = {Merton, Robert K.},
  date = {1974},
  edition = {4. Dr.},
  publisher = {Univ. of Chicago Pr},
  location = {Chicago},
  isbn = {978-0-226-52092-6},
  langid = {english},
  pagetotal = {605},
  keywords = {internship-project,project-acm-rep,project-repro-py,sociology}
}

@online{mesnardReproducibleReplicableCFD2016,
  title = {Reproducible and Replicable {{CFD}}: It's Harder than You Think},
  shorttitle = {Reproducible and Replicable {{CFD}}},
  author = {Mesnard, Olivier and Barba, Lorena A.},
  date = {2016-10-14},
  eprint = {1605.04339},
  eprinttype = {arXiv},
  eprintclass = {physics},
  doi = {10.48550/arXiv.1605.04339},
  url = {http://arxiv.org/abs/1605.04339},
  urldate = {2023-02-23},
  abstract = {Completing a full replication study of our previously published findings on bluff-body aerodynamics was harder than we thought. Despite the fact that we have good reproducible-research practices, sharing our code and data openly. Here's what we learned from three years, four CFD codes and hundreds of runs.},
  pubstate = {prepublished},
  keywords = {project-provenance-pp,reproducibility engineering},
  annotation = {interest: 94},
  file = {/home/sam/Zotero/storage/4C9ZRCHY/Mesnard and Barba - 2016 - Reproducible and replicable CFD it's harder than .pdf;/home/sam/Zotero/storage/7KT47NJL/1605.html}
}

@unpublished{meurerCondaCrossPlatform2014,
  title = {Conda: {{A Cross Platform Package Manager}} for Any {{Binary Distribution}}},
  shorttitle = {Conda},
  author = {Meurer, Aaron},
  date = {2014-07-09},
  url = {https://www.youtube.com/watch?v=UaIvrDWrIWM},
  urldate = {2023-04-06},
  eventtitle = {{{SciPy}} 2014},
  keywords = {project-provenance-pp}
}

@unpublished{meyerTypecheckedPythonReal2018,
  title = {Type-Checked {{Python}} in the Real World},
  author = {Meyer, Carl},
  date = {2018-05-13},
  url = {https://www.youtube.com/watch?v=pMgmKJyWKn8},
  urldate = {2022-04-18},
  abstract = {You've heard about Python type annotations, but wondered if they're useful in the real world? Worried you've got too much code and can't afford to annotate it?  Type-checked Python is here, it's for real, and it can help you catch bugs and make your code easier to understand. Come learn from our experience gradually typing a million-LOC production Python application! Type checking solves real world problems in production Python systems. We'll cover the benefits, how type checking in Python works, how to introduce it gradually and sustainably in a production Python application, and how to measure success and avoid common pitfalls. We'll even demonstrate how modern Python typechecking goes hand-in-hand with duck-typing! Join us for a deep dive into type-checked Python in the real world.},
  keywords = {industry practices,type checking},
  file = {/home/sam/Zotero/storage/QJIGATHB/13h10_-_Carl_Meyer_-_Type-checked_Python_in_the_real_world-1dsdxxyx8v32l.pdf}
}

@article{meynersEquivalenceTestsReview2012,
  title = {Equivalence Tests – {{A}} Review},
  author = {Meyners, Michael},
  date = {2012-12-01},
  journaltitle = {Food Quality and Preference},
  shortjournal = {Food Quality and Preference},
  volume = {26},
  number = {2},
  pages = {231--245},
  issn = {0950-3293},
  doi = {10.1016/j.foodqual.2012.05.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0950329312000961},
  urldate = {2024-01-29},
  abstract = {Equivalence tests are becoming increasingly popular in many application areas including sensory sciences. They should be applied whenever the aim of the study is not to show differences, but to conclude similarity. There has been quite some debate about pros and cons of different approaches, reaching the sensory and sensometrics community in recent years. Parts of the sometimes heated debate are, in our opinion, due to mutual misunderstandings, and to different objectives and hence selection criteria for an “optimal” test, boiling down to the question whether power is the only optimality criterion to apply, or whether somehow vague criteria like intuition should be taken into account as well. This review intends to give an introduction into equivalence tests, starting with some general considerations on the statistical testing of hypotheses. We will subsequently give an overview over the most common approaches, some of which are shown to be inappropriate (e.g. the power approach). Some valid and relatively simple methods will be introduced and their correspondence to confidence intervals clarified, while we will skip the mathematical details of some more recent tests. Instead, the pros and cons of different approaches will be discussed and recommendations given.},
  keywords = {statistics},
  file = {/home/sam/Zotero/storage/K8QQ8N3H/S0950329312000961.html}
}

@unpublished{miccoStateContinuousIntegration2017,
  title = {The {{State}} of {{Continuous Integration Testing}} @{{Google}}},
  author = {Micco, John},
  date = {2017},
  url = {https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45880.pdf},
  abstract = {This presentation deck speaks to some of the key problems and challenges facing the internal Google software engineering stack and invites industry and academic collaboration to help improve the state of the art of software testing throughout the industry.},
  keywords = {continuous integration,industry practices,software engineering},
  annotation = {score: 80},
  file = {/home/sam/Zotero/storage/RGWPC2NG/45880.pdf}
}

@online{michaelAskChefsOSTP2022,
  title = {Ask {{The Chefs}}: {{OSTP Policy Part I}}},
  shorttitle = {Ask {{The Chefs}}},
  author = {Michael, Ann},
  date = {2022-08-30T05:30:48-04:00},
  url = {https://scholarlykitchen.sspnet.org/2022/08/30/ask-the-chefs-ostp-policy-i/},
  urldate = {2022-09-06},
  abstract = {Everyone has an opinion about the OSTP Policy memo! Come over and hear what the Chefs have to say and share your opinions with us. Part 1 of a 2 part post.},
  langid = {american},
  organization = {The Scholarly Kitchen},
  annotation = {interest: 83},
  file = {/home/sam/Zotero/storage/RYAEA9XZ/ask-the-chefs-ostp-policy-i.html}
}

@online{mightIllustratedGuidePh,
  title = {The Illustrated Guide to a {{Ph}}.{{D}}.},
  author = {Might, Matt},
  url = {https://matt.might.net/articles/phd-school-in-pictures/},
  urldate = {2022-08-31},
  keywords = {academia,academic careers},
  file = {/home/sam/Zotero/storage/9FW2M5YD/phd-school-in-pictures.html}
}

@report{milewiczNegativePerceptionsApplicability2021,
  title = {Negative {{Perceptions About}} the {{Applicability}} of {{Source-to-Source Compilers}} in {{HPC}}: {{A Literature Review}}},
  shorttitle = {Negative {{Perceptions About}} the {{Applicability}} of {{Source-to-Source Compilers}} in {{HPC}}},
  author = {Milewicz, R. and Pirkelbauer, P. and Soundararajan, P. and Ahmed, H. and Skjellum, A.},
  date = {2021-04-08},
  number = {LLNL-CONF-821299},
  institution = {Lawrence Livermore National Lab. (LLNL), Livermore, CA (United States)},
  url = {https://www.osti.gov/biblio/1806418},
  urldate = {2022-09-06},
  abstract = {The U.S. Department of Energy's Office of Scientific and Technical Information},
  langid = {english},
  keywords = {high performance computing,research software engineering},
  annotation = {interest: 69},
  file = {/home/sam/Zotero/storage/3XQXP3YD/Milewicz et al. - 2021 - Negative Perceptions About the Applicability of So.pdf;/home/sam/Zotero/storage/785UAUJY/1806418.html}
}

@inproceedings{milewiczPositionPaperUsability2019,
  title = {Position {{Paper}}: {{Towards Usability}} as a {{First-Class Quality}} of {{HPC Scientific Software}}},
  shorttitle = {Position {{Paper}}},
  booktitle = {2019 {{IEEE}}/{{ACM}} 14th {{International Workshop}} on {{Software Engineering}} for {{Science}} ({{SE4Science}})},
  author = {Milewicz, Reed and Rodeghero, Paige},
  date = {2019-05},
  pages = {41--42},
  doi = {10.1109/SE4Science.2019.00012},
  abstract = {The modern HPC scientific software ecosystem is instrumental to the practice of science. However, software can only fulfill that role if it is readily usable. In this position paper, we discuss usability in the context of scientific software development, how usability engineering can be incorporated into current practice, and how software engineering research can help satisfy that objective.},
  eventtitle = {2019 {{IEEE}}/{{ACM}} 14th {{International Workshop}} on {{Software Engineering}} for {{Science}} ({{SE4Science}})},
  keywords = {research software engineering},
  annotation = {interest: 87},
  file = {/home/sam/Zotero/storage/UIYDAPK7/8823768.html}
}

@report{milewiczRequirementsElicitationTechniques,
  title = {Requirements {{Elicitation Techniques}}: {{Guidelines}} and {{Recommendations}}},
  author = {Milewicz, Reed},
  institution = {{Sandia National Laboratories, Software Engineering and Research Department}},
  url = {https://sandialabs-my.sharepoint.com/personal/rmilewi_sandia_gov/Documents/Microsoft%20Teams%20Chat%20Files/elicitation_rapid_review.pdf},
  keywords = {internship-project,software engineering process}
}

@article{milkowskiReplicabilityReproducibilityReplication2018,
  title = {Replicability or Reproducibility? {{On}} the Replication Crisis in Computational Neuroscience and Sharing Only Relevant Detail},
  shorttitle = {Replicability or Reproducibility?},
  author = {Miłkowski, Marcin and Hensel, Witold M. and Hohol, Mateusz},
  date = {2018-12-01},
  journaltitle = {Journal of Computational Neuroscience},
  shortjournal = {J Comput Neurosci},
  volume = {45},
  number = {3},
  pages = {163--172},
  issn = {1573-6873},
  doi = {10.1007/s10827-018-0702-z},
  url = {https://doi.org/10.1007/s10827-018-0702-z},
  urldate = {2025-01-14},
  abstract = {Replicability and reproducibility of computational models has been somewhat understudied by “the replication movement.” In this paper, we draw on methodological studies into the replicability of psychological experiments and on the mechanistic account of explanation to analyze the functions of model replications and model reproductions in computational neuroscience. We contend that model replicability, or independent researchers' ability to obtain the same output using original code and data, and model reproducibility, or independent researchers' ability to recreate a model without original code, serve different functions and fail for different reasons. This means that measures designed to improve model replicability may not enhance (and, in some cases, may actually damage) model reproducibility. We claim that although both are undesirable, low model reproducibility poses more of a threat to long-term scientific progress than low model replicability. In our opinion, low model reproducibility stems mostly from authors' omitting to provide crucial information in scientific papers and we stress that sharing all computer code and data is not a solution. Reports of computational studies should remain selective and include all and only relevant bits of code.},
  langid = {english},
  keywords = {Computational modeling,Direct and conceptual replication,Methodology of computational neuroscience,Replication and reproduction,Replication studies},
  file = {/home/sam/Zotero/storage/LCL8ZXAT/Miłkowski et al. - 2018 - Replicability or reproducibility On the replication crisis in computational neuroscience and sharin.pdf}
}

@article{millerScientistNightmareSoftware2006,
  title = {A {{Scientist}}'s {{Nightmare}}: {{Software Problem Leads}} to {{Five Retractions}}},
  shorttitle = {A {{Scientist}}'s {{Nightmare}}},
  author = {Miller, Greg},
  date = {2006-12-22},
  journaltitle = {Science},
  shortjournal = {Science},
  volume = {314},
  number = {5807},
  pages = {1856--1857},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.314.5807.1856},
  url = {https://www.science.org/doi/10.1126/science.314.5807.1856},
  urldate = {2022-05-26},
  langid = {english},
  keywords = {project-acm-rep,research software engineering,retraction},
  file = {/home/sam/Zotero/storage/6VPQSDMX/science.314.5807.1856.pdf}
}

@article{minerFormallyComparingTopic2023,
  title = {Formally Comparing Topic Models and Human-Generated Qualitative Coding  of Physician Mothers’ Experiences  of Workplace Discrimination},
  author = {Miner, Adam S and Stewart, Sheridan A and Halley, Meghan C and Nelson, Laura K and Linos, Eleni},
  date = {2023-01-01},
  journaltitle = {Big Data \& Society},
  volume = {10},
  number = {1},
  pages = {20539517221149106},
  publisher = {SAGE Publications Ltd},
  issn = {2053-9517},
  doi = {10.1177/20539517221149106},
  url = {https://doi.org/10.1177/20539517221149106},
  urldate = {2023-06-09},
  abstract = {Differences between computationally generated and human-generated themes in unstructured text are important to understand yet difficult to assess formally. In this study, we bridge these approaches through two contributions. First, we formally compare a primarily computational approach, topic modeling, to a primarily human-driven approach, qualitative thematic coding, in an impactful context: physician mothers’ experience of workplace discrimination. Second, we compare our chosen topic model to a principled alternative topic model to make explicit study design decisions meriting consideration in future research. By formally contrasting computationally generated (i.e. topic modeling) and human-generated (i.e. thematic coding) knowledge, we shed light on issues of interest to several audiences, notably computational social scientists who wish to understand study design tradeoffs, and qualitative researchers who may wish to leverage computational methods to improve the speed and reproducibility of labor-intensive coding. Although useful in other domains, we highlight the value of fast, reproducible methods to better understand experiences of workplace discrimination.},
  langid = {english},
  keywords = {qualitative analysis,topic modeling},
  file = {/home/sam/Zotero/storage/NNZAB9A7/Miner et al. - 2023 - Formally comparing topic models and human-generate.pdf}
}

@inproceedings{missierDPROVExtendingPROV2013,
  title = {D-{{PROV}}: {{Extending}} the {{PROV Provenance Model}} with {{Workﬂow Structure}}},
  booktitle = {5th {{Workshop}} on the {{Theory}} and {{Practice}} of {{Provenance}}},
  author = {Missier, Paulo and Dey, Sauman and Belhajjame, Khalid and Cuevas-Vicenttín, Victor and Ludäscher, Bertram},
  date = {2013-04-03},
  publisher = {Lombard, IL},
  url = {https://www.usenix.org/conference/tapp13/technical-sessions/presentation/missier},
  abstract = {This paper presents an extension to the W3C PROV provenance model, aimed at representing process structure. Although the modelling of process structure is out of the scope of the PROV specification, it is beneficial when capturing and analyzing the provenance of data that is produced by programs or other formally encoded processes. In the paper, we motivate the need for such and extended model in the context of an ongoing large data federation and preservation project, DataONE, where provenance traces of scientific workflow runs are captured and stored alongside the data products. We introduce new provenance relations for modelling process structure along with their usage patterns, and present sample queries that demonstrate their benefit.},
  keywords = {provenance,semantic web}
}

@inproceedings{missierFinegrainedEfficientLineage2010,
  title = {Fine-Grained and Efficient Lineage Querying of Collection-Based Workflow Provenance},
  booktitle = {Proceedings of the 13th {{International Conference}} on {{Extending Database Technology}} - {{EDBT}} '10},
  author = {Missier, Paolo and Paton, Norman W. and Belhajjame, Khalid},
  date = {2010},
  pages = {299},
  publisher = {ACM Press},
  location = {Lausanne, Switzerland},
  doi = {10.1145/1739041.1739079},
  url = {http://portal.acm.org/citation.cfm?doid=1739041.1739079},
  urldate = {2022-08-02},
  eventtitle = {The 13th {{International Conference}}},
  isbn = {978-1-60558-945-9},
  langid = {english}
}

@inproceedings{missierW3CPROVFamily2013,
  title = {The {{W3C PROV}} Family of Specifications for Modelling Provenance Metadata},
  booktitle = {Proceedings of the 16th {{International Conference}} on {{Extending Database Technology}} - {{EDBT}} '13},
  author = {Missier, Paolo and Belhajjame, Khalid and Cheney, James},
  date = {2013},
  pages = {773},
  publisher = {ACM Press},
  location = {Genoa, Italy},
  doi = {10.1145/2452376.2452478},
  url = {http://dl.acm.org/citation.cfm?doid=2452376.2452478},
  urldate = {2022-07-11},
  abstract = {Provenance, a form of structured metadata designed to record the origin or source of information, can be instrumental in deciding whether information is to be trusted, how it can be integrated with other diverse information sources, and how to establish attribution of information to authors throughout its history. The PROV set of specifications, produced by the World Wide Web Consortium (W3C), is designed to promote the publication of provenance information on the Web, and offers a basis for interoperability across diverse provenance management systems. The PROV provenance model is deliberately generic and domain-agnostic, but extension mechanisms are available and can be exploited for modelling specific domains. This tutorial provides an account of these specifications. Starting from intuitive and informal examples that present idiomatic provenance patterns, it progressively introduces the relational model of provenance along with the constraints model for validation of provenance documents, and concludes with example applications that show the extension points in use.},
  eventtitle = {The 16th {{International Conference}}},
  isbn = {978-1-4503-1597-5},
  langid = {english},
  keywords = {project-acm-rep,provenance,workflow managers}
}

@article{miyaokaEmergentCodingTopic2023,
  title = {Emergent {{Coding}} and {{Topic Modeling}}: {{A Comparison}} of {{Two Qualitative Analysis Methods}} on {{Teacher Focus Group Data}}},
  shorttitle = {Emergent {{Coding}} and {{Topic Modeling}}},
  author = {Miyaoka, Atsushi and Decker-Woodrow, Lauren and Hartman, Nancy and Booker, Barbara and Ottmar, Erin},
  date = {2023-01-01},
  journaltitle = {International Journal of Qualitative Methods},
  volume = {22},
  pages = {16094069231165950},
  publisher = {SAGE Publications Inc},
  issn = {1609-4069},
  doi = {10.1177/16094069231165950},
  url = {https://doi.org/10.1177/16094069231165950},
  urldate = {2023-06-14},
  abstract = {More than ever in the past, researchers have access to broad, educationally relevant text data from sources such as literature databases (e.g., ERIC), an open-ended response from online courses/surveys, online discussion forums, digital essays, and social media. These advances in data availability can dramatically increase the possibilities for discovering new patterns in the data and testing new theories through processing texts with emerging analytic techniques. In our study, we extended the application of Topic Modeling (TM) to data collected from focus groups within the context of a larger study. Specifically, we compared the results of emergent qualitative coding and TM. We found a high level of agreement between TM and emergent qualitative coding, suggesting TM is a viable method for coding focus group data when augmenting and validating manual qualitative coding. We also found that TM was ineffective in capturing more nuanced information than the qualitative coding was able to identify. This can be explained by two factors: (1) the word level tokenization we used in the study, and (2) variations in the terminology teachers used to identify the different technologies. Recommendations include additional data cleaning steps researchers should take and specifications within the topic modeling code when using topic modeling to analyze focus group data.},
  langid = {english},
  keywords = {qualitative analysis,topic modeling},
  file = {/home/sam/Zotero/storage/XBPQQ77Q/Miyaoka et al. - 2023 - Emergent Coding and Topic Modeling A Comparison o.pdf}
}

@article{mokhovBuildSystemsCarte2018,
  title = {Build Systems à La Carte},
  author = {Mokhov, Andrey and Mitchell, Neil and Peyton Jones, Simon},
  date = {2018-07-30},
  journaltitle = {Proceedings of the ACM on Programming Languages},
  shortjournal = {Proc. ACM Program. Lang.},
  volume = {2},
  pages = {1--29},
  issn = {2475-1421},
  doi = {10.1145/3236774},
  url = {https://dl.acm.org/doi/10.1145/3236774},
  urldate = {2023-05-06},
  abstract = {Build systems are awesome, terrifying -- and unloved. They are used by every developer around the world, but are rarely the object of study. In this paper we offer a systematic, and executable, framework for developing and comparing build systems, viewing them as related points in landscape rather than as isolated phenomena. By teasing apart existing build systems, we can recombine their components, allowing us to prototype new build systems with desired properties.},
  issue = {ICFP},
  langid = {english},
  file = {/home/sam/Zotero/storage/TA8MYX4D/Mokhov et al. - 2018 - Build systems à la carte.pdf}
}

@article{molderSustainableDataAnalysis2021,
  title = {Sustainable Data Analysis with {{Snakemake}}},
  author = {Mölder, Felix and Jablonski, Kim Philipp and Letcher, Brice and Hall, Michael B. and Tomkins-Tinch, Christopher H. and Sochat, Vanessa and Forster, Jan and Lee, Soohyun and Twardziok, Sven O. and Kanitz, Alexander and Wilm, Andreas and Holtgrewe, Manuel and Rahmann, Sven and Nahnsen, Sven and Köster, Johannes},
  date = {2021-04-19},
  number = {10:33},
  publisher = {F1000Research},
  doi = {10.12688/f1000research.29032.2},
  url = {https://f1000research.com/articles/10-33},
  urldate = {2022-05-12},
  abstract = {Data analysis often entails a multitude of heterogeneous steps, from the application of various command line tools to the usage of scripting languages like R or Python for the generation of plots and tables. It is widely recognized that data analyses should ideally be conducted in a reproducible way.\&nbsp;Reproducibility enables technical validation and regeneration of results on the original or even new data. However, reproducibility alone is by no means sufficient to deliver an analysis that is of lasting impact (i.e., sustainable) for the field, or even just one research group. We postulate that it is equally important to ensure adaptability and transparency. The former describes the ability to modify the analysis to answer extended or slightly different research questions. The latter describes the ability to understand the analysis in order to judge whether it is not only technically, but methodologically valid. Here, we analyze the properties needed for a data analysis to become reproducible, adaptable, and transparent. We show how the popular workflow management system Snakemake can be used to guarantee this, and how it enables an ergonomic, combined, unified representation of all steps involved in data analysis, ranging from raw data processing, to quality control and fine-grained, interactive exploration and plotting of final results.},
  langid = {english},
  keywords = {project-charmonium.cache,workflow managers},
  file = {/home/sam/Zotero/storage/9B3RG3QQ/Mölder et al. - 2021 - Sustainable data analysis with Snakemake.pdf}
}

@inproceedings{momotSevenTurretsBabel2016,
  title = {The {{Seven Turrets}} of {{Babel}}: {{A Taxonomy}} of {{LangSec Errors}} and {{How}} to {{Expunge Them}}},
  shorttitle = {The {{Seven Turrets}} of {{Babel}}},
  booktitle = {2016 {{IEEE Cybersecurity Development}} ({{SecDev}})},
  author = {Momot, Falcon and Bratus, Sergey and Hallberg, Sven M. and Patterson, Meredith L.},
  date = {2016-11},
  pages = {45--52},
  doi = {10.1109/SecDev.2016.019},
  url = {https://ieeexplore.ieee.org/abstract/document/7839788},
  urldate = {2024-01-11},
  abstract = {Input-handling bugs share two common patterns: insufficient recognition, where input-checking logic is unfit to validate a program's assumptions about inputs, \%leading to the code acting on invalid inputs, and parser differentials, wherein two or more components of a system fail to interpret input equivalently. We argue that these patterns are artifacts of avoidable weaknesses in the development process and explore these patterns both in general and via recent CVE instances. We break ground on defining the input-handling code weaknesses that should be actionable findings and propose a refactoring of existing CWEs to accommodate them. We propose a set of new CWEs to name such weaknesses that will help code auditors and penetration testers precisely express their findings of likely vulnerable code structures.},
  eventtitle = {2016 {{IEEE Cybersecurity Development}} ({{SecDev}})},
  keywords = {language engineering,security}
}

@inproceedings{mondelliCapturingSemanticallyDescribing2021,
  title = {Capturing and {{Semantically Describing Provenance}} to {{Tell}} the {{Story}} of {{R Scripts}}},
  booktitle = {2021 {{IEEE}} 17th {{International Conference}} on {{eScience}} ({{eScience}})},
  author = {Mondelli, Maria Luiza and Samuel, Sheeba and Konig-Ries, Birgitta and Gadelha, Luiz M. R.},
  date = {2021-09},
  pages = {283--288},
  publisher = {IEEE},
  location = {Innsbruck, Austria},
  doi = {10.1109/eScience51609.2021.00057},
  url = {https://ieeexplore.ieee.org/document/9582412/},
  urldate = {2022-07-26},
  abstract = {Scientific workflows are commonly used to model and execute large-scale scientific experiments. They represent key resources for scientists and are enacted and managed by Scientific Workflow Management Systems (SWfMS). Each SWfMS has its particular approach to execute workflows and to capture and manage their provenance data. Due to the large scale of experiments, it may be unviable to analyze provenance data only after the end of the execution. A single experiment may demand weeks to run, even in high performance computing environments. Thus scientists need to monitor the experiment during its execution, and this can be done through provenance data. Runtime provenance analysis allows for scientists to monitor workflow execution and to take actions before the end of it (i.e. workflow steering). This provenance data can also be used to fine-tune the parallel execution of the workflow dynamically. We use the PROV data model as a basic framework for modeling and providing runtime provenance as a database that can be queried even during the execution. This database is agnostic of SWfMS and workflow engine. We show the benefits of representing and sharing runtime provenance data for improving the experiment management as well as the analysis of the scientific data.},
  eventtitle = {2021 {{IEEE}} 17th {{International Conference}} on {{eScience}} ({{eScience}})},
  isbn = {978-1-6654-0361-0},
  keywords = {provenance}
}

@article{mooreDevelopmentInstrumentMeasure1991,
  title = {Development of an {{Instrument}} to {{Measure}} the {{Perceptions}} of {{Adopting}} an {{Information Technology Innovation}}},
  author = {Moore, Gary C. and Benbasat, Izak},
  date = {1991-09},
  journaltitle = {Information Systems Research},
  shortjournal = {Information Systems Research},
  volume = {2},
  number = {3},
  pages = {192--222},
  issn = {1047-7047, 1526-5536},
  doi = {10.1287/isre.2.3.192},
  url = {http://pubsonline.informs.org/doi/abs/10.1287/isre.2.3.192},
  urldate = {2022-06-02},
  abstract = {This paper reports on the development of an instrument designed to measure the various perceptions that an individual may have of adopting an information technology (IT) innovation. This instrument is intended to be a tool for the study of the initial adoption and eventual diffusion of IT innovations within organizations. While the adoption of information technologies by individuals and organizations has been an area of substantial research interest since the early days of computerization, research efforts to date have led to mixed and inconclusive outcomes. The lack of a theoretical foundation for such research and inadequate definition and measurement of constructs have been identified as major causes for such outcomes. In a recent study examining the diffusion of new end-user IT, we decided to focus on measuring the potential adopters' perceptions of the technology. Measuring such perceptions has been termed a “classic issue” in the innovation diffusion literature, and a key to integrating the various findings of diffusion research. The perceptions of adopting were initially based on the five characteristics of innovations derived by Rogers (1983) from the diffusion of innovations literature, plus two developed specifically within this study. Of the existing scales for measuring these characteristics, very few had the requisite levels of validity and reliability. For this study, both newly created and existing items were placed in a common pool and subjected to four rounds of sorting by judges to establish which items should be in the various scales. The objective was to verify the convergent and discriminant validity of the scales by examining how the items were sorted into various construct categories. Analysis of inter-judge agreement about item placement identified both bad items as well as weaknesses in some of the constructs' original definitions. These were subsequently redefined. Scales for the resulting constructs were subjected to three separate field tests. Following the final test, the scales all demonstrated acceptable levels of reliability. Their validity was further checked using factor analysis, as well as conducting discriminant analysis comparing responses between adopters and nonadopters of the innovation. The result is a parsimonious, 38-item instrument comprising eight scales which provides a useful tool for the study of the initial adoption and diffusion of innovations. A short, 25 item, version of the instrument is also suggested.},
  langid = {english},
  keywords = {internship-project,technology-acceptance},
  file = {/home/sam/Zotero/storage/PZG9C8UA/moore1991.pdf}
}

@article{moreauSpecialIssueFirst2008,
  title = {Special {{Issue}}: {{The First Provenance Challenge}}},
  shorttitle = {Special {{Issue}}},
  author = {Moreau, Luc and Ludäscher, Bertram and Altintas, Ilkay and Barga, Roger S. and Bowers, Shawn and Callahan, Steven and Chin, George and Clifford, Ben and Cohen, Shirley and Cohen-Boulakia, Sarah and Davidson, Susan and Deelman, Ewa and Digiampietri, Luciano and Foster, Ian and Freire, Juliana and Frew, James and Futrelle, Joe and Gibson, Tara and Gil, Yolanda and Goble, Carole and Golbeck, Jennifer and Groth, Paul and Holland, David A. and Jiang, Sheng and Kim, Jihie and Koop, David and Krenek, Ales and McPhillips, Timothy and Mehta, Gaurang and Miles, Simon and Metzger, Dominic and Munroe, Steve and Myers, Jim and Plale, Beth and Podhorszki, Norbert and Ratnakar, Varun and Santos, Emanuele and Scheidegger, Carlos and Schuchardt, Karen and Seltzer, Margo and Simmhan, Yogesh L. and Silva, Claudio and Slaughter, Peter and Stephan, Eric and Stevens, Robert and Turi, Daniele and Vo, Huy and Wilde, Mike and Zhao, Jun and Zhao, Yong},
  date = {2008-04-10},
  journaltitle = {Concurrency and Computation: Practice and Experience},
  shortjournal = {Concurrency Computat.: Pract. Exper.},
  volume = {20},
  number = {5},
  pages = {409--418},
  issn = {15320626, 15320634},
  doi = {10.1002/cpe.1233},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/cpe.1233},
  urldate = {2022-07-08},
  langid = {english},
  keywords = {project-provenance-pp,provenance},
  annotation = {interest: 87},
  file = {/home/sam/Zotero/storage/A8LJX7P3/Moreau et al. - 2008 - Special Issue The First Provenance Challenge.pdf;/home/sam/Zotero/storage/KS433NWN/Moreau et al. - 2008 - Special Issue The First Provenance Challenge.pdf}
}

@inproceedings{morenoSpeedingLargeScaleFinancial2014,
  title = {Speeding up {{Large-Scale Financial Recomputation}} with {{Memoization}}},
  booktitle = {2014 {{Seventh Workshop}} on {{High Performance Computational Finance}}},
  author = {Moreno, Alexander and Balch, Tucker},
  date = {2014-11},
  pages = {17--22},
  doi = {10.1109/WHPCF.2014.9},
  abstract = {Quantitative financial analysis requires repeated computations of the same functions with the same arguments when prototyping trading strategies; many of these functions involve resource intensive operations on large matrices. Reducing the number of repeated computations either within a program or across runs of the same program would allow analysts to build and debug trading strategies more quickly. We built a disk memoization library that caches function computations to files to avoid recomputation. Anymemoization solution should be easy to use, minimizing the need for users to think about whether caching is appropriate, while at the same time giving them control over speed, accuracy, and space used for caching. Guo and Engler proposed a similar tool that does automatic memoization by modifying the python interpreter, while the packages Jug and Joblib are distributed computing tools that have memoization options. Our library attempts to maintain the ease of use of the above packages for memoization, but at the same time give a higher degree of control of how caching is done for users who need it. We provide the same basic features as these other libraries, but allow control of how hashing is done, space usage for individual functions and all memoization, refreshing memoization for a specific function, and accuracy checking. This should lead to both increased productivity and speed increases for recomputation. We show that for several financial calculations, including Markowitz Optimization, Fama French, and the Singular Value Decomposition, memoization greatly speeds up recomputation, often by over 99\%. We also show that by using xxhash, a non-cryptographic hash function, instead of md5, and avoiding equality checks, our package greatly outperforms joblib, the best current package.},
  eventtitle = {2014 {{Seventh Workshop}} on {{High Performance Computational Finance}}},
  keywords = {project-charmonium.cache,software engineering},
  file = {/home/sam/Zotero/storage/KLFRZK4N/Moreno and Balch - 2014 - Speeding up Large-Scale Financial Recomputation wi.pdf;/home/sam/Zotero/storage/PMIR2BZ7/7016369.html}
}

@inproceedings{mukherjeeFixingDependencyErrors2021,
  title = {Fixing Dependency Errors for {{Python}} Build Reproducibility},
  booktitle = {Proceedings of the 30th {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Mukherjee, Suchita and Almanza, Abigail and Rubio-González, Cindy},
  date = {2021-07-11},
  series = {{{ISSTA}} 2021},
  pages = {439--451},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3460319.3464797},
  url = {https://dl.acm.org/doi/10.1145/3460319.3464797},
  urldate = {2023-07-03},
  abstract = {Software reproducibility is important for re-usability and the cumulative progress of research. An important manifestation of unreproducible software is the changed outcome of software builds over time. While enhancing code reuse, the use of open-source dependency packages hosted on centralized repositories such as PyPI can have adverse effects on build reproducibility. Frequent updates to these packages often cause their latest versions to have breaking changes for applications using them. Large Python applications risk their historical builds becoming unreproducible due to the widespread usage of Python dependencies, and the lack of uniform practices for dependency version specification. Manually fixing dependency errors requires expensive developer time and effort, while automated approaches face challenges of parsing unstructured build logs, finding transitive dependencies, and exploring an exponential search space of dependency versions. In this paper, we investigate how open-source Python projects specify dependency versions, and how their reproducibility is impacted by dependency packages. We propose a tool PyDFix to detect and fix unreproducibility in Python builds caused by dependency errors. PyDFix is evaluated on two bug datasets BugSwarm and BugsInPy, both of which are built from real-world open-source projects. PyDFix analyzes a total of 2,702 builds, identifying 1,921 (71.1\%) of them to be unreproducible due to dependency errors. From these, PyDFix provides a complete fix for 859 (44.7\%) builds, and partial fixes for an additional 632 (32.9\%) builds.},
  isbn = {978-1-4503-8459-9},
  keywords = {automated program repair,reproducibility engineering},
  annotation = {interest: 98},
  file = {/home/sam/Zotero/storage/52KV5K2V/Mukherjee et al. - 2021 - Fixing dependency errors for Python build reproduc.pdf}
}

@article{mulderCanSanctionsStop2022,
  entrysubtype = {magazine},
  title = {Can {{Sanctions Stop Russia}}?},
  author = {Mulder, Nicholas},
  namea = {Lowrey, Annie},
  nameatype = {collaborator},
  date = {2022-03-10},
  journaltitle = {The Atlantic},
  url = {https://www.theatlantic.com/ideas/archive/2022/03/russia-sanctions-economic-policy-effects/627009/},
  urldate = {2022-04-13},
  abstract = {Nicholas Mulder, the author of a new book on the history of sanctions, explains the West’s use of the “economic weapon.”},
  langid = {english},
  keywords = {economics,geopolitics},
  file = {/home/sam/Zotero/storage/IGWIWWTM/627009.html}
}

@article{munafoManifestoReproducibleScience2017,
  title = {A Manifesto for Reproducible Science},
  author = {Munafò, Marcus R. and Nosek, Brian A. and Bishop, Dorothy V. M. and Button, Katherine S. and Chambers, Christopher D. and Percie du Sert, Nathalie and Simonsohn, Uri and Wagenmakers, Eric-Jan and Ware, Jennifer J. and Ioannidis, John P. A.},
  date = {2017-01-10},
  journaltitle = {Nature Human Behaviour},
  shortjournal = {Nat Hum Behav},
  volume = {1},
  number = {1},
  pages = {1--9},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-016-0021},
  url = {https://www.nature.com/articles/s41562-016-0021},
  urldate = {2022-09-27},
  abstract = {Improving the reliability and efficiency of scientific research will increase the credibility of the published scientific literature and accelerate discovery. Here we argue for the adoption of measures to optimize key elements of the scientific process: methods, reporting and dissemination, reproducibility, evaluation and incentives. There is some evidence from both simulations and empirical studies supporting the likely effectiveness of these measures, but their broad adoption by researchers, institutions, funders and journals will require iterative evaluation and improvement. We discuss the goals of these measures, and how they can be implemented, in the hope that this will facilitate action toward improving the transparency, reproducibility and efficiency of scientific research.},
  issue = {1},
  langid = {english},
  keywords = {metascience,open data,reproducibility},
  annotation = {interest: 89},
  file = {/home/sam/Zotero/storage/REDCQKA3/Munafò et al. - 2017 - A manifesto for reproducible science.pdf;/home/sam/Zotero/storage/ICJA9STV/s41562-016-0021.html}
}

@article{munatLowercaseSemanticWeb,
  title = {The {{Lowercase Semantic Web}}},
  author = {Munat, Ben},
  abstract = {Despite the enthusiasm of certain technological utopians, building the Semantic Web is a hard problem. Capturing true, incontrovertible meaning of natural language remains an elusive goal. Also, the widespread adoption of complex data modeling languages like RDF or OWL is unlikely to happen overnight. A more feasible approach would be to focus first on specific domains and use the simpler technologies. XML and XHTML are two relatively simple technologies already on hand and can be used to lend a little more semantic value to the Web's existing content.},
  langid = {english},
  keywords = {semantic web},
  file = {/home/sam/Zotero/storage/VITBFWGP/Munat - The Lowercase Semantic Web.pdf}
}

@inproceedings{muniswamy-reddyLayeringProvenanceSystems2009,
  title = {Layering in Provenance Systems},
  booktitle = {Proceedings of the 2009 Conference on {{USENIX Annual}} Technical Conference},
  author = {Muniswamy-Reddy, Kiran-Kumar and Braun, Uri and Holland, David A. and Macko, Peter and Maclean, Diana and Margo, Daniel and Seltzer, Margo and Smogor, Robin},
  date = {2009-06-14},
  series = {{{USENIX}}'09},
  pages = {10},
  publisher = {USENIX Association},
  location = {USA},
  doi = {10.5555/1855807.1855817},
  abstract = {Digital provenance describes the ancestry or history of a digital object. Most existing provenance systems, however, operate at only one level of abstraction: the system call layer, a workflow specification, or the high-level constructs of a particular application. The provenance collectable in each of these layers is different, and all of it can be important. Single-layer systems fail to account for the different levels of abstraction at which users need to reason about their data and processes. These systems cannot integrate data provenance across layers and cannot answer questions that require an integrated view of the provenance. We have designed a provenance collection structure facilitating the integration of provenance across multiple levels of abstraction, including a workflow engine, a web browser, and an initial runtime Python provenance tracking wrapper. We layer these components atop provenance-aware network storage (NFS) that builds upon a Provenance-Aware Storage System (PASS). We discuss the challenges of building systems that integrate provenance across multiple layers of abstraction, present how we augmented systems in each layer to integrate provenance, and present use cases that demonstrate how provenance spanning multiple layers provides functionality not available in existing systems. Our evaluation shows that the overheads imposed by layering provenance systems are reasonable.},
  langid = {english},
  keywords = {project-provenance-pp,provenance,provenance-tool},
  file = {/home/sam/Zotero/storage/CB9P9RSJ/Muniswamy-Reddy et al. - Layering in Provenance Systems.pdf;/home/sam/Zotero/storage/GUGJKZNA/Muniswamy-Reddy et al. - Layering in Provenance Systems.pdf;/home/sam/Zotero/storage/EPL6YLK6/Muniswamy-Reddy et al. - Layering in Provenance Systems.html}
}

@inproceedings{muniswamy-reddyProvenanceAwareStorageSystems2006,
  title = {Provenance-{{Aware Storage Systems}}},
  booktitle = {2006 {{USENIX Annual Technical Conference}}},
  author = {Muniswamy-Reddy, Kiran-Kumar and Holland, David A and Braun, Uri and Seltzer, Margo},
  date = {2006},
  url = {https://dash.harvard.edu/handle/1/23853812},
  abstract = {A Provenance-Aware Storage System (PASS) is a storage system that automatically collects and maintains provenance or lineage, the complete history or ancestry of an item. We discuss the advantages of treating provenance as meta-data collected and maintained by the storage system, rather than as manual annotations stored in a separately administered database. We describe a PASS implementation, discussing the challenges it presents, performance cost it incurs, and the new functionality it enables. We show that with reasonable overhead, we can provide useful functionality not available in today’s file systems or provenance management systems.},
  eventtitle = {2006 {{USENIX Annual Technical Conference}}},
  langid = {american},
  keywords = {project-provenance-pp,provenance-tool},
  annotation = {Accepted: 2015-12-07T19:07:43Z},
  file = {/home/sam/Zotero/storage/2NZWHDHT/Muniswamy-Reddy et al. - 2006 - Provenance-Aware Storage Systems.pdf;/home/sam/Zotero/storage/BYWE4X2D/Muniswamy-Reddy et al. - 2006 - Provenance-Aware Storage Systems.pdf}
}

@article{murphy-hillWhatPredictsSoftware2019,
  title = {What {{Predicts Software Developers}}' {{Productivity}}?},
  author = {Murphy-Hill, Emerson and Jaspan, Ciera and Sadowski, Caitlin and Shepherd, David and Phillips, Michael and Winter, Collin and Knight, Andrea and Smith, Edward and Jorde, Matthew},
  date = {2019-02-19},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {47},
  number = {3},
  pages = {582--594},
  issn = {1939-3520},
  doi = {10.1109/TSE.2019.2900308},
  abstract = {Organizations have a variety of options to help their software developers become their most productive selves, from modifying office layouts, to investing in better tools, to cleaning up the source code. But which options will have the biggest impact? Drawing from the literature in software engineering and industrial/organizational psychology to identify factors that correlate with productivity, we designed a survey that asked 622 developers across 3 companies about these productivity factors and about self-rated productivity. Our results suggest that the factors that most strongly correlate with self-rated productivity were non-technical factors, such as job enthusiasm, peer support for new ideas, and receiving useful feedback about job performance. Compared to other knowledge workers, our results also suggest that software developers' self-rated productivity is more strongly related to task variety and ability to work remotely.},
  eventtitle = {{{IEEE Transactions}} on {{Software Engineering}}},
  keywords = {engineering productivity,software engineering},
  annotation = {score: 90},
  file = {/home/sam/Zotero/storage/ZDPEDW3I/What_Predicts_Software_Developers_Productivity.pdf}
}

@unpublished{murphy31NoobyHabits2021,
  title = {31 Nooby {{C}}++ Habits You Need to Ditch},
  author = {Murphy, James},
  date = {2021-12-13},
  url = {https://www.youtube.com/watch?v=i_wDa2AS_8w},
  urldate = {2022-04-10},
  abstract = {Up your C++ skill by recognizing and ditching these nooby C++ habits.},
  keywords = {industry practices,software engineering}
}

@inproceedings{murtaNoWorkflowCapturingAnalyzing2015,
  title = {{{noWorkflow}}: {{Capturing}} and {{Analyzing Provenance}} of {{Scripts}}},
  shorttitle = {{{noWorkflow}}},
  booktitle = {Provenance and {{Annotation}} of {{Data}} and {{Processes}}},
  author = {Murta, Leonardo and Braganholo, Vanessa and Chirigati, Fernando and Koop, David and Freire, Juliana},
  editor = {Ludäscher, Bertram and Plale, Beth},
  date = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {71--83},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-16462-5_6},
  abstract = {We propose noWorkflow, a tool that transparently captures provenance of scripts and enables reproducibility. Unlike existing approaches, noWorkflow is non-intrusive and does not require users to change the way they work – users need not wrap their experiments in scientific workflow systems, install version control systems, or instrument their scripts. The tool leverages Software Engineering techniques, such as abstract syntax tree analysis, reflection, and profiling, to collect different types of provenance, including detailed information about the underlying libraries. We describe how noWorkflow captures multiple kinds of provenance and the different classes of analyses it supports: graph-based visualization; differencing over provenance trails; and inference queries.},
  isbn = {978-3-319-16462-5},
  langid = {english},
  file = {/home/sam/Zotero/storage/QJPRKCX3/Murta et al. - 2015 - noWorkflow Capturing and Analyzing Provenance of .pdf}
}

@article{mytkowiczProducingWrongData2009,
  title = {Producing Wrong Data without Doing Anything Obviously Wrong!},
  author = {Mytkowicz, Todd and Diwan, Amer and Hauswirth, Matthias and Sweeney, Peter F.},
  date = {2009-03-07},
  journaltitle = {ACM SIGARCH Computer Architecture News},
  shortjournal = {SIGARCH Comput. Archit. News},
  volume = {37},
  number = {1},
  pages = {265--276},
  issn = {0163-5964},
  doi = {10.1145/2528521.1508275},
  url = {https://doi.org/10.1145/2528521.1508275},
  urldate = {2022-04-11},
  abstract = {This paper presents a surprising result: changing a seemingly innocuous aspect of an experimental setup can cause a systems researcher to draw wrong conclusions from an experiment. What appears to be an innocuous aspect in the experimental setup may in fact introduce a significant bias in an evaluation. This phenomenon is called measurement bias in the natural and social sciences. Our results demonstrate that measurement bias is significant and commonplace in computer system evaluation. By significant we mean that measurement bias can lead to a performance analysis that either over-states an effect or even yields an incorrect conclusion. By commonplace we mean that measurement bias occurs in all architectures that we tried (Pentium 4, Core 2, and m5 O3CPU), both compilers that we tried (gcc and Intel's C compiler), and most of the SPEC CPU2006 C programs. Thus, we cannot ignore measurement bias. Nevertheless, in a literature survey of 133 recent papers from ASPLOS, PACT, PLDI, and CGO, we determined that none of the papers with experimental results adequately consider measurement bias. Inspired by similar problems and their solutions in other sciences, we describe and demonstrate two methods, one for detecting (causal analysis) and one for avoiding (setup randomization) measurement bias.},
  keywords = {project-acm-rep,project-provenance-pp,software benchmarking,software engineering},
  annotation = {score: 95\\
interest: 70},
  file = {/home/sam/Zotero/storage/WRXTAESN/1508284.1508275.pdf}
}

@inproceedings{nakamuraProvenancebasedWorkflowDiagnostics2022,
  title = {Provenance-Based {{Workflow Diagnostics Using Program Specification}}},
  booktitle = {2022 {{IEEE}} 29th {{International Conference}} on {{High Performance Computing}}, {{Data}}, and {{Analytics}} ({{HiPC}})},
  author = {Nakamura, Yuta and Malik, Tanu and Kanj, Iyad and Gehani, Ashish},
  date = {2022-12},
  pages = {292--301},
  issn = {2640-0316},
  doi = {10.1109/HiPC56025.2022.00046},
  url = {https://ieeexplore.ieee.org/abstract/document/10106315?casa_token=WExXw0DqtskAAAAA:Es6Z26rbhv-4-J6_3TEOcEIFNw_F8FwLKAeP7CB_XUI8Ey5Lwnke6oUroqQR7WOlqF7VHn6x},
  urldate = {2023-11-14},
  abstract = {Workflow management systems (WMS) help automate and coordinate scientific modules and monitor their execution. WMSes are also used to repeat a workflow application with different inputs to test sensitivity and reproducibility of runs. However, when differences arise in outputs across runs, current WMSes do not audit sufficient provenance metadata to determine where the execution first differed. This increases diagnostic time and leads to poor quality diagnostic results. In this paper, we use program specification to precisely determine locations where workflow execution differs. We use existing provenance audited to isolate modules where execution differs. We show that using program specification comes at some increased storage overhead due to mapping of provenance data flows onto program specification, but leads to better quality diagnostics in terms of the number of differences found and their location relative to comparing provenance metadata audited within current WMSes.},
  eventtitle = {2022 {{IEEE}} 29th {{International Conference}} on {{High Performance Computing}}, {{Data}}, and {{Analytics}} ({{HiPC}})},
  keywords = {provenance,workflow managers},
  file = {/home/sam/Zotero/storage/GKXLZNQJ/10106315.html}
}

@inproceedings{namikiMethodConstructingResearch2023,
  title = {A {{Method}} for {{Constructing Research Data Provenance}} in {{High-Performance Computing Systems}}},
  booktitle = {2023 {{IEEE}} 19th {{International Conference}} on E-{{Science}} (e-{{Science}})},
  author = {Namiki, Yuta and Hosomi, Takeo and Tanushi, Hideyuki and Yamashita, Akihiro and Date, Susumu},
  date = {2023-10},
  pages = {1--10},
  issn = {2325-3703},
  doi = {10.1109/e-Science58273.2023.10254932},
  url = {https://ieeexplore.ieee.org/abstract/document/10254932?casa_token=pKmzTDmrxt0AAAAA:4_U3R82DbSFp5YSbx1tUWr86cMposZ_A-T4vbGHui1UR2Z9YRiFKeil9wQUDGhX3rlb3fb-h},
  urldate = {2024-02-14},
  abstract = {Research must be reproducible to be verifiable. Provenance, which describes how data was produced, is one of the metadata that can improve reproducibility. In this paper, we propose a method to construct the provenance of research data produced in high-performance computing (HPC) systems. Our method can construct a high-level and user-perspective provenance by integrating information available in HPC systems, such as a workload manager, with low-level data about running programs' behavior captured in an operating system kernel. The method enables users of HPC systems to collect the provenance without modifying assets such as programs and scripts.},
  eventtitle = {2023 {{IEEE}} 19th {{International Conference}} on E-{{Science}} (e-{{Science}})},
  keywords = {hpc,project-provenance-pp,provenance}
}

@unpublished{nathanyorkToolsContinuousIntegration2010,
  title = {Tools for {{Continuous Integration}} at {{Google Scale}}},
  author = {{Nathan York}},
  date = {2010-10-27},
  url = {https://www.youtube.com/watch?v=b52aXZ2yi08},
  urldate = {2022-04-07},
  abstract = {Software engineers rarely invoke compilers and lower-level tools directly. Instead they interact with a build system which analyzes dependency information and then orchestrates the overall build process. Yet build systems are often overlooked by industry and academia. This presents a challenge for large organizations as their code base grows and engineering processes struggle to keep up. This talk covers the key insights and technical design elements that enable us to scale the word's largest continuously integrated code base to thousands of engineers worldwide.},
  keywords = {build systems,continuous integration,industry practices,software engineering},
  annotation = {score: 50}
}

@article{naugleGroundTruthProgram2022,
  title = {The {{Ground Truth}} Program: Simulations as Test Beds for Social Science Research Methods},
  shorttitle = {The {{Ground Truth}} Program},
  author = {Naugle, Asmeret and Russell, Adam and Lakkaraju, Kiran and Swiler, Laura and Verzi, Stephen and Romero, Vicente},
  date = {2022-04-18},
  journaltitle = {Computational and Mathematical Organization Theory},
  shortjournal = {Comput Math Organ Theory},
  issn = {1381-298X, 1572-9346},
  doi = {10.1007/s10588-021-09346-9},
  url = {https://link.springer.com/10.1007/s10588-021-09346-9},
  urldate = {2022-06-30},
  abstract = {Social systems are uniquely complex and difficult to study, but understanding them is vital to solving the world’s problems. The Ground Truth program developed a new way of testing the research methods that attempt to understand and leverage the Human Domain and its associated complexities. The program developed simulations of social systems as virtual world test beds. Not only were these simulations able to produce data on future states of the system under various circumstances and scenarios, but their causal ground truth was also explicitly known. Research teams studied these virtual worlds, facilitating deep validation of causal inference, prediction, and prescription methods. The Ground Truth program model provides a way to test and validate research methods to an extent previously impossible, and to study the intricacies and interactions of different components of research.},
  langid = {english},
  keywords = {metascience,social science},
  annotation = {interest: 50},
  file = {/home/sam/Zotero/storage/G3ZQ5IWR/Naugle2022_Article_TheGroundTruthProgramSimulatio.pdf}
}

@article{naugleWhatCanSimulation2022,
  title = {What Can Simulation Test Beds Teach Us about Social Science? {{Results}} of the Ground Truth Program},
  shorttitle = {What Can Simulation Test Beds Teach Us about Social Science?},
  author = {Naugle, Asmeret and Krofcheck, Daniel and Warrender, Christina and Lakkaraju, Kiran and Swiler, Laura and Verzi, Stephen and Emery, Ben and Murdock, Jaimie and Bernard, Michael and Romero, Vicente},
  date = {2022-04-30},
  journaltitle = {Computational and Mathematical Organization Theory},
  shortjournal = {Comput Math Organ Theory},
  issn = {1381-298X, 1572-9346},
  doi = {10.1007/s10588-021-09349-6},
  url = {https://link.springer.com/10.1007/s10588-021-09349-6},
  urldate = {2022-06-30},
  abstract = {The ground truth program used simulations as test beds for social science research methods. The simulations had known ground truth and were capable of producing large amounts of data. This allowed research teams to run experiments and ask questions of these simulations similar to social scientists studying real-world systems, and enabled robust evaluation of their causal inference, prediction, and prescription capabilities. We tested three hypotheses about research effectiveness using data from the ground truth program, specifically looking at the influence of complexity, causal understanding, and data collection on performance. We found some evidence that system complexity and causal understanding influenced research performance, but no evidence that data availability contributed. The ground truth program may be the first robust coupling of simulation test beds with an experimental framework capable of teasing out factors that determine the success of social science research.},
  langid = {english},
  keywords = {metascience,social science},
  annotation = {interest: 50},
  file = {/home/sam/Zotero/storage/IE4M5XKV/Naugle2022_Article_WhatCanSimulationTestBedsTeach.pdf}
}

@inproceedings{navarroleijaReproducibleContainers2020,
  title = {Reproducible {{Containers}}},
  booktitle = {Proceedings of the {{Twenty-Fifth International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}},
  author = {Navarro Leija, Omar S. and Shiptoski, Kelly and Scott, Ryan G. and Wang, Baojun and Renner, Nicholas and Newton, Ryan R. and Devietti, Joseph},
  date = {2020-03-13},
  series = {{{ASPLOS}} '20},
  pages = {167--182},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3373376.3378519},
  url = {https://doi.org/10.1145/3373376.3378519},
  urldate = {2023-01-04},
  abstract = {We describe the design and implementation of DetTrace, a reproducible container abstraction for Linux implemented in user space. All computation that occurs inside a DetTrace container is a pure function of the initial filesystem state of the container. Reproducible containers can be used for a variety of purposes, including replication for fault-tolerance, reproducible software builds and reproducible data analytics. We use DetTrace to achieve, in an automatic fashion, reproducibility for 12,130 Debian package builds, containing over 800 million lines of code, as well as bioinformatics and machine learning workflows. We show that, while software in each of these domains is initially irreproducible, DetTrace brings reproducibility without requiring any hardware, OS or application changes. DetTrace's performance is dictated by the frequency of system calls: IO-intensive software builds have an average overhead of 3.49x, while a compute-bound bioinformatics workflow is under 2\%.},
  isbn = {978-1-4503-7102-5},
  keywords = {reproducibility},
  file = {/home/sam/Zotero/storage/C35QFKAG/Navarro Leija et al. - 2020 - Reproducible Containers.pdf}
}

@misc{nelsonEnsuringFreeImmediate2022,
  title = {Ensuring {{Free}}, {{Immediate}}, and {{Equitable Access}} to {{Federally Funded Research}}},
  author = {Nelson, Alondra},
  date = {2022-08-25},
  organization = {Executive Office of the President, Office of Science and Technology Policy},
  keywords = {open data,project-acm-rep},
  file = {/home/sam/Zotero/storage/EYGNDNRM/Nelson - 2022 - Ensuring Free, Immediate, and Equitable Access to .pdf}
}

@article{nelsonIllustrisSimulationPublic2015,
  title = {The Illustris Simulation: {{Public}} Data Release},
  shorttitle = {The Illustris Simulation},
  author = {Nelson, D. and Pillepich, A. and Genel, S. and Vogelsberger, M. and Springel, V. and Torrey, P. and Rodriguez-Gomez, V. and Sijacki, D. and Snyder, G. F. and Griffen, B. and Marinacci, F. and Blecha, L. and Sales, L. and Xu, D. and Hernquist, L.},
  date = {2015-11-01},
  journaltitle = {Astronomy and Computing},
  shortjournal = {Astronomy and Computing},
  volume = {13},
  pages = {12--37},
  issn = {2213-1337},
  doi = {10.1016/j.ascom.2015.09.003},
  url = {https://www.sciencedirect.com/science/article/pii/S2213133715000864},
  urldate = {2022-04-11},
  abstract = {We present the full public release of all data from the Illustris simulation project. Illustris is a suite of large volume, cosmological hydrodynamical simulations run with the moving-mesh code Arepo and including a comprehensive set of physical models critical for following the formation and evolution of galaxies across cosmic time. Each simulates a volume of (106.5 Mpc)3 and self-consistently evolves five different types of resolution elements from a starting redshift of z=127 to the present day, z=0. These components are: dark matter particles, gas cells, passive gas tracers, stars and stellar wind particles, and supermassive black holes. This data release includes the snapshots at all 136 available redshifts, halo and subhalo catalogs at each snapshot, and two distinct merger trees. Six primary realizations of the Illustris volume are released, including the flagship Illustris-1 run. These include three resolution levels with the fiducial “full” baryonic physics model, and a dark matter only analog for each. In addition, we provide four distinct, high time resolution, smaller volume “subboxes”. The total data volume is ∼265 TB, including ∼800 full volume snapshots and ∼30,000 subbox snapshots. We describe the released data products as well as tools we have developed for their analysis. All data may be directly downloaded in its native HDF5 format. Additionally, we release a comprehensive, web-based API which allows programmatic access to search and data processing tasks. In both cases we provide example scripts and a getting-started guide in several languages: currently, IDL, Python, and Matlab. This paper addresses scientific issues relevant for the interpretation of the simulations, serves as a pointer to published and on-line documentation of the project, describes planned future additional data releases, and discusses technical aspects of the release.},
  langid = {english},
  keywords = {astrophysics,cosmological simulation,project-astrophysics,research software engineering},
  annotation = {interest: 70},
  file = {/home/sam/Zotero/storage/95LXX5HZ/Nelson et al. - 2015 - The illustris simulation Public data release.pdf}
}

@article{nelsonLetPublishFewer2012,
  title = {Let's {{Publish Fewer Papers}}},
  author = {Nelson, Leif D. and Simmons, Joseph P. and Simonsohn, Uri},
  date = {2012-07-01},
  journaltitle = {Psychological Inquiry},
  volume = {23},
  number = {3},
  pages = {291--293},
  publisher = {Routledge},
  issn = {1047-840X},
  doi = {10.1080/1047840X.2012.705245},
  url = {https://doi.org/10.1080/1047840X.2012.705245},
  urldate = {2022-09-06},
  keywords = {academic publishing,metascience},
  annotation = {interest: 86}
}

@article{neupaneCharacterizationLeptazolinesPolar2019,
  title = {Characterization of {{Leptazolines A}}–{{D}}, {{Polar Oxazolines}} from the {{Cyanobacterium Leptolyngbya}} Sp., {{Reveals}} a {{Glitch}} with the “{{Willoughby}}–{{Hoye}}” {{Scripts}} for {{Calculating NMR Chemical Shifts}}},
  author = {Neupane, Jayanti Bhandari and Neupane, Ram P. and Luo, Yuheng and Yoshida, Wesley Y. and Sun, Rui and Williams, Philip G.},
  date = {2019-10-18},
  journaltitle = {Organic Letters},
  shortjournal = {Org. Lett.},
  volume = {21},
  number = {20},
  pages = {8449--8453},
  issn = {1523-7060, 1523-7052},
  doi = {10.1021/acs.orglett.9b03216},
  url = {https://pubs.acs.org/doi/10.1021/acs.orglett.9b03216},
  urldate = {2022-05-26},
  abstract = {The bioactivity-guided examination of a Leptolyngbya sp. led to the isolation of leptazolines A–D (1–4), from the culture media, along with two degradation products (5 and 6). Density functional theory nuclear magnetic resonance calculations established the relative configurations of 1 and 2 and revealed that the calculated shifts depended on the operating system when using the “Willoughby–Hoye” Python scripts to streamline the processing of the output files, a previously unrecognized flaw that could lead to incorrect conclusions.},
  langid = {english},
  keywords = {internship-project,project-acm-rep,research software engineering,retraction},
  file = {/home/sam/Zotero/storage/QWATD4UA/acs.orglett.9b03216.pdf}
}

@inproceedings{newmanMyExperimentOntologyEResearch2009,
  title = {{{myExperiment}}: {{An}} Ontology for e-{{Research}}},
  booktitle = {Semantic {{Web Applications}} in {{Scientific Discourse}}},
  author = {Newman, David and Bechhofer, Sean and De Roure, David},
  date = {2009-10-24},
  publisher = {Web \& Internet Science},
  location = {Washington, D.C., United States},
  url = {https://eprints.soton.ac.uk/267787/},
  abstract = {myExperiment describes itself as a "Social Virtual Research Environment" that provides the ability to share Research Objects (ROs) over a social infrastructure to facilitate actioning of research. The myExperiment Ontology is a logical representation of the data model used by this environment, allowing its data to be published in a standard RDF format, whilst providing a generic extensible framework that can be reused by similar projects. ROs are data structures designed to semantically enhance research publications by capturing and preserving the research method so that it can be reproduced in the future. This paper provides some motivation for an RO specification and briefly considers how existing domain-specifific ontologies might be integrated. It concludes by discussing the future direction of the myExperiment Ontology and how it will best support these ROs.},
  eventtitle = {Semantic {{Web Applications}} in {{Scientific Discourse}}},
  keywords = {provenance,workflow managers}
}

@article{nigrovicLymeVaccineCautionary2007,
  title = {The {{Lyme}} Vaccine: A Cautionary Tale},
  shorttitle = {The {{Lyme}} Vaccine},
  author = {Nigrovic, L. E. and Thompson, K. M.},
  date = {2007-01},
  journaltitle = {Epidemiology and Infection},
  shortjournal = {Epidemiol Infect},
  volume = {135},
  number = {1},
  eprint = {16893489},
  eprinttype = {pmid},
  pages = {1--8},
  issn = {0950-2688},
  doi = {10.1017/S0950268806007096},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2870557/},
  urldate = {2022-08-26},
  abstract = {People living in endemic areas acquire Lyme disease from the bite of an infected tick. This infection, when diagnosed and treated early in its course, usually responds well to antibiotic therapy. A minority of patients develops more serious disease, particularly after a delay in diagnosis or therapy, and sometimes chronic neurological, cardiac, or rheumatological manifestations. In 1998, the FDA approved a new recombinant Lyme vaccine, LYMErix™, which reduced new infections in vaccinated adults by nearly 80\%. Just 3 years later, the manufacturer voluntarily withdrew its product from the market amidst media coverage, fears of vaccine side-effects, and declining sales. This paper reviews these events in detail and focuses on the public communication of risks and benefits of the Lyme vaccine and important lessons learned.},
  pmcid = {PMC2870557},
  keywords = {public health},
  file = {/home/sam/Zotero/storage/AAWXX3EP/NIGROVIC and THOMPSON - 2007 - The Lyme vaccine a cautionary tale.pdf}
}

@article{nikolicMinimalRecomputationExploratory2018,
  title = {Minimal Re-Computation for Exploratory Data Analysis in Astronomy},
  author = {Nikolic, B. and Small, D. and Kettenis, M.},
  date = {2018-10-01},
  journaltitle = {Astronomy and Computing},
  shortjournal = {Astronomy and Computing},
  volume = {25},
  pages = {133--138},
  issn = {2213-1337},
  doi = {10.1016/j.ascom.2018.09.003},
  url = {https://www.sciencedirect.com/science/article/pii/S2213133718300246},
  urldate = {2022-04-12},
  abstract = {We present a technique to automatically minimise the re-computation when a data analysis program is iteratively changed, or added to, as is often the case in exploratory data analysis in astronomy. A typical example is flagging and calibration of demanding or unusual observations where visual inspection suggests improvement to the processing strategy. The technique is based on memoization and referentially transparent tasks. We describe the implementation of this technique for the CASA radio astronomy data reduction package. We also propose a technique for optimising efficiency of storage of memoized intermediate data products using copy-on-write and block level de-duplication and measure their practical efficiency. We find that the minimal recomputation technique improves the efficiency of data analysis while reducing the possibility for user error and improving the reproducibility of the final result. It also aids exploratory data analysis on batch-schedule cluster computer systems.},
  langid = {english},
  keywords = {research software engineering,software engineering},
  annotation = {interest: 95},
  file = {/home/sam/Zotero/storage/AHRY49WY/Nikolic et al. - 2018 - Minimal re-computation for exploratory data analys.pdf;/home/sam/Zotero/storage/8483SGX7/S2213133718300246.html}
}

@online{nissenWhatBadJulia2023,
  title = {What's Bad about {{Julia}}?},
  author = {Nissen, Jakob Nybo},
  date = {2023-02-24},
  url = {https://viralinstruction.com/posts/badjulia/},
  urldate = {2023-03-09},
  abstract = {Julia is my favorite programming language. More than that actually, perhaps I'm a bit of a fanboy. Sometimes, though, the ceaseless celebration of Julia by fans like me can be a bit too much. It papers over legitimate problems in the language, hindering progress. And from an outsider perspective, it's not only insufferable (I would guess), but also obfuscates the true pros and cons of the language. Learning why you may not want to choose to use a tool is just as important as learning why you may. This post is about all the major disadvantages of Julia. Some of it will just be rants about things I particularly don't like - hopefully they will be informative, too. A post like this is necessarily subjective. For example, some people believe Julia's lack of a Java-esque OOP is a design mistake. I don't, so the post won't go into that.},
  organization = {virtualinstruction},
  keywords = {programming languages}
}

@online{nissenWhatGreatJulia2023,
  title = {What's Great about {{Julia}}?},
  author = {Nissen, Jakob Nybo},
  date = {2023-02-24},
  url = {https://viralinstruction.com/posts/goodjulia/},
  urldate = {2023-03-09},
  abstract = {The first post on this blog was "What's bad about Julia" - a collection of the worst things about my favourite language, which turned out to be quite the Hacker News bait. The most common responses I got was along the lines of: "If Julia has all these flaws, why not just use another language?". At the time, I just said that despite its flaws, Julia was still amazing, that it would take another 4,000 word post to elaborate on why, and then I left it at that. Recently I've been thinking a lot about one of Julia's major drawbacks, and have been drafting up a post that goes in depth about the subject. But honestly, posting another verbose criticism of Julia would risk giving a misleadingly bad impression of my experience with the lovely language, even if I bracket a wall of criticism with a quick endorsement. After all, I've chosen to use the language for my daily work about two years ago, and I don't regret that choice in the slightest. Now is the right time for that 4,000 word post on the best parts of Julia.},
  organization = {virtualinstruction},
  keywords = {programming languages},
  file = {/home/sam/Zotero/storage/WZTLZBJN/goodjulia.html}
}

@inproceedings{noonanGhostsDepartedProofs2018,
  title = {Ghosts of Departed Proofs (Functional Pearl)},
  booktitle = {Proceedings of the 11th {{ACM SIGPLAN International Symposium}} on {{Haskell}}},
  author = {Noonan, Matt},
  date = {2018-09-17},
  series = {Haskell 2018},
  pages = {119--131},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3242744.3242755},
  url = {https://dl.acm.org/doi/10.1145/3242744.3242755},
  urldate = {2024-01-11},
  abstract = {Library authors often are faced with a design choice: should a function with preconditions be implemented as a partial function, or by returning a failure condition on incorrect use? Neither option is ideal. Partial functions lead to frustrating run-time errors. Failure conditions must be checked at the use-site, placing an unfair tax on the users who have ensured that the function's preconditions were correctly met. In this paper, we introduce an API design concept called ``ghosts of departed proofs'' based on the following observation: sophisticated preconditions can be encoded in Haskell's type system with no run-time overhead, by using proofs that inhabit phantom type parameters attached to newtype wrappers. The user expresses correctness arguments by constructing proofs to inhabit these phantom types. Critically, this technique allows the library user to decide when and how to validate that the API's preconditions are met. The ``ghosts of departed proofs'' approach to API design can achieve many of the benefits of dependent types and refinement types, yet only requires some minor and well-understood extensions to Haskell 2010. We demonstrate the utility of this approach through a series of case studies, showing how to enforce novel invariants for lists, maps, graphs, shared memory regions, and more.},
  isbn = {978-1-4503-5835-4},
  keywords = {industry practices,language engineering},
  file = {/home/sam/Zotero/storage/D5JI3MT4/Noonan - 2018 - Ghosts of departed proofs (functional pearl).pdf}
}

@inproceedings{norgaardWhatUsabilityEvaluators2006,
  title = {What Do Usability Evaluators Do in Practice?: An Explorative Study of Think-Aloud Testing},
  shorttitle = {What Do Usability Evaluators Do in Practice?},
  booktitle = {Proceedings of the 6th {{ACM}} Conference on {{Designing Interactive}} Systems  - {{DIS}} '06},
  author = {Nørgaard, Mie and Hornbæk, Kasper},
  date = {2006},
  pages = {209},
  publisher = {ACM Press},
  location = {University Park, PA, USA},
  doi = {10.1145/1142405.1142439},
  url = {http://portal.acm.org/citation.cfm?doid=1142405.1142439},
  urldate = {2022-06-13},
  abstract = {Think-aloud testing is a widely employed usability evaluation method, yet its use in practice is rarely studied. We report an explorative study of 14 think-aloud sessions, the audio recordings of which were examined in detail. The study shows that immediate analysis of observations made in the think-aloud sessions is done only sporadically, if at all. When testing, evaluators seem to seek confirmation of problems that they are already aware of. During testing, evaluators often ask users about their expectations and about hypothetical situations, rather than about experienced problems. In addition, evaluators learn much about the usability of the tested system but little about its utility. The study shows how practical realities rarely discussed in the literature on usability evaluation influence sessions. We discuss implications for usability researchers and professionals, including techniques for fast-paced analysis and tools for capturing observations during sessions.},
  eventtitle = {The 6th {{ACM}} Conference},
  isbn = {978-1-59593-367-6},
  langid = {english},
  keywords = {internship-project,usability},
  annotation = {interest: 80},
  file = {/home/sam/Zotero/storage/JP9BIKMJ/1142405.1142439.pdf}
}

@online{nosekStrategyCultureChange,
  title = {Strategy for {{Culture Change}}},
  author = {Nosek, Brian},
  url = {https://www.cos.io/blog/strategy-for-culture-change},
  urldate = {2022-09-06},
  abstract = {Strategy for Culture Change},
  langid = {english},
  keywords = {research software engineering},
  annotation = {interest: 89},
  file = {/home/sam/Zotero/storage/TKBTZ23J/strategy-for-culture-change.html}
}

@article{nowakowskiCollageAuthoringEnvironment2011,
  title = {The {{Collage Authoring Environment}}},
  author = {Nowakowski, Piotr and Ciepiela, Eryk and Harężlak, Daniel and Kocot, Joanna and Kasztelnik, Marek and Bartyński, Tomasz and Meizner, Jan and Dyk, Grzegorz and Malawski, Maciej},
  date = {2011},
  journaltitle = {Procedia Computer Science},
  shortjournal = {Procedia Computer Science},
  volume = {4},
  pages = {608--617},
  issn = {18770509},
  doi = {10.1016/j.procs.2011.04.064},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050911001220},
  urldate = {2024-10-04},
  langid = {english},
  file = {/home/sam/Zotero/storage/6PMQPAGV/Nowakowski et al. - 2011 - The Collage Authoring Environment.pdf}
}

@article{nuzzoHowScientistsFool2015,
  title = {How Scientists Fool Themselves – and How They Can Stop},
  author = {Nuzzo, Regina},
  date = {2015-10-01},
  journaltitle = {Nature},
  volume = {526},
  number = {7572},
  pages = {182--185},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/526182a},
  url = {https://www.nature.com/articles/526182a},
  urldate = {2022-09-27},
  abstract = {Humans are remarkably good at self-deception. But growing concern about reproducibility is driving many researchers to seek ways to fight their own worst instincts.},
  issue = {7572},
  langid = {english},
  keywords = {metascience},
  annotation = {interest: 74},
  file = {/home/sam/Zotero/storage/KA7Y2XD8/Nuzzo - 2015 - How scientists fool themselves – and how they can .pdf;/home/sam/Zotero/storage/8SF66E25/526182a.html}
}

@article{nuzzoScientificMethodStatistical2014,
  title = {Scientific Method: {{Statistical}} Errors},
  shorttitle = {Scientific Method},
  author = {Nuzzo, Regina},
  date = {2014-02-13},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {506},
  number = {7487},
  pages = {150--152},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/506150a},
  url = {https://www.nature.com/articles/506150a},
  urldate = {2022-07-28},
  abstract = {P values, the 'gold standard' of statistical validity, are not as reliable as many scientists assume.},
  langid = {english},
  keywords = {metascience,statistics},
  annotation = {interest: 82},
  file = {/home/sam/Zotero/storage/SBAFYTLK/Nuzzo - 2014 - Scientific method Statistical errors.pdf;/home/sam/Zotero/storage/3X8W6C9F/506150a.html}
}

@online{ocallahanEngineeringRecordReplay2017,
  title = {Engineering {{Record And Replay For Deployability}}: {{Extended Technical Report}}},
  shorttitle = {Engineering {{Record And Replay For Deployability}}},
  author = {O'Callahan, Robert and Jones, Chris and Froyd, Nathan and Huey, Kyle and Noll, Albert and Partush, Nimrod},
  date = {2017-05-16},
  eprint = {1705.05937},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1705.05937},
  url = {http://arxiv.org/abs/1705.05937},
  urldate = {2024-01-26},
  abstract = {The ability to record and replay program executions with low overhead enables many applications, such as reverse-execution debugging, debugging of hard-to-reproduce test failures, and "black box" forensic analysis of failures in deployed systems. Existing record-and-replay approaches limit deployability by recording an entire virtual machine (heavyweight), modifying the OS kernel (adding deployment and maintenance costs), requiring pervasive code instrumentation (imposing significant performance and complexity overhead), or modifying compilers and runtime systems (limiting generality). We investigated whether it is possible to build a practical record-and-replay system avoiding all these issues. The answer turns out to be yes - if the CPU and operating system meet certain non-obvious constraints. Fortunately modern Intel CPUs, Linux kernels and user-space frameworks do meet these constraints, although this has only become true recently. With some novel optimizations, our system 'rr' records and replays real-world low-parallelism workloads with low overhead, with an entirely user-space implementation, using stock hardware, compilers, runtimes and operating systems. "rr" forms the basis of an open-source reverse-execution debugger seeing significant use in practice. We present the design and implementation of 'rr', describe its performance on a variety of workloads, and identify constraints on hardware and operating system design required to support our approach.},
  pubstate = {prepublished},
  keywords = {project-provenance-pp,reproducibility},
  file = {/home/sam/Zotero/storage/Z8KUXE23/O'Callahan et al. - 2017 - Engineering Record And Replay For Deployability E.pdf;/home/sam/Zotero/storage/TVC4MPID/1705.html}
}

@online{ocallahanRrdebuggerRelatedWork,
  title = {Rr-Debugger: {{Related}} Work},
  author = {O'Callahan, Robert},
  url = {https://github.com/rr-debugger/rr/wiki/Related-work}
}

@article{oconnorDockstoreEnablingModular2017,
  title = {The {{Dockstore}}: Enabling Modular, Community-Focused Sharing of {{Docker-based}} Genomics Tools and Workflows},
  shorttitle = {The {{Dockstore}}},
  author = {O'Connor, Brian D. and Yuen, Denis and Chung, Vincent and Duncan, Andrew G. and Liu, Xiang Kun and Patricia, Janice and Paten, Benedict and Stein, Lincoln and Ferretti, Vincent},
  date = {2017-01-18},
  journaltitle = {F1000Research},
  shortjournal = {F1000Res},
  volume = {6},
  pages = {52},
  issn = {2046-1402},
  doi = {10.12688/f1000research.10137.1},
  url = {https://f1000research.com/articles/6-52/v1},
  urldate = {2022-10-31},
  abstract = {As genomic datasets continue to grow, the feasibility of downloading data to a local organization and running analysis on a traditional compute environment is becoming increasingly problematic. Current large-scale projects, such as the ICGC PanCancer Analysis of Whole Genomes (PCAWG), the Data Platform for the U.S. Precision Medicine Initiative, and the NIH Big Data to Knowledge Center for Translational Genomics, are using cloud-based infrastructure to both host and perform analysis across large data sets. In PCAWG, over 5,800 whole human genomes were aligned and variant called across 14 cloud and HPC environments; the processed data was then made available on the cloud for further analysis and sharing. If run locally, an operation at this scale would have monopolized a typical academic data centre for many months, and would have presented major challenges for data storage and distribution. However, this scale is increasingly typical for genomics projects and necessitates a rethink of how analytical tools are packaged and moved to the data. For PCAWG, we embraced the use of highly portable Docker images for encapsulating and sharing complex alignment and variant calling workflows across highly variable environments. While successful, this endeavor revealed a limitation in Docker containers, namely the lack of a standardized way to describe and execute the tools encapsulated inside the container. As a result, we created the Dockstore (               https://dockstore.org               ), a project that brings together Docker images with standardized, machine-readable ways of describing and running the tools contained within. This service greatly improves the sharing and reuse of genomics tools and promotes interoperability with similar projects through emerging web service standards developed by the Global Alliance for Genomics and Health (GA4GH).},
  langid = {english},
  keywords = {project-acm-rep,reproducibility engineering,workflow managers},
  file = {/home/sam/Zotero/storage/A2EM7HZ5/O'Connor et al. - 2017 - The Dockstore enabling modular, community-focused.pdf}
}

@inproceedings{odenLessonsLearnedComparing2020,
  title = {Lessons Learned from Comparing {{C-CUDA}} and {{Python-Numba}} for {{GPU-Computing}}},
  booktitle = {2020 28th {{Euromicro International Conference}} on {{Parallel}}, {{Distributed}} and {{Network-Based Processing}} ({{PDP}})},
  author = {Oden, Lena},
  date = {2020-03},
  pages = {216--223},
  issn = {2377-5750},
  doi = {10.1109/PDP50117.2020.00041},
  abstract = {Python as programming language is increasingly gaining importance, especially in data science, scientific, and parallel programming. It is faster and easier to learn than classical programming languages such as C. However, usability often comes at the cost of performance and applications written in Python are considered to be much slower than applications written in C or FORTRAN. Further, it does not allow the usage of GPUs-besides of pre-compiled libraries.However, the Numba package promises performance similar to C code for compute intensive parts of a Python application and it supports CUDA, which allows the use of GPUs inside a Python application.In this paper we compare the performance of Numba-CUDA and C -CUDA for different kinds of applications. For compute intensive benchmarks, the performance of the Numba version only reaches between 50\% and 85\% performance of the CCUDA version, despite the reduction operation, where the Numba version outperforms CUDA. Analyzing the PTX code and CUDA performance counters revealed that index-calculation is one limiting factor in Numba. Another problem is the type interference for single precision computations, as some values are computed in double precision. By optimizing this within the Numba package, the performance of Numba improves. However, C-CUDA applications still outperform the Numba versions. Further analysis with the CloverLeav Mini App shows that Numba performance further decreases for applications with multiple different compute kernels. The non-GPU part slows down these applications, due to the slow Python interpreter. This leads to a worse GPU utilization.Today Python is widely used in industry and academia and has been the first choice of coding languages among software programmers in the last years. Currently, according to the TIOBE index [5], it is the 3rd most popular programming language and the number one in IEEE Spectrum's fifth annual interactive ranking of the top programming languages [4]. One reason for this is that is easier to learn than classical programming languages like C. However, the other reason is the increasing popularity of Data Science, where Python is the most used language. A collection of libraries such as NumPy [22], and Matplotlib [1] or Scipy [8] provide a rich set of functions for scientific computing [16]. Packages like Dask [19], PyCompss [21] and MPI for Python [6] allow running Python applications on large, parallel machines, promising high performance. However, the performance of Python is considered slow compared to compiled languages such as C, C++, and FORTRAN, especially for heavy computations. In recent years, more and more tools have been developed to counter this prejudice. Numpy [22], for example, uses C-like arrays to store data and offers fast functions implemented in C to speed up calculations. The CuPy [14] package provides a similar set of functions, but these functions are implemented for GPUs using CUDA. The SciPy library is based on NumPy and provides a rich set on functionalities for scientific computing. Still, the high performance of these libraries is provided by the underling C-implementations. Internally, they use libraries like OpenBlas or IntelMKL to reach high performance and therefore, they are limited by the functions which are provided by theses libraries. Therefore, a performance problem always arises when the required functionality is not implemented within these libraries. In this case, the application falls back to the Python interpreter. Compared to "bare metal" code, interpreted code is slow. In addition, in Python it is not possible to use GPUs or other accelerators directly, as the Python interpreter cannot execute code on these machines. Therefore, the usage is only possible with precompiled libraries. To overcome this limitation, different approaches where developed to mix C, CUDA or OpenCL with Python. Cython [2] allows integrating C-code in Python applications to improve performance of critical sections. It also allows an easy development of wrappers for C-libraries. Similar, packages such as PyCuda and PyOpenCL [9] support wrappers for CUDA or OpenCL code within a Python script. Both approaches require the mixture of different programming languages.Numba [10] follows a different approach. Instead of merging C/CUDA code with Python, it allows the development of efficient applications for both, CPUs and GPUs in Python style. When a Python script using Numba is executed, marked functions are compiled just-in-time (JIT) using the LLVM framework. Using Python for GPU programming can mean a considerable simplification in the development of parallel applications.But often a simplification of comes at the expense of performance, and one expects a performance loss from Python compared to pure C code. In this paper, we want to understand the differences between native C-CUDA code and CUDA-code written in Python with Numba. We also want to share some basic tips how to improve the performance of applications written in Numba.We will first analyse a few micro benchmarks in detail. We are using these simple benchmarks, as it is easier to understand the differences with small code examples. We will use the collected information to derive some optimization for Numba. Finally, we evaluate and compare the performance of a more application like mini-app, written in C-CUDA and Numba accelerated Python. We will evaluate if our insights from the microbenchmarks to real applications.},
  eventtitle = {2020 28th {{Euromicro International Conference}} on {{Parallel}}, {{Distributed}} and {{Network-Based Processing}} ({{PDP}})},
  keywords = {performance engineering},
  annotation = {interest: 71},
  file = {/home/sam/Zotero/storage/L44F22LB/9092407.html}
}

@article{oliphantPythonScientificComputing2007,
  title = {Python for {{Scientific Computing}}},
  author = {Oliphant, Travis E.},
  date = {2007-05},
  journaltitle = {Computing in Science \& Engineering},
  volume = {9},
  number = {3},
  pages = {10--20},
  issn = {1558-366X},
  doi = {10.1109/MCSE.2007.58},
  url = {https://ieeexplore.ieee.org/abstract/document/4160250},
  urldate = {2024-02-23},
  abstract = {Python is an excellent "steering" language for scientific codes written in other languages. However, with additional basic tools, Python transforms into a high-level language suited for scientific and engineering code that's often fast enough to be immediately useful but also flexible enough to be sped up with additional extensions.},
  eventtitle = {Computing in {{Science}} \& {{Engineering}}},
  keywords = {project-repro-py,research software engineering},
  file = {/home/sam/Zotero/storage/3B26SAKA/Oliphant - 2007 - Python for Scientific Computing.pdf;/home/sam/Zotero/storage/6MC97EID/4160250.html}
}

@article{oliveiraProvenanceAnalyticsWorkflowBased2018,
  title = {Provenance {{Analytics}} for {{Workflow-Based Computational Experiments}}: {{A Survey}}},
  shorttitle = {Provenance {{Analytics}} for {{Workflow-Based Computational Experiments}}},
  author = {Oliveira, Wellington and Oliveira, Daniel De and Braganholo, Vanessa},
  date = {2018-05-23},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {51},
  number = {3},
  pages = {53:1--53:25},
  issn = {0360-0300},
  doi = {10.1145/3184900},
  url = {https://doi.org/10.1145/3184900},
  urldate = {2023-02-23},
  abstract = {Until not long ago, manually capturing and storing provenance from scientific experiments were constant concerns for scientists. With the advent of computational experiments (modeled as scientific workflows) and Scientific Workflow Management Systems, produced and consumed data, as well as the provenance of a given experiment, are automatically managed, so provenance capturing and storing in such a context is no longer a major concern. Similarly to several existing big data problems, the bottom line is now on how to analyze the large amounts of provenance data generated by workflow executions and how to be able to extract useful knowledge of this data. In this context, this article surveys the current state of the art on provenance analytics by presenting the key initiatives that have been taken to support provenance data analysis. We also contribute by proposing a taxonomy to classify elements related to provenance analytics.},
  keywords = {project-provenance-pp,provenance},
  annotation = {interest: 95},
  file = {/home/sam/Zotero/storage/IT8P4NF7/Oliveira et al. - 2018 - Provenance Analytics for Workflow-Based Computatio.pdf}
}

@inproceedings{olmsted-hawalaThinkaloudProtocolsComparison2010,
  title = {Think-Aloud Protocols: A Comparison of Three Think-Aloud Protocols for Use in Testing Data-Dissemination Web Sites for Usability},
  shorttitle = {Think-Aloud Protocols},
  booktitle = {Proceedings of the 28th International Conference on {{Human}} Factors in Computing Systems - {{CHI}} '10},
  author = {Olmsted-Hawala, Erica L. and Murphy, Elizabeth D. and Hawala, Sam and Ashenfelter, Kathleen T.},
  date = {2010},
  pages = {2381},
  publisher = {ACM Press},
  location = {Atlanta, Georgia, USA},
  doi = {10.1145/1753326.1753685},
  url = {http://portal.acm.org/citation.cfm?doid=1753326.1753685},
  urldate = {2022-06-13},
  eventtitle = {The 28th International Conference},
  isbn = {978-1-60558-929-9},
  langid = {english},
  keywords = {internship-project,usability},
  annotation = {interest: 75}
}

@online{omorainWhyWeRe2015,
  title = {Why We’re No Longer Using {{Core}}.Typed},
  author = {O'Morain, Marc},
  date = {2015-09-24T12:15:53Z},
  url = {https://archive.is/KiBXb},
  urldate = {2024-01-03},
  organization = {The Circle Blog},
  keywords = {industry practices,type checking}
}

@article{onionsSubhaloesGoingNotts2012,
  title = {Subhaloes Going {{Notts}}: The Subhalo-Finder Comparison Project: {{Subhalo-finder}} Comparison},
  shorttitle = {Subhaloes Going {{Notts}}},
  author = {Onions, Julian and Knebe, Alexander and Pearce, Frazer R. and Muldrew, Stuart I. and Lux, Hanni and Knollmann, Steffen R. and Ascasibar, Yago and Behroozi, Peter and Elahi, Pascal and Han, Jiaxin and Maciejewski, Michal and Merchán, Manuel E. and Neyrinck, Mark and Ruiz, Andrés N. and Sgró, Mario A. and Springel, Volker and Tweed, Dylan},
  date = {2012-06-21},
  journaltitle = {Monthly Notices of the Royal Astronomical Society},
  volume = {423},
  number = {2},
  pages = {1200--1214},
  issn = {00358711},
  doi = {10.1111/j.1365-2966.2012.20947.x},
  url = {https://academic.oup.com/mnras/article-lookup/doi/10.1111/j.1365-2966.2012.20947.x},
  urldate = {2022-07-22},
  abstract = {We present a detailed comparison of the substructure properties of a single Milky Way sized dark matter halo from the Aquarius suite at five different resolutions, as identified by a variety of different (sub)halo finders for simulations of cosmic structure formation. These finders span a wide range of techniques and methodologies to extract and quantify substructures within a larger non-homogeneous background density (e.g. a host halo). This includes real-space-, phase-space-, velocity-space- and time-space-based finders, as well as finders employing a Voronoi tessellation, Friends-of-Friends techniques or refined meshes as the starting point for locating substructure. A common post-processing pipeline was used to uniformly analyse the particle lists provided by each finder. We extract quantitative and comparable measures for the subhaloes, primarily focusing on mass and the peak of the rotation curve for this particular study. We find that all of the finders agree extremely well in the presence and location of substructure and even for properties relating to the inner part of the subhalo (e.g. the maximum value of the rotation curve). For properties that rely on particles near the outer edge of the subhalo the agreement is at around the 20 per cent level. We find that the basic properties (mass and maximum circular velocity) of a subhalo can be reliably recovered if the subhalo contains more than 100 particles although its presence can be reliably inferred for a lower particle number limit of 20. We finally note that the logarithmic slope of the subhalo cumulative number count is remarkably consistent and {$<$}1 for all the finders that reached high resolution. If correct, this would indicate that the larger and more massive, respectively, substructures are the most dynamically interesting and that higher levels of the (sub)subhalo hierarchy become progressively less important.},
  langid = {english},
  keywords = {astrophysics,dark matter halos}
}

@online{ORESpecificationAbstract2008,
  title = {{{ORE Specification}} - {{Abstract Data Model}}},
  date = {2008-10-17},
  url = {https://www.openarchives.org/ore/1.0/datamodel},
  abstract = {Open Archives Initiative Object Reuse and Exchange (OAI-ORE) defines standards for the description and exchange of aggregations of Web resources. This document describes the abstract data model that is the foundation for these standards. This model is conformant with the Architecture of the World Wide Web [Web Architecture] and leverages concepts from the Semantic Web including RDF descriptions [RDF Concepts] and Linked Data [Linked Data Tutorial]. This specification is one of several documents comprising the OAI-ORE specifications and user guides. The intended audience for this document is implementers that have an understanding of Semantic Web Concepts. Readers that want a high-level understanding of the motivation for ORE, and of the solution it provides should read the ORE Primer.},
  keywords = {provenance,semantic web}
}

@article{osheaIntroducingEnzoAMR2004,
  title = {Introducing {{Enzo}}, an {{AMR Cosmology Application}}},
  author = {O'Shea, Brian W. and Bryan, Greg and Bordner, James and Norman, Michael L. and Abel, Tom and Harkness, Robert and Kritsuk, Alexei},
  date = {2004-03},
  journaltitle = {arXiv e-prints},
  pages = {astro-ph/0403044},
  url = {https://ui.adsabs.harvard.edu/abs/2004astro.ph..3044O/abstract},
  urldate = {2022-04-11},
  abstract = {In this paper we introduce Enzo, a 3D MPI-parallel Eulerian block-structured adaptive mesh refinement cosmology code. Enzo is designed to simulate cosmological structure formation, but can also be used to simulate a wide range of astrophysical situations. Enzo solves dark matter N-body dynamics using the particle-mesh technique. The Poisson equation is solved using a combination of fast fourier transform (on a periodic root grid) and multigrid techniques (on non-periodic subgrids). Euler's equations of hydrodynamics are solved using a modified version of the piecewise parabolic method. Several additional physics packages are implemented in the code, including several varieties of radiative cooling, a metagalactic ultraviolet background, and prescriptions for star formation and feedback. We also show results illustrating properties of the adaptive mesh portion of the code. Enzo is publicly available and can be downloaded at http://cosmos.ucsd.edu/enzo/ .},
  langid = {english},
  keywords = {astrophysics,cosmological simulation,project-astrophysics},
  file = {/home/sam/Zotero/storage/X84V55KX/abstract.html}
}

@article{oswaldLargeInequalityInternational2020,
  title = {Large Inequality in International and Intranational Energy Footprints between Income Groups and across Consumption Categories},
  author = {Oswald, Yannick and Owen, Anne and Steinberger, Julia K.},
  date = {2020-03},
  journaltitle = {Nature Energy},
  shortjournal = {Nat Energy},
  volume = {5},
  number = {3},
  pages = {231--239},
  publisher = {Nature Publishing Group},
  issn = {2058-7546},
  doi = {10.1038/s41560-020-0579-8},
  url = {https://www.nature.com/articles/s41560-020-0579-8},
  urldate = {2024-09-10},
  abstract = {Inequality in energy consumption, both direct and indirect, affects the distribution of benefits that result from energy use. Detailed measures of this inequality are required to ensure an equitable and just energy transition. Here we calculate final energy footprints; that is, the energy embodied in goods and services across income classes in 86 countries, both highly industrialized and developing. We analyse the energy intensity of goods and services used by different income groups, as well as their income elasticity of demand. We find that inequality in the distribution of energy footprints varies across different goods and services. Energy-intensive goods tend to be more elastic, leading to higher energy footprints of high-income individuals. Our results consequently expose large inequality in international energy footprints: the consumption share of the bottom half of the population is less than 20\% of final energy footprints, which in turn is less than what the top 5\% consume.},
  langid = {english},
  keywords = {economics,Economics,energy,Energy and society,Energy science and technology,Environmental social sciences},
  file = {/home/sam/Zotero/storage/M945Y4CL/Oswald et al. - 2020 - Large inequality in international and intranationa.pdf}
}

@article{otterRoadmapComputationPersistent2017,
  title = {A Roadmap for the Computation of Persistent Homology},
  author = {Otter, Nina and Porter, Mason A and Tillmann, Ulrike and Grindrod, Peter and Harrington, Heather A},
  date = {2017},
  journaltitle = {Epj Data Science},
  shortjournal = {EPJ Data Sci},
  volume = {6},
  number = {1},
  eprint = {32025466},
  eprinttype = {pmid},
  pages = {17},
  issn = {2193-1127},
  doi = {10.1140/epjds/s13688-017-0109-5},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6979512/},
  urldate = {2022-09-06},
  abstract = {Persistent homology (PH) is a method used in topological data analysis (TDA) to study qualitative features of data that persist across multiple scales. It is robust to perturbations of input data, independent of dimensions and coordinates, and provides a compact representation of the qualitative features of the input. The computation of PH is an open area with numerous important and fascinating challenges. The field of PH computation is evolving rapidly, and new algorithms and software implementations are being updated and released at a rapid pace. The purposes of our article are to (1) introduce theory and computational methods for PH to a broad range of computational scientists and (2) provide benchmarks of state-of-the-art implementations for the computation of PH. We give a friendly introduction to PH, navigate the pipeline for the computation of PH with an eye towards applications, and use a range of synthetic and real-world data sets to evaluate currently available open-source implementations for the computation of PH. Based on our benchmarking, we indicate which algorithms and implementations are best suited to different types of data sets. In an accompanying tutorial, we provide guidelines for the computation of PH. We make publicly available all scripts that we wrote for the tutorial, and we make available the processed version of the data sets used in the benchmarking.},
  pmcid = {PMC6979512},
  keywords = {topological data analysis},
  annotation = {interest: 61},
  file = {/home/sam/Zotero/storage/TC4J2I3Z/Otter et al. - 2017 - A roadmap for the computation of persistent homolo.pdf}
}

@article{paineWhoHasPlots2017,
  title = {"{{Who Has Plots}}?": {{Contextualizing Scientific Software}}, {{Practice}}, and {{Visualizations}}},
  shorttitle = {"{{Who Has Plots}}?},
  author = {Paine, Drew and Lee, Charlotte P.},
  date = {2017-12-06},
  journaltitle = {Proceedings of the ACM on Human-Computer Interaction},
  shortjournal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {1},
  pages = {85:1--85:21},
  doi = {10.1145/3134720},
  url = {https://doi.org/10.1145/3134720},
  urldate = {2022-08-25},
  abstract = {Software is an integral element of the work of science yet it is not commonly an object of inquiry in studies of scientific infrastructures. This paper presents findings from an ethnographic study of a cosmology group's collaborative scientific software production. We demonstrate how these cosmologists use plots to simultaneously test their software and analyze data while interrogating multiple layers of infrastructural components. We broaden perspectives on scientific software development using a sociotechnical, software studies lens to examine this work of scientific discovery as a creative and embodied, yet exacting and methodical, activity that requires a 'human in the loop'. We offer a new reading of scientific software practices to convey how creating scientific software is often really the act of doing science itself--an intervention we believe is necessary to more successfully support scientific software sharing and infrastructure production.},
  issue = {CSCW},
  keywords = {metascience},
  annotation = {interest: 82},
  file = {/home/sam/Zotero/storage/YZDBTUSI/Paine and Lee - 2017 - Who Has Plots Contextualizing Scientific Softw.pdf}
}

@inproceedings{parisMerkleHashGrids2020,
  title = {Merkle {{Hash Grids Instead}} of {{Merkle Trees}}},
  booktitle = {2020 28th {{International Symposium}} on {{Modeling}}, {{Analysis}}, and {{Simulation}} of {{Computer}} and {{Telecommunication Systems}} ({{MASCOTS}})},
  author = {Pâris, Jehan-François and Schwarz, Thomas},
  date = {2020-11},
  pages = {1--8},
  issn = {2375-0227},
  doi = {10.1109/MASCOTS50786.2020.9285942},
  abstract = {Merkle grids are a new data organization that replicates the functionality of Merkle trees while reducing their transmission and storage costs by up to 50 percent. All Merkle grids organize the objects whose conformity they monitor in a square array. They add row and column hashes to it such that (a) all row hashes contain the hash of the concatenation of the hashes of all the objects in their respective row and (b) all column hashes contain the hash of the concatenation of the hashes of all the objects in their respective column. In addition, a single signed master hash contains the hash of the concatenation of all row and column hashes. Extended Merkle grids add two auxiliary Merkle trees to speed up searches among both row hashes and column hashes. While both basic and extended Merkle grids perform authentication of all blocks better than Merkle trees, only extended Merkle grids can locate individual non-conforming objects or authenticate a single non-conforming object as fast as Merkle trees.},
  eventtitle = {2020 28th {{International Symposium}} on {{Modeling}}, {{Analysis}}, and {{Simulation}} of {{Computer}} and {{Telecommunication Systems}} ({{MASCOTS}})},
  keywords = {algorithms,distributed systems},
  file = {/home/sam/Zotero/storage/T4BSG8K2/Pâris and Schwarz - 2020 - Merkle Hash Grids Instead of Merkle Trees.pdf;/home/sam/Zotero/storage/HMGYR286/9285942.html}
}

@inproceedings{parrEnforcingStrictModelview2004,
  title = {Enforcing Strict Model-View Separation in Template Engines},
  booktitle = {Proceedings of the 13th Conference on {{World Wide Web}}  - {{WWW}} '04},
  author = {Parr, Terence John},
  date = {2004},
  pages = {224},
  publisher = {ACM Press},
  location = {New York, NY, USA},
  doi = {10.1145/988672.988703},
  url = {http://portal.acm.org/citation.cfm?doid=988672.988703},
  urldate = {2022-07-11},
  abstract = {The mantra of every experienced web application developer is the same: thou shalt separate business logic from display. Ironically, almost all template engines allow violation of this separation principle, which is the very impetus for HTML template engine development. This situation is due mostly to a lack of formal definition of separation and fear that enforcing separation emasculates a template's power. I show that not only is strict separation a worthy design principle, but that we can enforce separation while providing a potent template engine. I demonstrate my StringTemplate engine, used to build jGuru.com and other commercial sites, at work solving some nontrivial generational tasks. My goal is to formalize the study of template engines, thus, providing a common nomenclature, a means of classifying template generational power, and a way to leverage interesting results from formal language theory. I classify three types of restricted templates analogous to Chomsky's type 1..3 grammar classes and formally define separation including the rules that embody separation. Because this paper provides a clear definition of model-view separation, template engine designers may no longer blindly claim enforcement of separation. Moreover, given theoretical arguments and empirical evidence, programmers no longer have an excuse to entangle model and view.},
  eventtitle = {The 13th Conference},
  isbn = {978-1-58113-844-3},
  langid = {english},
  keywords = {software engineering,web programming},
  annotation = {interest: 45}
}

@article{parrLLFoundationANTLR2011,
  title = {{{LL}}(*): The Foundation of the {{ANTLR}} Parser Generator},
  shorttitle = {{{LL}}(*)},
  author = {Parr, Terence and Fisher, Kathleen},
  date = {2011-06-04},
  journaltitle = {ACM SIGPLAN Notices},
  shortjournal = {SIGPLAN Not.},
  volume = {46},
  number = {6},
  pages = {425--436},
  issn = {0362-1340},
  doi = {10.1145/1993316.1993548},
  url = {https://doi.org/10.1145/1993316.1993548},
  urldate = {2022-09-21},
  abstract = {Despite the power of Parser Expression Grammars (PEGs) and GLR, parsing is not a solved problem. Adding nondeterminism (parser speculation) to traditional LL and LR parsers can lead to unexpected parse-time behavior and introduces practical issues with error handling, single-step debugging, and side-effecting embedded grammar actions. This paper introduces the LL(*) parsing strategy and an associated grammar analysis algorithm that constructs LL(*) parsing decisions from ANTLR grammars. At parse-time, decisions gracefully throttle up from conventional fixed k{$>$}=1 lookahead to arbitrary lookahead and, finally, fail over to backtracking depending on the complexity of the parsing decision and the input symbols. LL(*) parsing strength reaches into the context-sensitive languages, in some cases beyond what GLR and PEGs can express. By statically removing as much speculation as possible, LL(*) provides the expressivity of PEGs while retaining LL's good error handling and unrestricted grammar actions. Widespread use of ANTLR (over 70,000 downloads/year) shows that it is effective for a wide variety of applications.},
  keywords = {parsing},
  file = {/home/sam/Zotero/storage/IPRVUEQS/Parr and Fisher - 2011 - LL() the foundation of the ANTLR parser generato.pdf}
}

@inproceedings{pasquierPracticalWholesystemProvenance2017,
  title = {Practical Whole-System Provenance Capture},
  booktitle = {Proceedings of the 2017 {{Symposium}} on {{Cloud Computing}}},
  author = {Pasquier, Thomas and Han, Xueyuan and Goldstein, Mark and Moyer, Thomas and Eyers, David and Seltzer, Margo and Bacon, Jean},
  date = {2017-09-24},
  series = {{{SoCC}} '17},
  pages = {405--418},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3127479.3129249},
  url = {https://dl.acm.org/doi/10.1145/3127479.3129249},
  urldate = {2023-07-07},
  abstract = {Data provenance describes how data came to be in its present form. It includes data sources and the transformations that have been applied to them. Data provenance has many uses, from forensics and security to aiding the reproducibility of scientific experiments. We present CamFlow, a whole-system provenance capture mechanism that integrates easily into a PaaS offering. While there have been several prior whole-system provenance systems that captured a comprehensive, systemic and ubiquitous record of a system's behavior, none have been widely adopted. They either A) impose too much overhead, B) are designed for long-outdated kernel releases and are hard to port to current systems, C) generate too much data, or D) are designed for a single system. CamFlow addresses these shortcoming by: 1) leveraging the latest kernel design advances to achieve efficiency; 2) using a self-contained, easily maintainable implementation relying on a Linux Security Module, NetFilter, and other existing kernel facilities; 3) providing a mechanism to tailor the captured provenance data to the needs of the application; and 4) making it easy to integrate provenance across distributed systems. The provenance we capture is streamed and consumed by tenant-built auditor applications. We illustrate the usability of our implementation by describing three such applications: demonstrating compliance with data regulations; performing fault/intrusion detection; and implementing data loss prevention. We also show how CamFlow can be leveraged to capture meaningful provenance without modifying existing applications.},
  isbn = {978-1-4503-5028-0},
  keywords = {project-provenance-pp,provenance,provenance-tool},
  file = {/home/sam/Zotero/storage/DYSRHKYG/Pasquier et al. - 2017 - Practical whole-system provenance capture.pdf}
}

@online{patilStatisticalDefinitionReproducibility2016,
  title = {A Statistical Definition for Reproducibility and Replicability},
  author = {Patil, Prasad and Peng, Roger D. and Leek, Jeffrey T.},
  date = {2016-07-29},
  doi = {10.1101/066803},
  url = {http://biorxiv.org/lookup/doi/10.1101/066803},
  urldate = {2024-10-04},
  abstract = {Abstract           Everyone agrees that reproducibility and replicability are fundamental characteristics of scientific studies. These topics are attracting increasing attention, scrutiny, and debate both in the popular press and the scientific literature. But there are no formal statistical definitions for these concepts, which leads to confusion since the same words are used for different concepts by different people in different fields. We provide formal and informal definitions of scientific studies, reproducibility, and replicability that can be used to clarify discussions around these concepts in the scientific and popular press.},
  langid = {english},
  pubstate = {prepublished},
  file = {/home/sam/Zotero/storage/QPGQ7KKH/Patil et al. - 2016 - A statistical definition for reproducibility and replicability.pdf}
}

@unpublished{pattersonLinkingTypesMultiLanguage2017,
  title = {Linking {{Types}} for {{Multi-Language Software}}: {{Have Your Cake}} and {{Eat It Too}}},
  shorttitle = {Linking {{Types}} for {{Multi-Language Software}}},
  author = {Patterson, Daniel and Ahmed, Amal},
  date = {2017},
  eprint = {1711.04559},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {15 pages},
  doi = {10.4230/LIPIcs.SNAPL.2017.12},
  url = {http://arxiv.org/abs/1711.04559},
  urldate = {2023-03-10},
  abstract = {Software developers compose systems from components written in many different languages. A business-logic component may be written in Java or OCaml, a resource-intensive component in C or Rust, and a high-assurance component in Coq. In this multi-language world, program execution sends values from one linguistic context to another. This boundary-crossing exposes values to contexts with unforeseen behavior---that is, behavior that could not arise in the source language of the value. For example, a Rust function may end up being applied in an ML context that violates the memory usage policy enforced by Rust's type system. This leads to the question of how developers ought to reason about code in such a multi-language world where behavior inexpressible in one language is easily realized in another. This paper proposes the novel idea of linking types to address the problem of reasoning about single-language components in a multi-lingual setting. Specifically, linking types allow programmers to annotate where in a program they can link with components inexpressible in their unadulterated language. This enables developers to reason about (behavioral) equality using only their own language and the annotations, even though their code may be linked with code written in a language with more expressive power.},
  annotation = {interest: 75},
  file = {/home/sam/Zotero/storage/8PKPFUR9/Patterson and Ahmed - 2017 - Linking Types for Multi-Language Software Have Yo.pdf;/home/sam/Zotero/storage/UFPTED3C/1711.html}
}

@book{pearlCausality2009,
  title = {Causality},
  author = {Pearl, Judea},
  date = {2009},
  publisher = {Cambridge University Press},
  isbn = {978-0-521-89560-6},
  langid = {english},
  annotation = {Open Library ID: OL27305967M\\
interest: 80}
}

@online{pearlUnderstandingSimpsonParadox2013,
  type = {SSRN Scholarly Paper},
  title = {Understanding {{Simpson}}'s {{Paradox}}},
  author = {Pearl, Judea},
  date = {2013-09-19},
  number = {2343788},
  location = {Rochester, NY},
  doi = {10.2139/ssrn.2343788},
  url = {https://papers.ssrn.com/abstract=2343788},
  urldate = {2022-09-09},
  abstract = {Simpson's paradox is often presented as a compelling demonstration of why we need statistics education in our schools. It is a reminder of how easy it is to fall into a web of paradoxical conclusions when relying solely on intuition, unaided by rigorous statistical methods. In recent years, ironically, the paradox assumed an added dimension when educators began using it to demonstrate the limits of statistical methods, and why causal, rather than statistical considerations are necessary to avoid those paradoxical conclusions (Arah, 2008; Pearl, 2009, pp. 173-182; Wasserman, 2004).},
  langid = {english},
  pubstate = {prepublished},
  keywords = {causality,statistics},
  annotation = {interest: 91},
  file = {/home/sam/Zotero/storage/LQ9YWZBS/Pearl - 2013 - Understanding Simpson's Paradox.pdf;/home/sam/Zotero/storage/RHG957NU/papers.html}
}

@inproceedings{pediaditisExplicitProvenanceManagement2009,
  title = {On {{Explicit Provenance Management}} in {{RDF}}/{{S Graphs}}},
  booktitle = {First {{Workshop}} on the {{Theory}} and {{Practice}} of {{Provenance}} ({{TaPP}} '09)},
  author = {Pediaditis, P and Flouris, G and Fundulaki, I and Christophides, V},
  date = {2009-02-23},
  publisher = {USENIX},
  location = {San Francisco, CA},
  abstract = {The notion of RDF Named Graphs has been proposed in order to assign provenance information to data described using RDF triples. In this paper, we argue that named graphs alone cannot capture provenance information in the presence of RDFS reasoning and updates. In order to address this problem, we introduce the notion of RDF/S Graphsets: a graphset is associated with a set of RDF named graphs and contain the triples that are jointly owned by the named graphs that constitute the graphset. We formalize the notions of RDF named graphs and RDF/S graphsets and propose query and update languages that can be used to handle provenance information for RDF/S graphs taking into account RDFS semantics.},
  langid = {english},
  file = {/home/sam/Zotero/storage/2YM7WH53/Pediaditis et al. - On Explicit Provenance Management in RDFS Graphs.pdf;/home/sam/Zotero/storage/G6GM7BGA/index.html}
}

@article{pengReproducibleResearchComputational2011,
  title = {Reproducible {{Research}} in {{Computational Science}}},
  author = {Peng, Roger D.},
  date = {2011-12-02},
  journaltitle = {Science},
  shortjournal = {Science},
  volume = {334},
  number = {6060},
  pages = {1226--1227},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1213847},
  url = {https://www.science.org/doi/10.1126/science.1213847},
  urldate = {2022-06-30},
  abstract = {Computational science has led to exciting new developments, but the nature of the work has exposed limitations in our ability to evaluate published findings. Reproducibility has the potential to serve as a minimum standard for judging scientific claims when full independent replication of a study is not possible.},
  langid = {english},
  keywords = {metascience,reproducibility engineering},
  annotation = {interest: 95},
  file = {/home/sam/Zotero/storage/XT2M7FCD/science.1213847.pdf}
}

@online{pennarunMtimeComparisonConsidered,
  title = {Mtime Comparison Considered Harmful},
  author = {Pennarun, Avery},
  url = {https://apenwarr.ca/log/20181113},
  urldate = {2024-02-11},
  abstract = {tl;dr: Rebuilding a target because its mtime is older than the mtimes of its dependencies, like `make` does, is very error prone.  redo does...},
  file = {/home/sam/Zotero/storage/3NURRWCB/20181113.html}
}

@unpublished{pereraImpactConsideringHuman2021,
  title = {The {{Impact}} of {{Considering Human Values}} during {{Requirements Engineering Activities}}},
  author = {Perera, Harsha and Hoda, Rashina and Shams, Rifat Ara and Nurwidyantoro, Arif and Shahin, Mojtaba and Hussain, Waqar and Whittle, Jon},
  date = {2021-11-30},
  eprint = {2111.15293},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2111.15293},
  urldate = {2022-06-24},
  abstract = {Human values, or what people hold important in their life, such as freedom, fairness, and social responsibility, often remain unnoticed and unattended during software development. Ignoring values can lead to values violations in software that can result in financial losses, reputation damage, and widespread social and legal implications. However, embedding human values in software is not only non-trivial but also generally an unclear process. Commencing as early as during the Requirements Engineering (RE) activities promises to ensure fit-for-purpose and quality software products that adhere to human values. But what is the impact of considering human values explicitly during early RE activities? To answer this question, we conducted a scenario-based survey where 56 software practitioners contextualised requirements analysis towards a proposed mobile application for the homeless and suggested values-laden software features accordingly. The suggested features were qualitatively analysed. Results show that explicit considerations of values can help practitioners identify applicable values, associate purpose with the features they develop, think outside-the-box, and build connections between software features and human values. Finally, drawing from the results and experiences of this study, we propose a scenario-based values elicitation process -- a simple four-step takeaway as a practical implication of this study.},
  keywords = {Computer Science - Software Engineering},
  annotation = {interest: 80}
}

@inproceedings{perezderossoWhatWrongGit2013,
  title = {What's Wrong with Git? A Conceptual Design Analysis},
  shorttitle = {What's Wrong with Git?},
  booktitle = {Proceedings of the 2013 {{ACM}} International Symposium on {{New}} Ideas, New Paradigms, and Reflections on Programming \& Software},
  author = {Perez De Rosso, Santiago and Jackson, Daniel},
  date = {2013-10-29},
  series = {Onward! 2013},
  pages = {37--52},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2509578.2509584},
  url = {https://dl.acm.org/doi/10.1145/2509578.2509584},
  urldate = {2023-07-06},
  abstract = {It is commonly asserted that the success of a software development project, and the usability of the final product, depend on the quality of the concepts that underlie its design. Yet this hypothesis has not been systematically explored by researchers, and conceptual design has not played the central role in the research and teaching of software engineering that one might expect. As part of a new research project to explore conceptual design, we are engaging in a series of case studies. This paper reports on the early stages of our first study, on the Git version control system. Despite its widespread adoption, Git puzzles even experienced developers and is not regarded as easy to use. In an attempt to understand the root causes of its complexity, we analyze its conceptual model and identify some undesirable properties; we then propose a reworking of the conceptual model that forms the basis of (the first version of) Gitless, an ongoing effort to redesign Git and experiment with the effects of conceptual simplifications.},
  isbn = {978-1-4503-2472-4},
  keywords = {human computer interaction,software engineering},
  file = {/home/sam/Zotero/storage/MRHCFZPB/Perez De Rosso and Jackson - 2013 - What's wrong with git a conceptual design analysi.pdf}
}

@article{perezgonzalezFisherNeymanPearsonNHST2015,
  title = {Fisher, {{Neyman-Pearson}} or {{NHST}}? {{A}} Tutorial for Teaching Data Testing},
  shorttitle = {Fisher, {{Neyman-Pearson}} or {{NHST}}?},
  author = {Perezgonzalez, Jose D.},
  date = {2015},
  journaltitle = {Frontiers in Psychology},
  volume = {6},
  issn = {1664-1078},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2015.00223},
  urldate = {2024-01-29},
  abstract = {Despite frequent calls for the overhaul of null hypothesis significance testing (NHST), this controversial procedure remains ubiquitous in behavioral, social and biomedical teaching and research. Little change seems possible once the procedure becomes well ingrained in the minds and current practice of researchers; thus, the optimal opportunity for such change is at the time the procedure is taught, be this at undergraduate or at postgraduate levels. This paper presents a tutorial for the teaching of data testing procedures, often referred to as hypothesis testing theories. The first procedure introduced is Fisher's approach to data testing—tests of significance; the second is Neyman-Pearson's approach—tests of acceptance; the final procedure is the incongruent combination of the previous two theories into the current approach—NSHT. For those researchers sticking with the latter, two compromise solutions on how to improve NHST conclude the tutorial.},
  keywords = {statistics},
  file = {/home/sam/Zotero/storage/XYMUNLVG/Perezgonzalez - 2015 - Fisher, Neyman-Pearson or NHST A tutorial for tea.pdf}
}

@article{perezPythonEcosystemScientific2011,
  title = {Python: {{An Ecosystem}} for {{Scientific Computing}}},
  shorttitle = {Python},
  author = {Pérez, Fernando and Granger, Brian E. and Hunter, John D.},
  date = {2011-03},
  journaltitle = {Computing in Science \& Engineering},
  volume = {13},
  number = {2},
  pages = {13--21},
  issn = {1558-366X},
  doi = {10.1109/MCSE.2010.119},
  url = {https://ieeexplore.ieee.org/abstract/document/5582063},
  urldate = {2024-02-23},
  abstract = {As the relationship between research and computing evolves, new tools are required to not only treat numerical problems, but also to solve various problems that involve large datasets in different formats, new algorithms, and computational systems such as databases and Internet servers. Python can help develop these computational research tools by providing a balance of clarity and flexibility without sacrificing performance.},
  eventtitle = {Computing in {{Science}} \& {{Engineering}}},
  keywords = {project-repro-py,research software engineering},
  file = {/home/sam/Zotero/storage/E8E3RTLY/Pérez et al. - 2011 - Python An Ecosystem for Scientific Computing.pdf;/home/sam/Zotero/storage/P9ERS24J/5582063.html}
}

@article{perkelSixTipsBetter2022,
  title = {Six Tips for Better Spreadsheets},
  author = {Perkel, Jeffrey M.},
  date = {2022-08-02},
  journaltitle = {Nature},
  volume = {608},
  number = {7921},
  pages = {229--230},
  publisher = {Nature Publishing Group},
  doi = {10.1038/d41586-022-02076-1},
  url = {https://www.nature.com/articles/d41586-022-02076-1},
  urldate = {2022-08-25},
  abstract = {Microsoft Excel and Google Sheets are powerful and widely used. But there’s a right way and a wrong way to use them, data scientists say.},
  issue = {7921},
  langid = {english},
  annotation = {Bandiera\_abtest: a\\
Cg\_type: Technology Feature\\
Subject\_term: Software, Computational biology and bioinformatics, Research data},
  file = {/home/sam/Zotero/storage/AGHEEAPK/Perkel - 2022 - Six tips for better spreadsheets.pdf}
}

@article{peroniOpenCitationsInfrastructureOrganization2020,
  title = {{{OpenCitations}}, an Infrastructure Organization for Open Scholarship},
  author = {Peroni, Silvio and Shotton, David},
  date = {2020-02-01},
  journaltitle = {Quantitative Science Studies},
  shortjournal = {Quantitative Science Studies},
  volume = {1},
  number = {1},
  pages = {428--444},
  issn = {2641-3337},
  doi = {10.1162/qss_a_00023},
  url = {https://doi.org/10.1162/qss_a_00023},
  urldate = {2025-01-13},
  abstract = {OpenCitations is an infrastructure organization for open scholarship dedicated to the publication of open citation data as Linked Open Data using Semantic Web technologies, thereby providing a disruptive alternative to traditional proprietary citation indexes. Open citation data are valuable for bibliometric analysis, increasing the reproducibility of large-scale analyses by enabling publication of the source data. Following brief introductions to the development and benefits of open scholarship and to Semantic Web technologies, this paper describes OpenCitations and its data sets, tools, services, and activities. These include the OpenCitations Data Model; the SPAR (Semantic Publishing and Referencing) Ontologies; OpenCitations’ open software of generic applicability for searching, browsing, and providing REST APIs over resource description framework (RDF) triplestores; Open Citation Identifiers (OCIs) and the OpenCitations OCI Resolution Service; the OpenCitations Corpus (OCC), a database of open downloadable bibliographic and citation data made available in RDF under a Creative Commons public domain dedication; and the OpenCitations Indexes of open citation data, of which the first and largest is COCI, the OpenCitations Index of Crossref Open DOI-to-DOI Citations, which currently contains over 624 million bibliographic citations and is receiving considerable usage by the scholarly community.},
  file = {/home/sam/Zotero/storage/PY32AAI4/Peroni and Shotton - 2020 - OpenCitations, an infrastructure organization for open scholarship.pdf;/home/sam/Zotero/storage/QRH6NRDJ/OpenCitations-an-infrastructure-organization-for.html}
}

@online{perryCrossValidationUnsupervisedLearning2009,
  title = {Cross-{{Validation}} for {{Unsupervised Learning}}},
  author = {Perry, Patrick O.},
  date = {2009-09-16},
  eprint = {0909.3052},
  eprinttype = {arXiv},
  eprintclass = {math, stat},
  url = {http://arxiv.org/abs/0909.3052},
  urldate = {2024-02-10},
  abstract = {Cross-validation (CV) is a popular method for model-selection. Unfortunately, it is not immediately obvious how to apply CV to unsupervised or exploratory contexts. This thesis discusses some extensions of cross-validation to unsupervised learning, specifically focusing on the problem of choosing how many principal components to keep. We introduce the latent factor model, define an objective criterion, and show how CV can be used to estimate the intrinsic dimensionality of a data set. Through both simulation and theory, we demonstrate that cross-validation is a valuable tool for unsupervised learning.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {machine learning,project-provenance-pp},
  file = {/home/sam/Zotero/storage/5M3Y9SK7/Perry - 2009 - Cross-Validation for Unsupervised Learning.pdf}
}

@unpublished{petersohnScalableDataframeSystems2020,
  title = {Towards {{Scalable Dataframe Systems}}},
  author = {Petersohn, Devin and Macke, Stephen and Xin, Doris and Ma, William and Lee, Doris and Mo, Xiangxi and Gonzalez, Joseph E. and Hellerstein, Joseph M. and Joseph, Anthony D. and Parameswaran, Aditya},
  date = {2020-06-02},
  eprint = {2001.00888},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2001.00888},
  urldate = {2022-05-25},
  abstract = {Dataframes are a popular abstraction to represent, prepare, and analyze data. Despite the remarkable success of dataframe libraries in Rand Python, dataframes face performance issues even on moderately large datasets. Moreover, there is significant ambiguity regarding dataframe semantics. In this paper we lay out a vision and roadmap for scalable dataframe systems. To demonstrate the potential in this area, we report on our experience building MODIN, a scaled-up implementation of the most widely-used and complex dataframe API today, Python's pandas. With pandas as a reference, we propose a simple data model and algebra for dataframes to ground discussion in the field. Given this foundation, we lay out an agenda of open research opportunities where the distinct features of dataframes will require extending the state of the art in many dimensions of data management. We discuss the implications of signature data-frame features including flexible schemas, ordering, row/column equivalence, and data/metadata fluidity, as well as the piecemeal, trial-and-error-based approach to interacting with dataframes.},
  keywords = {data science,research software engineering},
  annotation = {interest: 71},
  file = {/home/sam/Zotero/storage/6ZTS7RSJ/2001.00888.pdf}
}

@article{petersPhenoMeNalProcessingAnalysis2019,
  title = {{{PhenoMeNal}}: Processing and Analysis of Metabolomics Data in the Cloud},
  shorttitle = {{{PhenoMeNal}}},
  author = {Peters, Kristian and Bradbury, James and Bergmann, Sven and Capuccini, Marco and Cascante, Marta and {de~Atauri}, Pedro and Ebbels, Timothy M D and Foguet, Carles and Glen, Robert and Gonzalez-Beltran, Alejandra and Günther, Ulrich L and Handakas, Evangelos and Hankemeier, Thomas and Haug, Kenneth and Herman, Stephanie and Holub, Petr and Izzo, Massimiliano and Jacob, Daniel and Johnson, David and Jourdan, Fabien and Kale, Namrata and Karaman, Ibrahim and Khalili, Bita and Emami~Khonsari, Payam and Kultima, Kim and Lampa, Samuel and Larsson, Anders and Ludwig, Christian and Moreno, Pablo and Neumann, Steffen and Novella, Jon Ander and O'Donovan, Claire and Pearce, Jake T M and Peluso, Alina and Piras, Marco Enrico and Pireddu, Luca and Reed, Michelle A C and Rocca-Serra, Philippe and Roger, Pierrick and Rosato, Antonio and Rueedi, Rico and Ruttkies, Christoph and Sadawi, Noureddin and Salek, Reza M and Sansone, Susanna-Assunta and Selivanov, Vitaly and Spjuth, Ola and Schober, Daniel and Thévenot, Etienne A and Tomasoni, Mattia and {van~Rijswijk}, Merlijn and {van~Vliet}, Michael and Viant, Mark R and Weber, Ralf J M and Zanetti, Gianluigi and Steinbeck, Christoph},
  date = {2019-02-01},
  journaltitle = {GigaScience},
  volume = {8},
  number = {2},
  issn = {2047-217X},
  doi = {10.1093/gigascience/giy149},
  url = {https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giy149/5232984},
  urldate = {2024-10-04},
  langid = {english},
  file = {/home/sam/Zotero/storage/QL8H36AW/Peters et al. - 2019 - PhenoMeNal processing and analysis of metabolomics data in the cloud.pdf}
}

@inproceedings{phamUsingProvenanceRepeatability2013,
  title = {Using {{Provenance}} for {{Repeatability}}},
  author = {Pham, Quan and Malik, Tanu and Foster, Ian},
  date = {2013},
  url = {https://www.usenix.org/conference/tapp13/technical-sessions/presentation/pham},
  urldate = {2024-02-14},
  eventtitle = {5th {{USENIX Workshop}} on the {{Theory}} and {{Practice}} of {{Provenance}} ({{TaPP}} 13)},
  langid = {english},
  keywords = {project-provenance-pp,provenance},
  file = {/home/sam/Zotero/storage/SNTQWLYD/Pham et al. - 2013 - Using Provenance for Repeatability.pdf}
}

@inproceedings{phungNotAllTasks2021,
  title = {Not {{All Tasks Are Created Equal}}: {{Adaptive Resource Allocation}} for {{Heterogeneous Tasks}} in {{Dynamic Workflows}}},
  shorttitle = {Not {{All Tasks Are Created Equal}}},
  booktitle = {2021 {{IEEE Workshop}} on {{Workflows}} in {{Support}} of {{Large-Scale Science}} ({{WORKS}})},
  author = {Phung, Thanh Son and Ward, Logan and Chard, Kyle and Thain, Douglas},
  date = {2021-11},
  pages = {17--24},
  doi = {10.1109/WORKS54523.2021.00008},
  abstract = {Users running dynamic workflows in distributed systems usually have inadequate expertise to correctly size the allocation of resources (cores, memory, disk) to each task due to the difficulty in uncovering the obscure yet important correlation between tasks and their resource consumption. Thus, users typically pay little attention to this problem of allocation sizing and either simply apply an error-prone upper bound of resource allocation to all tasks, or delegate this responsibility to underlying distributed systems, resulting in substantial waste from allocated yet unused resources. In this paper, we will first show that tasks performing different work may have significantly different resource consumption. We will then show that exploiting the heterogeneity of tasks is a desirable way to reveal and predict the relationship between tasks and their resource consumption, reduce waste from resource misallocation, increase tasks' consumption efficiency, and incentivize users' cooperation. We have developed two info-aware allocation strategies capitalizing on this characteristic and will show their effectiveness through simulations on two modern applications with dynamic workflows and five synthetic datasets of resource consumption. Our results show that info-aware strategies can cut down up to 98.7\% of the total waste incurred by a best-effort strategy, and increase the efficiency in resource consumption of each task on average anywhere up to 93.9\%.},
  eventtitle = {2021 {{IEEE Workshop}} on {{Workflows}} in {{Support}} of {{Large-Scale Science}} ({{WORKS}})},
  keywords = {scheduling,workflow managers},
  annotation = {interest: 71},
  file = {/home/sam/Zotero/storage/AEWTGMN4/9652605.html}
}

@online{pillerPatientsEndangeredLaw2015,
  title = {Patients Endangered as Law Is Ignored},
  author = {Piller, Charles},
  date = {2015-12-14T02:00:14+00:00},
  url = {https://www.statnews.com/2015/12/13/clinical-trials-investigation/},
  urldate = {2022-08-30},
  abstract = {A STAT analysis has found that some of the top research institutions are not reporting clinical trial results as required by law.},
  langid = {american},
  organization = {STAT},
  keywords = {science policy}
}

@inproceedings{pimentelLargeScaleStudyQuality2019,
  title = {A {{Large-Scale Study About Quality}} and {{Reproducibility}} of {{Jupyter Notebooks}}},
  booktitle = {Proceedings of the 16th {{International Conference}} on {{Mining Software Repositories}}},
  author = {Pimentel, João Felipe and Murta, Leonardo and Braganholo, Vanessa and Freire, Juliana},
  date = {2019-05-26},
  series = {{{MSR}} '19},
  pages = {507--517},
  publisher = {IEEE Press},
  location = {Montreal, Quebec, Canada},
  issn = {2574-3864},
  doi = {10.1109/MSR.2019.00077},
  url = {https://doi.org/10.1109/MSR.2019.00077},
  abstract = {Jupyter Notebooks have been widely adopted by many different communities, both in science and industry. They support the creation of literate programming documents that combine code, text, and execution results with visualizations and all sorts of rich media. The self-documenting aspects and the ability to reproduce results have been touted as significant benefits of notebooks. At the same time, there has been growing criticism that the way notebooks are being used leads to unexpected behavior, encourage poor coding practices, and that their results can be hard to reproduce. To understand good and bad practices used in the development of real notebooks, we studied 1.4 million notebooks from GitHub. We present a detailed analysis of their characteristics that impact reproducibility. We also propose a set of best practices that can improve the rate of reproducibility and discuss open challenges that require further research and development.},
  eventtitle = {2019 {{IEEE}}/{{ACM}} 16th {{International Conference}} on {{Mining Software Repositories}} ({{MSR}})},
  keywords = {project-acm-rep,reproducibility engineering},
  file = {/home/sam/Zotero/storage/PT3DYAV2/8816763.html}
}

@article{pimentelSurveyCollectingManaging2019,
  title = {A {{Survey}} on {{Collecting}}, {{Managing}}, and {{Analyzing Provenance}} from {{Scripts}}},
  author = {Pimentel, João Felipe and Freire, Juliana and Murta, Leonardo and Braganholo, Vanessa},
  date = {2019-06-18},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {52},
  number = {3},
  pages = {47:1--47:38},
  issn = {0360-0300},
  doi = {10.1145/3311955},
  url = {https://dl.acm.org/doi/10.1145/3311955},
  urldate = {2024-01-20},
  abstract = {Scripts are widely used to design and run scientific experiments. Scripting languages are easy to learn and use, and they allow complex tasks to be specified and executed in fewer steps than with traditional programming languages. However, they also have important limitations for reproducibility and data management. As experiments are iteratively refined, it is challenging to reason about each experiment run (or trial), to keep track of the association between trials and experiment instances as well as the differences across trials, and to connect results to specific input data and parameters. Approaches have been proposed that address these limitations by collecting, managing, and analyzing the provenance of scripts. In this article, we survey the state of the art in provenance for scripts. We have identified the approaches by following an exhaustive protocol of forward and backward literature snowballing. Based on a detailed study, we propose a taxonomy and classify the approaches using this taxonomy.},
  keywords = {project-provenance-pp,provenance},
  file = {/home/sam/Zotero/storage/YF83IWDC/Pimentel et al. - 2019 - A Survey on Collecting, Managing, and Analyzing Pr.pdf}
}

@inproceedings{pintoHowScientistsDevelop2018,
  title = {How Do Scientists Develop Scientific Software? {{An}} External Replication},
  shorttitle = {How Do Scientists Develop Scientific Software?},
  booktitle = {2018 {{IEEE}} 25th {{International Conference}} on {{Software Analysis}}, {{Evolution}} and {{Reengineering}} ({{SANER}})},
  author = {Pinto, Gustavo and Wiese, Igor and Dias, Luiz Felipe},
  date = {2018-03},
  pages = {582--591},
  doi = {10.1109/SANER.2018.8330263},
  abstract = {Although the goal of scientists is to do science, not to develop software, many scientists have extended their roles to include software development to their skills. However, since scientists have different background, it remains unclear how do they perceive software engineering practices or how do they acquire software engineering knowledge. In this paper we conducted an external replication of one influential 10 years paper about how scientists develop and use scientific software. In particular, we employed the same method (an on-line questionnaire) in a different population (R developers). When analyzing the more than 1,574 responses received, enriched with data gathered from their GitHub repositories, we correlated our findings with the original study. We found that the results were consistent in many ways, including: (1) scientists that develop software work mostly alone, (2) they decide themselves what they want to work on next, and (3) most of what they learnt came from self-study, rather than a formal education. However, we also uncover new facts, such as: some of the "pain points" regarding software development are not related to technical activities (e.g., interruptions, lack of collaborators, and lack of a reward system play a role). Our replication can help researchers, practitioners, and educators to better focus their efforts on topics that are important to the scientific community that develops software.},
  eventtitle = {2018 {{IEEE}} 25th {{International Conference}} on {{Software Analysis}}, {{Evolution}} and {{Reengineering}} ({{SANER}})},
  keywords = {research software engineering},
  annotation = {interest: 90},
  file = {/home/sam/Zotero/storage/VZL624JR/Pinto et al. - 2018 - How do scientists develop scientific software An .pdf;/home/sam/Zotero/storage/XJLPMXFR/8330263.html}
}

@inproceedings{plankensteinerIWIRLanguageEnabling2011,
  title = {{{IWIR}}: A Language Enabling Portability across Grid Workflow Systems},
  shorttitle = {{{IWIR}}},
  booktitle = {Proceedings of the 6th Workshop on {{Workflows}} in Support of Large-Scale Science - {{WORKS}} '11},
  author = {Plankensteiner, Kassian and Montagnat, Johan and Prodan, Radu},
  date = {2011},
  pages = {97},
  publisher = {ACM Press},
  location = {Seattle, Washington, USA},
  doi = {10.1145/2110497.2110509},
  url = {http://dl.acm.org/citation.cfm?doid=2110497.2110509},
  urldate = {2022-08-03},
  eventtitle = {The 6th Workshop},
  isbn = {978-1-4503-1100-7},
  langid = {english},
  keywords = {workflow managers}
}

@article{plenzHowTradeServer2019,
  title = {How to {{Trade}} off {{Server Utilization}} and {{Tail Latency}}},
  author = {Plenz, Julius},
  date = {2019},
  journaltitle = {SRE Con},
  url = {https://www.usenix.org/conference/srecon19asia/presentation/plenz},
  urldate = {2023-12-15},
  abstract = {When running large scale systems, we strive to deliver both low tail latency and high utilization of servers. However, these two dimenions are at odds: increasing the average utilization of a system will have a detrimental impact on the tail latency. This talk provides a light-weight walkthrough of the important basics of queueing theory (avoiding unnecessary formalism), illustrates graphically several typical outcomes of this analysis, and closes with a few basic rules on how to think about utilization and tail latency.},
  langid = {english},
  keywords = {industry practices,queuing theory},
  file = {/home/sam/Zotero/storage/LHKBUC7P/plenz.html}
}

@article{plesserReproducibilityVsReplicability2018,
  title = {Reproducibility vs. {{Replicability}}: {{A Brief History}} of a {{Confused Terminology}}},
  shorttitle = {Reproducibility vs. {{Replicability}}},
  author = {Plesser, Hans E.},
  date = {2018},
  journaltitle = {Frontiers in Neuroinformatics},
  volume = {11},
  issn = {1662-5196},
  url = {https://www.frontiersin.org/articles/10.3389/fninf.2017.00076},
  urldate = {2022-10-11},
  keywords = {reproducibility engineering},
  file = {/home/sam/Zotero/storage/JDIE62JR/Plesser - 2018 - Reproducibility vs. Replicability A Brief History.pdf}
}

@article{plouffeResearchReportRichness2001,
  title = {Research {{Report}}: {{Richness Versus Parsimony}} in {{Modeling Technology Adoption Decisions}}—{{Understanding Merchant Adoption}} of a {{Smart Card-Based Payment System}}},
  shorttitle = {Research {{Report}}},
  author = {Plouffe, Christopher R. and Hulland, John S. and Vandenbosch, Mark},
  date = {2001-06},
  journaltitle = {Information Systems Research},
  shortjournal = {Information Systems Research},
  volume = {12},
  number = {2},
  pages = {208--222},
  issn = {1047-7047, 1526-5536},
  doi = {10.1287/isre.12.2.208.9697},
  url = {http://pubsonline.informs.org/doi/abs/10.1287/isre.12.2.208.9697},
  urldate = {2022-06-02},
  langid = {english},
  keywords = {internship-project,technology-acceptance},
  file = {/home/sam/Zotero/storage/SKAR4BUB/isre.12.2.208.9697.pdf}
}

@online{poeWhatKnowDebating2010,
  title = {What to Know before Debating Type Systems | {{Ovid}} [Blogs.Perl.Org]},
  author = {Poe, Curtis "Ovid"},
  date = {2010-08-19},
  url = {https://blogs.perl.org/users/ovid/2010/08/what-to-know-before-debating-type-systems.html},
  urldate = {2024-01-03},
  organization = {Ovid: A blog about the Perl programming language},
  file = {/home/sam/Zotero/storage/JVFC4ART/what-to-know-before-debating-type-systems.html}
}

@inproceedings{pohlyHiFiCollectingHighfidelity2012,
  title = {Hi-{{Fi}}: Collecting High-Fidelity Whole-System Provenance},
  shorttitle = {Hi-{{Fi}}},
  booktitle = {Proceedings of the 28th {{Annual Computer Security Applications Conference}}},
  author = {Pohly, Devin J. and McLaughlin, Stephen and McDaniel, Patrick and Butler, Kevin},
  date = {2012-12-03},
  series = {{{ACSAC}} '12},
  pages = {259--268},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2420950.2420989},
  url = {https://dl.acm.org/doi/10.1145/2420950.2420989},
  urldate = {2023-08-23},
  abstract = {Data provenance---a record of the origin and evolution of data in a system---is a useful tool for forensic analysis. However, existing provenance collection mechanisms fail to achieve sufficient breadth or fidelity to provide a holistic view of a system's operation over time. We present Hi-Fi, a kernel-level provenance system which leverages the Linux Security Modules framework to collect high-fidelity whole-system provenance. We demonstrate that Hi-Fi is able to record a variety of malicious behavior within a compromised system. In addition, our benchmarks show the collection overhead from Hi-Fi to be less than 1\% for most system calls and 3\% in a representative workload, while simultaneously generating a system measurement that fully reflects system evolution. In this way, we show that we can collect broad, high-fidelity provenance data which is capable of supporting detailed forensic analysis.},
  isbn = {978-1-4503-1312-4},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/HBBVHQER/Pohly et al. - 2012 - Hi-Fi collecting high-fidelity whole-system prove.pdf}
}

@article{pordesOpenScienceGrid2007,
  title = {The Open Science Grid},
  author = {Pordes, Ruth and Petravick, Don and Kramer, Bill and Olson, Doug and Livny, Miron and Roy, Alain and Avery, Paul and Blackburn, Kent and Wenaus, Torre and Würthwein, Frank and Foster, Ian and Gardner, Rob and Wilde, Mike and Blatecky, Alan and McGee, John and Quick, Rob},
  date = {2007-07-01},
  journaltitle = {Journal of Physics: Conference Series},
  volume = {78},
  number = {1},
  issn = {1742-6588},
  doi = {10.1088/1742-6596/78/1/012057},
  url = {http://www.scopus.com/inward/record.url?scp=36049001139&partnerID=8YFLogxK},
  urldate = {2022-10-18},
  abstract = {The Open Science Grid (OSG) provides a distributed facility where the Consortium members provide guaranteed and opportunistic access to shared computing and storage resources. OSG provides support for and evolution of the infrastructure through activities that cover operations, security, software, troubleshooting, addition of new capabilities, and support for existing and engagement with new communities. The OSG SciDAC-2 project provides specific activities to manage and evolve the distributed infrastructure and support it's use. The innovative aspects of the project are the maintenance and performance of a collaborative (shared \& common) petascale national facility over tens of autonomous computing sites, for many hundreds of users, transferring terabytes of data a day, executing tens of thousands of jobs a day, and providing robust and usable resources for scientific groups of all types and sizes. More information can be found at the OSG web site: www.opensciencegrid. org.},
  keywords = {cloud computing},
  annotation = {interest: 84},
  file = {/home/sam/Zotero/storage/GR4FYSIJ/Pordes et al. - 2007 - The open science grid.pdf}
}

@article{pouchardComputationalReproducibilityScientific2019,
  title = {Computational Reproducibility of Scientific Workflows at Extreme Scales},
  author = {Pouchard, Line and Baldwin, Sterling and Elsethagen, Todd and Jha, Shantenu and Raju, Bibi and Stephan, Eric and Tang, Li and Van Dam, Kerstin Kleese},
  date = {2019-04-08},
  journaltitle = {International Journal of High Performance Computing Applications},
  volume = {33},
  number = {BNL-211854-2019-JAAM},
  pages = {763--776},
  publisher = {SAGE},
  issn = {1094-3420},
  doi = {10.1177/1094342019839124},
  url = {https://www.osti.gov/pages/biblio/1542776},
  urldate = {2023-02-20},
  abstract = {We propose an approach for improved reproducibility that includes capturing and relating provenance characteristics and performance metrics. We discuss two use cases: scientific reproducibility of results in the Energy Exascale Earth System Model (E3SM – previously ACME), and performance reproducibility in molecular dynamics workflows on HPC computing platforms. In order to capture and persist the provenance and performance data of these workflows, we have designed and developed the Chimbuko and ProvEn frameworks. Chimbuko captures provenance and enables detailed single workflow performance analysis. ProvEn is a hybrid, queriable system for storing and analyzing the provenance and performance metrics of multiple runs in workflow performance analysis campaigns. Workflow provenance and performance data output from Chimbuko can be visualized in a dynamic, multi-level visualization providing overview and zoom-in capabilities for areas of interest. Provenance and related performance data ingested into ProvEn is queriable and can be used to reproduce runs. In conclusion, our provenance-based approach highlights challenges in extracting information and gaps in the information collected. It is agnostic to the type of provenance data it captures so that both the reproducibility of scientific results and that of performance can be explored with our tools.},
  issue = {5},
  langid = {english},
  keywords = {project-acm-rep,reproducibility engineering,workflow managers},
  file = {/home/sam/Zotero/storage/EUIUMYIZ/Pouchard et al. - 2019 - Computational reproducibility of scientific workfl.pdf}
}

@article{powellFuturePostdoc2015,
  title = {The Future of the Postdoc},
  author = {Powell, Kendall},
  date = {2015-04-01},
  journaltitle = {Nature},
  volume = {520},
  number = {7546},
  pages = {144--147},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/520144a},
  url = {https://www.nature.com/articles/520144a},
  urldate = {2022-08-30},
  abstract = {There is a growing number of postdocs and few places in academia for them to go. But change could be on the way.},
  issue = {7546},
  langid = {english},
  file = {/home/sam/Zotero/storage/BL3GX89J/Powell - 2015 - The future of the postdoc.pdf;/home/sam/Zotero/storage/AA8GDPLG/520144a.html}
}

@inproceedings{prabhakaranIRONFileSystems2005,
  title = {{{IRON}} File Systems},
  booktitle = {Proceedings of the Twentieth {{ACM}} Symposium on {{Operating}} Systems Principles},
  author = {Prabhakaran, Vijayan and Bairavasundaram, Lakshmi N. and Agrawal, Nitin and Gunawi, Haryadi S. and Arpaci-Dusseau, Andrea C. and Arpaci-Dusseau, Remzi H.},
  date = {2005-10-20},
  series = {{{SOSP}} '05},
  pages = {206--220},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1095810.1095830},
  url = {https://doi.org/10.1145/1095810.1095830},
  urldate = {2023-01-18},
  abstract = {Commodity file systems trust disks to either work or fail completely, yet modern disks exhibit more complex failure modes. We suggest a new fail-partial failure model for disks, which incorporates realistic localized faults such as latent sector errors and block corruption. We then develop and apply a novel failure-policy fingerprinting framework, to investigate how commodity file systems react to a range of more realistic disk failures. We classify their failure policies in a new taxonomy that measures their Internal RObustNess (IRON), which includes both failure detection and recovery techniques. We show that commodity file system failure policies are often inconsistent, sometimes buggy, and generally inadequate in their ability to recover from partial disk failures. Finally, we design, implement, and evaluate a prototype IRON file system, Linux ixt3, showing that techniques such as in-disk checksumming, replication, and parity greatly enhance file system robustness while incurring minimal time and space overheads.},
  isbn = {978-1-59593-079-8},
  keywords = {filesystems,operating systems},
  annotation = {interest: 40},
  file = {/home/sam/Zotero/storage/NX2L3N7H/Prabhakaran et al. - 2005 - IRON file systems.pdf}
}

@inproceedings{prasadLocatingSystemProblems2005,
  title = {Locating {{System Problems Using Dynamic Instrumentation}}},
  booktitle = {Proceedings of the {{Linux Symposium}}},
  author = {Prasad, Vara and Cohen, William and Eigler, Frank and Hunt, Martin and Keniston, Jim and Chen, Brad},
  date = {2005-07-20},
  volume = {2},
  pages = {49--64},
  publisher = {kernel.org},
  location = {Ottawa, Ontario, Canada},
  abstract = {Diagnosing complex performance or kernel debugging problems often requires kernel modifications with multiple rebuilds and reboots. This is tedious, time-consuming work that most developers would prefer to minimize. Systemtap uses the kprobes infrastructure to dynamically instrument the kernel and user applications. Systemtap instrumentation incurs low overhead when enabled, and zero overhead when disabled. SystemTap provides facilities to define instrumentation points in a high-level language, and to aggregate and analyze the instrumentation data. Details of the SystemTap architecture and implementation are presented, along with an example of its application.},
  keywords = {operating systems,project-provenance-pp},
  file = {/home/sam/Zotero/storage/ENCHXEJI/ols2005v2.pdf}
}

@unpublished{priceHDF5GoodFormat2014,
  title = {Is {{HDF5}} a Good Format to Replace {{UVFITS}}?},
  author = {Price, Danny C. and Barsdell, Benjamin R. and Greenhill, Lincoln J.},
  date = {2014-11-03},
  eprint = {1411.0507},
  eprinttype = {arXiv},
  eprintclass = {astro-ph},
  url = {http://arxiv.org/abs/1411.0507},
  urldate = {2022-04-12},
  abstract = {The FITS (Flexible Image Transport System) data format was developed in the late 1970s for storage and exchange of astronomy-related image data. Since then, it has become a standard file format not only for images, but also for radio interferometer data (e.g. UVFITS, FITS-IDI). But is FITS the right format for next-generation telescopes to adopt? The newer Hierarchical Data Format (HDF5) file format offers considerable advantages over FITS, but has yet to gain widespread adoption within radio astronomy. One of the major holdbacks is that HDF5 is not well supported by data reduction software packages. Here, we present a comparison of FITS, HDF5, and the MeasurementSet (MS) format for storage of interferometric data. In addition, we present a tool for converting between formats. We show that the underlying data model of FITS can be ported to HDF5, a first step toward achieving wider HDF5 support.},
  keywords = {data mining,research software engineering},
  annotation = {interest: 30},
  file = {/home/sam/Zotero/storage/8JKDD2RQ/Price et al. - 2014 - Is HDF5 a good format to replace UVFITS.pdf;/home/sam/Zotero/storage/6QTINXZ7/1411.html}
}

@inproceedings{priedhorskyCharliecloudUnprivilegedContainers2017,
  title = {Charliecloud: Unprivileged Containers for User-Defined Software Stacks in {{HPC}}},
  shorttitle = {Charliecloud},
  booktitle = {Proceedings of the {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  author = {Priedhorsky, Reid and Randles, Tim},
  date = {2017-11-12},
  pages = {1--10},
  publisher = {ACM},
  location = {Denver Colorado},
  doi = {10.1145/3126908.3126925},
  url = {https://dl.acm.org/doi/10.1145/3126908.3126925},
  urldate = {2022-05-26},
  abstract = {Supercomputing centers are seeing increasing demand for user-defined software stacks (UDSS), instead of or in addition to the stack provided by the center. These UDSS support user needs such as complex dependencies or build requirements, externally required configurations, portability, and consistency. The challenge for centers is to provide these services in a usable manner while minimizing the risks: security, support burden, missing functionality, and performance. We present Charliecloud, which uses the Linux user and mount namespaces to run industry-standard Docker containers with no privileged operations or daemons on center resources. Our simple approach avoids most security risks while maintaining access to the performance and functionality already on offer, doing so in just 800 lines of code. Charliecloud promises to bring an industry-standard UDSS user workflow to existing, minimally altered HPC resources.},
  eventtitle = {{{SC}} '17: {{The International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  isbn = {978-1-4503-5114-0},
  langid = {english},
  keywords = {containers,hpc,operating systems,project-acm-rep,project-provenance-pp},
  file = {/home/sam/Zotero/storage/T2VWSVNT/3126908.3126925.pdf}
}

@online{ProFTPDUsingAuthUserFiles,
  title = {{{ProFTPD}}: {{Using AuthUserFiles}}},
  url = {http://www.proftpd.org/docs/howto/AuthFiles.html},
  urldate = {2024-01-23},
  file = {/home/sam/Zotero/storage/H5K7G3JM/AuthFiles.html}
}

@online{ProvONEDataModel,
  title = {The {{ProvONE Data Model}} for {{Scientific Workflow Provenance}}},
  url = {http://jenkins-1.dataone.org/jenkins/view/Documentation%20Projects/job/ProvONE-Documentation-trunk/ws/provenance/ProvONE/v1/provone.html},
  urldate = {2022-07-26},
  abstract = {Provenance describes the origin and processing history of an artifact. Data provenance is an important form of metadata that explains how a particular data product was generated, by detailing the steps in the computational process producing it. Provenance information brings transparency and helps to audit and interpret data products. The state of the art scientific workflow systems (e.g. Kepler, Taverna, VisTrails, etc.) provide environments for specifying and enacting complex computational pipelines commonly referred to as scientific workflows. In such systems, provenance information is automatically captured in the form of execution traces. However, they often rely on proprietary formats that make the interchange of provenance information difficult. Furthermore, the workflow itself, which represents very useful information, may be disregarded in provenance traces. The evolution history of the workflow (i.e. its provenance) can likewise be missing. To address these shortcomings we propose ProvONE, a standard for scientific workflow provenance representation. ProvONE is defined as an extension of the W3C recommended standard PROV, aiming to capture the most relevant information concerning scientific workflow computational processes, and providing extension points to accommodate the specificities of particular scientific workflow systems. This document specifies the ProvONE model and details how its constituting parts are related to the W3C PROV standard. The description provided is complemented by examples including queries on ProvONE data.},
  keywords = {provenance,semantic web}
}

@online{Ptrace,
  title = {Ptrace},
  url = {https://man7.org/linux/man-pages/man2/ptrace.2.html},
  urldate = {2023-08-24},
  organization = {Linux manual page},
  keywords = {operating systems,project-provenance-pp},
  file = {/home/sam/Zotero/storage/NVK5RHL3/ptrace.2.html}
}

@article{qiuRetractionNoteLimited2019,
  title = {Retraction {{Note}}: {{Limited}} Individual Attention and Online Virality of Low-Quality Information},
  shorttitle = {Retraction {{Note}}},
  author = {Qiu, Xiaoyan and Oliveira, Diego F. M. and Shirazi, Alireza Sahami and Flammini, Alessandro and Menczer, Filippo},
  date = {2019-01},
  journaltitle = {Nature Human Behaviour},
  shortjournal = {Nat Hum Behav},
  volume = {3},
  number = {1},
  pages = {102--102},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0507-0},
  url = {https://www.nature.com/articles/s41562-018-0507-0},
  urldate = {2023-01-19},
  abstract = {The authors wish to retract this Letter as follow-up work has highlighted that two errors were committed in the analyses used to produce Figs 4d and 5. In Fig. 4d, a software bug led to an incorrect value of the discriminative power represented by the blue bar. The correct value is τ = 0.17, as opposed to the value τ = 0.15 reported in the Letter. In Fig. 5, the model plot was produced with erroneous data. Produced with the correct data, the authors’ model does not account for the virality of both high- and low-quality information observed in the empirical Facebook data (inset). In the revised figure shown in the correction notice, the distribution of high-quality meme popularity predicted by the model is substantially broader than that of low-quality memes, which do not become popular. Thus, the original conclusion, that the model predicts that low-quality information is just as likely to go viral as high-quality information, is not supported. All other results in the Letter remain valid.},
  issue = {1},
  langid = {english},
  keywords = {project-acm-rep,retraction},
  file = {/home/sam/Zotero/storage/9JL4ZV7W/Qiu et al. - 2019 - Retraction Note Limited individual attention and .pdf}
}

@online{quansightlabsandopen-sourcecontributorsNativeDependencies2023,
  title = {Native Dependencies},
  author = {Quansight Labs {and} open-source contributors},
  date = {2023-01-02},
  url = {https://pypackaging-native.github.io/key-issues/native-dependencies/},
  urldate = {2023-02-24},
  organization = {pypackaging-native},
  keywords = {package managers,project-python-packaging}
}

@article{rahmanAssessingUtilityTAM2017,
  title = {Assessing the Utility of {{TAM}}, {{TPB}}, and {{UTAUT}} for Advanced Driver Assistance Systems},
  author = {Rahman, Md Mahmudur and Lesch, Mary F. and Horrey, William J. and Strawderman, Lesley},
  date = {2017-11},
  journaltitle = {Accident Analysis \& Prevention},
  shortjournal = {Accident Analysis \& Prevention},
  volume = {108},
  pages = {361--373},
  issn = {00014575},
  doi = {10.1016/j.aap.2017.09.011},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0001457517303329},
  urldate = {2022-06-03},
  abstract = {Advanced Driver Assistance Systems (ADAS) are intended to enhance driver performance and improve transportation safety. The potential benefits of these technologies, such as reduction in number of crashes, enhancing driver comfort or convenience, decreasing environmental impact, etc., have been acknowledged by transportation safety researchers and federal transportation agencies. Although these systems afford safety advantages, they may also challenge the traditional role of drivers in operating vehicles. Driver acceptance, therefore, is essential for the implementation of these systems into the transportation system. Recognizing the need for research into the factors affecting driver acceptance, this study assessed the utility of the Technology Acceptance Model (TAM), the Theory of Planned Behavior (TPB), and the Unified Theory of Acceptance and Use of Technology (UTAUT) for modelling driver acceptance in terms of Behavioral Intention to use an ADAS. Each of these models propose a set of factors that influence acceptance of a technology. Data collection was done using two approaches: a driving simulator approach and an online survey approach. In both approaches, participants interacted with either a fatigue monitoring system or an adaptive cruise control system combined with a lane-keeping system. Based on their experience, participants responded to several survey questions to indicate their attitude toward using the ADAS and their perception of its usefulness, usability, etc. A sample of 430 surveys were collected for this study. Results found that all the models (TAM, TPB, and UTAUT) can explain driver acceptance with their proposed sets of factors, each explaining 71\% or more of the variability in Behavioral Intention. Among the models, TAM was found to perform the best in modelling driver acceptance followed by TPB. The findings of this study confirm that these models can be applied to ADAS technologies and that they provide a basis for understanding driver acceptance.},
  langid = {english},
  keywords = {internship-project,technology-acceptance},
  file = {/home/sam/Zotero/storage/W942RKZ8/1-s2.0-S0001457517303329-main.pdf}
}

@online{ramReportFirstURSSI2018,
  title = {Report from the First {{URSSI}} Workshop},
  author = {Ram, Karthik and Katz, Daniel S. and Carver, Jeffrey and Weber, Nic and Gesing, Sandra},
  date = {2018-08-23},
  url = {https://urssi.us/blog/2018/08/23/report-from-the-first-urssi-workshop/},
  urldate = {2022-08-25},
  keywords = {research software engineering},
  annotation = {interest: 80},
  file = {/home/sam/Zotero/storage/BVKHRMLB/report-from-the-first-urssi-workshop.html}
}

@report{raybournIncentivizingAdoptionSoftware2022,
  title = {Incentivizing {{Adoption}} of {{Software Quality Practices}}.},
  author = {Raybourn, Elaine M. and Milewicz, Reed and Mundt, Miranda R.},
  date = {2022-02-01},
  number = {SAND2022-1691},
  institution = {Sandia National Lab. (SNL-NM), Albuquerque, NM (United States)},
  doi = {10.2172/1845193},
  url = {https://www.osti.gov/biblio/1845193/},
  urldate = {2022-06-30},
  abstract = {Although many software teams across the laboratories comply with yearly software quality engineering (SQE) assessments, the practice of introducing quality into each phase of the software lifecycle, or the team processes, may vary substantially. Even with the support of a quality engineer, many teams struggle to adapt and right-size software engineering best practices in quality to ?t their context, and these activities aren?t framed in a way that motivates teams to take action. In short, software quality is often a ?check the box for compliance? activity instead of a cultural practice that both values software quality and knows how to achieve it. In this report, we present the results of our 6600 VISTA Innovation Tournament project, "Incentivizing and Motivating High Con?dence and Research Software Teams to Adopt the Practice of Quality." We present our ?ndings and roadmap for future work based on 1) a rapid review of relevant literature, 2) lessons learned from an internal design thinking workshop, and 3) an external Collegeville 2021 workshop. These activities provided an opportunity for team ideation and community engagement/feedback. Based on our ?ndings, we believe a coordinated effort (e.g. strategic communication campaign) aimed at diffusing the innovation of the practice of quality across Sandia National Laboratories could over time effect meaningful organizational change. As such, our roadmap addresses strategies for motivating and incentivizing individuals ranging from early career to seasoned software developers/scientists.},
  langid = {english},
  keywords = {research software engineering},
  annotation = {interest: 90},
  file = {/home/sam/Zotero/storage/V2XEMF6G/document.pdf}
}

@report{RecommendationsBestPractices,
  title = {Recommendations on the {{Best Practices}} for the {{Collection}} of {{Sexual Orientation}} and {{Gender Identity Data}} on {{Federal Statistical Surveys}}},
  langid = {english},
  keywords = {survey methodology},
  file = {/home/sam/Zotero/storage/B6JWG4UB/Recommendations on the Best Practices for the Coll.pdf}
}

@online{redheadOASPASecondStatement2013,
  title = {{{OASPA}}’s Second Statement Following the Article in {{Science}} Entitled “{{Who}}’s {{Afraid}} of {{Peer Review}}?”},
  shorttitle = {{{OASPA}}’s Second Statement Following the Article in {{Science}} Entitled “{{Who}}’s {{Afraid}} of {{Peer Review}}?},
  author = {Redhead, Claire},
  date = {2013-11-11T10:54:14+00:00},
  url = {https://oaspa.org/oaspas-second-statement-following-the-article-in-science-entitled-whos-afraid-of-peer-review/},
  urldate = {2022-08-30},
  abstract = {Since OASPA released its first response to the Science ‘Sting’ article published in October, the OASPA Board has been looking at the implications of the findings for the organisation and its members. There has also been much discussion of the Science article, exploring the strengths and weaknesses of the exercise and associated data, along with...~Read full article {$>$}},
  langid = {british},
  organization = {OASPA},
  keywords = {academic publishing,predatory journals},
  file = {/home/sam/Zotero/storage/THNNW8Q5/oaspas-second-statement-following-the-article-in-science-entitled-whos-afraid-of-peer-review.html}
}

@article{reichGenePattern202006,
  title = {{{GenePattern}} 2.0},
  author = {Reich, Michael and Liefeld, Ted and Gould, Joshua and Lerner, Jim and Tamayo, Pablo and Mesirov, Jill P},
  date = {2006-05},
  journaltitle = {Nature Genetics},
  shortjournal = {Nat Genet},
  volume = {38},
  number = {5},
  pages = {500--501},
  issn = {1061-4036, 1546-1718},
  doi = {10.1038/ng0506-500},
  url = {https://www.nature.com/articles/ng0506-500},
  urldate = {2024-10-04},
  langid = {english}
}

@online{reynoldsHamSandwichNation2013,
  type = {SSRN Scholarly Paper},
  title = {Ham {{Sandwich Nation}}: {{Due Process When Everything}} Is a {{Crime}}},
  shorttitle = {Ham {{Sandwich Nation}}},
  author = {Reynolds, Glenn Harlan},
  date = {2013-01-20},
  number = {2203713},
  location = {Rochester, NY},
  doi = {10.2139/ssrn.2203713},
  url = {https://papers.ssrn.com/abstract=2203713},
  urldate = {2023-06-07},
  abstract = {Though extensive due process protections apply to the investigation of crimes, and to criminal trials, perhaps the most important part of the criminal process -- the decision whether to charge a defendant, and with what -- is almost entirely discretionary.  Given the plethora of criminal laws and regulations in today's society, this due process gap allows prosecutors to charge almost anyone they take a deep interest in.  This Essay discusses the problem in the context of recent prosecutorial controversies involving the cases of Aaron Swartz and David Gregory, and offers some suggested remedies, along with a call for further discussion.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {civil rights},
  file = {/home/sam/Zotero/storage/YZWAHC6B/Reynolds - 2013 - Ham Sandwich Nation Due Process When Everything i.pdf}
}

@article{riordanBridgeTooFar2016,
  title = {A Bridge Too Far: {{The}} Demise of the {{Superconducting Super Collider}}},
  shorttitle = {A Bridge Too Far},
  author = {Riordan, Michael},
  date = {2016-10-01},
  journaltitle = {Physics Today},
  shortjournal = {Physics Today},
  volume = {69},
  number = {10},
  pages = {48--54},
  issn = {0031-9228},
  doi = {10.1063/PT.3.3329},
  url = {https://doi.org/10.1063/PT.3.3329},
  urldate = {2023-09-20},
  abstract = {That fateful decision, made by the leader of the world’s most powerful government, established the founding rhetoric for the SSC project, which proved difficult to abandon when it came time to enlist foreign partners.6Some physicists will counter that the SSC was in fact being pursued as an international project, with the US taking the lead in anticipation that other nations would follow; it had done so on large physics projects in the past and was doing so with the much costlier International Space Station.5 But that argument ignores the inconvenient truth that the gargantuan project was launched by the Reagan administration as a deliberate attempt to reestablish US leadership in a scientific discipline the nation had long dominated. If other nations were to become involved, they would have had to do so as junior partners in a multibillion-dollar enterprise led by US physicists.},
  keywords = {science history},
  file = {/home/sam/Zotero/storage/VBVT9ZWG/Riordan - 2016 - A bridge too far The demise of the Superconductin.pdf}
}

@book{ritchieScienceFictionsHow2020,
  title = {Science {{Fictions}}: {{How Fraud}}, {{Bias}}, {{Negligence}}, and {{Hype Undermine}} the {{Search}} for {{Truth}}},
  shorttitle = {Science {{Fictions}}},
  author = {Ritchie, Stuart},
  date = {2020-07-21},
  edition = {Illustrated edition},
  publisher = {Metropolitan Books},
  location = {New York},
  isbn = {978-1-250-22269-5},
  langid = {english},
  pagetotal = {368},
  keywords = {metascience}
}

@inproceedings{rizziTechniquesWeCreate2016,
  title = {On the Techniques We Create, the Tools We Build, and Their Misalignments: A Study of {{KLEE}}},
  shorttitle = {On the Techniques We Create, the Tools We Build, and Their Misalignments},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Software Engineering}}},
  author = {Rizzi, Eric F. and Elbaum, Sebastian and Dwyer, Matthew B.},
  date = {2016-05-14},
  pages = {132--143},
  publisher = {ACM},
  location = {Austin Texas},
  doi = {10.1145/2884781.2884835},
  url = {https://dl.acm.org/doi/10.1145/2884781.2884835},
  urldate = {2022-06-30},
  eventtitle = {{{ICSE}} '16: 38th {{International Conference}} on {{Software Engineering}}},
  isbn = {978-1-4503-3900-1},
  langid = {english},
  keywords = {metascience,research software engineering},
  file = {/home/sam/Zotero/storage/XRXMEWQM/2884781.2884835.pdf}
}

@article{robertsPublicationScientificFortran1969,
  title = {The Publication of Scientific Fortran Programs},
  author = {Roberts, K. V.},
  date = {1969-07-01},
  journaltitle = {Computer Physics Communications},
  shortjournal = {Computer Physics Communications},
  volume = {1},
  number = {1},
  pages = {1--9},
  issn = {0010-4655},
  doi = {10.1016/0010-4655(69)90011-3},
  url = {https://www.sciencedirect.com/science/article/pii/0010465569900113},
  urldate = {2023-02-23},
  abstract = {This article outlines some general principles which appear to be necessary if an international literature of published scientific programs is to be successfully established. Programming conventions are suggested for Fortran, together with several automatic documentation tools which have already been tried out and found useful.},
  langid = {english},
  keywords = {artifact evaluation,reproducibility engineering},
  file = {/home/sam/Zotero/storage/CT9MGYIV/Roberts - 1969 - The publication of scientific fortran programs.pdf;/home/sam/Zotero/storage/3VXZ6MTL/0010465569900113.html}
}

@online{rocklinBiasedBenchmarks2017,
  type = {Blog},
  title = {Biased {{Benchmarks}}},
  author = {Rocklin, Matthew},
  date = {2017-03-09},
  url = {https://matthewrocklin.com/blog/work/2017/03/09/biased-benchmarks},
  urldate = {2022-04-11},
  keywords = {software benchmarking,software engineering},
  file = {/home/sam/Zotero/storage/SJU2II9P/biased-benchmarks.html}
}

@inproceedings{rocklinDaskParallelComputation2015,
  title = {Dask: {{Parallel Computation}} with {{Blocked}} Algorithms and {{Task Scheduling}}},
  shorttitle = {Dask},
  author = {Rocklin, Matthew},
  date = {2015},
  pages = {126--132},
  location = {Austin, Texas},
  doi = {10.25080/Majora-7b98e3ed-013},
  url = {https://conference.scipy.org/proceedings/scipy2015/matthew_rocklin.html},
  urldate = {2023-05-03},
  abstract = {Dask enables parallel and out-of-core computation. We couple blocked algorithms with dynamic and memory aware task scheduling to achieve a parallel and out-of-core NumPy clone. We show how this extends the effective scale of modern hardware to larger datasets and discuss how these ideas can be more broadly applied to other parallel collections.},
  eventtitle = {Python in {{Science Conference}}},
  langid = {english},
  keywords = {data analysis,distributed systems},
  file = {/home/sam/Zotero/storage/3P6PEZE6/Rocklin - 2015 - Dask Parallel Computation with Blocked algorithms.pdf}
}

@article{rodriguez-perezReproducibilityCredibilityEmpirical2018,
  title = {Reproducibility and Credibility in Empirical Software Engineering: {{A}} Case Study Based on a Systematic Literature Review of the Use of the {{SZZ}} Algorithm},
  shorttitle = {Reproducibility and Credibility in Empirical Software Engineering},
  author = {Rodríguez-Pérez, Gema and Robles, Gregorio and González-Barahona, Jesús M.},
  date = {2018-07-01},
  journaltitle = {Information and Software Technology},
  shortjournal = {Information and Software Technology},
  volume = {99},
  pages = {164--176},
  issn = {0950-5849},
  doi = {10.1016/j.infsof.2018.03.009},
  url = {https://www.sciencedirect.com/science/article/pii/S0950584917304275},
  urldate = {2022-12-18},
  abstract = {Context Reproducibility of Empirical Software Engineering (ESE) studies is an essential part for improving their credibility, as it offers the opportunity to the research community to verify, evaluate and improve their research outcomes. Objective We aim to study reproducibility and credibility in ESE with a case study, by investigating how they have been addressed in studies where SZZ, a widely-used algorithm by Śliwerski, Zimmermann and Zeller to detect the origin of a bug, has been applied. Methodology We have performed a systematic literature review to evaluate publications that use SZZ. In total, 187 papers have been analyzed for reproducibility, reporting of limitations and use of improved versions of the algorithm. Results We have found a situation with a lot of room for improvement in ESE as reproducibility is not commonly found; factors that undermine the credibility of results are common. We offer some lessons learned and guidelines for researchers and reviewers to address this problem. Conclusion Reproducibility and other related aspects that ensure a high quality scientific process should be taken more into consideration by the ESE community in order to increase the credibility of the research results.},
  langid = {english},
  keywords = {reproducibility engineering,resaearch and practice},
  annotation = {interest: 93},
  file = {/home/sam/Zotero/storage/7MP38MC3/S0950584917304275.html}
}

@book{rogersDiffusionInnovations1983,
  title = {Diffusion of Innovations},
  author = {Rogers, Everett M.},
  date = {1983},
  edition = {3rd ed},
  publisher = {Free Press ; Collier Macmillan},
  location = {New York : London},
  isbn = {978-0-02-926650-2},
  pagetotal = {453},
  keywords = {internship-project,technology-acceptance}
}

@article{roperTestingReproducibilityRobustness,
  title = {Testing the Reproducibility and Robustness of the Cancer Biology Literature by Robot},
  author = {Roper, Katherine and Abdel-Rehim, A. and Hubbard, Sonya and Carpenter, Martin and Rzhetsky, Andrey and Soldatova, Larisa and King, Ross D.},
  journaltitle = {Journal of The Royal Society Interface},
  volume = {19},
  number = {189},
  pages = {20210821},
  publisher = {Royal Society},
  doi = {10.1098/rsif.2021.0821},
  url = {https://royalsocietypublishing.org/doi/10.1098/rsif.2021.0821},
  urldate = {2022-04-14},
  abstract = {Scientific results should not just be ‘repeatable’ (replicable in the same laboratory under identical conditions), but also ‘reproducible’ (replicable in other laboratories under similar conditions). Results should also, if possible, be ‘robust’ (replicable under a wide range of conditions). The reproducibility and robustness of only a small fraction of published biomedical results has been tested; furthermore, when reproducibility is tested, it is often not found. This situation is termed ‘the reproducibility crisis', and it is one the most important issues facing biomedicine. This crisis would be solved if it were possible to automate reproducibility testing. Here, we describe the semi-automated testing for reproducibility and robustness of simple statements (propositions) about cancer cell biology automatically extracted from the literature. From 12 260 papers, we automatically extracted statements predicted to describe experimental results regarding a change of gene expression in response to drug treatment in breast cancer, from these we selected 74 statements of high biomedical interest. To test the reproducibility of these statements, two different teams used the laboratory automation system Eve and two breast cancer cell lines (MCF7 and MDA-MB-231). Statistically significant evidence for repeatability was found for 43 statements, and significant evidence for reproducibility/robustness in 22 statements. In two cases, the automation made serendipitous discoveries. The reproduced/robust knowledge provides significant insight into cancer. We conclude that semi-automated reproducibility testing is currently achievable, that it could be scaled up to generate a substantive source of reliable knowledge and that automation has the potential to mitigate the reproducibility crisis.},
  keywords = {cancer,scientific method},
  annotation = {interest: 20},
  file = {/home/sam/Zotero/storage/6ZFYNATN/Roper et al. - Testing the reproducibility and robustness of the .pdf;/home/sam/Zotero/storage/4UBWT9B8/2022-04-robot-scientist-eve-one-third-scientific.html}
}

@unpublished{roquesSysMLVsUML2011,
  title = {{{SysML}} vs. {{UML}} 2: {{A Detailed Comparison}}},
  author = {Roques, Pascal},
  date = {2011-10-16},
  url = {https://ecs.wgtn.ac.nz/foswiki/pub/Events/MODELS2011/Material/MODELS_2011_T2-Roques-SysML_UML2.pdf},
  eventtitle = {{{MoDELS}}’11 {{Tutorial}}},
  file = {/home/sam/Zotero/storage/EHGHMTUK/MODELS_2011_T2-Roques-SysML_UML2.pdf}
}

@article{rosenbergNextFrontierMaking2019,
  title = {The {{Next Frontier}}: {{Making Research More Reproducible}}},
  shorttitle = {The {{Next Frontier}}},
  author = {Rosenberg, David and Fillion, Yves and Teasley, Rebecca and Sandoval-Solis, Samuel and Hecht, Jory and family=Zyl, given=Jakobus, prefix=van, useprefix=false and McMahon, George and Horsburgh, Jeffery and Kasprzyk, Joseph and Tarboton, David},
  date = {2019-11-27},
  journaltitle = {Journal of Water Resources Planning and Management},
  pages = {1--10},
  doi = {10.1061/(ASCE)WR.1943-5452.0001215},
  url = {https://digitalcommons.usu.edu/water_pubs/156},
  keywords = {reproducibility engineering},
  file = {/home/sam/Zotero/storage/VF8HC4ZF/Rosenberg et al. - 2019 - The Next Frontier Making Research More Reproducib.pdf;/home/sam/Zotero/storage/VD8WB9MF/156.html}
}

@article{roseneSoftwareMaintainabilityWhat1981,
  title = {Software {{Maintainability}} - {{What It Means}} and {{How}} to {{Achieve It}}},
  author = {Rosene, A.F. and Connolly, J.E. and Bracy, K.M.},
  date = {1981-08},
  journaltitle = {IEEE Transactions on Reliability},
  shortjournal = {IEEE Trans. Rel.},
  volume = {R-30},
  number = {3},
  pages = {240--245},
  issn = {0018-9529, 1558-1721},
  doi = {10.1109/TR.1981.5221065},
  url = {http://ieeexplore.ieee.org/document/5221065/},
  urldate = {2022-05-26},
  abstract = {The terms reliability and maintainability are often misunderstood in the software field since software does not `break' or `wear out' in the physical sense; it either works in a given environment or it does not. This means that the program is either right or wrong in the environment. However, it does not follow that a program that is right is reliable or maintainable. For the purposes of this paper a program is maintainable if it meets the following two conditions: There is a high probability of determining the cause of a problem in a timely manner the first time it occurs, and There is a high probability of being able to modify the program without causing an error in some other part of the program. There are three important topics in developing a program which fulfills the above criteria: documentation, standards, and system architecture. This paper discusses the third topic since it is the real key to the development of maintainable software. A particular architecture of structured modular design using controlled communication between modules is presented together with its relationship to maintainability and reliability. The benefits of this approach are: - High isolation between modules. - Communication visibility and monitoring. - Error location. - Overload control. - Simplified control program.  Transparency to multicomputer configurations. A method is presented for calculating maintainability parameters related to this architecture, and examples of these calculations are given and interpreted.},
  keywords = {internship-project,software engineering},
  file = {/home/sam/Zotero/storage/AF4P5QPS/Software_Maintainability_-_What_It_Means_and_How_to_Achieve_It.pdf}
}

@video{roslingBestStatsYou2007,
  entrysubtype = {video},
  title = {The Best Stats You've Ever Seen},
  editor = {Rosling, Hans},
  editortype = {director},
  date = {2007-01-16},
  location = {Monterey, CA},
  url = {https://www.youtube.com/watch?v=hVimVzgtD6w},
  urldate = {2022-09-06},
  keywords = {current events},
  annotation = {interest: 98}
}

@video{roslingHowNotBe2014,
  entrysubtype = {video},
  title = {How Not to Be Ignorant about the World},
  editor = {Rosling, Hans and Rosling, Ola},
  editortype = {director},
  date = {2014-09-11},
  url = {https://www.youtube.com/watch?v=Sm5xF-UYgdg},
  urldate = {2022-09-06},
  annotation = {interest: 98}
}

@online{rossantMovingAwayHDF5,
  title = {Moving Away from {{HDF5}}},
  author = {Rossant, Cyrille},
  url = {https://cyrille.rossant.net/moving-away-hdf5/},
  urldate = {2022-04-14},
  file = {/home/sam/Zotero/storage/EEIN6J4G/moving-away-hdf5.html}
}

@article{rothermelAnalyzingRegressionTest1996,
  title = {Analyzing Regression Test Selection Techniques},
  author = {Rothermel, G. and Harrold, M.J.},
  date = {1996-08},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {22},
  number = {8},
  pages = {529--551},
  issn = {1939-3520},
  doi = {10.1109/32.536955},
  abstract = {Regression testing is a necessary but expensive maintenance activity aimed at showing that code has not been adversely affected by changes. Regression test selection techniques reuse tests from an existing test suite to test a modified program. Many regression test selection techniques have been proposed, however, it is difficult to compare and evaluate these techniques because they have different goals. This paper outlines the issues relevant to regression test selection techniques, and uses these issues as the basis for a framework within which to evaluate the techniques. The paper illustrates the application of the framework by using it to evaluate existing regression test selection techniques. The evaluation reveals the strengths and weaknesses of existing techniques, and highlights some problems that future work in this area should address.},
  eventtitle = {{{IEEE Transactions}} on {{Software Engineering}}},
  keywords = {software testing},
  annotation = {interest: 73},
  file = {/home/sam/Zotero/storage/PGUS477U/Rothermel and Harrold - 1996 - Analyzing regression test selection techniques.pdf;/home/sam/Zotero/storage/7V2GEBLI/536955.html}
}

@article{rothermelMethodologyTestingSpreadsheets2001,
  title = {A Methodology for Testing Spreadsheets},
  author = {Rothermel, Gregg and Burnett, Margaret and Li, Lixin and Dupuis, Christopher and Sheretov, Andrei},
  date = {2001-01-01},
  journaltitle = {ACM Transactions on Software Engineering and Methodology},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  volume = {10},
  number = {1},
  pages = {110--147},
  issn = {1049-331X},
  doi = {10.1145/366378.366385},
  url = {https://doi.org/10.1145/366378.366385},
  urldate = {2022-08-31},
  abstract = {Spreadsheet languages, which include commercial spreadsheets and various research systems, have had a substantial impact on end-user computing. Research shows, however, that spreadsheets often contain faults; thus, we would like to provide at least some of the benefits of formal testing methodologies to the creators of spreadsheets. This article presents a testing methodology that adapts data flow adequacy criteria and coverage monitoring to the task of testing spreadsheets. To accommodate the evaluation model used with spreadsheets, and the interactive process by which they are created, our methodology is incremental. To accommodate the users of spreadsheet languages, we provide an interface to our methodology that does not require an understanding of testing theory. We have implemented our testing methodology in the context of the Forms/3 visual spreadsheet language. We report on the methodology, its time and space costs, and the mapping from the testing strategy to the user interface. In an empirical study, we found that test suites created according to our methodology detected, on average, 81\% of the faults in a set of faulty spreadsheets, significantly outperforming randomly generated test suites.},
  keywords = {research software engineering,software testing},
  annotation = {interest: 54},
  file = {/home/sam/Zotero/storage/4CX6TW9G/Rothermel et al. - 2001 - A methodology for testing spreadsheets.pdf}
}

@inproceedings{rougierReScienceJournalReproducible2019,
  title = {{{ReScience C}}: {{A Journal}} for {{Reproducible Replications}} in {{Computational Science}}},
  shorttitle = {{{ReScience C}}},
  booktitle = {Reproducible {{Research}} in {{Pattern Recognition}}},
  author = {Rougier, Nicolas P. and Hinsen, Konrad},
  editor = {Kerautret, Bertrand and Colom, Miguel and Lopresti, Daniel and Monasse, Pascal and Talbot, Hugues},
  date = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {150--156},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-23987-9_14},
  abstract = {Independent replication is one of the most powerful methods to verify published scientific studies. In computational science, it requires the reimplementation of the methods described in the original article by a different team of researchers. Replication is often performed by scientists who wish to gain a better understanding of a published method, but its results are rarely made public. ReScience~C is a peer-reviewed journal dedicated to the publication of high-quality computational replications that provide added value to the scientific community. To this end, ReScience~C requires replications to be reproducible and implemented using Open Source languages and libraries. In this article, we provide an overview of ReScience~C’s goals and quality standards, outline the submission and reviewing processes, and summarize the experience of its first three years of operation, concluding with an outlook towards evolutions envisaged for the near future.},
  isbn = {978-3-030-23987-9},
  langid = {english},
  keywords = {project-acm-rep,project-provenance-pp,reproducibility engineering},
  file = {/home/sam/Zotero/storage/3YZGFA62/Rougier and Hinsen - 2019 - ReScience C A Journal for Reproducible Replicatio.pdf}
}

@article{rougierSustainableComputationalScience2017,
  title = {Sustainable Computational Science: The {{ReScience}} Initiative},
  shorttitle = {Sustainable Computational Science},
  author = {Rougier, Nicolas P. and Hinsen, Konrad and Alexandre, Frédéric and Arildsen, Thomas and Barba, Lorena and Benureau, Fabien C. Y. and Brown, C. Titus and family=Buyl, given=Pierre, prefix=de, useprefix=true and Caglayan, Ozan and Davison, Andrew P. and Delsuc, Marc André and Detorakis, Georgios and Diem, Alexandra K. and Drix, Damien and Enel, Pierre and Girard, Benoît and Guest, Olivia and Hall, Matt G. and Henriques, Rafael Neto and Hinaut, Xavier and Jaron, Kamil S. and Khamassi, Mehdi and Klein, Almar and Manninen, Tiina and Marchesi, Pietro and McGlinn, Dan and Metzner, Christoph and Petchey, Owen L. and Plesser, Hans Ekkehard and Poisot, Timothée and Ram, Karthik and Ram, Yoav and Roesch, Etienne and Rossant, Cyrille and Rostami, Vahid and Shifman, Aaron and Stachelek, Joseph and Stimberg, Marcel and Stollmeier, Frank and Vaggi, Federico and Viejo, Guillaume and Vitay, Julien and Vostinar, Anya and Yurchak, Roman and Zito, Tiziano},
  date = {2017-12-18},
  journaltitle = {PeerJ Computer Science},
  volume = {3},
  eprint = {1707.04393},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {e142},
  issn = {2376-5992},
  doi = {10.7717/peerj-cs.142},
  url = {http://arxiv.org/abs/1707.04393},
  urldate = {2024-10-04},
  abstract = {Computer science offers a large set of tools for prototyping, writing, running, testing, validating, sharing and reproducing results, however computational science lags behind. In the best case, authors may provide their source code as a compressed archive and they may feel confident their research is reproducible. But this is not exactly true. James Buckheit and David Donoho proposed more than two decades ago that an article about computational results is advertising, not scholarship. The actual scholarship is the full software environment, code, and data that produced the result. This implies new workflows, in particular in peer-reviews. Existing journals have been slow to adapt: source codes are rarely requested, hardly ever actually executed to check that they produce the results advertised in the article. ReScience is a peer-reviewed journal that targets computational research and encourages the explicit replication of already published research, promoting new and open-source implementations in order to ensure that the original research can be replicated from its description. To achieve this goal, the whole publishing chain is radically different from other traditional scientific journals. ReScience resides on GitHub where each new implementation of a computational study is made available together with comments, explanations, and software tests.},
  keywords = {Computer Science - Digital Libraries},
  file = {/home/sam/Zotero/storage/ZLR5EW56/Rougier et al. - 2017 - Sustainable computational science the ReScience initiative.pdf;/home/sam/Zotero/storage/77GTZLFH/1707.html}
}

@article{rubinCausalInferenceUsing2005,
  title = {Causal {{Inference Using Potential Outcomes}}},
  author = {Rubin, Donald B},
  date = {2005-03-01},
  journaltitle = {Journal of the American Statistical Association},
  volume = {100},
  number = {469},
  pages = {322--331},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1198/016214504000001880},
  url = {https://doi.org/10.1198/016214504000001880},
  urldate = {2022-09-09},
  abstract = {Causal effects are defined as comparisons of potential outcomes under different treatments on a common set of units. Observed values of the potential outcomes are revealed by the assignment mechanism—a probabilistic model for the treatment each unit receives as a function of covariates and potential outcomes. Fisher made tremendous contributions to causal inference through his work on the design of randomized experiments, but the potential outcomes perspective applies to other complex experiments and nonrandomized studies as well. As noted by Kempthorne in his 1976 discussion of Savage's Fisher lecture, Fisher never bridged his work on experimental design and his work on parametric modeling, a bridge that appears nearly automatic with an appropriate view of the potential outcomes framework, where the potential outcomes and covariates are given a Bayesian distribution to complete the model specification. Also, this framework crisply separates scientific inference for causal effects and decisions based on such inference, a distinction evident in Fisher's discussion of tests of significance versus tests in an accept/reject framework. But Fisher never used the potential outcomes framework, originally proposed by Neyman in the context of randomized experiments, and as a result he provided generally flawed advice concerning the use of the analysis of covariance to adjust for posttreatment concomitants in randomized trials.},
  keywords = {causality},
  annotation = {interest: 85}
}

@inproceedings{rubinsteynParakeetJustInTimeParallel2012,
  title = {Parakeet: {{A}} \{\vphantom\}{{Just-In-Time}}\vphantom\{\} {{Parallel Accelerator}} for {{Python}}},
  shorttitle = {Parakeet},
  author = {Rubinsteyn, Alex and Hielscher, Eric and Weinman, Nathaniel and Shasha, Dennis},
  date = {2012},
  url = {https://www.usenix.org/conference/hotpar12/workshop-program/presentation/rubinsteyn},
  urldate = {2022-10-18},
  eventtitle = {4th {{USENIX Workshop}} on {{Hot Topics}} in {{Parallelism}} ({{HotPar}} 12)},
  langid = {english},
  annotation = {interest: 75},
  file = {/home/sam/Zotero/storage/CZICVIUB/Rubinsteyn et al. - 2012 - Parakeet A Just-In-Time Parallel Accelerator fo.pdf;/home/sam/Zotero/storage/7YQN6C8P/rubinsteyn.html}
}

@inproceedings{ruleExplorationExplanationComputational2018,
  title = {Exploration and {{Explanation}} in {{Computational Notebooks}}},
  booktitle = {Proceedings of the 2018 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Rule, Adam and Tabard, Aurélien and Hollan, James D.},
  date = {2018-04-19},
  pages = {1--12},
  publisher = {ACM},
  location = {Montreal QC Canada},
  doi = {10.1145/3173574.3173606},
  url = {https://dl.acm.org/doi/10.1145/3173574.3173606},
  urldate = {2022-07-07},
  eventtitle = {{{CHI}} '18: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-5620-6},
  langid = {english},
  keywords = {computational notebooks},
  annotation = {interest: 61}
}

@article{rupprechtImprovingReproducibilityData2020,
  title = {Improving Reproducibility of Data Science Pipelines through Transparent Provenance Capture},
  author = {Rupprecht, Lukas and Davis, James C. and Arnold, Constantine and Gur, Yaniv and Bhagwat, Deepavali},
  date = {2020-08-01},
  journaltitle = {Proceedings of the VLDB Endowment},
  shortjournal = {Proc. VLDB Endow.},
  volume = {13},
  number = {12},
  pages = {3354--3368},
  issn = {2150-8097},
  doi = {10.14778/3415478.3415556},
  url = {https://dl.acm.org/doi/10.14778/3415478.3415556},
  urldate = {2023-08-24},
  abstract = {Data science has become prevalent in a large variety of domains. Inherent in its practice is an exploratory, probing, and fact finding journey, which consists of the assembly, adaptation, and execution of complex data science pipelines. The trustworthiness of the results of such pipelines rests entirely on their ability to be reproduced with fidelity, which is difficult if pipelines are not documented or recorded minutely and consistently. This difficulty has led to a reproducibility crisis and presents a major obstacle to the safe adoption of the pipeline results in production environments. The crisis can be resolved if the provenance for each data science pipeline is captured transparently as pipelines are executed. However, due to the complexity of modern data science pipelines, transparently capturing sufficient provenance to allow for reproducibility is challenging. As a result, most existing systems require users to augment their code or use specific tools to capture provenance, which hinders productivity and results in a lack of adoption. In this paper, we present Ursprung,1 a transparent provenance collection system designed for data science environments.2 The Ursprung philosophy is to capture provenance and build lineage by integrating with the execution environment to automatically track static and runtime configuration parameters of data science pipelines. Rather than requiring data scientists to make changes to their code, Ursprung records basic provenance information from system-level sources and combines it with provenance from application-level sources (e.g., log files, stdout), which can be accessed and recorded through a domain-specific language. In our evaluation, we show that Ursprung is able to capture sufficient provenance for a variety of use cases and only adds an overhead of up to 4\%.},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/YVLSLMIG/Rupprecht et al. - 2020 - Improving reproducibility of data science pipeline.pdf}
}

@online{russelHowMakeThis2008,
  title = {How {{Do I Make This Hard}} to {{Misuse}}?},
  author = {Russel, Rusty},
  date = {2008-03-30},
  url = {https://ozlabs.org/~rusty/index.cgi/tech/2008-03-30.html},
  urldate = {2024-01-08},
  file = {/home/sam/Zotero/storage/4QQMLETX/2008-03-30.html}
}

@legislation{ryanFoundationsEvidenceBasedPolicymaking2019,
  title = {Foundations for {{Evidence-Based Policymaking Act}} of 2018},
  author = {Ryan, Paul D.},
  date = {2019-01-14},
  number = {H.R. 4174},
  keywords = {project-acm-rep}
}

@article{saeedizadeDDBWSDynamicDeadline2021,
  title = {{{DDBWS}}: A Dynamic Deadline and Budget-Aware Workflow Scheduling Algorithm in Workflow-as-a-Service Environments},
  shorttitle = {{{DDBWS}}},
  author = {Saeedizade, Ehsan and Ashtiani, Mehrdad},
  date = {2021-12-01},
  journaltitle = {The Journal of Supercomputing},
  shortjournal = {J Supercomput},
  volume = {77},
  number = {12},
  pages = {14525--14564},
  issn = {1573-0484},
  doi = {10.1007/s11227-021-03858-6},
  url = {https://doi.org/10.1007/s11227-021-03858-6},
  urldate = {2022-09-06},
  abstract = {Workflow scheduling has been excessively studied in different environments like clusters, grids, and clouds. Cloud is a scalable, cost-effective environment that allows users to access an unlimited amount of resources and offers a pay-as-you-go model. An increase in the users’ desire to run their workflow applications on clouds leads to the development of multi-tenant environments like workflow-as-a-service platforms (WaaS). By leveraging cloud features, WaaS offers an environment where users can submit their workflows for execution with different quality of service (QoS) attributes at different. The problem of finding an appropriate scheduling algorithm considering factors like resource heterogeneity and QoS requirements is an NP-complete problem. Most of the existing algorithms in the literature are designed to schedule a single instance of a workflow or have a static behavior. Using static scheduling in dynamic environments like WaaS can lead to a low planning success rate. Besides, while it is possible to share resources between users, for simplicity purposes a majority of proposed algorithms schedule at most one task on a computing resource at any given point in time. They also consider either the time or cost as a hard constraint during scheduling. To cover these limitations in this study, we propose DDBWS, a Dynamic, Deadline and Budget-aware, Workflow Scheduling algorithm that is designed specifically for the WaaS environments. DDBWS schedules workflows by solving a multi-resource packing problem. Unlike several existing algorithms, it considers both CPU and memory demands for tasks simultaneously. Also, it leverages containers to run multiple tasks on a VM concurrently. It uses a bi-factor to control the tradeoff between cost and resource utilization during the mapping of tasks to resources. Based on real-world workflow traces, we have generated 6 different datasets of synthetic workflows. To compare the performance of the proposed scheduling algorithm, we chose two of the state-of-the-art dynamic concurrent workflow scheduling algorithms called EPSM and MW-HBDCS. We have conducted several experiments on these datasets. The results of the performed experiments show that DDBWS achieves at least 96\% planning success rate on 6 different workloads which is a comparable PSR. The proposed algorithm decreases the total leased VM numbers considerably. It also outperforms its rivals in terms of the total execution cost and decreases the overall execution cost by at least 8.03\% and on average 32.08\%. The 95\% confidence interval for this decrease is 32.08\,±\,14.1 based on 12 samples.},
  langid = {english},
  keywords = {scheduling,workflow managers},
  annotation = {interest: 76}
}

@inproceedings{sakalisSplash3ProperlySynchronized2016,
  title = {Splash-3: {{A}} Properly Synchronized Benchmark Suite for Contemporary Research},
  shorttitle = {Splash-3},
  booktitle = {2016 {{IEEE International Symposium}} on {{Performance Analysis}} of {{Systems}} and {{Software}} ({{ISPASS}})},
  author = {Sakalis, Christos and Leonardsson, Carl and Kaxiras, Stefanos and Ros, Alberto},
  date = {2016-04},
  pages = {101--111},
  doi = {10.1109/ISPASS.2016.7482078},
  url = {https://ieeexplore.ieee.org/abstract/document/7482078?casa_token=Uy0X7NXLNqcAAAAA:6l5GoGgYgAIopiywBBMNih_IIiS0P1olFyxxW7r_cHaeB_eB6bsyUpQ1KQk0083iqBPJ0vt8},
  urldate = {2024-02-08},
  abstract = {Benchmarks are indispensable in evaluating the performance implications of new research ideas. However, their usefulness is compromised if they do not work correctly on a system under evaluation or, in general, if they cannot be used consistently to compare different systems. A well-known benchmark suite of parallel applications is the Splash-2 suite. Since its creation in the context of the DASH project, Splash-2 benchmarks have been widely used in research. However, Splash-2 was released over two decades ago and does not adhere to the recent C memory consistency model. This leads to unexpected and often incorrect behavior when some Splash-2 benchmarks are used in conjunction with contemporary compilers and hardware (simulated or real). Most importantly, we discovered critical performance bugs that may question some of the reported benchmark results. In this work, we analyze the Splash-2 benchmarks and expose data races and related performance bugs. We rectify the problematic benchmarks and evaluate the resulting performance. Our work contributes to the community a new sanitized version of the Splash-2 benchmarks, called the Splash-3 benchmark suite.},
  eventtitle = {2016 {{IEEE International Symposium}} on {{Performance Analysis}} of {{Systems}} and {{Software}} ({{ISPASS}})},
  keywords = {benchmarking,project-provenance-pp},
  file = {/home/sam/Zotero/storage/8GZ8FS6H/Sakalis et al. - 2016 - Splash-3 A properly synchronized benchmark suite .pdf;/home/sam/Zotero/storage/GBWPKZA6/7482078.html}
}

@article{samuelEndtoEndProvenanceRepresentation2022,
  title = {End-to-{{End}} Provenance Representation for the Understandability and Reproducibility of Scientific Experiments Using a Semantic Approach},
  author = {Samuel, Sheeba and König-Ries, Birgitta},
  date = {2022-12},
  journaltitle = {Journal of Biomedical Semantics},
  shortjournal = {J Biomed Semant},
  volume = {13},
  number = {1},
  pages = {1},
  issn = {2041-1480},
  doi = {10.1186/s13326-021-00253-1},
  url = {https://jbiomedsem.biomedcentral.com/articles/10.1186/s13326-021-00253-1},
  urldate = {2022-08-03},
  abstract = {Abstract                              Background                The advancement of science and technologies play an immense role in the way scientific experiments are being conducted. Understanding how experiments are performed and how results are derived has become significantly more complex with the recent explosive growth of heterogeneous research data and methods. Therefore, it is important that the provenance of results is tracked, described, and managed throughout the research lifecycle starting from the beginning of an experiment to its end to ensure reproducibility of results described in publications. However, there is a lack of interoperable representation of end-to-end provenance of scientific experiments that interlinks data, processing steps, and results from an experiment’s computational and non-computational processes.                                            Results                We present the “REPRODUCE-ME” data model and ontology to describe the end-to-end provenance of scientific experiments by extending existing standards in the semantic web. The ontology brings together different aspects of the provenance of scientific studies by interlinking non-computational data and steps with computational data and steps to achieve understandability and reproducibility. We explain the important classes and properties of the ontology and how they are mapped to existing ontologies like PROV-O and P-Plan. The ontology is evaluated by answering competency questions over the knowledge base of scientific experiments consisting of computational and non-computational data and steps.                                            Conclusion                We have designed and developed an interoperable way to represent the complete path of a scientific experiment consisting of computational and non-computational steps. We have applied and evaluated our approach to a set of scientific experiments in different subject domains like computational science, biological imaging, and microscopy.},
  langid = {english},
  keywords = {provenance,reproducibility engineering}
}

@article{sandersModifiedNewtonianDynamics2002,
  title = {Modified {{Newtonian Dynamics}} as an {{Alternative}} to {{Dark Matter}}},
  author = {Sanders, Robert H. and McGaugh, Stacy S.},
  date = {2002},
  journaltitle = {Annual Review of Astronomy and Astrophysics},
  volume = {40},
  number = {1},
  pages = {263--317},
  doi = {10.1146/annurev.astro.40.060401.093923},
  url = {https://doi.org/10.1146/annurev.astro.40.060401.093923},
  urldate = {2022-05-02},
  abstract = {Modified Newtonian dynamics (MOND) is an empirically motivated modification of Newtonian gravity or inertia suggested by Milgrom as an alternative to cosmic dark matter. The basic idea is that at accelerations below ao ≈ 10−8 cm/s2 ≈ cHo/6 the effective gravitational attraction approaches , where gn is the usual Newtonian acceleration. This simple algorithm yields flat rotation curves for spiral galaxies and a mass-rotation velocity relation of the form M ∝ V4 that forms the basis for the observed luminosity–rotation velocity relation—the Tully-Fisher law. We review the phenomenological success of MOND on scales ranging from dwarf spheroidal galaxies to superclusters and demonstrate that the evidence for dark matter can be equally well interpreted as evidence for MOND. We discuss the possible physical basis for an acceleration-based modification of Newtonian dynamics as well as the extention of MOND to cosmology and structure formation.},
  keywords = {astrophysics,cosmology,project-astrophysics},
  file = {/home/sam/Zotero/storage/42RDEUCW/Sanders and McGaugh - 2002 - Modified Newtonian Dynamics as an Alternative to D.pdf}
}

@online{SandiaAnalysisWorkbench,
  title = {Sandia {{Analysis Workbench}} ({{SAW}})},
  url = {https://www.sandia.gov/saw/},
  urldate = {2022-06-14},
  abstract = {Integration Workflow Analysis The Sandia Analysis Workbench (SAW) is a family of software applications that boost productivity and quality by making modeling and simulation easier while enforcing best practices and supporting ubiquitous V\&V. Capabilities include workflow management, model bui...},
  langid = {american},
  organization = {Sandia Analysis Workbench},
  keywords = {internship-project,workflow managers}
}

@article{sandveTenSimpleRules2013,
  title = {Ten {{Simple Rules}} for {{Reproducible Computational Research}}},
  author = {Sandve, Geir Kjetil and Nekrutenko, Anton and Taylor, James and Hovig, Eivind},
  date = {2013-10-24},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  volume = {9},
  number = {10},
  pages = {e1003285},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003285},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003285},
  urldate = {2023-11-20},
  langid = {english},
  file = {/home/sam/Zotero/storage/FWAF4U9S/Sandve et al. - 2013 - Ten Simple Rules for Reproducible Computational Re.pdf}
}

@article{santana-perezReproducibilityScientificWorkflows2015,
  title = {Towards {{Reproducibility}} in {{Scientific Workflows}}: {{An Infrastructure-Based Approach}}},
  shorttitle = {Towards {{Reproducibility}} in {{Scientific Workflows}}},
  author = {Santana-Perez, Idafen and Pérez-Hernández, María S.},
  date = {2015-02-24},
  journaltitle = {Scientific Programming},
  volume = {2015},
  pages = {e243180},
  publisher = {Hindawi},
  issn = {1058-9244},
  doi = {10.1155/2015/243180},
  url = {https://www.hindawi.com/journals/sp/2015/243180/},
  urldate = {2022-12-18},
  abstract = {It is commonly agreed that in silico scientific experiments should be executable and repeatable processes. Most of the current approaches for computational experiment conservation and reproducibility have focused so far on two of the main components of the experiment, namely, data and method. In this paper, we propose a new approach that addresses the third cornerstone of experimental reproducibility: the equipment. This work focuses on the equipment of a computational experiment, that is, the set of software and hardware components that are involved in the execution of a scientific workflow. In order to demonstrate the feasibility of our proposal, we describe a use case scenario on the Text Analytics domain and the application of our approach to it. From the original workflow, we document its execution environment, by means of a set of semantic models and a catalogue of resources, and generate an equivalent infrastructure for reexecuting it.},
  langid = {english},
  annotation = {interest: 90},
  file = {/home/sam/Zotero/storage/UJBNBBZR/Santana-Perez and Pérez-Hernández - 2015 - Towards Reproducibility in Scientific Workflows A.pdf}
}

@unpublished{sarLineageFileSystem,
  title = {Lineage {{File System}}},
  author = {Sar, Can and Cao, Pei},
  url = {http://crypto.stanford.edu/~cao/lineage.html},
  urldate = {2023-08-23},
  keywords = {project-provenance-pp,provenance-tool}
}

@unpublished{schaureckerSuperresolvingDarkMatter2021,
  title = {Super-Resolving {{Dark Matter Halos}} Using {{Generative Deep Learning}}},
  author = {Schaurecker, David and Li, Yin and Tinker, Jeremy and Ho, Shirley and Refregier, Alexandre},
  date = {2021-11-11},
  eprint = {2111.06393},
  eprinttype = {arXiv},
  eprintclass = {astro-ph},
  url = {http://arxiv.org/abs/2111.06393},
  urldate = {2022-04-11},
  abstract = {Generative deep learning methods built upon Convolutional Neural Networks (CNNs) provide a great tool for predicting non-linear structure in cosmology. In this work we predict high resolution dark matter halos from large scale, low resolution dark matter only simulations. This is achieved by mapping lower resolution to higher resolution density fields of simulations sharing the same cosmology, initial conditions and box-sizes. To resolve structure down to a factor of 8 increase in mass resolution, we use a variation of U-Net with a conditional GAN, generating output that visually and statistically matches the high resolution target extremely well. This suggests that our method can be used to create high resolution density output over Gpc/h box-sizes from low resolution simulations with negligible computational effort.},
  keywords = {astrophysics,machine learning,project-astrophysics},
  file = {/home/sam/Zotero/storage/KZ4RBGIG/Schaurecker et al. - 2021 - Super-resolving Dark Matter Halos using Generative.pdf;/home/sam/Zotero/storage/VBN36AJ2/Schaurecker et al. - 2022 - Super-resolving Dark Matter Halos using Generative.pdf}
}

@article{scheideggerTacklingProvenanceChallenge2008,
  title = {Tackling the {{Provenance Challenge}} One Layer at a Time},
  author = {Scheidegger, Carlos and Koop, David and Santos, Emanuele and Vo, Huy and Callahan, Steven and Freire, Juliana and Silva, Cláudio},
  date = {2008},
  journaltitle = {Concurrency and Computation: Practice and Experience},
  volume = {20},
  number = {5},
  pages = {473--483},
  issn = {1532-0634},
  doi = {10.1002/cpe.1237},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.1237},
  urldate = {2023-07-18},
  abstract = {VisTrails is a new workflow and provenance management system that provides support for scientific data exploration and visualization. Whereas workflows have been traditionally used to automate repetitive tasks, for applications that are exploratory in nature, change is the norm. VisTrails uses a new change-based provenance mechanism, which was designed to handle rapidly evolving workflows. It uniformly and automatically captures provenance information for data products and for the evolution of the workflows used to generate these products. In this paper, we describe how the VisTrails provenance data are organized in layers and present a first approach for querying this data that we developed to tackle the Provenance Challenge queries. Copyright © 2007 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {provenance,workflow managers},
  file = {/home/sam/Zotero/storage/SRZ85ZHY/Scheidegger et al. - 2008 - Tackling the Provenance Challenge one layer at a t.pdf;/home/sam/Zotero/storage/M2RA2N8F/cpe.html}
}

@online{schimmackMostPublishedResults2021,
  title = {Most Published Results in Medical Journals Are Not False},
  author = {Schimmack, Ulrich},
  date = {2021-08-10T15:24:29+00:00},
  url = {https://replicationindex.com/2021/08/10/fpr-medicine/},
  urldate = {2022-11-14},
  abstract = {Peer Reviewed by Editors of Biostatistics “You have produced a nicely written paper that seems to be mathematically correct and I enjoyed reading” (Professor Dimitris Rizopoulos \& P…},
  langid = {american},
  organization = {Replicability-Index},
  keywords = {metascience},
  annotation = {interest: 87},
  file = {/home/sam/Zotero/storage/W5Z6CBJE/fpr-medicine.html}
}

@online{schlawackSemanticVersioningWill2021,
  title = {Semantic {{Versioning Will Not Save You}}},
  author = {Schlawack, Hynek},
  date = {2021-03-02T00:00:00Z},
  url = {https://hynek.me/articles/semver-will-not-save-you/},
  urldate = {2022-04-20},
  abstract = {The widely used Python package cryptography changed their build system to use Rust for low-level code which caused an emotional GitHub thread. Enthusiasts of 32-bit hardware from the 1990s aside, there was a vocal faction that stipulated adherence to Semantic Versioning from the maintainers – claiming it would’ve prevented all grief. I will show you not only why this is wrong, but also how relying on Semantic Versioning hurts you.},
  langid = {american},
  organization = {Hynek Schlawack},
  keywords = {industry practices,package versioning,software engineering},
  file = {/home/sam/Zotero/storage/B33TN9CF/semver-will-not-save-you.html}
}

@online{schlueterKikLeftpadNpm,
  title = {Kik, Left-Pad, and Npm},
  shorttitle = {Kik, Left-Pad, and Npm},
  author = {Schlueter, Isaac},
  url = {https://blog.npmjs.org/post/141577284765/kik-left-pad-and-npm},
  urldate = {2022-04-18},
  organization = {npm Blog (Archive)},
  file = {/home/sam/Zotero/storage/B8FXB68P/kik-left-pad-and-npm.html}
}

@online{schreinerShouldYouUse,
  title = {Should {{You Use Upper Bound Version Constraints}}?},
  author = {Schreiner, Henry},
  url = {https://iscinumpy.dev/post/bound-version-constraints/},
  urldate = {2022-04-20},
  abstract = {Bound version constraints (upper caps) are starting to show up in the Python ecosystem. This is causing real world problems with libraries following this recommendation, and is likely to continue to get worse; this practice does not scale to large numbers of libraries or large numbers of users. In this discussion I would like to explain why always providing an upper limit causes far more harm than good even for true SemVer libraries, why libraries that pin upper limits require more frequent updates rather than less, and why it is not scalable.  After reading this, hopefully you will always consider every cap you add, you will know the (few) places where pinning an upper limit is reasonable, and will possibly even avoid using libraries that pin upper limits needlessly until the author updates them to remove these pins. If this 10,000 word behemoth is a bit long for you, then skip around using the table of contents, or see the TL;DR section at the end, or read version numbers by Bernát Gábor, which is shorter but is a fantastic read with good examples and cute dog pictures. Or Hynek’s Semantic Versioning Will Not Save You Be sure to check at least the JavaScript project analysis before you leave! Also be warned, I pick on Poetry quite a bit.  The rising popularity of Poetry is likely due to the simplicity of having one tool vs. many for packaging, but it happens to also have a special dependency solver, a new upper bound syntax, and a strong recommendation to always limit upper versions - in direct opposition to members of the Python core developer team and PyPA developers. Not all libraries with excessive version capping are Poetry projects (like TensorFlow), but many, many of them are. To be clear, Poetry doesn’t force version pinning on you, but it does push you really, really hard to always version cap, and it’s targeting new Python users that don’t know any better yet than to accept bad recommendations. And these affect the whole ecosystem, including users who do not use poetry, but want to depend on libraries that do! I do really like other aspects of Poetry, and would like to eventually help it build binary packages with Scikit-build (CMake) via a plugin, and I use it on some of my projects happily. If I don’t pick on Poetry enough for you, don’t worry, I have a follow-up post that picks on it in much more detail. Also, check out pdm, which gives many of the benefits of Poetry while following PEP standards.},
  langid = {english},
  organization = {ISciNumPy.dev},
  keywords = {industry practices,package versioning,project-python-packaging,software engineering},
  file = {/home/sam/Zotero/storage/SFE6H5BU/bound-version-constraints.html}
}

@article{schuirmannComparisonTwoOneSided1987,
  title = {A Comparison of the {{Two One-Sided Tests Procedure}} and the {{Power Approach}} for Assessing the Equivalence of Average Bioavailability},
  author = {Schuirmann, Donald J.},
  date = {1987-12-01},
  doi = {10.1007/bf01068419},
  url = {https://zenodo.org/records/1232484},
  urldate = {2024-01-29},
  abstract = {The statistical test of the hypothesis of no difference between the average bioavailabilities of two drug formulations, usually supplemented by an assessment of what the power of the statistical test would have been if the true averages had been inequivalent, continues to be used in the statistical analysis of bioavailability/bioequivalence studies. In the present article, this Power Approach (which in practice usually consists of testing the hypothesis of no difference at level 0.05 and requiring an estimated power of 0.80) is compared to another statistical approach, the Two One-Sided Tests Procedure, which leads to the same conclusion as the approach proposed by Westlake (2) based on the usual (shortest) 1–2α confidence interval for the true average difference. It is found that for the specific choice of α=0.05 as the nominal level of the one-sided tests, the two one-sided tests procedure has uniformly superior properties to the power approach in most cases. The only cases where the power approach has superior properties when the true averages are equivalent correspond to cases where the chance of concluding equivalence with the power approach when the true averages are notequivalent exceeds 0.05. With appropriate choice of the nominal level of significance of the one-sided tests, the two one-sided tests procedure always has uniformly superior properties to the power approach. The two one-sided tests procedure is compared to the procedure proposed by Hauck and Anderson (1).},
  keywords = {statistics},
  file = {/home/sam/Zotero/storage/XI2JVVKR/Schuirmann - 1987 - A comparison of the Two One-Sided Tests Procedure .pdf}
}

@article{schunemannReviewsRapidRapid2015,
  title = {Reviews: {{Rapid}}! {{Rapid}}! {{Rapid}}! …and Systematic},
  shorttitle = {Reviews},
  author = {Schünemann, Holger J. and Moja, Lorenzo},
  date = {2015-01-14},
  journaltitle = {Systematic Reviews},
  shortjournal = {Systematic Reviews},
  volume = {4},
  number = {1},
  pages = {4},
  issn = {2046-4053},
  doi = {10.1186/2046-4053-4-4},
  url = {https://doi.org/10.1186/2046-4053-4-4},
  urldate = {2023-10-27},
  keywords = {project-provenance-pp,rapid reviews},
  file = {/home/sam/Zotero/storage/SVE98P9A/Schünemann and Moja - 2015 - Reviews Rapid! Rapid! Rapid! …and systematic.pdf;/home/sam/Zotero/storage/TJK8MEUF/2046-4053-4-4.html}
}

@article{schwabMakingScientificComputations2000,
  title = {Making Scientific Computations Reproducible},
  author = {Schwab, M. and Karrenbach, N. and Claerbout, J.},
  date = {2000-11},
  journaltitle = {Computing in Science \& Engineering},
  volume = {2},
  number = {6},
  pages = {61--67},
  issn = {1558-366X},
  doi = {10.1109/5992.881708},
  abstract = {To verify a research paper's computational results, readers typically have to recreate them from scratch. ReDoc is a simple software filing system for authors that lets readers easily reproduce computational results using standardized rules and commands.},
  eventtitle = {Computing in {{Science}} \& {{Engineering}}},
  keywords = {academic publishing,reproducibility engineering},
  annotation = {interest: 87},
  file = {/home/sam/Zotero/storage/G522JI7Y/Schwab et al. - 2000 - Making scientific computations reproducible.pdf}
}

@unpublished{scottmeyerWhySailsWhen2014,
  title = {Why {{C}}++ {{Sails When}} the {{Vasa Sank}}},
  author = {{Scott Meyer}},
  date = {2014-06-23},
  url = {https://www.youtube.com/watch?v=ltCgzYcpFUI},
  urldate = {2023-04-04},
  abstract = {Source: http://tech.yandex.ru/events/cpp-part... I especially like 36:58 The Vasa was a 17th-century Swedish warship which suffered such feature creep during construction that it sank shortly after leaving the harbour on its maiden voyage. In the early 1990s, the C++ standardisation committee adopted the Vasa as a cautionary tale, discouraging prospective language extensions with "Remember the Vasa!" Yet C++ continued to grow, and by the time C++ was standardised, its complexity made the Vasa look like a rowboat. The Vasa sank, however, while C++ cruised, and it looks likely to continue doing so even as the latest revised standards (C++11 and C++14) add dozens of new features, each with its own idiosyncrasies. Clearly, C++ has gotten some important things right. In this talk, Scott Meyers considers the lessons to be learned from the ongoing success of a complex programming language that's over 30 years old, yet very much alive and kicking.},
  keywords = {industry practices,programming languages}
}

@article{seagerNewCalculationRecombination1999,
  title = {A {{New Calculation}} of the {{Recombination Epoch}}},
  author = {Seager, S. and Sasselov, D. D. and Scott, D.},
  date = {1999-09-01},
  journaltitle = {The Astrophysical Journal},
  volume = {523},
  pages = {L1-L5},
  issn = {0004-637X},
  doi = {10.1086/312250},
  url = {https://ui.adsabs.harvard.edu/abs/1999ApJ...523L...1S},
  urldate = {2022-04-11},
  abstract = {We have developed an improved recombination calculation of H, He I, and He II in the early universe that involves a line-by-line treatment of each atomic level. We find two major differences compared with previous calculations. First, the ionization fraction xe is approximately 10\% smaller for redshifts {$<$}\textasciitilde 800 because of nonequilibrium processes in the excited states of H. Second, He I recombination is much slower than previously thought, and it is delayed until just before H recombines. We describe the basic physics behind the new results and present a simple way to reproduce our calculation. This should enable a fast computation of the ionization history (and of the quantities such as the power spectrum of cosmic microwave background anisotropies that depend on it) for arbitrary cosmologies, without the need to consider the hundreds of atomic levels used in our complete model.},
  keywords = {astronomical observations,astrophysics,cosmic microwave background,project-astrophysics},
  annotation = {ADS Bibcode: 1999ApJ...523L...1S},
  file = {/home/sam/Zotero/storage/7UIJ2GG4/Seager et al. - 1999 - A New Calculation of the Recombination Epoch.pdf}
}

@article{seljakLineofSightIntegrationApproach1996,
  title = {A {{Line-of-Sight Integration Approach}} to {{Cosmic Microwave Background Anisotropies}}},
  author = {Seljak, Uros and Zaldarriaga, Matias},
  date = {1996-10-01},
  journaltitle = {The Astrophysical Journal},
  volume = {469},
  pages = {437},
  issn = {0004-637X},
  doi = {10.1086/177793},
  url = {https://ui.adsabs.harvard.edu/abs/1996ApJ...469..437S},
  urldate = {2022-04-11},
  abstract = {We present a new method for calculating linear cosmic microwave background (CM B) anisotropy spectra based on integration over sources along the photon past light cone. In this approach the temperature anisotropy is written as a time integral over the product of a geometrical term and a source term. The geometrical term is given by radial eigenfunctions, which do not depend on the particular cosmological model. The source term can be expressed in terms of photon, baryon, and metric perturbations, all of which can be calculated using a small number of differential equations. This split clearly separates the dynamical from the geometrical effects on the CMB anisotropies. More importantly, it allows us to significantly reduce the computational time compared to standard methods. This is achieved because the source term, which depends on the model and is generally the most time-consuming part of calculation, is a slowly varying function of wavelength and needs to be evaluated only in a small number of points. The geometrical term, which oscillates much more rapidly than the source term, does not depend on the particular model and can be precomputed in advance. Standard methods that do not separate the two terms require a much higher number of evaluations. The new method leads to about 2 orders of magnitude reduction in CPU time when compared to standard methods and typically requires a few minutes on a workstation for a single model. The method should be especially useful for accurate determinations of cosmological parameters from CMB anisotropy and polarization measurements that will become with the next generation of experiments. A program implementing this method can be obtained from the authors.},
  keywords = {astronomical observations,astrophysics,cosmic microwave background,numerical methods,project-astrophysics},
  annotation = {ADS Bibcode: 1996ApJ...469..437S},
  file = {/home/sam/Zotero/storage/KSQTHNZ3/Seljak and Zaldarriaga - 1996 - A Line-of-Sight Integration Approach to Cosmic Mic.pdf}
}

@inreference{SenateReportPrewar2022,
  title = {Senate {{Report}} on {{Pre-war Intelligence}} on {{Iraq}}},
  booktitle = {Wikipedia},
  date = {2022-08-21T03:05:50Z},
  url = {https://en.wikipedia.org/w/index.php?title=Senate_Report_on_Pre-war_Intelligence_on_Iraq&oldid=1105631972},
  urldate = {2022-08-28},
  abstract = {The Senate Report on Iraqi WMD Intelligence (formally, the "Report of the Select Committee on Intelligence on the U.S. Intelligence Community's Prewar Intelligence Assessments on Iraq") was the report by the United States Senate Select Committee on Intelligence concerning the U.S. intelligence community's assessments of Iraq during the time leading up to the 2003 invasion of Iraq. The report, which was released on July 9, 2004, identified numerous failures in the intelligence-gathering and -analysis process. The report found that these failures led to the creation of inaccurate materials that misled both government policy makers and the American public. The Committee's nine Republicans and eight Democrats agreed on the report's major conclusions and unanimously endorsed its findings. They disagreed, though, on the impact that statements on Iraq by senior members of the Bush administration had on the intelligence process.  The second phase of the investigation, addressing the way senior policymakers used the intelligence, was published on May 25, 2007.  Portions of the phase II report not released at that time include the review of public statements by U.S. government leaders prior to the war, and the assessment of the activities of Douglas Feith and the Pentagon's Office of Special Plans.},
  langid = {english},
  keywords = {current events,geopolitics},
  annotation = {Page Version ID: 1105631972}
}

@inproceedings{serebryanyAddressSanitizerFastAddress2012,
  title = {\{\vphantom\}{{AddressSanitizer}}\vphantom\{\}: {{A Fast Address Sanity Checker}}},
  shorttitle = {\{\vphantom\}{{AddressSanitizer}}\vphantom\{\}},
  author = {Serebryany, Konstantin and Bruening, Derek and Potapenko, Alexander and Vyukov, Dmitriy},
  date = {2012},
  pages = {309--318},
  url = {https://www.usenix.org/conference/atc12/technical-sessions/presentation/serebryany},
  urldate = {2025-01-14},
  eventtitle = {2012 {{USENIX Annual Technical Conference}} ({{USENIX ATC}} 12)},
  langid = {english},
  file = {/home/sam/Zotero/storage/9CEDEAL4/Serebryany et al. - 2012 - AddressSanitizer A Fast Address Sanity Checker.pdf}
}

@inproceedings{sfiligoiAcceleratingKeyBioinformatics2021,
  title = {Accelerating {{Key Bioinformatics Tasks}} 100-Fold by {{Improving Memory Access}}},
  booktitle = {Practice and {{Experience}} in {{Advanced Research Computing}}},
  author = {Sfiligoi, Igor and McDonald, Daniel and Knight, Rob},
  date = {2021-07-17},
  series = {{{PEARC}} '21},
  pages = {1--5},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3437359.3465562},
  url = {https://doi.org/10.1145/3437359.3465562},
  urldate = {2022-10-18},
  abstract = {Most experimental sciences now rely on computing, and biological sciences are no exception. As datasets get bigger, so do the computing costs, making proper optimization of the codes used by scientists increasingly important. Many of the codes developed in recent years are based on the Python-based NumPy, due to its ease of use and good performance characteristics. The composable nature of NumPy, however, does not generally play well with the multi-tier nature of modern CPUs, making any non-trivial multi-step algorithm limited by the external memory access speeds, which are hundreds of times slower than the CPU's compute capabilities. In order to fully utilize the CPU compute capabilities, one must keep the working memory footprint small enough to fit in the CPU caches, which requires splitting the problem into smaller portions and fusing together as many steps as possible. In this paper, we present changes based on these principles to two important functions in the scikit-bio library, principal coordinates analysis and the Mantel test, that resulted in over 100x speed improvement in these widely used, general-purpose tools.},
  isbn = {978-1-4503-8292-2},
  keywords = {performance engineering,research software engineering},
  annotation = {interest: 91},
  file = {/home/sam/Zotero/storage/42KGCFZC/Sfiligoi et al. - 2021 - Accelerating Key Bioinformatics Tasks 100-fold by .pdf}
}

@inproceedings{shafferLightweightFunctionMonitors2021,
  title = {Lightweight {{Function Monitors}} for {{Fine-Grained Management}} in {{Large Scale Python Applications}}},
  booktitle = {2021 {{IEEE International Parallel}} and {{Distributed Processing Symposium}} ({{IPDPS}})},
  author = {Shaffer, Tim and Li, Zhuozhao and Tovar, Ben and Babuji, Yadu and family=Dasso, given=TJ, given-i=TJ and Surma, Zoe and Chard, Kyle and Foster, Ian and Thain, Douglas},
  date = {2021-05},
  pages = {786--796},
  issn = {1530-2075},
  doi = {10.1109/IPDPS49936.2021.00088},
  abstract = {Python has become a widely used programming language for research, not only for small one-off analyses, but also for complex application pipelines running at supercomputer-scale. Modern parallel programming frameworks for Python present users with a more granular unit of management than traditional Unix processes and batch submissions: the Python function. We review the challenges involved in running native Python functions at scale, and present techniques for dynamically determining a minimal set of dependencies and for assembling a lightweight function monitor (LFM) that captures the software environment and manages resources at the granularity of single functions. We evaluate these techniques in a range of environments, from campus cluster to supercomputer, and show that our advanced dependency management planning and dynamic resource management methods provide superior performance and utilization relative to coarser-grained management approaches, achieving several-fold decrease in execution time for several large Python applications.},
  eventtitle = {2021 {{IEEE International Parallel}} and {{Distributed Processing Symposium}} ({{IPDPS}})},
  keywords = {cloud computing,workflow managers},
  annotation = {interest: 89},
  file = {/home/sam/Zotero/storage/UF3PP8J6/Shaffer et al. - 2021 - Lightweight Function Monitors for Fine-Grained Man.pdf;/home/sam/Zotero/storage/WFBGZFRG/9460530.html}
}

@article{shamirPracticesSourceCode2013,
  title = {Practices in Source Code Sharing in Astrophysics},
  author = {Shamir, Lior and Wallin, John F. and Allen, Alice and Berriman, Bruce and Teuben, Peter and Nemiroff, Robert J. and Mink, Jessica and Hanisch, Robert J. and DuPrie, Kimberly},
  date = {2013-02-01},
  journaltitle = {Astronomy and Computing},
  shortjournal = {Astronomy and Computing},
  volume = {1},
  pages = {54--58},
  issn = {2213-1337},
  doi = {10.1016/j.ascom.2013.04.001},
  url = {https://www.sciencedirect.com/science/article/pii/S2213133713000073},
  urldate = {2023-01-19},
  abstract = {While software and algorithms have become increasingly important in astronomy, the majority of authors who publish computational astronomy research do not share the source code they develop, making it difficult to replicate and reuse the work. In this paper we discuss the importance of sharing scientific source code with the entire astrophysics community, and propose that journals require authors to make their code publicly available when a paper is published. That is, we suggest that a paper that involves a computer program not be accepted for publication unless the source code becomes publicly available. The adoption of such a policy by editors, editorial boards, and reviewers will improve the ability to replicate scientific results, and will also make computational astronomy methods more available to other researchers who wish to apply them to their data.},
  langid = {english},
  keywords = {open data,reproducibility engineering},
  file = {/home/sam/Zotero/storage/AF4QKCAA/Shamir et al. - 2013 - Practices in source code sharing in astrophysics.pdf;/home/sam/Zotero/storage/A5K2QJJ2/S2213133713000073.html}
}

@article{shamseerPotentialPredatoryLegitimate2017,
  title = {Potential Predatory and Legitimate Biomedical Journals: Can You Tell the Difference? {{A}} Cross-Sectional Comparison},
  shorttitle = {Potential Predatory and Legitimate Biomedical Journals},
  author = {Shamseer, Larissa and Moher, David and Maduekwe, Onyi and Turner, Lucy and Barbour, Virginia and Burch, Rebecca and Clark, Jocalyn and Galipeau, James and Roberts, Jason and Shea, Beverley J.},
  date = {2017-03-16},
  journaltitle = {BMC Medicine},
  shortjournal = {BMC Med},
  volume = {15},
  eprint = {28298236},
  eprinttype = {pmid},
  pages = {28},
  issn = {1741-7015},
  doi = {10.1186/s12916-017-0785-9},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5353955/},
  urldate = {2022-08-30},
  abstract = {Background The Internet has transformed scholarly publishing, most notably, by the introduction of open access publishing. Recently, there has been a rise of online journals characterized as ‘predatory’, which actively solicit manuscripts and charge publications fees without providing robust peer review and editorial services. We carried out a cross-sectional comparison of characteristics of potential predatory, legitimate open access, and legitimate subscription-based biomedical journals. Methods On July 10, 2014, scholarly journals from each of the following groups were identified – potential predatory journals (source: Beall’s List), presumed legitimate, fully open access journals (source: PubMed Central), and presumed legitimate subscription-based (including hybrid) journals (source: Abridged Index Medicus). MEDLINE journal inclusion criteria were used to screen and identify biomedical journals from within the potential predatory journals group. One hundred journals from each group were randomly selected. Journal characteristics (e.g., website integrity, look and feel, editors and staff, editorial/peer review process, instructions to authors, publication model, copyright and licensing, journal location, and contact) were collected by one assessor and verified by a second. Summary statistics were calculated. Results Ninety-three predatory journals, 99 open access, and 100 subscription-based journals were analyzed; exclusions were due to website unavailability. Many more predatory journals’ homepages contained spelling errors (61/93, 66\%) and distorted or potentially unauthorized images (59/93, 63\%) compared to open access journals (6/99, 6\% and 5/99, 5\%, respectively) and subscription-based journals (3/100, 3\% and 1/100, 1\%, respectively). Thirty-one (33\%) predatory journals promoted a bogus impact metric – the Index Copernicus Value – versus three (3\%) open access journals and no subscription-based journals. Nearly three quarters (n\,=\,66, 73\%) of predatory journals had editors or editorial board members whose affiliation with the journal was unverified versus two (2\%) open access journals and one (1\%) subscription-based journal in which this was the case. Predatory journals charge a considerably smaller publication fee (median \$100 USD, IQR \$63–\$150) than open access journals (\$1865 USD, IQR \$800–\$2205) and subscription-based hybrid journals (\$3000 USD, IQR \$2500–\$3000). Conclusions We identified 13 evidence-based characteristics by which predatory journals may potentially be distinguished from presumed legitimate journals. These may be useful for authors who are assessing journals for possible submission or for others, such as universities evaluating candidates’ publications as part of the hiring process.},
  pmcid = {PMC5353955},
  keywords = {academic publishing,metascience,predatory journals},
  file = {/home/sam/Zotero/storage/YD3BEVKP/Shamseer et al. - 2017 - Potential predatory and legitimate biomedical jour.pdf}
}

@article{shankarDITTOAutomaticIncrementalization2007,
  title = {{{DITTO}}: Automatic Incrementalization of Data Structure Invariant Checks (in {{Java}})},
  shorttitle = {{{DITTO}}},
  author = {Shankar, Ajeet and Bodík, Rastislav},
  date = {2007-06-10},
  journaltitle = {ACM SIGPLAN Notices},
  shortjournal = {SIGPLAN Not.},
  volume = {42},
  number = {6},
  pages = {310--319},
  issn = {0362-1340},
  doi = {10.1145/1273442.1250770},
  url = {https://doi.org/10.1145/1273442.1250770},
  urldate = {2022-09-06},
  abstract = {We present DITTO, an automatic incrementalizer for dynamic, side-effect-free data structure invariant checks. Incrementalization speeds up the execution of a check by reusing its previous executions, checking the invariant anew only the changed parts of the data structure. DITTO exploits properties specific to the domain of invariant checks to automate and simplify the process without restricting what mutations the program can perform. Our incrementalizer works for modern imperative languages such as Java and C\#. It can incrementalize,for example, verification of red-black tree properties and the consistency of the hash code in a hash table bucket. Our source-to-source implementation for Java is automatic, portable, and efficient. DITTO provides speedups on data structures with as few as 100 elements; on larger data structures, its speedups are characteristic of non-automatic incrementalizers: roughly 5-fold at 5,000 elements,and growing linearly with data structure size.},
  keywords = {incremental computation},
  annotation = {interest: 61}
}

@article{shawGoldenAgeSoftware2006,
  title = {The Golden Age of Software Architecture},
  author = {Shaw, M. and Clements, P.},
  date = {2006-03},
  journaltitle = {IEEE Software},
  volume = {23},
  number = {2},
  pages = {31--39},
  issn = {1937-4194},
  doi = {10.1109/MS.2006.58},
  abstract = {Since the late 1980s, software architecture has emerged as the principled understanding of the large-scale structures of software systems. From its roots in qualitative descriptions of empirically observed useful system organizations, software architecture has matured to encompass a broad set of notations, tools, and analysis techniques. Whereas initially the research area interpreted software practice, it now offers concrete guidance for complex software design and development. It has made the transition from basic research to an essential element of software system design and construction. This retrospective examines software architecture's growth in the context of a technology maturation model, matching its significant accomplishments to the model's stages to gain perspective on where the field stands today. This trajectory has taken architecture to its golden age.},
  eventtitle = {{{IEEE Software}}},
  keywords = {software engineering},
  annotation = {interest: 85},
  file = {/home/sam/Zotero/storage/3WSHWM3I/Shaw and Clements - 2006 - The golden age of software architecture.pdf}
}

@online{shawHasPythonGIL2019,
  title = {Has the {{Python GIL}} Been Slain?},
  shorttitle = {Has the {{Python GIL}} Been Slain?},
  author = {Shaw, Anthony},
  date = {2019-05-15},
  url = {https://hackernoon.com/has-the-python-gil-been-slain-9440d28fa93d},
  urldate = {2022-04-18},
  langid = {english},
  organization = {HackerNoon},
  keywords = {language engineering},
  file = {/home/sam/Zotero/storage/NYS6ARJ4/has-the-python-gil-been-slain-9440d28fa93d.html}
}

@article{shawWhatMakesGood2002,
  title = {What Makes Good Research in Software Engineering?},
  author = {Shaw, Mary},
  date = {2002-10-01},
  journaltitle = {International Journal on Software Tools for Technology Transfer},
  shortjournal = {STTT},
  volume = {4},
  number = {1},
  pages = {1--7},
  issn = {1433-2779},
  doi = {10.1007/s10009-002-0083-4},
  url = {https://doi.org/10.1007/s10009-002-0083-4},
  urldate = {2022-09-06},
  abstract = {Physics, biology, and medicine have well-refined public explanations of their research processes. Even in simplified form, these provide guidance about what counts as “good research” both inside and outside the field. Software engineering has not yet explicitly identified and explained either our research processes or the ways we recognize excellent work. Science and engineering research fields can be characterized in terms of the kinds of questions they find worth investigating, the research methods they adopt, and the criteria by which they evaluate their results. I will present such a characterization for software engineering, showing the diversity of research strategies and the way they shift as ideas mature. Understanding these strategies should help software engineers design research plans and report the results clearly; it should also help explain the character of software engineering research to computer science at large and to other scientists.},
  langid = {english},
  keywords = {research software engineering},
  annotation = {interest: 90}
}

@inproceedings{shawWritingGoodSoftware2003,
  title = {Writing Good Software Engineering Research Papers: Minitutorial},
  shorttitle = {Writing Good Software Engineering Research Papers},
  booktitle = {Proceedings of the 25th {{International Conference}} on {{Software Engineering}}},
  author = {Shaw, Mary},
  date = {2003-05-03},
  series = {{{ICSE}} '03},
  pages = {726--736},
  publisher = {IEEE Computer Society},
  location = {USA},
  abstract = {Software engineering researchers solve problems of several different kinds. To do so, they produce several different kinds of results, and they should develop appropriate evidence to validate these results. They often report their research in conference papers. I analyzed the abstracts of research papers submitted to ICSE 2002 in order to identify the types of research reported in the submitted and accepted papers, and I observed the program committee discussions about which papers to accept. This report presents the research paradigms of the papers, common concerns of the program committee, and statistics on success rates. This information should help researchers design better research projects and write papers that present their results to best advantage.},
  isbn = {978-0-7695-1877-0},
  keywords = {research,software engineering},
  file = {/home/sam/Zotero/storage/DCFWHZS6/Shaw - 2003 - Writing good software engineering research papers.pdf}
}

@online{shewchukThreeSinsAuthors,
  title = {Three {{Sins}} of {{Authors}} in {{Computer Science}} and {{Math}}},
  author = {Shewchuk, Jonathan},
  url = {http://www.cs.cmu.edu/~jrs/sins.html},
  urldate = {2023-01-20},
  keywords = {academic writing},
  file = {/home/sam/Zotero/storage/G82PUVRX/sins.html}
}

@article{shiExperienceReportProducing2022,
  title = {An {{Experience Report}} on {{Producing Verifiable Builds}} for {{Large-Scale Commercial Systems}}},
  author = {Shi, Yong and Wen, Mingzhi and Cogo, Filipe R. and Chen, Boyuan and Jiang, Zhen Ming},
  date = {2022-09},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {48},
  number = {9},
  pages = {3361--3377},
  issn = {1939-3520},
  doi = {10.1109/TSE.2021.3092692},
  url = {https://ieeexplore.ieee.org/abstract/document/9465650?casa_token=thRn5ctU4g0AAAAA:RFJ6sU_u8vmnEeKIDwZws5s4n75cs9oWEbC6bLWw_zNr-NZKLGUtGIolRbCnA3GograW75NKOQ},
  urldate = {2023-12-18},
  abstract = {Build verifiability is a safety property for a software system which can be used to check against various security-related issues during the build process. In summary, a verifiable build generates equivalent build artifacts for every build instance, allowing independent auditors to verify that the generated artifacts correspond to their source code. Producing a verifiable build is a very challenging problem, as non-equivalences in the build artifacts can be caused by non-determinsm from the build environment, the build toolchain, or the system implementation. Existing research and practices on build verifiability mainly focus on remediating sources of non-determinism. However, such a process does not work well with large-scale commercial systems (LSCSs) due to their stringent security requirements, complex third party dependencies, and large volumes of code changes. In this paper, we present an experience report on using a unified process and a toolkit to produce verifiable builds for LSCSs. A unified process contrasts with the existing practices in which recommendations to mitigate sources of non-determinism are proposed on a case-by-case basis and are not codified in a comprehensive tool. Our approach supports the following three strategies to systematically mitigate non-equivalences in the build artifacts: remediation, controlling, and interpretation. Case study on three LSCSs within \textbackslash sf HuaweiHuawei shows that our approach is able to increase the proportion of verified build artifacts from less than 50 to 100 percent. To cross-validate our approach, we successfully applied our approach to build 2,218 open source packages distributed under \textbackslash sf CentOSCentOS 7.8, increasing the proportion of verified build artifacts from 85 to 99 percent with minimal human intervention. We also provide an overview of our mitigation guideline, which describes the recommended strategies to mitigate various non-equivalences. Finally, we present some discussions and open research problems in this area based on our experience and lessons learned in the past few years of applying our approach within the company. This paper will be useful for practitioners and software engineering researchers who are interested in build verifiability.},
  eventtitle = {{{IEEE Transactions}} on {{Software Engineering}}},
  keywords = {project-provenance-pp,reproducibility},
  file = {/home/sam/Zotero/storage/GGEJY2E8/Shi et al. - 2022 - An Experience Report on Producing Verifiable Build.pdf;/home/sam/Zotero/storage/359TFW2E/9465650.html}
}

@article{shiReflectionawareStaticRegression2019,
  title = {Reflection-Aware Static Regression Test Selection},
  author = {Shi, August and Hadzi-Tanovic, Milica and Zhang, Lingming and Marinov, Darko and Legunsen, Owolabi},
  date = {2019-10-10},
  journaltitle = {Proceedings of the ACM on Programming Languages},
  shortjournal = {Proc. ACM Program. Lang.},
  volume = {3},
  pages = {1--29},
  issn = {2475-1421},
  doi = {10.1145/3360613},
  url = {https://dl.acm.org/doi/10.1145/3360613},
  urldate = {2022-04-12},
  abstract = {Regression test selection (RTS) aims to speed up regression testing by rerunning only tests that are affected by code changes. RTS can be performed using static or dynamic analysis techniques. Our prior study showed that static and dynamic RTS perform similarly for medium-sized Java projects. However, the results of that prior study also showed that static RTS can be unsafe, missing to select tests that dynamic RTS selects, and that reflection was the only cause of unsafety observed among the evaluated projects. In this paper, we investigate five techniques—three purely static techniques and two hybrid static-dynamic techniques—that aim to make static RTS safe with respect to reflection. We implement these reflection-aware (RA) techniques by extending the reflection-unaware (RU) class-level static RTS technique in a tool called STARTS. To evaluate these RA techniques, we compare their end-to-end times with RU, and with RetestAll, which reruns all tests after every code change. We also compare safety and precision of the RA techniques with Ekstazi, a state-of-the-art dynamic RTS technique; precision is a measure of unaffected tests selected. Our evaluation on 1173 versions of 24 open-source Java projects shows negative results. The RA techniques improve the safety of RU but at very high costs. The purely static techniques are safe in our experiments but decrease the precision of RU, with end-to-end time at best 85.8\% of RetestAll time, versus 69.1\% for RU. One hybrid static-dynamic technique improves the safety of RU but at high cost, with end-to-end time that is 91.2\% of RetestAll. The other hybrid static-dynamic technique provides better precision, is safer than RU, and incurs lower end-to-end time—75.8\% of RetestAll, but it can still be unsafe in the presence of test-order dependencies. Our study highlights the challenges involved in making static RTS safe with respect to reflection.},
  issue = {OOPSLA},
  langid = {english},
  keywords = {software engineering,software testing,static analysis},
  annotation = {interest: 93},
  file = {/home/sam/Zotero/storage/7KA92V4X/Shi et al. - 2019 - Reflection-aware static regression test selection.pdf}
}

@article{shirtsLessonsLearnedComparing2017,
  title = {Lessons Learned from Comparing Molecular Dynamics Engines on the {{SAMPL5}} Dataset},
  author = {Shirts, Michael R. and Klein, Christoph and Swails, Jason M. and Yin, Jian and Gilson, Michael K. and Mobley, David L. and Case, David A. and Zhong, Ellen D.},
  date = {2017-01-01},
  journaltitle = {Journal of Computer-Aided Molecular Design},
  shortjournal = {J Comput Aided Mol Des},
  volume = {31},
  number = {1},
  pages = {147--161},
  issn = {1573-4951},
  doi = {10.1007/s10822-016-9977-1},
  url = {https://doi.org/10.1007/s10822-016-9977-1},
  urldate = {2023-02-23},
  abstract = {We describe our efforts to prepare common starting structures and models for the SAMPL5 blind prediction challenge. We generated the starting input files and single configuration potential energies for the host-guest in the SAMPL5 blind prediction challenge for the GROMACS, AMBER, LAMMPS, DESMOND and CHARMM molecular simulation programs. All conversions were fully automated from the originally prepared AMBER input files using a combination of the ParmEd and InterMol conversion programs. We find that the energy calculations for all molecular dynamics engines for this molecular set agree to better than 0.1~\% relative absolute energy for all energy components, and in most cases an order of magnitude better, when reasonable choices are made for different cutoff parameters. However, there are some surprising sources of statistically significant differences. Most importantly, different choices of Coulomb’s constant between programs are one of the largest sources of discrepancies in energies. We discuss the measures required to get good agreement in the energies for equivalent starting configurations between the simulation programs, and the energy differences that occur when simulations are run with program-specific default simulation parameter values. Finally, we discuss what was required to automate this conversion and comparison.},
  langid = {english},
  keywords = {research software engineering},
  file = {/home/sam/Zotero/storage/N3YEZBN9/Shirts et al. - 2017 - Lessons learned from comparing molecular dynamics .pdf}
}

@article{shottonCiTOCitationTyping2010,
  title = {{{CiTO}}, the {{Citation Typing Ontology}}},
  author = {Shotton, David},
  date = {2010-06-22},
  journaltitle = {Journal of Biomedical Semantics},
  shortjournal = {J Biomed Semant},
  volume = {1},
  number = {1},
  pages = {S6},
  issn = {2041-1480},
  doi = {10.1186/2041-1480-1-S1-S6},
  url = {https://doi.org/10.1186/2041-1480-1-S1-S6},
  urldate = {2023-05-25},
  abstract = {CiTO, the Citation Typing Ontology, is an ontology for describing the nature of reference citations in scientific research articles and other scholarly works, both to other such publications and also to Web information resources, and for publishing these descriptions on the Semantic Web. Citation are described in terms of the factual and rhetorical relationships between citing publication and cited publication, the in-text and global citation frequencies of each cited work, and the nature of the cited work itself, including its publication and peer review status. This paper describes CiTO and illustrates its usefulness both for the annotation of bibliographic reference lists and for the visualization of citation networks. The latest version of CiTO, which this paper describes, is CiTO Version 1.6, published on 19 March 2010. CiTO is written in the Web Ontology Language OWL, uses the namespace http://purl.org/net/cito/, and is available from http://purl.org/net/cito/. This site uses content negotiation to deliver to the user an OWLDoc Web version of the ontology if accessed via a Web browser, or the OWL ontology itself if accessed from an ontology management tool such as Protégé 4 (http://protege.stanford.edu/). Collaborative work is currently under way to harmonize CiTO with other ontologies describing bibliographies and the rhetorical structure of scientific discourse.},
  langid = {english},
  keywords = {knowledge engineering,project-provenance-pp,semantic web},
  file = {/home/sam/Zotero/storage/NBZPC9S5/Shotton - 2010 - CiTO, the Citation Typing Ontology.pdf}
}

@unpublished{shresthaSLNETRedistributableCorpus2022,
  title = {{{SLNET}}: {{A Redistributable Corpus}} of 3rd-Party {{Simulink Models}}},
  shorttitle = {{{SLNET}}},
  author = {Shrestha, Sohil Lal and Chowdhury, Shafiul Azam and Csallner, Christoph},
  date = {2022-03-31},
  eprint = {2203.17112},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.1145/3524842.3528001},
  url = {http://arxiv.org/abs/2203.17112},
  urldate = {2022-04-12},
  abstract = {MATLAB/Simulink is widely used for model-based design. Engineers create Simulink models and compile them to embedded code, often to control safety-critical cyber-physical systems in automotive, aerospace, and healthcare applications. Despite Simulink's importance, there are few large-scale empirical Simulink studies, perhaps because there is no large readily available corpus of third-party open-source Simulink models. To enable empirical Simulink studies, this paper introduces SLNET, the largest corpus of freely available third-party Simulink models. SLNET has several advantages over earlier collections. Specifically, SLNET is 8 times larger than the largest previous corpus of Simulink models, includes fine-grained metadata, is constructed automatically, is self-contained, and allows redistribution. SLNET is available under permissive open-source licenses and contains all of its collection and analysis tools.},
  keywords = {research software engineering,software engineering},
  file = {/home/sam/Zotero/storage/PRA8FN8T/Shrestha et al. - 2022 - SLNET A Redistributable Corpus of 3rd-party Simul.pdf;/home/sam/Zotero/storage/RS8SZSEZ/2203.html}
}

@article{siciliaCommunityCurationOpen2017,
  title = {Community {{Curation}} in {{Open Dataset Repositories}}: {{Insights}} from {{Zenodo}}},
  shorttitle = {Community {{Curation}} in {{Open Dataset Repositories}}},
  author = {Sicilia, Miguel-Angel and García-Barriocanal, Elena and Sánchez-Alonso, Salvador},
  date = {2017},
  journaltitle = {Procedia Computer Science},
  shortjournal = {Procedia Computer Science},
  volume = {106},
  pages = {54--60},
  issn = {18770509},
  doi = {10.1016/j.procs.2017.03.009},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050917302776},
  urldate = {2024-10-04},
  langid = {english},
  file = {/home/sam/Zotero/storage/RDKHPLCE/Sicilia et al. - 2017 - Community Curation in Open Dataset Repositories Insights from Zenodo.pdf}
}

@inproceedings{siegmundViewsInternalExternal2015,
  title = {Views on {{Internal}} and {{External Validity}} in {{Empirical Software Engineering}}},
  booktitle = {2015 {{IEEE}}/{{ACM}} 37th {{IEEE International Conference}} on {{Software Engineering}}},
  author = {Siegmund, Janet and Siegmund, Norbert and Apel, Sven},
  date = {2015-05},
  pages = {9--19},
  publisher = {IEEE},
  location = {Florence, Italy},
  doi = {10.1109/ICSE.2015.24},
  url = {http://ieeexplore.ieee.org/document/7194557/},
  urldate = {2022-06-30},
  abstract = {Empirical methods have grown common in software engineering, but there is no consensus on how to apply them properly. Is practical relevance key? Do internally valid studies have any value? Should we replicate more to address the tradeoff between internal and external validity? We asked the community how empirical research should take place in software engineering, with a focus on the tradeoff between internal and external validity and replication, complemented with a literature review about the status of empirical research in software engineering. We found that the opinions differ considerably, and that there is no consensus in the community when to focus on internal or external validity and how to conduct and review replications.},
  eventtitle = {2015 {{IEEE}}/{{ACM}} 37th {{IEEE International Conference}} on {{Software Engineering}} ({{ICSE}})},
  isbn = {978-1-4799-1934-5},
  keywords = {human computer interaction,metascience},
  annotation = {interest: 78},
  file = {/home/sam/Zotero/storage/KX4S6DCK/Views_on_Internal_and_External_Validity_in_Empirical_Software_Engineering.pdf}
}

@online{sierraTroubleKoolaidPoint2014,
  title = {Trouble at the {{Koolaid Point}}},
  author = {Sierra, Kathy},
  date = {2014-10-07},
  url = {https://web.archive.org/web/20211031133101/http://seriouspony.com/trouble-at-the-koolaid-point},
  urldate = {2023-09-27},
  organization = {Serious Pony},
  file = {/home/sam/Zotero/storage/95Z3IUU6/trouble-at-the-koolaid-point.html}
}

@article{sikosProvenanceAwareKnowledgeRepresentation2020,
  title = {Provenance-{{Aware Knowledge Representation}}: {{A Survey}} of {{Data Models}} and {{Contextualized Knowledge Graphs}}},
  shorttitle = {Provenance-{{Aware Knowledge Representation}}},
  author = {Sikos, Leslie F. and Philp, Dean},
  date = {2020-09},
  journaltitle = {Data Science and Engineering},
  shortjournal = {Data Sci. Eng.},
  volume = {5},
  number = {3},
  pages = {293--316},
  issn = {2364-1185, 2364-1541},
  doi = {10.1007/s41019-020-00118-0},
  url = {https://link.springer.com/10.1007/s41019-020-00118-0},
  urldate = {2022-08-02},
  abstract = {Abstract              Expressing machine-interpretable statements in the form of subject-predicate-object triples is a well-established practice for capturing semantics of structured data. However, the standard used for representing these triples, RDF, inherently lacks the mechanism to attach provenance data, which would be crucial to make automatically generated and/or processed data authoritative. This paper is a critical review of data models, annotation frameworks, knowledge organization systems, serialization syntaxes, and algebras that enable provenance-aware RDF statements. The various approaches are assessed in terms of standard compliance, formal semantics, tuple type, vocabulary term usage, blank nodes, provenance granularity, and scalability. This can be used to advance existing solutions and help implementers to select the most suitable approach (or a combination of approaches) for their applications. Moreover, the analysis of the mechanisms and their limitations highlighted in this paper can serve as the basis for novel approaches in RDF-powered applications with increasing provenance needs.},
  langid = {english},
  keywords = {formal semantics,provenance,workflow managers}
}

@article{simeonEssenceXML2003,
  title = {The Essence of {{XML}}},
  author = {Siméon, Jérôme and Wadler, Philip},
  date = {2003-01-15},
  journaltitle = {ACM SIGPLAN Notices},
  shortjournal = {SIGPLAN Not.},
  volume = {38},
  number = {1},
  pages = {1--13},
  issn = {0362-1340},
  doi = {10.1145/640128.604132},
  url = {https://dl.acm.org/doi/10.1145/640128.604132},
  urldate = {2023-09-11},
  abstract = {The World-Wide Web Consortium (W3C) promotes XML and related standards, including XML Schema, XQuery, and XPath. This paper describes a formalization of XML Schema. A formal semantics based on these ideas is part of the official XQuery and XPath specification, one of the first uses of formal methods by a standards body. XML Schema features both named and structural types, with structure based on tree grammars. While structural types and matching have been studied in other work (notably XDuce, Relax NG, and a previous formalization of XML Schema), this is the first work to study the relation between named types and structural types, and the relation between matching and validation.},
  keywords = {programming languages},
  file = {/home/sam/Zotero/storage/7QKK7WAV/Siméon and Wadler - 2003 - The essence of XML.pdf}
}

@article{simmhanSurveyDataProvenance2005,
  title = {A Survey of Data Provenance in E-Science},
  author = {Simmhan, Yogesh L. and Plale, Beth and Gannon, Dennis},
  date = {2005-09},
  journaltitle = {ACM SIGMOD Record},
  shortjournal = {SIGMOD Rec.},
  volume = {34},
  number = {3},
  pages = {31--36},
  issn = {0163-5808},
  doi = {10.1145/1084805.1084812},
  url = {https://dl.acm.org/doi/10.1145/1084805.1084812},
  urldate = {2022-07-08},
  abstract = {Data management is growing in complexity as large-scale applications take advantage of the loosely coupled resources brought together by grid middleware and by abundant storage capacity. Metadata describing the data products used in and generated by these applications is essential to disambiguate the data and enable reuse. Data provenance, one kind of metadata, pertains to the derivation history of a data product starting from its original sources.In this paper we create a taxonomy of data provenance characteristics and apply it to current research efforts in e-science, focusing primarily on scientific workflow approaches. The main aspect of our taxonomy categorizes provenance systems based on why they record provenance, what they describe, how they represent and store provenance, and ways to disseminate it. The survey culminates with an identification of open research problems in the field.},
  langid = {english},
  keywords = {provenance}
}

@article{simmonsFalsePositivePsychologyUndisclosed2011,
  title = {False-{{Positive Psychology}}: {{Undisclosed Flexibility}} in {{Data Collection}} and {{Analysis Allows Presenting Anything}} as {{Significant}}},
  shorttitle = {False-{{Positive Psychology}}},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  date = {2011-11-01},
  journaltitle = {Psychological Science},
  shortjournal = {Psychol Sci},
  volume = {22},
  number = {11},
  pages = {1359--1366},
  publisher = {SAGE Publications Inc},
  issn = {0956-7976},
  doi = {10.1177/0956797611417632},
  url = {https://doi.org/10.1177/0956797611417632},
  urldate = {2022-09-06},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists’ nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  langid = {english},
  keywords = {metascience,psychology,statistics},
  file = {/home/sam/Zotero/storage/D6IV7FGU/Simmons et al. - 2011 - False-Positive Psychology Undisclosed Flexibility.pdf}
}

@report{simsResearchSoftwareEngineering2021,
  title = {Research Software Engineering: {{Professionalization}}, Roles, and Identity},
  shorttitle = {Research Software Engineering},
  author = {Sims, Benjamin},
  date = {2021-02-08},
  number = {LA-UR-22-21250, 1845242},
  pages = {LA-UR-22-21250, 1845242},
  institution = {Los Alamos National Laboratory},
  doi = {10.2172/1845242},
  url = {https://www.osti.gov/servlets/purl/1845242/},
  urldate = {2022-04-12},
  abstract = {This report summarizes the results of a study of research software professionals based on 17 interviews conducted at U.S. national laboratories and academic institutions. The study focuses on understanding the emerging professional role of research software engineer (RSE) and examining how the history of software engineering might be relevant to the emergence of research software engineering as a professional movement. The report first uses interview data to identify several models of professional identity for research software professionals. These include dominance models in which a person strongly identifies with a single professional identity (such as RSE), and multiplicity models in which an individual sees an identity like RSE as just one facet of their overall professional role. Next, it identifies three types of work situations for software professionals at national laboratories, covering those who work in dedicated RSE organizations, those who play an integral role in large multiphysics code projects, and those who work in a more isolated capacity to provide software development support for less computationally intensive research projects. It then reviews some key ideas from the sociology of professions in the context of the software engineering profession, suggesting parallels between the current RSE professional movement and aspects of the history of software engineering. It concludes by suggesting some potential practical implications of these findings for the RSE movement and professional organizations.},
  langid = {english},
  keywords = {research software engineering,sociology of professions},
  annotation = {interst: 50},
  file = {/home/sam/Zotero/storage/VPA8H7WM/Sims - 2022 - Research software engineering Professionalization.pdf}
}

@article{singhFigShare2011,
  title = {{{FigShare}}},
  author = {Singh, Jatinder},
  date = {2011-06},
  journaltitle = {Journal of Pharmacology and Pharmacotherapeutics},
  shortjournal = {Journal of Pharmacology and Pharmacotherapeutics},
  volume = {2},
  number = {2},
  pages = {138--139},
  issn = {0976-500X, 0976-5018},
  doi = {10.4103/0976-500X.81919},
  url = {http://journals.sagepub.com/doi/10.4103/0976-500X.81919},
  urldate = {2024-10-04},
  langid = {english},
  file = {/home/sam/Zotero/storage/7A2LC6MG/Singh - 2011 - FigShare.pdf}
}

@online{singularitydevelopersSecuritySingularityCESingularityCE2023,
  title = {Security in {{SingularityCE}} — {{SingularityCE Admin Guide}} 3.11 Documentation},
  author = {Singularity Developers},
  date = {2023},
  url = {https://docs.sylabs.io/guides/latest/admin-guide/security.html},
  urldate = {2023-02-18},
  keywords = {containers,operating systems,project-acm-rep},
  file = {/home/sam/Zotero/storage/NBP7QE8A/security.html}
}

@online{sitakerMemoryModelsThat,
  title = {The Memory Models That Underlie Programming Languages},
  author = {Sitaker, Kragen Javier},
  url = {http://canonical.org/~kragen/memory-models/},
  urldate = {2022-06-24},
  abstract = {There are about six major conceptualizations of memory, which I’m calling “memory models”, that dominate today’s programming. Three of them derive from the three most historically important programming languages of the 1950s\,—\,COBOL, LISP, and FORTRAN\,—\,and the other three derive from the three historically important data storage systems: magnetic tape, Unix-style hierarchical filesystems, and relational databases. These models shape what our programming languages can or cannot do at a much deeper layer than mere syntax or even type systems. Mysteriously, I’ve never seen a good explanation of them\,—\,you pretty much just have to absorb them by osmosis instead of having them explained to you\,—\,and so I’m going to try now. Then I’m going to explain some possible alternatives to the mainstream options and why they might be interesting. - Nested records, the COBOL memory model: memory is a tax form - Object graphs, the LISP memory model: memory is a labeled directed graph - Parallel arrays, the FORTRAN memory model: memory is a bunch of arrays - Pipes, the magnetic tape memory model: the moving finger writes, and, having writen, moves on - Directories, the Multics memory model: memory is a string-labeled tree with blob leaves - Relations, the SQL memory model: memory is a collection of mutable multivalued finite functions - Also: why is there no Lua, Erlang, or Forth memory model?},
  keywords = {programming languages}
}

@inproceedings{slaughterRegentHighproductivityProgramming2015,
  title = {Regent: A High-Productivity Programming Language for {{HPC}} with Logical Regions},
  shorttitle = {Regent},
  booktitle = {Proceedings of the {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  author = {Slaughter, Elliott and Lee, Wonchan and Treichler, Sean and Bauer, Michael and Aiken, Alex},
  date = {2015-11-15},
  series = {{{SC}} '15},
  pages = {1--12},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2807591.2807629},
  url = {https://dl.acm.org/doi/10.1145/2807591.2807629},
  urldate = {2023-07-14},
  abstract = {We present Regent, a high-productivity programming language for high performance computing with logical regions. Regent users compose programs with tasks (functions eligible for parallel execution) and logical regions (hierarchical collections of structured objects). Regent programs appear to execute sequentially, require no explicit synchronization, and are trivially deadlock-free. Regent's type system catches many common classes of mistakes and guarantees that a program with correct serial execution produces identical results on parallel and distributed machines. We present an optimizing compiler for Regent that translates Regent programs into efficient implementations for Legion, an asynchronous task-based model. Regent employs several novel compiler optimizations to minimize the dynamic overhead of the runtime system and enable efficient operation. We evaluate Regent on three benchmark applications and demonstrate that Regent achieves performance comparable to hand-tuned Legion.},
  isbn = {978-1-4503-3723-6},
  keywords = {hpc,programming languages},
  file = {/home/sam/Zotero/storage/VG4D9XWS/Slaughter et al. - 2015 - Regent a high-productivity programming language f.pdf}
}

@online{slusherLifeTimesGus,
  title = {The {{Life}} and {{Times}} of {{Gus Garcia}}},
  author = {Slusher, Daryl and {FRI.} and June 2 and {2000}},
  url = {https://www.austinchronicle.com/news/2000-06-02/the-life-and-times-of-gus-garcia/},
  urldate = {2022-10-12},
  abstract = {City Council Member Gus Garcia retires after more than 30 years of public service},
  langid = {american},
  keywords = {current events}
}

@online{smalleyLinuxSecurityModules,
  title = {Linux {{Security Modules}}: {{General Security Hooks}} for {{Linux}}},
  author = {Smalley, Stephen and Fraser, Timothy and Vance, Chris},
  url = {https://docs.kernel.org/security/lsm.html},
  urldate = {2023-08-24},
  organization = {The Linux Kernel documentation},
  keywords = {operating systems,project-provenance-pp},
  file = {/home/sam/Zotero/storage/ZPXUQP9J/lsm.html}
}

@software{smithYt_astro_analysisVersion2022,
  title = {Yt\_astro\_analysis Version 1.1.1},
  shorttitle = {Yt-Project/Yt\_astro\_analysis},
  author = {Smith, Britton and Turk, Matthew and ZuHone, John and Goldbaum, Nathan and Hummels, Cameron and Egan, Hilary and Wise, John and Scopatz, Anthony and family=Val-Borro, given=Miguel, prefix=de, useprefix=false and Keller, Ben and Richardson, Mark and Robert, Clément},
  date = {2022-01-27},
  doi = {10.5281/zenodo.5911048},
  url = {https://zenodo.org/record/5911048},
  urldate = {2022-05-03},
  abstract = {yt\_astro\_analysis is a yt extension package for astrophysical analysis. This package contains functionality for: Halo finding and analysis Lightcones Planning cosmological simulations for making lightcones and lightrays Exporting to the RADMC-3D radiation transport code Creating PPV FITS cubes This is a bugfix release; no new features have been added. Bugfixes Make sure to initialize index before checking particle types https://github.com/yt-project/yt\_astro\_analysis/pull/127 Fix broken example with halo plotting https://github.com/yt-project/yt\_astro\_analysis/pull/132 Make total particles a 64 bit integer https://github.com/yt-project/yt\_astro\_analysis/pull/133 Set output directory properly for rockstar halo finder https://github.com/yt-project/yt\_astro\_analysis/pull/134 Full Changelog: https://github.com/yt-project/yt\_astro\_analysis/compare/yt\_astro\_analysis-1.1.0...yt\_astro\_analysis-1.1.1},
  organization = {Zenodo},
  keywords = {astronomy,data mining,project-astrophysics,research software engineering},
  file = {/home/sam/Zotero/storage/PY6VPFGJ/5911048.html}
}

@article{sochatResearchSoftwareEncyclopedia2022,
  title = {The {{Research Software Encyclopedia}}: {{A Community Framework}} to {{Define Research Software}}},
  shorttitle = {The {{Research Software Encyclopedia}}},
  author = {Sochat, Vanessa and May, Nicholas and Cosden, Ian and Martinez-Ortiz, Carlos and Bartholomew, Sadie},
  date = {2022-03-04},
  journaltitle = {Journal of Open Research Software},
  volume = {10},
  number = {1},
  pages = {2},
  publisher = {Ubiquity Press},
  issn = {2049-9647},
  doi = {10.5334/jors.359},
  url = {http://openresearchsoftware.metajnl.com/articles/10.5334/jors.359/},
  urldate = {2022-04-12},
  abstract = {The Research Software Encyclopedia is a community driven, open source strategy to define the term “research software” in different contexts. It consists of several elements: a base library to manage a database of software, criteria and taxonomy items that can be used to answer questions about the software in the database, and several ways for an interested party to interact. A community database is stored in version control (GitHub), and by way of providing and updating this database, the Research Software Encyclopedia takes a strategy of small contributions over time to grow a valuable resource. Using a community-driven open source approach offers a number of advantages over attempting to derive a single, holistic definition for research software. First, it takes into account the context under which the definition is considered. Second, community and scoped contributions to specific components of the task are easy. Third, it provides a resource that can be extended to other use cases. Finally, this initiative creates a solution that requires no grants or other funding to maintain, increasing its ability to grow, adapt, and evolve over time.},
  issue = {1},
  langid = {english},
  keywords = {research software engineering},
  annotation = {interest: 73},
  file = {/home/sam/Zotero/storage/7GNWL7FR/Sochat et al. - 2022 - The Research Software Encyclopedia A Community Fr.pdf;/home/sam/Zotero/storage/YTR2IN6P/jors.359.html}
}

@article{soiland-reyesPackagingResearchArtefacts2022,
  title = {Packaging Research Artefacts with {{RO-Crate}}},
  author = {Soiland-Reyes, Stian and Sefton, Peter and Crosas, Mercè and Castro, Leyla Jael and Coppens, Frederik and Fernández, José M. and Garijo, Daniel and Grüning, Björn and La Rosa, Marco and Leo, Simone and Ó Carragáin, Eoghan and Portier, Marc and Trisovic, Ana and RO-Crate Community and Groth, Paul and Goble, Carole},
  date = {2022-01-01},
  journaltitle = {Data Science},
  volume = {5},
  number = {2},
  pages = {97--138},
  publisher = {IOS Press},
  issn = {2451-8484},
  doi = {10.3233/DS-210053},
  url = {https://content.iospress.com/articles/data-science/ds210053},
  urldate = {2023-05-26},
  abstract = {An increasing number of researchers support reproducibility by including pointers to and descriptions of datasets, software and methods in their publications. However, scientific articles may be ambiguous, incomplete and difficult to process by autom},
  langid = {english},
  keywords = {project-acm-rep,project-provenance-pp,reproducibility engineering},
  annotation = {interest: 99},
  file = {/home/sam/Zotero/storage/YHYB8T2Y/Soiland-Reyes et al. - 2022 - Packaging research artefacts with RO-Crate.pdf}
}

@article{soiland-reyesWf4EverResearchObject2013,
  title = {{{Wf4Ever Research Object Model}}},
  author = {Soiland-Reyes, Stian and Bechhofer, Sean and Belhajjame, Khalid and Klyne, Graham and Garijo, Daniel and Coricho, Oscar and García Cuesta, Esteban and Palma, Raul},
  date = {2013-11-30},
  publisher = {Zenodo},
  doi = {10.5281/ZENODO.12744},
  url = {https://zenodo.org/record/12744},
  urldate = {2023-05-26},
  abstract = {The Wf4Ever Research Object Model provides a vocabulary for the description of workflow-centric Research Objects: aggregations of resources relating to scientific workflows. {$<$}strong{$>$}Permalink{$<$}/strong{$>$}: https://w3id.org/ro/2013-11-30/},
  keywords = {project-provenance-pp,provenance,semantic web}
}

@article{sollaciIntroductionMethodsResults2004,
  title = {The Introduction, Methods, Results, and Discussion ({{IMRAD}}) Structure: A Fifty-Year Survey},
  shorttitle = {The Introduction, Methods, Results, and Discussion ({{IMRAD}}) Structure},
  author = {Sollaci, Luciana B. and Pereira, Mauricio G.},
  date = {2004-07},
  journaltitle = {Journal of the Medical Library Association},
  shortjournal = {J Med Libr Assoc},
  volume = {92},
  number = {3},
  eprint = {15243643},
  eprinttype = {pmid},
  pages = {364--371},
  issn = {1536-5050},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC442179/},
  urldate = {2022-05-31},
  abstract = {Background: The scientific article in the health sciences evolved from the letter form and purely descriptive style in the seventeenth century to a very standardized structure in the twentieth century known as introduction, methods, results, and discussion (IMRAD). The pace in which this structure began to be used and when it became the most used standard of today's scientific discourse in the health sciences is not well established., Purpose: The purpose of this study is to point out the period in time during which the IMRAD structure was definitively and widely adopted in medical scientific writing., Methods: In a cross-sectional study, the frequency of articles written under the IMRAD structure was measured from 1935 to 1985 in a randomly selected sample of articles published in four leading journals in internal medicine: the British Medical Journal, JAMA, The Lancet, and the New England Journal of Medicine., Results: The IMRAD structure, in those journals, began to be used in the 1940s. In the 1970s, it reached 80\% and, in the 1980s, was the only pattern adopted in original papers., Conclusions: Although recommended since the beginning of the twentieth century, the IMRAD structure was adopted as a majority only in the 1970s. The influence of other disciplines and the recommendations of editors are among the facts that contributed to authors adhering to it.},
  pmcid = {PMC442179},
  keywords = {academia},
  file = {/home/sam/Zotero/storage/2JF2BZT4/i0025-7338-092-03-0364.pdf}
}

@online{SomeoneBeenMessing,
  title = {Someone’s {{Been Messing With My Subnormals}}!},
  url = {https://moyix.blogspot.com/2022/09/someones-been-messing-with-my-subnormals.html},
  urldate = {2022-11-14},
  organization = {Someone’s Been Messing With My Subnormals!},
  keywords = {numerical methods,research software engineering},
  annotation = {interest: 90},
  file = {/home/sam/Zotero/storage/SRUEUJQ6/someones-been-messing-with-my-subnormals.html}
}

@article{sorosFallibilityReflexivityHuman2013,
  title = {Fallibility, Reflexivity, and the Human Uncertainty Principle},
  author = {Soros, George},
  date = {2013-12-01},
  journaltitle = {Journal of Economic Methodology},
  volume = {20},
  number = {4},
  pages = {309--329},
  publisher = {Routledge},
  issn = {1350-178X},
  doi = {10.1080/1350178X.2013.859415},
  url = {https://doi.org/10.1080/1350178X.2013.859415},
  urldate = {2022-08-30},
  keywords = {economics,epistemology,social science}
}

@article{souzaDataReductionScientific2020,
  title = {Data Reduction in Scientific Workflows Using Provenance Monitoring and User Steering},
  author = {Souza, Renan and Silva, Vítor and Coutinho, Alvaro L. G. A. and Valduriez, Patrick and Mattoso, Marta},
  date = {2020-09-01},
  journaltitle = {Future Generation Computer Systems},
  shortjournal = {Future Generation Computer Systems},
  volume = {110},
  pages = {481--501},
  issn = {0167-739X},
  doi = {10.1016/j.future.2017.11.028},
  url = {https://www.sciencedirect.com/science/article/pii/S0167739X16308238},
  urldate = {2022-11-29},
  abstract = {Scientific workflows need to be iteratively, and often interactively, executed for large input datasets. Reducing data from input datasets is a powerful way to reduce overall execution time in such workflows. When this is accomplished online (i.e., without requiring the user to stop execution to reduce the data, and then resume), it can save much time. However, determining which subsets of the input data should be removed becomes a major problem. A related problem is to guarantee that the workflow system will maintain execution and data consistent with the reduction. Keeping track of how users interact with the workflow is essential for data provenance purposes. In this paper, we adopt the “human-in-the-loop” approach, which enables users to steer the running workflow and reduce subsets from datasets online. We propose an adaptive workflow monitoring approach that combines provenance data monitoring and computational steering to support users in analyzing the evolution of key parameters and determining the subset of data to remove. We extend a provenance data model to keep track of users’ interactions when they reduce data at runtime. In our experimental validation, we develop a test case from the oil and gas domain, using a 936-cores cluster. The results on this test case show that the approach yields reductions of 32\% of execution time and 14\% of the data processed.},
  langid = {english},
  keywords = {input reduction,workflow managers},
  annotation = {interest: 99},
  file = {/home/sam/Zotero/storage/3LJDGW4X/Souza et al. - 2020 - Data reduction in scientific workflows using prove.pdf;/home/sam/Zotero/storage/A89HKTW4/S0167739X16308238.html}
}

@inproceedings{souzaOnlineInputData2016,
  title = {Online {{Input Data Reduction}} in {{Scientific Workflows}}},
  author = {Souza, Renan and Silva, Vítor and Coutinho, Alvaro L. G. A. and Valduriez, Patrick and Mattoso, Marta},
  date = {2016-11-14},
  url = {https://hal-lirmm.ccsd.cnrs.fr/lirmm-01400538},
  urldate = {2022-11-29},
  abstract = {Many scientific workflows are data-intensive and need be iteratively executed for large input sets of data elements. Reducing input data is a powerful way to reduce overall execution time in such workflows. When this is accomplished online (i.e., without requiring users to stop execution to reduce the data and resume execution), it can save much time and user interactions can integrate within workflow execution. Then, a major problem is to determine which subset of the input data should be removed. Other related problems include guaranteeing that the workflow system will maintain execution and data consistent after reduction, and keeping track of how users interacted with execution. In this paper, we adopt the approach " human-in-the-loop " for scientific workflows by enabling users to steer the workflow execution and reduce input elements from datasets at runtime. We propose an adaptive monitoring approach that combines workflow provenance monitoring and computational steering to support users in analyzing the evolution of key parameters and determining which subset of the data should be removed. We also extend a provenance data model to keep track of user interactions when users reduce data at runtime. In our experimental validation, we develop a test case from the oil and gas industry, using a 936-cores cluster. The results on our parameter sweep test case show that the user interactions for online data reduction yield a 37\% reduction of execution time.},
  eventtitle = {{{WORKS}}: {{Workflows}} in {{Support}} of {{Large-scale Science}}},
  langid = {english},
  keywords = {input reduction,workflow managers},
  annotation = {interest: 99},
  file = {/home/sam/Zotero/storage/VBMGS3IM/Souza et al. - 2016 - Online Input Data Reduction in Scientific Workflow.pdf;/home/sam/Zotero/storage/NZ532LFG/lirmm-01400538.html}
}

@article{speerMeetingComputerHalfway,
  title = {Meeting the {{Computer Halfway}}: {{Language Processing}} in the {{Artiﬁcial Language Lojban}}},
  author = {Speer, Rob and Havasi, Catherine},
  pages = {3},
  langid = {english},
  keywords = {artificial intelligence,conlang},
  file = {/home/sam/Zotero/storage/XNQEUU5C/Speer and Havasi - Meeting the Computer Halfway Language Processing .pdf}
}

@unpublished{speicherWhatUsabilityCharacterization2015,
  title = {What Is {{Usability}}? {{A Characterization}} Based on {{ISO}} 9241-11 and {{ISO}}/{{IEC}} 25010},
  shorttitle = {What Is {{Usability}}?},
  author = {Speicher, Maximilian},
  date = {2015-02-24},
  eprint = {1502.06792},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1502.06792},
  urldate = {2022-06-01},
  abstract = {According to Brooke* "Usability does not exist in any absolute sense; it can only be defined with reference to particular contexts." That is, one cannot speak of usability without specifying what that particular usability is characterized by. Driven by the feedback of a reviewer at an international conference, I explore in which way one can precisely specify the kind of usability they are investigating in a given setting. Finally, I come up with a formalism that defines usability as a quintuple comprising the elements level of usability metrics, product, users, goals and context of use. Providing concrete values for these elements then constitutes the investigated type of usability. The use of this formalism is demonstrated in two case studies. * J. Brooke. SUS: A "quick and dirty" usability scale. In P. W. Jordan, B. Thomas, B. A. Weerdmeester, and A. L. McClelland, editors, Usability Evaluation in Industry. Taylor and Francis, 1996.},
  keywords = {human computer interaction}
}

@online{spornyJSONLDWhyHate2014,
  title = {{{JSON-LD}} and {{Why I Hate}} the {{Semantic Web}}},
  author = {Sporny, Manu},
  date = {2014-01-21},
  url = {https://web.archive.org/web/20230526160411/https://manu.sporny.org/2014/json-ld-origins-2/},
  abstract = {The desire for better Web APIs is what motivated the creation of JSON-LD, not the Semantic Web. If you want to make the Semantic Web a reality, stop making the case for it and spend your time doing something more useful, like actually making machines smarter or helping people publish data in a way that’s useful to them.},
  organization = {The Beautiful, Tormented Machine: Art, technology and leaving the world better off than we found it.},
  keywords = {semantic web}
}

@article{springelPurSiMuove2010,
  title = {E Pur Si Muove: {{Galilean-invariant}} Cosmological Hydrodynamical Simulations on a Moving Mesh},
  shorttitle = {E Pur Si Muove},
  author = {Springel, Volker},
  date = {2010-01-11},
  journaltitle = {Monthly Notices of the Royal Astronomical Society},
  shortjournal = {Monthly Notices of the Royal Astronomical Society},
  volume = {401},
  number = {2},
  pages = {791--851},
  issn = {0035-8711},
  doi = {10.1111/j.1365-2966.2009.15715.x},
  url = {https://doi.org/10.1111/j.1365-2966.2009.15715.x},
  urldate = {2022-04-11},
  abstract = {Hydrodynamic cosmological simulations at present usually employ either the Lagrangian smoothed particle hydrodynamics (SPH) technique or Eulerian hydrodynamics on a Cartesian mesh with (optional) adaptive mesh refinement (AMR). Both of these methods have disadvantages that negatively impact their accuracy in certain situations, for example the suppression of fluid instabilities in the case of SPH, and the lack of Galilean invariance and the presence of overmixing in the case of AMR. We here propose a novel scheme which largely eliminates these weaknesses. It is based on a moving unstructured mesh defined by the Voronoi tessellation of a set of discrete points. The mesh is used to solve the hyperbolic conservation laws of ideal hydrodynamics with a finite-volume approach, based on a second-order unsplit Godunov scheme with an exact Riemann solver. The mesh-generating points can in principle be moved arbitrarily. If they are chosen to be stationary, the scheme is equivalent to an ordinary Eulerian method with second-order accuracy. If they instead move with the velocity of the local flow, one obtains a Lagrangian formulation of continuum hydrodynamics that does not suffer from the mesh distortion limitations inherent in other mesh-based Lagrangian schemes. In this mode, our new method is fully Galilean invariant, unlike ordinary Eulerian codes, a property that is of significant importance for cosmological simulations where highly supersonic bulk flows are common. In addition, the new scheme can adjust its spatial resolution automatically and continuously, and hence inherits the principal advantage of SPH for simulations of cosmological structure growth. The high accuracy of Eulerian methods in the treatment of shocks is also retained, while the treatment of contact discontinuities improves. We discuss how this approach is implemented in our new code arepo, both in 2D and in 3D, and is parallelized for distributed memory computers. We also discuss techniques for adaptive refinement or de-refinement of the unstructured mesh. We introduce an individual time-step approach for finite-volume hydrodynamics, and present a high-accuracy treatment of self-gravity for the gas that allows the new method to be seamlessly combined with a high-resolution treatment of collisionless dark matter. We use a suite of test problems to examine the performance of the new code and argue that the hydrodynamic moving-mesh scheme proposed here provides an attractive and competitive alternative to current SPH and Eulerian techniques.},
  keywords = {computational fluid dynamics,cosmological simulation,numerical methods,project-astrophysics},
  file = {/home/sam/Zotero/storage/V3SZCTVX/Springel - 2010 - E pur si muove Galilean-invariant cosmological hy.pdf;/home/sam/Zotero/storage/X27GUXK6/1147356.html}
}

@online{srivastavaGroundbreakingDefinitiveJournals2011,
  title = {Groundbreaking or {{Definitive}}? {{Journals Need}} to {{Pick One}}},
  shorttitle = {Groundbreaking or {{Definitive}}?},
  author = {Srivastava, Sanjay},
  date = {2011-12-31T16:52:38+00:00},
  url = {https://spsptalks.wordpress.com/2011/12/31/groundbreaking-or-definitive-journals-need-to-pick-one/},
  urldate = {2022-08-30},
  abstract = {Do our top journals need to rethink their missions of publishing research that is both groundbreaking and definitive? And as a part of that, do they — and we scientists — need to recons…},
  langid = {english},
  organization = {SPSP},
  keywords = {academic publishing},
  file = {/home/sam/Zotero/storage/VQKZ2QYS/groundbreaking-or-definitive-journals-need-to-pick-one.html}
}

@inproceedings{stamatogiannakisDecouplingProvenanceCapture2015,
  title = {Decoupling Provenance Capture and Analysis from Execution},
  booktitle = {Proceedings of the 7th {{USENIX Conference}} on {{Theory}} and {{Practice}} of {{Provenance}}},
  author = {Stamatogiannakis, Manolis and Groth, Paul and Bos, Herbert},
  date = {2015-07-08},
  series = {{{TaPP}}'15},
  pages = {3},
  publisher = {USENIX Association},
  location = {USA},
  abstract = {Capturing provenance usually involves the direct observation and instrumentation of the execution of a program or workflow. However, this approach restricts provenance analysis to pre-determined programs and methods. This may not pose a problem when one is interested in the provenance of a well-defined workflow, but may limit the analysis of unstructured processes such as interactive desktop computing. In this paper, we present a new approach to capturing provenance based on full execution record and replay. Our approach leverages full-system execution trace logging and replay, which allows the complete decoupling of analysis from the original execution. This enables the selective analysis of the execution using progressively heavier instrumentation.},
  langid = {english},
  keywords = {introspection,project-provenance-pp,provenance,provenance-tool,reverse engineering},
  file = {/home/sam/Zotero/storage/XVTAMHUA/Stamatogiannakis et al. - Decoupling Provenance Capture and Analysis from Ex.pdf}
}

@inproceedings{stamatogiannakisLookingBlackBoxCapturing2015,
  title = {Looking {{Inside}} the {{Black-Box}}: {{Capturing Data Provenance Using Dynamic Instrumentation}}},
  shorttitle = {Looking {{Inside}} the {{Black-Box}}},
  booktitle = {Provenance and {{Annotation}} of {{Data}} and {{Processes}}},
  author = {Stamatogiannakis, Manolis and Groth, Paul and Bos, Herbert},
  editor = {Ludäscher, Bertram and Plale, Beth},
  date = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {155--167},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-16462-5_12},
  abstract = {Knowing the provenance of a data item helps in ascertaining its trustworthiness. Various approaches have been proposed to track or infer data provenance. However, these approaches either treat an executing program as a black-box, limiting the fidelity of the captured provenance, or require developers to modify the program to make it provenance-aware. In this paper, we introduce DataTracker, a new approach to capturing data provenance based on taint tracking, a technique widely used in the security and reverse engineering fields. Our system is able to identify data provenance relations through dynamic instrumentation of unmodified binaries, without requiring access to, or knowledge of, their source code. Hence, we can track provenance for a variety of well-known applications. Because DataTracker looks inside the executing program, it captures high-fidelity and accurate data provenance.},
  isbn = {978-3-319-16462-5},
  langid = {english},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/ULXR9GF5/Stamatogiannakis et al. - 2015 - Looking Inside the Black-Box Capturing Data Prove.pdf}
}

@online{SteveysBlogRants,
  title = {Stevey's {{Blog Rants}}: {{Lisp}} Is {{Not}} an {{Acceptable Lisp}}},
  shorttitle = {Stevey's {{Blog Rants}}},
  url = {https://steve-yegge.blogspot.com/2006/04/lisp-is-not-acceptable-lisp.html},
  urldate = {2024-09-22},
  langid = {english},
  keywords = {programming languages,programming pedagogy},
  file = {/home/sam/Zotero/storage/EEWC7WA3/lisp-is-not-acceptable-lisp.html}
}

@online{stigauthorsRedHatEnterprise2020,
  title = {Red {{Hat Enterprise Linux}} 8 {{Security Technical Implementation Guide}}},
  author = {STIG Authors},
  date = {2020-11-25},
  url = {https://www.stigviewer.com/stig/red_hat_enterprise_linux_8/2020-11-25/},
  urldate = {2023-02-18},
  abstract = {Security Technical Implementation Guides (STIGs) that provides a methodology for standardized secure installation and maintenance of DOD IA and IA-enabled devices and systems.},
  langid = {american},
  organization = {STIG Viewer | Unified Compliance Framework®},
  keywords = {operating systems,project-acm-rep,security},
  file = {/home/sam/Zotero/storage/5MW6IHB4/2020-11-25.html}
}

@online{stinnerBenchmarks,
  title = {Benchmarks},
  author = {Stinner, Victor},
  url = {https://vstinner.readthedocs.io/benchmark.html},
  urldate = {2022-04-11},
  organization = {Victor Stinner's Notes 1.0},
  keywords = {industry practices,software benchmarking,software engineering},
  file = {/home/sam/Zotero/storage/PBBTIIV2/benchmark.html}
}

@article{stoddenBestPracticesComputational2014,
  title = {Best {{Practices}} for {{Computational Science}}: {{Software Infrastructure}} and {{Environments}} for {{Reproducible}} and {{Extensible Research}}},
  shorttitle = {Best {{Practices}} for {{Computational Science}}},
  author = {Stodden, Victoria and Miguez, Sheila},
  date = {2014-07-09},
  journaltitle = {Journal of Open Research Software},
  volume = {2},
  number = {1},
  pages = {e21},
  publisher = {Ubiquity Press},
  issn = {2049-9647},
  doi = {10.5334/jors.ay},
  url = {http://openresearchsoftware.metajnl.com/articles/10.5334/jors.ay/},
  urldate = {2023-01-19},
  abstract = {The goal of this article is to coalesce a discussion around best practices for scholarly research that utilizes computational methods, by providing a formalized set of best practice recommendations to guide computational scientists and other stakeholders wishing to disseminate reproducible research, facilitate innovation by enabling data and code re-use, and enable broader communication of the output of computational scientific research. Scholarly dissemination and communication standards are changing to reflect the increasingly computational nature of scholarly research, primarily to include the sharing of the data and code associated with published results. We also present these Best Practices as a living, evolving, and changing document at http://wiki.stodden.net/Best\_Practices.},
  issue = {1},
  langid = {english},
  keywords = {project-acm-rep,reproducibility engineering,research software engineering},
  file = {/home/sam/Zotero/storage/5YM2UMKA/Stodden and Miguez - 2014 - Best Practices for Computational Science Software.pdf}
}

@article{stoddenEmpiricalAnalysisJournal2018,
  title = {An Empirical Analysis of Journal Policy Effectiveness for Computational Reproducibility},
  author = {Stodden, Victoria and Seiler, Jennifer and Ma, Zhaokun},
  date = {2018-03-13},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {11},
  pages = {2584--2589},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1708290115},
  url = {https://www.pnas.org/doi/full/10.1073/pnas.1708290115},
  urldate = {2023-01-19},
  abstract = {A key component of scientific communication is sufficient information for other researchers in the field to reproduce published findings. For computational and data-enabled research, this has often been interpreted to mean making available the raw data from which results were generated, the computer code that generated the findings, and any additional information needed such as workflows and input parameters. Many journals are revising author guidelines to include data and code availability. This work evaluates the effectiveness of journal policy that requires the data and code necessary for reproducibility be made available postpublication by the authors upon request. We assess the effectiveness of such a policy by (i) requesting data and code from authors and (ii) attempting replication of the published findings. We chose a random sample of 204 scientific papers published in the journal Science after the implementation of their policy in February 2011. We found that we were able to obtain artifacts from 44\% of our sample and were able to reproduce the findings for 26\%. We find this policy—author remission of data and code postpublication upon request—an improvement over no policy, but currently insufficient for reproducibility.},
  keywords = {project-acm-rep,reproducibility engineering},
  file = {/home/sam/Zotero/storage/T537TK9E/Stodden et al. - 2018 - An empirical analysis of journal policy effectiven.pdf}
}

@article{stoddenEnhancingReproducibilityComputational2016,
  title = {Enhancing Reproducibility for Computational Methods},
  author = {Stodden, Victoria and McNutt, Marcia and Bailey, David H. and Deelman, Ewa and Gil, Yolanda and Hanson, Brooks and Heroux, Michael A. and Ioannidis, John P.A. and Taufer, Michela},
  date = {2016-12-09},
  journaltitle = {Science},
  volume = {354},
  number = {6317},
  pages = {1240--1241},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.aah6168},
  url = {https://www.science.org/doi/full/10.1126/science.aah6168},
  urldate = {2022-12-18},
  keywords = {reproducibility},
  annotation = {interest: 96}
}

@article{stoddenReproducibleResearchAddressing2010,
  title = {Reproducible {{Research}}: {{Addressing}} the {{Need}} for {{Data}} and {{Code Sharing}} in {{Computational Science}}},
  shorttitle = {Reproducible {{Research}}},
  author = {Stodden, Victoria C.},
  date = {2010},
  doi = {10.7916/D8WH30H4},
  url = {https://academiccommons.columbia.edu/doi/10.7916/D8WH30H4},
  urldate = {2022-07-08},
  abstract = {Roundtable participants identified ways of making computational research details readily available, which is a crucial step in addressing the current credibility crisis.},
  keywords = {reproducibility engineering},
  file = {/home/sam/Zotero/storage/WRRVNIHJ/Stodden - 2010 - Reproducible Research Addressing the Need for Dat.pdf}
}

@inproceedings{stoddenRunMyCodeorgNovelDissemination2012,
  title = {{{RunMyCode}}.Org: {{A}} Novel Dissemination and Collaboration Platform for Executing Published Computational Results},
  shorttitle = {{{RunMyCode}}.Org},
  booktitle = {2012 {{IEEE}} 8th {{International Conference}} on {{E-Science}}},
  author = {Stodden, Victoria and Hurlin, Christophe and Perignon, Christophe},
  date = {2012-10},
  pages = {1--8},
  publisher = {IEEE},
  location = {Chicago, IL, USA},
  doi = {10.1109/eScience.2012.6404455},
  url = {http://ieeexplore.ieee.org/document/6404455/},
  urldate = {2024-10-04},
  eventtitle = {2012 {{IEEE}} 8th {{International Conference}} on {{E-Science}} (e-{{Science}})},
  isbn = {978-1-4673-4466-1 978-1-4673-4467-8 978-1-4673-4465-4},
  file = {/home/sam/Zotero/storage/H4KPFR37/Stodden et al. - 2012 - RunMyCode.org A novel dissemination and collaboration platform for executing published computationa.pdf}
}

@online{stoddenScientificMethodPractice2010,
  type = {SSRN Scholarly Paper},
  title = {The {{Scientific Method}} in {{Practice}}: {{Reproducibility}} in the {{Computational Sciences}}},
  shorttitle = {The {{Scientific Method}} in {{Practice}}},
  author = {Stodden, Victoria},
  date = {2010-02-09},
  number = {1550193},
  location = {Rochester, NY},
  doi = {10.2139/ssrn.1550193},
  url = {https://papers.ssrn.com/abstract=1550193},
  urldate = {2023-01-24},
  abstract = {Since the 1660’s the scientific method has included reproducibility as a mainstay in its effort to root error from scientific discovery. With the explosive growth of digitization in scientific research and communication, it is easier than ever to satisfy this requirement. In computational research experimental details and methods can be recorded in code and scripts, data is digital, papers are frequently online, and the result is the potential for “really reproducible research.”1 Imagine the ability to routinely inspect code and data and recreate others’ results: Every step taken to achieve the findings can potentially be transparent. Now imagine anyone with an Internet connection and the capability of running the code being able to do this. This paper investigates the obstacles blocking the sharing of code and data to understand conditions under which computational scientists reveal their full research compendium. A survey of registrants at a top machine learning conference (NIPS) was used to discover the strength of underlying factors that affect the decision to reveal code, data, and ideas. Sharing of code and data is becoming more common as about a third of respondents post some on their websites, and about 85\% self report to have some code or data publicly available on the web. Contrary to theoretical expectations, the decision to share work is grounded in communitarian norms, although when work remains hidden private incentives dominate the decision. We find that code, data, and ideas are each regarded differently in terms of how they are revealed and that guidance from scientific norms varies with pervasiveness of computation in the field. The largest barriers to sharing are time involved in preparation of work and the legal Intellectual Property framework scientists face. This paper does two things. It provides evidence in the debate about whether scientists’ research revealing behavior is wholly governed by considerations of personal impact or whether the reasoning behind the revealing decision involves larger scientific ideals, and secondly, this research describes the actual sharing behavior in the Machine Learning community.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {metascience,reproducibility},
  annotation = {interst: 90},
  file = {/home/sam/Zotero/storage/2IRDMJS5/Stodden - 2010 - The Scientific Method in Practice Reproducibility.pdf}
}

@report{stoddenSettingDefaultReproducible2013,
  title = {Setting the {{Default}} to {{Reproducible}}: {{Reproducibility}} in {{Computational}} and {{Experimental Mathematics}}},
  shorttitle = {Setting the {{Default}} to {{Reproducible}}},
  author = {Stodden, V. and Bailey, D. H. and Borwein, J. and LeVeque, R. J. and Rider, W. and Stein, W.},
  date = {2013-02-16},
  institution = {{Reproducibility in Computational and Experimental Mathematics (ICERM) Workshop}},
  url = {https://icerm.brown.edu/topical_workshops/tw12-5-rcem/icerm_report.pdf},
  abstract = {Science is built upon foundations of theory and experiment validated and improved through open, transparent communication. With the increasingly central role of computation in scientific discovery this means communicating all details of the computations needed for others to replicate the experiment, i.e. making available to others the associated data and code. The "reproducible research" movement recognizes that traditional scientific research and publication practices now fall short of this ideal, and encourages all those involved in the production of computational science --- scientists who use computational methods and the institutions that employ them, journals and dissemination mechanisms, and funding agencies --- to facilitate and practice really reproducible research},
  langid = {american},
  keywords = {reproducibility engineering},
  annotation = {interest: 88},
  file = {/home/sam/Zotero/storage/ZIT27H7Q/icerm-report.pdf}
}

@article{stoneZEUS2DRadiationMagnetohydrodynamics1992,
  title = {{{ZEUS-2D}}: {{A Radiation Magnetohydrodynamics Code}} for {{Astrophysical Flows}} in {{Two Space Dimensions}}. {{I}}. {{The Hydrodynamic Algorithms}} and {{Tests}}},
  shorttitle = {{{ZEUS-2D}}},
  author = {Stone, James M. and Norman, Michael L.},
  date = {1992-06-01},
  journaltitle = {The Astrophysical Journal Supplement Series},
  volume = {80},
  pages = {753},
  issn = {0067-0049},
  doi = {10.1086/191680},
  url = {https://ui.adsabs.harvard.edu/abs/1992ApJS...80..753S},
  urldate = {2022-04-11},
  abstract = {A detailed description of ZEUS-2D, a numerical code for the simulation of fluid dynamical flows including a self-consistent treatment of the effects of magnetic fields and radiation transfer is presented. Attention is given to the hydrodynamic (HD) algorithms which form the foundation for the more complex MHD and radiation HD algorithms. The effect of self-gravity on the flow dynamics is accounted for by an iterative solution of the sparse-banded matrix resulting from discretizing the Poisson equation in multidimensions. The results of an extensive series of HD test problems are presented. A detailed description of the MHD algorithms in ZEUS-2D is presented. A new method of computing the electromotive force is developed using the method of characteristics (MOC). It is demonstrated through the results of an extensive series of MHD test problems that the resulting hybrid MOC-constrained transport method provides for the accurate evolution of all modes of MHD wave families.},
  keywords = {astrophysics,computational fluid dynamics},
  annotation = {ADS Bibcode: 1992ApJS...80..753S},
  file = {/home/sam/Zotero/storage/QHFN3D6Z/Stone and Norman - 1992 - ZEUS-2D A Radiation Magnetohydrodynamics Code for.pdf}
}

@article{stoneZEUS2DRadiationMagnetohydrodynamics1992a,
  title = {{{ZEUS-2D}}: {{A Radiation Magnetohydrodynamics Code}} for {{Astrophysical Flows}} in {{Two Space Dimensions}}. {{II}}. {{The Magnetohydrodynamic Algorithms}} and {{Tests}}},
  shorttitle = {{{ZEUS-2D}}},
  author = {Stone, James M. and Norman, Michael L.},
  date = {1992-06-01},
  journaltitle = {The Astrophysical Journal Supplement Series},
  volume = {80},
  pages = {791},
  issn = {0067-0049},
  doi = {10.1086/191681},
  url = {https://ui.adsabs.harvard.edu/abs/1992ApJS...80..791S},
  urldate = {2022-04-11},
  abstract = {In this, the second of a series of three papers, we continue a detailed description of ZEUS-2D, a numerical code for the simulation of fluid dynamical flows in astrophysics including a self-consistent treatment of the effects of magnetic fields and radiation transfer. In this paper, we give a detailed description of the magnetohydrodynamical (MHD) algorithms in ZEUS-2D. The recently developed constrained transport (CT) algorithm is implemented for the numerical evolution of the components of the magnetic field for MHD simulations. This formalism guarantees the numerically evolved field components will satisfy the divergence-free constraint at all times. We find, however, that the method used to compute the electromotive forces must be chosen carefully to propagate accurately all modes of MHD wave families (in particular shear Alfvén waves). A new method of computing the electromotive force is developed using the method of characteristics (MOC). It is demonstrated through the results of an extensive series of MHD test problems that the resulting hybrid MOC-CT method provides for the accurate evolution of all modes of MHD wave families.},
  keywords = {astrophysics,computational fluid dynamics,project-astrophysics},
  annotation = {ADS Bibcode: 1992ApJS...80..791S},
  file = {/home/sam/Zotero/storage/TZIARM3B/Stone and Norman - 1992 - ZEUS-2D A Radiation Magnetohydrodynamics Code for.pdf}
}

@article{strangConstructionComparisonDifference1968,
  title = {On the {{Construction}} and {{Comparison}} of {{Difference Schemes}}},
  author = {Strang, Gilbert},
  date = {1968-09},
  journaltitle = {SIAM Journal on Numerical Analysis},
  shortjournal = {SIAM J. Numer. Anal.},
  volume = {5},
  number = {3},
  pages = {506--517},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0036-1429},
  doi = {10.1137/0705041},
  url = {https://epubs.siam.org/doi/10.1137/0705041},
  urldate = {2022-04-11},
  abstract = {In this paper, we present Approximation Schemes for Capacitated Vehicle Routing Problem (CVRP) on several classes of graphs. In CVRP, introduced by Dantzig and Ramser in 1959 [13], we are given a graph G = (V, E) with metric edges costs, a depot r ∊ V, and a vehicle of bounded capacity Q. The goal is to find a minimum cost collection of tours for the vehicle that returns to the depot, each visiting at most Q nodes, such that they cover all the nodes. This generalizes classic TSP and has been studied extensively. In the more general setting, each node v has a demand dv and the total demand of each tour must be no more than Q. Either the demand of each node must be served by one tour (unsplittable) or can be served by multiple tours (splittable). The best known approximation algorithm for general graphs has ratio α + 2(1–∊) (for the unsplittable) and α + 1–∊ (for the splittable) for some fixed  is the best approximation for TSP. Even for the case of trees, the best approximation ratio is 4/3 [5], and it has been an open question if there is an approximation scheme for this simple class of graphs. Das and Mathieu [14] presented an approximation scheme with time  for Euclidean plane ℝ2. No other approximation scheme is known for any other class of metrics (without further restrictions on Q). In this paper, we make significant progress on this classic problem by presenting Quasi-Polynomial Time Approximation Schemes (QPTAS) for graphs of bounded treewidth, graphs of bounded highway dimensions, and graphs of bounded doubling dimensions. For comparison, our result implies an approximation scheme for Euclidean plane with run time .},
  keywords = {astrophysics,numerical methods,project-astrophysics},
  file = {/home/sam/Zotero/storage/KBFXXV8M/Strang - 1968 - On the Construction and Comparison of Difference S.pdf}
}

@inproceedings{suenS2LoggerEndtoEndData2013,
  title = {{{S2Logger}}: {{End-to-End Data Tracking Mechanism}} for {{Cloud Data Provenance}}},
  shorttitle = {{{S2Logger}}},
  booktitle = {2013 12th {{IEEE International Conference}} on {{Trust}}, {{Security}} and {{Privacy}} in {{Computing}} and {{Communications}}},
  author = {Suen, Chun Hui and Ko, Ryan K.L. and Tan, Yu Shyang and Jagadpramana, Peter and Lee, Bu Sung},
  date = {2013-07},
  pages = {594--602},
  issn = {2324-9013},
  doi = {10.1109/TrustCom.2013.73},
  abstract = {The inability to effectively track data in cloud computing environments is becoming one of the top concerns for cloud stakeholders. This inability is due to two main reasons. Firstly, the lack of data tracking tools built for clouds. Secondly, current logging mechanisms are only designed from a system-centric perspective. There is a need for data-centric logging techniques which can trace data activities (e.g. file creation, edition, duplication, transfers, deletions, etc.) within and across all cloud servers. This will effectively enable full transparency and accountability for data movements in the cloud. In this paper, we introduce S2Logger, a data event logging mechanism which captures, analyses and visualizes data events in the cloud from the data point of view. By linking together atomic data events captured at both file and block level, the resulting sequence of data events depicts the cloud data provenance records throughout the data lifecycle. With this information, we can then detect critical data-related cloud security problems such as malicious actions, data leakages and data policy violations by analysing the data provenance. S2Logger also enables us to address the gaps and inadequacies of existing system-centric security tools.},
  eventtitle = {2013 12th {{IEEE International Conference}} on {{Trust}}, {{Security}} and {{Privacy}} in {{Computing}} and {{Communications}}},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/IV6JM6YF/Suen et al. - 2013 - S2Logger End-to-End Data Tracking Mechanism for C.pdf}
}

@article{suhEMPExecutionTime2017,
  title = {{{EMP}}: Execution Time Measurement Protocol for Compute-Bound Programs},
  shorttitle = {{{EMP}}},
  author = {Suh, Young-Kyoon and Snodgrass, Richard T. and Kececioglu, John D. and Downey, Peter J. and Maier, Robert S. and Yi, Cheng},
  date = {2017-04},
  journaltitle = {Software: Practice and Experience},
  volume = {47},
  number = {4},
  pages = {559--597},
  issn = {0038-0644, 1097-024X},
  doi = {10.1002/spe.2476},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2476},
  urldate = {2023-08-22},
  abstract = {Measuring execution time is one of the most used performance evaluation techniques in computer science research. Inaccurate measurements cannot be used for a fair performance comparison between programs. Despite the prevalence of its use, the intrinsic variability in the time measurement makes it hard to obtain repeatable and accurate timing results of a program running on an operating system. We propose a novel execution time measurement protocol (termed EMP) for measuring the execution time of a compute-bound program on Linux, while minimizing that measurement's variability. During the development of execution time measurement protocol, we identified several factors that disturb execution time measurement. We introduce successive refinements to the protocol by addressing each of these factors, in concert, reducing variability by more than an order of magnitude. We also introduce a new visualization technique, what we term ‘dual-execution scatter plot’ that highlights infrequent, long-running daemons, differentiating them from frequent and/or short-running daemons. Our empirical results show that the proposed protocol successfully achieves three major aspects—precision, accuracy, and scalability—in execution time measurement that can work for open-source and proprietary software. Copyright © 2017 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {benchmarking,project-provenance-pp,software benchmarking},
  annotation = {interest: 71},
  file = {/home/sam/Zotero/storage/FKY79VS6/Suh et al. - 2017 - EMP execution time measurement protocol for compu.pdf;/home/sam/Zotero/storage/LERXZLRE/spe.html}
}

@inproceedings{sultanaFileProvenanceSystem2013,
  title = {A File Provenance System},
  booktitle = {Proceedings of the Third {{ACM}} Conference on {{Data}} and Application Security and Privacy},
  author = {Sultana, Salmin and Bertino, Elisa},
  date = {2013-02-18},
  series = {{{CODASPY}} '13},
  pages = {153--156},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2435349.2435368},
  url = {https://dl.acm.org/doi/10.1145/2435349.2435368},
  urldate = {2023-08-23},
  abstract = {A file provenance system supports the automatic collection and management of provenance i.e. the complete processing history of a data object. File system level provenance provides functionality unavailable in the existing provenance systems. In this paper, we discuss the design objectives for a flexible and efficient file provenance system and then propose the design of such a system, called FiPS. We design FiPS as a thin stackable file system for capturing provenance in a portable manner. FiPS can capture provenance at various degrees of granularity, can transform provenance records into secure information, and can direct the resulting provenance data to various persistent storage systems.},
  isbn = {978-1-4503-1890-7},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/L543Q7CC/Sultana and Bertino - 2013 - A file provenance system.pdf}
}

@online{SystemdJournal,
  title = {Systemd Journal},
  url = {https://docs.google.com/document/u/0/d/1IC9yOXj7j6cdLLxWEBAGRL6wl97tFxgjLUEHIX3MSTs/pub},
  urldate = {2023-09-17},
  file = {/home/sam/Zotero/storage/NG2IYAPU/pub.html}
}

@unpublished{szalaySloanDigitalSky1999,
  title = {The {{Sloan Digital Sky Survey}} and Its {{Archive}}},
  author = {Szalay, Alexander S. and Kunszt, Peter and Thakar, Anirudha and Gray, Jim and Slutz, Don},
  date = {1999-12-17},
  eprint = {astro-ph/9912382},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/astro-ph/9912382},
  urldate = {2022-04-12},
  abstract = {The next-generation astronomy archives will cover most of the universe at fine resolution in many wavelengths. One of the first of these projects, the Sloan Digital Sky Survey (SDSS) will create a 5-wavelength catalog over 10,000 square degrees of the sky. The 200 million objects in the multi-terabyte database will have mostly numerical attributes, defining a space of 100+ dimensions. Points in this space have highly correlated distributions. The archive will enable astronomers to explore the data interactively. Data access will be aided by multidimensional spatial indices. The data will be partitioned in many ways. Small tag objects consisting of the most popular attributes speed up frequent searches. Splitting the data among multiple servers enables parallel, scalable I/O. Hashing techniques allow efficient clustering and pairwise comparison algorithms. Randomly sampled subsets allow debugging otherwise large queries at the desktop. Central servers will operate a data pump that supports sweeping searches that touch most of the data.},
  keywords = {astronomical observations,data mining,project-astrophysics,research software engineering},
  annotation = {interest: 80},
  file = {/home/sam/Zotero/storage/QY7P9KDH/Szalay et al. - 1999 - The Sloan Digital Sky Survey and its Archive.pdf;/home/sam/Zotero/storage/IU2SSQPU/9912382.html}
}

@online{targetHowLispBecame2018,
  title = {How {{Lisp Became God}}'s {{Own Programming Language}}},
  author = {Target, Sinclair},
  date = {2018-10-14},
  url = {https://twobithistory.org/2018/10/14/lisp.html},
  urldate = {2024-01-03},
  abstract = {A look at the fascinating history behind the one programming language with magical powers.},
  keywords = {history of computing,programming languages},
  file = {/home/sam/Zotero/storage/XHHBNRT2/lisp.html}
}

@online{targetWhateverHappenedSemantic2018,
  title = {Whatever {{Happened}} to the {{Semantic Web}}?},
  author = {Target, Sinclair},
  date = {2018-05-27},
  url = {https://twobithistory.org/2018/05/27/semantic-web.html},
  urldate = {2023-06-06},
  file = {/home/sam/Zotero/storage/HBIFISEH/semantic-web.html}
}

@inproceedings{tariqAutomatedCollectionApplicationLevel2012,
  title = {Towards {{Automated Collection}} of \{\vphantom\}{{Application-Level}}\vphantom\{\} {{Data Provenance}}},
  author = {Tariq, Dawood and Masaim, Ali and Gehani, Ashish},
  date = {2012},
  url = {https://www.usenix.org/conference/tapp12/workshop-program/presentation/tariq},
  urldate = {2023-08-23},
  eventtitle = {4th {{USENIX Workshop}} on the {{Theory}} and {{Practice}} of {{Provenance}} ({{TaPP}} 12)},
  langid = {english},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/PG74HTCE/2012 - Towards Automated Collection of Application-Level.pdf}
}

@book{taylorWorkflowsEScienceScientific2014,
  title = {Workflows for E-{{Science}}: {{Scientific Workflows}} for {{Grids}}},
  shorttitle = {Workflows for E-{{Science}}},
  editor = {Taylor, Ian J. and Deelman, Ewa and Gannon, Dennis B. and Shields, Matthew},
  date = {2014-03-27},
  edition = {2007th edition},
  publisher = {Springer},
  url = {https://link.springer.com/book/10.1007/978-1-84628-757-2},
  abstract = {Workflows for e-Science is divided into four parts, which represent four broad but distinct areas of scientific workflows. In the first part, Background, we introduce the concept of scientific workflows and set the scene by describing how they differ from their business workflow counterpart. In Part II, Application and User Perspective, we provide a number of scientific examples that currently use workflows for their e-Science experiments. In Workflow Representation and Common Structure (Part III), we describe core workflow themes, such as control flow or dataflow and the use of components or services. In this part, we also provide overviews for a number of common workflow languages, such as Petri Nets, the Business Process Execution Language (BPEL), and the Virtual Data Language (VDL), along with service interfaces. In Part IV, Frameworks and Tools, we take a look at many of the popular environments that are currently being used for e-Science applications by paying particular attention to their workflow capabilities. The following four sections describe the chapters in each part and therefore provide a comprehensive summary of the book as a whole.},
  isbn = {978-1-84996-619-1},
  langid = {english},
  pagetotal = {548},
  keywords = {workflow managers},
  file = {/home/sam/Zotero/storage/9KLYDT65/Taylor et al. - Workflows for e-Science.pdf;/home/sam/Zotero/storage/92UFJE7C/978-1-84628-757-2.html}
}

@article{tennantMultidisciplinaryPerspectiveEmergent2017,
  title = {A Multi-Disciplinary Perspective on Emergent and Future Innovations in Peer Review},
  author = {Tennant, Jonathan P. and Dugan, Jonathan M. and Graziotin, Daniel and Jacques, Damien C. and Waldner, François and Mietchen, Daniel and Elkhatib, Yehia and B. Collister, Lauren and Pikas, Christina K. and Crick, Tom and Masuzzo, Paola and Caravaggi, Anthony and Berg, Devin R. and Niemeyer, Kyle E. and Ross-Hellauer, Tony and Mannheimer, Sara and Rigling, Lillian and Katz, Daniel S. and Greshake Tzovaras, Bastian and Pacheco-Mendoza, Josmel and Fatima, Nazeefa and Poblet, Marta and Isaakidis, Marios and Irawan, Dasapta Erwin and Renaut, Sébastien and Madan, Christopher R. and Matthias, Lisa and Nørgaard Kjær, Jesper and O'Donnell, Daniel Paul and Neylon, Cameron and Kearns, Sarah and Selvaraju, Manojkumar and Colomb, Julien},
  date = {2017-11-29},
  journaltitle = {F1000Research},
  shortjournal = {F1000Res},
  volume = {6},
  eprint = {29188015},
  eprinttype = {pmid},
  pages = {1151},
  issn = {2046-1402},
  doi = {10.12688/f1000research.12037.3},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5686505/},
  urldate = {2022-09-06},
  abstract = {Peer review of research articles is a core part of our scholarly communication system. In spite of its importance, the status and purpose of peer review is often contested. What is its role in our modern digital research and communications infrastructure? Does it perform to the high standards with which it is generally regarded? Studies of peer review have shown that it is prone to bias and abuse in numerous dimensions, frequently unreliable, and can fail to detect even fraudulent research. With the advent of web technologies, we are now witnessing a phase of innovation and experimentation in our approaches to peer review. These developments prompted us to examine emerging models of peer review from a range of disciplines and venues, and to ask how they might address some of the issues with our current systems of peer review. We examine the functionality of a range of social Web platforms, and compare these with the traits underlying a viable peer review system: quality control, quantified performance metrics as engagement incentives, and certification and reputation. Ideally, any new systems will demonstrate that they out-perform and reduce the biases of existing models as much as possible. We conclude that there is considerable scope for new peer review initiatives to be developed, each with their own potential issues and advantages. We also propose a novel hybrid platform model that could, at least partially, resolve many of the socio-technical issues associated with peer review, and potentially disrupt the entire scholarly communication system. Success for any such development relies on reaching a critical threshold of research community engagement with both the process and the platform, and therefore cannot be achieved without a significant change of incentives in research environments.},
  pmcid = {PMC5686505},
  keywords = {academic publishing,metascience},
  annotation = {interest: 87},
  file = {/home/sam/Zotero/storage/XMS7M5E8/Tennant et al. - 2017 - A multi-disciplinary perspective on emergent and f.pdf}
}

@misc{terrellPeonageUnitedStates1907,
  title = {Peonage in the {{United States}}},
  author = {Terrell, Mary Church},
  date = {1907},
  url = {https://awpc.cattcenter.iastate.edu/2019/11/22/peonage-in-the-united-states-1907/},
  urldate = {2022-05-10},
  langid = {english},
  keywords = {american history,civil rights},
  file = {/home/sam/Zotero/storage/5QNUI662/peonage-in-the-united-states-1907.html}
}

@online{thainCommonModelHighly2014,
  title = {Toward a {{Common Model}} of {{Highly Concurrent Programming}}},
  shorttitle = {Prof. {{Douglas Thain}}},
  author = {Thain, Douglas},
  date = {2014-05-19},
  url = {https://dthain.blogspot.com/2014/05/toward-common-model-of-highly.html},
  urldate = {2022-09-06},
  organization = {Prof. Douglas Thain},
  keywords = {programming languages},
  annotation = {interest: 64},
  file = {/home/sam/Zotero/storage/JR6BETI8/toward-common-model-of-highly.html}
}

@online{thalheimInspectorDataProvenance2016a,
  title = {Inspector: {{A Data Provenance Library}} for {{Multithreaded Programs}}},
  shorttitle = {Inspector},
  author = {Thalheim, Jörg and Bhatotia, Pramod and Fetzer, Christof},
  date = {2016-05-02},
  eprint = {1605.00498},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1605.00498},
  url = {http://arxiv.org/abs/1605.00498},
  urldate = {2024-01-21},
  abstract = {Data provenance strives for explaining how the computation was performed by recording a trace of the execution. The provenance trace is useful across a wide-range of workflows to improve the dependability, security, and efficiency of software systems. In this paper, we present Inspector, a POSIX-compliant data provenance library for shared-memory multithreaded programs. The Inspector library is completely transparent and easy to use: it can be used as a replacement for the pthreads library by a simple exchange of libraries linked, without even recompiling the application code. To achieve this result, we present a parallel provenance algorithm that records control, data, and schedule dependencies using a Concurrent Provenance Graph (CPG). We implemented our algorithm to operate at the compiled binary code level by leveraging a combination of OS-specific mechanisms, and recently released Intel PT ISA extensions as part of the Broadwell micro-architecture. Our evaluation on a multicore platform using applications from multithreaded benchmarks suites (PARSEC and Phoenix) shows reasonable provenance overheads for a majority of applications. Lastly, we briefly describe three case-studies where the generic interface exported by Inspector is being used to improve the dependability, security, and efficiency of systems. The Inspector library is publicly available for further use in a wide range of other provenance workflows.},
  pubstate = {prepublished},
  keywords = {computer architecture,project-provenance-pp},
  file = {/home/sam/Zotero/storage/VGWTF9CE/Thalheim et al. - 2016 - Inspector A Data Provenance Library for Multithre.pdf;/home/sam/Zotero/storage/GSDDFS9S/1605.html}
}

@article{thegalaxycommunityGalaxyPlatformAccessible2024,
  title = {The {{Galaxy}} Platform for Accessible, Reproducible, and Collaborative Data Analyses: 2024 Update},
  shorttitle = {The {{Galaxy}} Platform for Accessible, Reproducible, and Collaborative Data Analyses},
  author = {{The Galaxy Community} and Abueg, Linelle Ann L and Afgan, Enis and Allart, Olivier and Awan, Ahmed H and Bacon, Wendi A and Baker, Dannon and Bassetti, Madeline and Batut, Bérénice and Bernt, Matthias and Blankenberg, Daniel and Bombarely, Aureliano and Bretaudeau, Anthony and Bromhead, Catherine J and Burke, Melissa L and Capon, Patrick K and Čech, Martin and Chavero-Díez, María and Chilton, John M and Collins, Tyler J and Coppens, Frederik and Coraor, Nate and Cuccuru, Gianmauro and Cumbo, Fabio and Davis, John and De Geest, Paul F and De Koning, Willem and Demko, Martin and DeSanto, Assunta and Begines, José Manuel Domínguez and Doyle, Maria A and Droesbeke, Bert and Erxleben-Eggenhofer, Anika and Föll, Melanie C and Formenti, Giulio and Fouilloux, Anne and Gangazhe, Rendani and Genthon, Tanguy and Goecks, Jeremy and Beltran, Alejandra N Gonzalez and Goonasekera, Nuwan A and Goué, Nadia and Griffin, Timothy J and Grüning, Björn A and Guerler, Aysam and Gundersen, Sveinung and Gustafsson, Ove Johan Ragnar and Hall, Christina and Harrop, Thomas W and Hecht, Helge and Heidari, Alireza and Heisner, Tillman and Heyl, Florian and Hiltemann, Saskia and Hotz, Hans-Rudolf and Hyde, Cameron J and Jagtap, Pratik D and Jakiela, Julia and Johnson, James E and Joshi, Jayadev and Jossé, Marie and Jum’ah, Khaled and Kalaš, Matúš and Kamieniecka, Katarzyna and Kayikcioglu, Tunc and Konkol, Markus and Kostrykin, Leonid and Kucher, Natalie and Kumar, Anup and Kuntz, Mira and Lariviere, Delphine and Lazarus, Ross and Bras, Yvan Le and Corguillé, Gildas Le and Lee, Justin and Leo, Simone and Liborio, Leandro and Libouban, Romane and Tabernero, David López and Lopez-Delisle, Lucille and Los, Laila S and Mahmoud, Alexandru and Makunin, Igor and Marin, Pierre and Mehta, Subina and Mok, Winnie and Moreno, Pablo A and Morier-Genoud, François and Mosher, Stephen and Müller, Teresa and Nasr, Engy and Nekrutenko, Anton and Nelson, Tiffanie M and Oba, Asime J and Ostrovsky, Alexander and Polunina, Polina V and Poterlowicz, Krzysztof and Price, Elliott J and Price, Gareth R and Rasche, Helena and Raubenolt, Bryan and Royaux, Coline and Sargent, Luke and Savage, Michelle T and Savchenko, Volodymyr and Savchenko, Denys and Schatz, Michael C and Seguineau, Pauline and Serrano-Solano, Beatriz and Soranzo, Nicola and Srikakulam, Sanjay Kumar and Suderman, Keith and Syme, Anna E and Tangaro, Marco Antonio and Tedds, Jonathan A and Tekman, Mehmet and Cheng (Mike) Thang, Wai and Thanki, Anil S and Uhl, Michael and Van Den Beek, Marius and Varshney, Deepti and Vessio, Jenn and Videm, Pavankumar and Von Kuster, Greg and Watson, Gregory R and Whitaker-Allen, Natalie and Winter, Uwe and Wolstencroft, Martin and Zambelli, Federico and Zierep, Paul and Zoabi, Rand},
  date = {2024-07-05},
  journaltitle = {Nucleic Acids Research},
  volume = {52},
  number = {W1},
  pages = {W83-W94},
  issn = {0305-1048, 1362-4962},
  doi = {10.1093/nar/gkae410},
  url = {https://academic.oup.com/nar/article/52/W1/W83/7676834},
  urldate = {2024-10-04},
  abstract = {Abstract             Galaxy (https://galaxyproject.org) is deployed globally, predominantly through free-to-use services, supporting user-driven research that broadens in scope each year. Users are attracted to public Galaxy services by platform stability, tool and reference dataset diversity, training, support~and integration, which enables complex, reproducible, shareable data analysis. Applying the principles of user experience design (UXD), has driven improvements in accessibility, tool discoverability through Galaxy Labs/subdomains, and a redesigned Galaxy ToolShed. Galaxy tool capabilities are progressing in two strategic directions: integrating general purpose graphical processing units (GPGPU) access for cutting-edge methods, and licensed tool support. Engagement with global research consortia is being increased by developing more workflows in Galaxy and by resourcing the public Galaxy services to run them. The Galaxy Training Network (GTN) portfolio has grown in both size, and accessibility, through learning paths and direct integration with Galaxy tools that feature in training courses. Code development continues in line with the Galaxy Project roadmap, with improvements to job scheduling and the user interface. Environmental impact assessment is also helping engage users and developers, reminding them of their role in sustainability, by displaying estimated CO2 emissions generated by each Galaxy job.},
  langid = {english},
  file = {/home/sam/Zotero/storage/I8XI4DFP/The Galaxy Community et al. - 2024 - The Galaxy platform for accessible, reproducible, and collaborative data analyses 2024 update.pdf}
}

@inproceedings{thiesStreamItLanguageStreaming2002,
  title = {{{StreamIt}}: {{A Language}} for {{Streaming Applications}}},
  shorttitle = {{{StreamIt}}},
  booktitle = {Compiler {{Construction}}},
  author = {Thies, William and Karczmarek, Michal and Amarasinghe, Saman},
  editor = {Horspool, R. Nigel},
  date = {2002},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {179--196},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/3-540-45937-5_14},
  abstract = {We characterize high-performance streaming applications as a new and distinct domain of programs that is becoming increasingly important. The StreamIt language provides novel high-level representations to improve programmer productivity and program robustness within the streaming domain. At the same time, the StreamIt compiler aims to improve the performance of streaming applications via stream-specific analyses and optimizations. In this paper, we motivate, describe and justify the language features of StreamIt, which include: a structured model of streams, a messaging system for control, a re-initialization mechanism, and a natural textual syntax.},
  isbn = {978-3-540-45937-8},
  langid = {english},
  keywords = {parallel programming,programming languages},
  annotation = {interest: 75},
  file = {/home/sam/Zotero/storage/PJPU9DJJ/Thies et al. - 2002 - StreamIt A Language for Streaming Applications.pdf}
}

@article{timperleyUnderstandingImprovingArtifact2021,
  title = {Understanding and {{Improving Artifact Sharing}} in {{Software Engineering Research}}},
  author = {Timperley, Christopher S. and Herckis, Lauren and Goues, Claire Le and Hilton, Michael},
  date = {2021-07},
  journaltitle = {Empirical Software Engineering},
  shortjournal = {Empir Software Eng},
  volume = {26},
  number = {4},
  eprint = {2008.01046},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {67},
  issn = {1382-3256, 1573-7616},
  doi = {10.1007/s10664-021-09973-5},
  url = {http://arxiv.org/abs/2008.01046},
  urldate = {2023-05-06},
  abstract = {In recent years, many software engineering researchers have begun to include artifacts alongside their research papers. Ideally, artifacts, including tools, benchmarks, and data, support the dissemination of ideas, provide evidence for research claims, and serve as a starting point for future research. However, in practice, artifacts suffer from a variety of issues that prevent the realization of their full potential. To help the software engineering community realize the full potential of artifacts, we seek to understand the challenges involved in the creation, sharing, and use of artifacts. To that end, we perform a mixed-methods study including a survey of artifacts in software engineering publications, and an online survey of 153 software engineering researchers. By analyzing the perspectives of artifact creators, users, and reviewers, we identify several high-level challenges that affect the quality of artifacts including mismatched expectations between these groups, and a lack of sufficient reward for both creators and reviewers. Using Diffusion of Innovations as an analytical framework, we examine how these challenges relate to one another, and build an understanding of the factors that affect the sharing and success of artifacts. Finally, we make recommendations to improve the quality of artifacts based on our results and existing best practices.},
  keywords = {artifact evaluation,project-provenance-pp,reproducibility engineering,research software engineering},
  annotation = {interest: 90},
  file = {/home/sam/Zotero/storage/BM7CHTK8/Timperley et al. - 2021 - Understanding and Improving Artifact Sharing in So.pdf;/home/sam/Zotero/storage/ID3NVV6E/2008.html}
}

@inproceedings{tomassiBugSwarmMiningContinuously2019,
  title = {{{BugSwarm}}: {{Mining}} and {{Continuously Growing}} a {{Dataset}} of {{Reproducible Failures}} and {{Fixes}}},
  shorttitle = {{{BugSwarm}}},
  booktitle = {2019 {{IEEE}}/{{ACM}} 41st {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Tomassi, David A. and Dmeiri, Naji and Wang, Yichen and Bhowmick, Antara and Liu, Yen-Chuan and Devanbu, Premkumar T. and Vasilescu, Bogdan and Rubio-González, Cindy},
  date = {2019-05},
  pages = {339--349},
  issn = {1558-1225},
  doi = {10.1109/ICSE.2019.00048},
  abstract = {Fault-detection, localization, and repair methods are vital to software quality; but it is difficult to evaluate their generality, applicability, and current effectiveness. Large, diverse, realistic datasets of durably-reproducible faults and fixes are vital to good experimental evaluation of approaches to software quality, but they are difficult and expensive to assemble and keep current. Modern continuous-integration (CI) approaches, like TRAVIS-CI, which are widely used, fully configurable, and executed within custom-built containers, promise a path toward much larger defect datasets. If we can identify and archive failing and subsequent passing runs, the containers will provide a substantial assurance of durable future reproducibility of build and test. Several obstacles, however, must be overcome to make this a practical reality. We describe BUGSWARM, a toolset that navigates these obstacles to enable the creation of a scalable, diverse, realistic, continuously growing set of durably reproducible failing and passing versions of real-world, open-source systems. The BUGSWARM toolkit has already gathered 3,091 fail-pass pairs, in Java and Python, all packaged within fully reproducible containers. Furthermore, the toolkit can be run periodically to detect fail-pass activities, thus growing the dataset continually.},
  eventtitle = {2019 {{IEEE}}/{{ACM}} 41st {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  keywords = {automated program repair},
  annotation = {interest: 98},
  file = {/home/sam/Zotero/storage/R95ZK6ER/Tomassi et al. - 2019 - BugSwarm Mining and Continuously Growing a Datase.pdf;/home/sam/Zotero/storage/QKP83YZ3/8812141.html}
}

@inproceedings{tonthatSciunitsReusableResearch2017,
  title = {Sciunits: {{Reusable Research Objects}}},
  shorttitle = {Sciunits},
  booktitle = {2017 {{IEEE}} 13th {{International Conference}} on E-{{Science}} (e-{{Science}})},
  author = {Ton That, Dai Hai and Fils, Gabriel and Yuan, Zhihao and Malik, Tanu},
  date = {2017-10},
  pages = {374--383},
  doi = {10.1109/eScience.2017.51},
  abstract = {Science is conducted collaboratively, often requiring knowledge sharing about computational experiments. When experiments include only datasets, they can be shared using Uniform Resource Identifiers (URIs) or Digital Object Identifiers (DOIs). An experiment, however, seldom includes only datasets, but more often includes software, its past execution, provenance, and associated documentation. The Research Object has recently emerged as a comprehensive and systematic method for aggregation and identification of diverse elements of computational experiments. While a necessary method, mere aggregation is not sufficient for the sharing of computational experiments. Other users must be able to easily recompute on these shared research objects. In this paper, we present the sciunit, a reusable research object in which aggregated content is recomputable. We describe a Git-like client that efficiently creates, stores, and repeats sciunits. We show through analysis that sciunits repeat computational experiments with minimal storage and processing overhead. Finally, we provide an overview of sharing and reproducible cyberinfrastructure based on sciunits gaining adoption in the domain of geosciences.},
  eventtitle = {2017 {{IEEE}} 13th {{International Conference}} on E-{{Science}} (e-{{Science}})},
  keywords = {project-provenance-pp,provenance,record-replay},
  file = {/home/sam/Zotero/storage/4RA2L32H/Ton That et al. - 2017 - Sciunits Reusable Research Objects.pdf;/home/sam/Zotero/storage/ILYKBCUH/8109156.html}
}

@online{treatEverythingYouKnow2015,
  title = {Everything {{You Know About Latency Is Wrong}}},
  author = {Treat, Tyler},
  date = {2015-12-12T21:12:12+00:00},
  url = {https://bravenewgeek.com/everything-you-know-about-latency-is-wrong/},
  urldate = {2024-09-24},
  abstract = {Okay, maybe not everything you know about latency is wrong. But now that I have your attention, we can talk about why the tools and methodologies you use to measure and reason about latency are lik…},
  langid = {american},
  organization = {Brave New Geek},
  keywords = {software benchmarking,software engineering,systems},
  file = {/home/sam/Zotero/storage/HQU2CY4M/everything-you-know-about-latency-is-wrong.html}
}

@article{trisovicLargescaleStudyResearch2022,
  title = {A Large-Scale Study on Research Code Quality and Execution},
  author = {Trisovic, Ana and Lau, Matthew K. and Pasquier, Thomas and Crosas, Mercè},
  date = {2022-02-21},
  journaltitle = {Scientific Data},
  shortjournal = {Sci Data},
  volume = {9},
  number = {1},
  pages = {60},
  publisher = {Nature Publishing Group},
  issn = {2052-4463},
  doi = {10.1038/s41597-022-01143-6},
  url = {https://www.nature.com/articles/s41597-022-01143-6},
  urldate = {2022-12-13},
  abstract = {This article presents a study on the quality and execution of research code from publicly-available replication datasets at the Harvard Dataverse repository. Research code is typically created by a group of scientists and published together with academic papers to facilitate research transparency and reproducibility. For this study, we define ten questions to address aspects impacting research reproducibility and reuse. First, we retrieve and analyze more than 2000 replication datasets with over 9000 unique R files published from 2010 to 2020. Second, we execute the code in a clean runtime environment to assess its ease of reuse. Common coding errors were identified, and some of them were solved with automatic code cleaning to aid code execution. We find that 74\% of R files failed to complete without error in the initial execution, while 56\% failed when code cleaning was applied, showing that many errors can be prevented with good coding practices. We also analyze the replication datasets from journals’ collections and discuss the impact of the journal policy strictness on the code re-execution rate. Finally, based on our results, we propose a set of recommendations for code dissemination aimed at researchers, journals, and repositories.},
  issue = {1},
  langid = {english},
  keywords = {project-acm-rep,project-provenance-pp,reproducibility engineering},
  file = {/home/sam/Zotero/storage/YI4U9WQW/Trisovic et al. - 2022 - A large-scale study on research code quality and e.pdf}
}

@online{tsangWelcomeNewERA2020,
  title = {Welcome to a New {{ERA}} of Reproducible Publishing},
  author = {Tsang, Emily and Maciocci, Giuliano},
  date = {2020-08-24},
  publisher = {eLife Sciences Publications Limited},
  url = {https://elifesciences.org/labs/dc5acbde/welcome-to-a-new-era-of-reproducible-publishing},
  urldate = {2022-09-06},
  abstract = {New open-source technology lets eLife authors publish Executable Research Articles that treat live code and data as first-class citizens.},
  langid = {english},
  organization = {eLife},
  keywords = {reproducibility engineering,research software engineering},
  annotation = {interest: 76},
  file = {/home/sam/Zotero/storage/GWB3W5LJ/welcome-to-a-new-era-of-reproducible-publishing.html}
}

@article{turkYtMulticodeAnalysis2010,
  title = {Yt: {{A Multi-code Analysis Toolkit}} for {{Astrophysical Simulation Data}}},
  shorttitle = {Yt},
  author = {Turk, Matthew J. and Smith, Britton D. and Oishi, Jeffrey S. and Skory, Stephen and Skillman, Samuel W. and Abel, Tom and Norman, Michael L.},
  date = {2010-12},
  journaltitle = {The Astrophysical Journal Supplement Series},
  shortjournal = {ApJS},
  volume = {192},
  number = {1},
  pages = {9},
  publisher = {American Astronomical Society},
  issn = {0067-0049},
  doi = {10.1088/0067-0049/192/1/9},
  url = {https://doi.org/10.1088/0067-0049/192/1/9},
  urldate = {2022-05-03},
  abstract = {The analysis of complex multiphysics astrophysical simulations presents a unique and rapidly growing set of challenges: reproducibility, parallelization, and vast increases in data size and complexity chief among them. In order to meet these challenges, and in order to open up new avenues for collaboration between users of multiple simulation platforms, we present yt (available at http://yt.enzotools.org/) an open source, community-developed astrophysical analysis and visualization toolkit. Analysis and visualization with yt are oriented around physically relevant quantities rather than quantities native to astrophysical simulation codes. While originally designed for handling Enzo's structure adaptive mesh refinement data, yt has been extended to work with several different simulation methods and simulation codes including Orion, RAMSES, and FLASH. We report on its methods for reading, handling, and visualizing data, including projections, multivariate volume rendering, multi-dimensional histograms, halo finding, light cone generation, and topologically connected isocontour identification. Furthermore, we discuss the underlying algorithms yt uses for processing and visualizing data, and its mechanisms for parallelization of analysis tasks.},
  langid = {english},
  keywords = {astronomy,data mining,project-astrophysics,research software engineering},
  file = {/home/sam/Zotero/storage/7TXAK8NK/Turk et al. - 2010 - yt A MULTI-CODE ANALYSIS TOOLKIT FOR ASTROPHYSICA.pdf}
}

@article{turmonTestsTolerancesHighperformance2003,
  title = {Tests and Tolerances for High-Performance Software-Implemehted Fault Detection},
  author = {Turmon, M. and Granat, R. and Katz, D.S. and Lou, J.Z.},
  date = {2003-05},
  journaltitle = {IEEE Transactions on Computers},
  volume = {52},
  number = {5},
  pages = {579--591},
  issn = {1557-9956},
  doi = {10.1109/TC.2003.1197125},
  abstract = {We describe and test a software approach to fault detection in common numerical algorithms. Such result checking or algorithm-based fault tolerance (ABFT) methods may be used, for example, to overcome single-event upsets in computational hardware or to detect errors in complex, high-efficiency implementations of the algorithms. Following earlier work, we use checksum methods to validate results returned by a numerical subroutine operating subject to unpredictable errors in data. We consider common matrix and Fourier algorithms which return results satisfying a necessary condition having a linear form; the checksum tests compliance with this condition. We discuss the theory and practice of setting numerical tolerances to separate errors caused by a fault from those inherent in finite-precision floating-point calculations. We concentrate on comprehensively defining and evaluating tests having various accuracy/computational burden tradeoffs, and we emphasize average-case algorithm behavior rather than using worst-case upper, bounds on error.},
  eventtitle = {{{IEEE Transactions}} on {{Computers}}},
  annotation = {interest: 95}
}

@unpublished{uhrieAutomatedParallelKernel2020,
  title = {Automated {{Parallel Kernel Extraction}} from {{Dynamic Application Traces}}},
  author = {Uhrie, Richard and Chakrabarti, Chaitali and Brunhaver, John},
  date = {2020-01-27},
  eprint = {2001.09995},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2001.09995},
  urldate = {2022-04-06},
  abstract = {Modern program runtime is dominated by segments of repeating code called kernels. Kernels are accelerated by increasing memory locality, increasing data-parallelism, and exploiting producer-consumer parallelism among kernels - which requires hardware specialized for a particular class of kernels. Programming this hardware can be difficult, requiring that the kernels be identified and annotated in the code or translated to a domain-specific language. This paper describes a technique to automatically localize parallel kernels from a dynamic application trace, facilitating further code optimization. Dynamic trace collection is fast and compact. With optimization, it only incurs a time-dilation of a factor on nine and file-size of one megabyte per second, addressing a significant criticism of this approach. Kernel extraction is accurate and performed in linear time within logarithmic memory, detecting a wide range of kernels. This approach was validated across 16 libraries, comprised of 10,507 kernels instances. To validate the accuracy of our detected kernels, five test programs were written that spans traditional kernel definitions and were certified to contain all the kernels that were expected.},
  keywords = {compilers,computer architecture,heterogeneous computing},
  file = {/home/sam/Zotero/storage/TADVDXRE/Uhrie et al. - 2020 - Automated Parallel Kernel Extraction from Dynamic .pdf}
}

@article{UniversityMaastrichtSays2020,
  entrysubtype = {newspaper},
  title = {University of {{Maastricht}} Says It Paid Hackers 200,000-Euro Ransom},
  date = {2020-02-05T17:22:22Z},
  journaltitle = {Reuters},
  url = {https://www.reuters.com/article/us-cybercrime-netherlands-university-idUSKBN1ZZ2HH},
  urldate = {2022-05-23},
  abstract = {The University of Maastricht on Wednesday disclosed that it had paid hackers a ransom of 30 bitcoin -- at the time worth 200,000 euros (\$220,000) -- to unblock its computer systems, including email and computers, after an attack that unfolded on Dec. 24.},
  journalsubtitle = {Technology News},
  langid = {english},
  keywords = {cybersecurity,internship-project,project-devsecops},
  file = {/home/sam/Zotero/storage/7HMMMGFR/us-cybercrime-netherlands-university-idUSKBN1ZZ2HH.html}
}

@online{UsagePythonPerformance,
  title = {Usage — {{Python Performance Benchmark Suite}} 1.0.4 Documentation},
  url = {https://pyperformance.readthedocs.io/usage.html#how-to-get-stable-benchmarks},
  urldate = {2022-04-11},
  organization = {Python Performance Benchmark Suite 1.0.4 documentation},
  file = {/home/sam/Zotero/storage/BHS9C68U/usage.html}
}

@online{UserspaceRCU,
  title = {Userspace {{RCU}}},
  url = {http://liburcu.org/},
  urldate = {2022-09-06},
  abstract = {liburcu is a LGPLv2.1 userspace RCU (read-copy-update) library. This data synchronization library provides read-side access which scales linearly with the number of cores. liburcu-cds provides efficient data structures based on RCU and lock-free algorithms. Those structures include hash tables, queues, stacks, and doubly-linked lists.},
  organization = {UserspaceRCU},
  keywords = {operating systems,software},
  annotation = {interest: 53},
  file = {/home/sam/Zotero/storage/UBWEG3CS/liburcu.org.html}
}

@inproceedings{vahdatTransparentResultCaching1998,
  title = {Transparent Result Caching},
  booktitle = {Proceedings of the Annual Conference on {{USENIX Annual Technical Conference}}},
  author = {Vahdat, Amin and Anderson, Thomas},
  date = {1998-06-15},
  series = {{{ATEC}} '98},
  pages = {3},
  publisher = {USENIX Association},
  location = {USA},
  abstract = {The goal of this work is to develop a general framework for transparently managing the interactions and dependencies among input files, development tools, and output files. By unobtrusively monitoring the execution of unmodified programs, we are able to track process lineage--each process's parent, children, input files, and output files, and file dependency--for each file, the sequence of operations and the set of input files used to create the file. We use this information to implement Transparent Result Caching (TREC) and describe how TREC is used to build a number of useful user utilities. Unmake allows users to query TREC for file lineage information, including the full sequence of programs executed to create a particular output file. Transparent Make uses TREC to automatically generate dependency information by observing program execution, freeing end users from the need to explicitly specify dependency information (i.e., Makefiles can be replaced by shell scripts). Dynamic Web Object Caching allows for the caching of certain dynamically generated web pages, improving server performance and client latency.},
  langid = {english},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/UWUW3GY2/Vahdat and Anderson - Transparent Result Caching.pdf}
}

@article{valletPracticalTransparentVerifiable2022,
  title = {Toward Practical Transparent Verifiable and Long-Term Reproducible Research Using {{Guix}}},
  author = {Vallet, Nicolas and Michonneau, David and Tournier, Simon},
  date = {2022-10-04},
  journaltitle = {Scientific Data},
  shortjournal = {Sci Data},
  volume = {9},
  number = {1},
  pages = {597},
  publisher = {Nature Publishing Group},
  issn = {2052-4463},
  doi = {10.1038/s41597-022-01720-9},
  url = {https://www.nature.com/articles/s41597-022-01720-9},
  urldate = {2023-05-06},
  abstract = {Reproducibility crisis urge scientists to promote transparency which allows peers to draw same conclusions after performing identical steps from hypothesis to results. Growing resources are developed to open the access to methods, data and source codes. Still, the computational environment, an interface between data and source code running analyses, is not addressed. Environments are usually described with software and library names associated with version labels or provided as an opaque container image. This is not enough to describe the complexity of the dependencies on which they rely to operate on. We describe this issue and illustrate how open tools like Guix can be used by any scientist to share their environment and allow peers to reproduce it. Some steps of research might not be fully reproducible, but at least, transparency for computation is technically addressable. These tools should be considered by scientists willing to promote transparency and open science.},
  issue = {1},
  langid = {english},
  keywords = {project-acm-rep,project-provenance-pp,reproducibility,research software engineering},
  annotation = {interest: 99},
  file = {/home/sam/Zotero/storage/NTCF5E96/Vallet et al. - 2022 - Toward practical transparent verifiable and long-t.pdf}
}

@online{vanderplasWhyPythonSlow,
  title = {Why {{Python}} Is {{Slow}}: {{Looking Under}} the {{Hood}}},
  author = {VanderPlas, Jake},
  url = {http://jakevdp.github.io/blog/2014/05/09/why-python-is-slow/},
  urldate = {2024-01-15},
  organization = {Pythonic Perambulations},
  file = {/home/sam/Zotero/storage/EPUPUPUC/why-python-is-slow.html}
}

@article{vandewalleReproducibleResearchSignal2009,
  title = {Reproducible Research in Signal Processing},
  author = {Vandewalle, Patrick and Kovacevic, Jelena and Vetterli, Martin},
  date = {2009-05},
  journaltitle = {IEEE Signal Processing Magazine},
  shortjournal = {IEEE Signal Process. Mag.},
  volume = {26},
  number = {3},
  pages = {37--47},
  issn = {1053-5888, 1558-0792},
  doi = {10.1109/MSP.2009.932122},
  url = {https://ieeexplore.ieee.org/document/4815541/},
  urldate = {2024-09-04},
  keywords = {reproducibility},
  file = {/home/sam/Zotero/storage/PHFKKHSD/Vandewalle et al. - 2009 - Reproducible research in signal processing.pdf}
}

@inproceedings{vangoorFUSENotFUSE2017,
  title = {To \{\vphantom\}{{FUSE}}\vphantom\{\} or {{Not}} to \{\vphantom\}{{FUSE}}\vphantom\{\}: {{Performance}} of \{\vphantom\}{{User-Space}}\vphantom\{\} {{File Systems}}},
  shorttitle = {To \{\vphantom\}{{FUSE}}\vphantom\{\} or {{Not}} to \{\vphantom\}{{FUSE}}\vphantom\{\}},
  author = {Vangoor, Bharath Kumar Reddy and Tarasov, Vasily and Zadok, Erez},
  date = {2017},
  pages = {59--72},
  url = {https://www.usenix.org/conference/fast17/technical-sessions/presentation/vangoor},
  urldate = {2023-07-25},
  eventtitle = {15th {{USENIX Conference}} on {{File}} and {{Storage Technologies}} ({{FAST}} 17)},
  isbn = {978-1-931971-36-2},
  langid = {english},
  keywords = {filesystems,operating systems,project-provenance-pp},
  file = {/home/sam/Zotero/storage/3N3VXAWI/Vangoor et al. - 2017 - To FUSE or Not to FUSE Performance of User-S.pdf;/home/sam/Zotero/storage/5RCSUA7B/fast17_slides_vangoor.pdf}
}

@article{vangorpSHAREWebPortal2011,
  title = {{{SHARE}}: A Web Portal for Creating and Sharing Executable Research Papers},
  shorttitle = {{{SHARE}}},
  author = {Van Gorp, Pieter and Mazanek, Steffen},
  date = {2011},
  journaltitle = {Procedia Computer Science},
  shortjournal = {Procedia Computer Science},
  volume = {4},
  pages = {589--597},
  issn = {18770509},
  doi = {10.1016/j.procs.2011.04.062},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050911001207},
  urldate = {2024-10-04},
  langid = {english},
  file = {/home/sam/Zotero/storage/TEFCLKDA/Van Gorp and Mazanek - 2011 - SHARE a web portal for creating and sharing executable research papers.pdf}
}

@article{vanleerRelationUpwindDifferencingSchemes1984,
  title = {On the {{Relation Between}} the {{Upwind-Differencing Schemes}} of {{Godunov}}, {{Engquist}}–{{Osher}} and {{Roe}}},
  author = {family=Leer, given=Bram, prefix=van, useprefix=true},
  date = {1984-03},
  journaltitle = {SIAM Journal on Scientific and Statistical Computing},
  shortjournal = {SIAM J. Sci. and Stat. Comput.},
  volume = {5},
  number = {1},
  pages = {1--20},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0196-5204},
  doi = {10.1137/0905001},
  url = {https://epubs.siam.org/doi/10.1137/0905001},
  urldate = {2022-04-11},
  abstract = {The upwind-differencing first-order schemes of Godunov, Engquist–Osher and Roe are discussed on the basis of the inviscid Burgers equations. The differences between the schemes are interpreted as differences between the approximate Riemann solutions on which their numerical flux-functions are based. Special attention is given to the proper formulation of these schemes when a source term is present. Second-order two-step schemes, based on the numerical flux-functions of the first-order schemes are also described. The schemes are compared in a numerical experiment and recommendations on their use are included.},
  keywords = {astrophysics,numerical methods,project-astrophysics},
  file = {/home/sam/Zotero/storage/DIJ7BDCD/0905001.pdf}
}

@incollection{vanleerUpwindHighResolutionMethods2012,
  title = {Upwind and {{High-Resolution Methods}} for {{Compressible Flow}}: {{From Donor Cell}} to {{Residual-Distribution Schemes}}},
  shorttitle = {Upwind and {{High-Resolution Methods}} for {{Compressible Flow}}},
  booktitle = {16th {{AIAA Computational Fluid Dynamics Conference}}},
  author = {family=Leer, given=Bram, prefix=van, useprefix=true},
  date = {2012-06-25},
  series = {Session: {{CFD-8}}: {{Thirty Years}} of {{CFD II}}},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  doi = {10.2514/6.2003-3559},
  url = {https://arc.aiaa.org/doi/abs/10.2514/6.2003-3559},
  urldate = {2022-04-11},
  keywords = {astrophysics,numerical methods,project-astrophysics},
  file = {/home/sam/Zotero/storage/IKU9QGRF/AIAA-2003-3559-978.pdf}
}

@article{vannoordenOpenaccessWebsiteGets2014,
  title = {Open-Access Website Gets Tough},
  author = {Van Noorden, Richard},
  date = {2014-08-01},
  journaltitle = {Nature},
  volume = {512},
  number = {7512},
  pages = {17--17},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/512017a},
  url = {https://www.nature.com/articles/512017a},
  urldate = {2022-08-30},
  abstract = {Leading directory tightens listing criteria to weed out rogue journals.},
  issue = {7512},
  langid = {english},
  keywords = {academic publishing,metascience,predatory journals},
  file = {/home/sam/Zotero/storage/NCTCW7DW/Van Noorden - 2014 - Open-access website gets tough.pdf;/home/sam/Zotero/storage/84DPICKD/512017a.html}
}

@online{vanpeltHowClearEnvironment2023,
  title = {How to {{Clear}} the {{Environment}} in {{R}} with the Rm(List=ls()) {{Command}}},
  author = {Vanpelt, Gary},
  date = {2023-01-31T15:15:13},
  url = {https://lxadm.com/r-rm-list-ls/},
  urldate = {2023-05-03},
  abstract = {The rm(list=ls()) command allows you to remove all variables and functions from the current environment in R. This can be useful if you have been working on a project with many variables and functions and you want to start the project with a blank slate.},
  langid = {english},
  organization = {Lxadm.com},
  file = {/home/sam/Zotero/storage/PVZKWK4S/r-rm-list-ls.html}
}

@online{vardaCurlBashInsecure2015,
  title = {Is Curl|bash Insecure?},
  author = {Varda, Kenton},
  date = {2015-09-24},
  url = {https://sandstorm.io/news/2015-09-24-is-curl-bash-insecure-pgp-verified-install},
  urldate = {2022-08-22},
  abstract = {Take control of your web by running your own personal cloud server with Sandstorm.},
  langid = {english},
  file = {/home/sam/Zotero/storage/V25EX4NJ/2015-09-24-is-curl-bash-insecure-pgp-verified-install.html}
}

@inproceedings{vasicFilelevelVsModulelevel2017,
  title = {File-Level vs. Module-Level Regression Test Selection for .{{NET}}},
  booktitle = {Proceedings of the 2017 11th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  author = {Vasic, Marko and Parvez, Zuhair and Milicevic, Aleksandar and Gligoric, Milos},
  date = {2017-08-21},
  series = {{{ESEC}}/{{FSE}} 2017},
  pages = {848--853},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3106237.3117763},
  url = {https://doi.org/10.1145/3106237.3117763},
  urldate = {2022-04-07},
  abstract = {Regression testing is used to check the correctness of evolving software. With the adoption of Agile development methodology, the number of tests and software revisions has dramatically increased, and hence has the cost of regression testing. Researchers proposed regression test selection (RTS) techniques that optimize regression testing by skipping tests that are not impacted by recent program changes. Ekstazi is one such state-of-the art technique; Ekstazi is implemented for the Java programming language and has been adopted by several companies and open-source projects. We report on our experience implementing and evaluating Ekstazi\#, an Ekstazi-like tool for .NET. We describe the key challenges of bringing the Ekstazi idea to the .NET platform. We evaluate Ekstazi\# on 11 open-source projects, as well as an internal Microsoft project substantially larger than each of the open-source projects. Finally, we compare Ekstazi\# to an incremental build system (also developed at Microsoft), which, out of the box, provides module-level dependency tracking and skipping tasks (including test execution) whenever dependencies of a task do not change between the current and the last successful build. Ekstazi\# on average reduced regression testing time by 43.70\% for the open-source projects and by 65.26\% for the Microsoft project (the latter is in addition to the savings provided by incremental builds).},
  isbn = {978-1-4503-5105-8},
  keywords = {software engineering,software testing},
  file = {/home/sam/Zotero/storage/WM697GEF/Vasic et al. - 2017 - File-level vs. module-level regression test select.pdf}
}

@inproceedings{venkatagiriApproxilyzerSystematicFramework2016,
  title = {Approxilyzer: {{Towards}} a Systematic Framework for Instruction-Level Approximate Computing and Its Application to Hardware Resiliency},
  shorttitle = {Approxilyzer},
  booktitle = {2016 49th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  author = {Venkatagiri, Radha and Mahmoud, Abdulrahman and Hari, Siva Kumar Sastry and Adve, Sarita V.},
  date = {2016-10},
  pages = {1--14},
  doi = {10.1109/MICRO.2016.7783745},
  abstract = {Approximate computing environments trade off computational accuracy for improvements in performance, energy, and resiliency cost. For widespread adoption of approximate computing, a fundamental requirement is to understand how perturbations to a computation affect the outcome of the execution in terms of its output quality. This paper presents a framework for approximate computing, called Approxilyzer, that quantifies the quality impact of a single-bit error in all dynamic instructions of an execution with high accuracy (95\% on average). We demonstrate two uses of Approxilyzer. First, we show how Approxilyzer can be used to quantitatively tune output quality vs. resiliency vs. overhead to enable ultra-low cost resiliency solutions (with a single bit error model). For example, we show that Approxilyzer determines that a very small loss in output quality (1\%) can yield large resiliency overhead reduction (up to 55\%) for 99\% resiliency coverage. Second, we show how Approxilyzer can be used to provide a first-order estimate of the approximation potential of general-purpose programs. It does so in an automated way while requiring minimal user input and no program modifications. This enables programmers or other tools to focus on the promising subset of approximable instructions for further analysis.},
  eventtitle = {2016 49th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  keywords = {approximate computing,compilers,computer architecture},
  file = {/home/sam/Zotero/storage/MWCUDZ4E/Approxilyzer_Towards_a_systematic_framework_for_instruction-level_approximate_computing_and_its_application_to_hardware_resiliency.pdf}
}

@article{venkateshUserAcceptanceInformation2003,
  title = {User {{Acceptance}} of {{Information Technology}}: {{Toward}} a {{Unified View}}},
  shorttitle = {User {{Acceptance}} of {{Information Technology}}},
  author = {Venkatesh, Viswanath and Morris, Michael G. and Davis, Gordon B. and Davis, Fred D.},
  date = {2003},
  journaltitle = {MIS Quarterly},
  shortjournal = {MIS Quarterly},
  volume = {27},
  number = {3},
  eprint = {10.2307/30036540},
  eprinttype = {jstor},
  pages = {425},
  issn = {02767783},
  doi = {10.2307/30036540},
  url = {https://www.jstor.org/stable/10.2307/30036540},
  urldate = {2022-05-27},
  abstract = {Information technology (IT) acceptance research has yielded many competing models, each with different sets of acceptance determinants. In this paper, we (1) review user acceptance literature and discuss eight prominent models, (2) empirically compare the eight models and their extensions, (3) formulate a unified model that integrates elements across the eight models, and (4) empirically validate the unified model. The eight models reviewed are the theory of reasoned action, the technology acceptance model, the motivational model, the theory of planned behavior, a model combining the technology acceptance model and the theory of planned behavior, the model of PC utilization, the innovation diffusion theory, and the social cognitive theory. Using data from four organizations over a six-month period with three points of measurement, the eight models explained between 17 percent and 53 percent of the variance in user intentions to use information technology. Next, a unified model, called the Unified Theory of Acceptance and Use of Technology (UTAUT), was formulated, with four core determinants of intention and usage, and up to four moderators of key relationships. UTAUT was then tested using the original data and found to outperform the eight individual models (adjusted R of 69 percent). UTAUT was then confirmed with data from two new organizations with similar results (adjusted R of 70 percent). UTAUT thus provides a useful tool for managers needing to assess the likelihood of success for new technology introductions and helps them understand the drivers of acceptance in order to proactively design interventions (including training, marketing, etc.) targeted at populations of users that may be less inclined to adopt and use new systems. The paper also makes several recommendations for future research including developing a deeper understanding of the dynamic influences studied here, refining measurement of the core constructs used in UTAUT, and understanding the organizational outcomes associated with new technology use.},
  keywords = {internship-project,technology-acceptance},
  file = {/home/sam/Zotero/storage/T6KS9WWD/30036540.pdf;/home/sam/Zotero/storage/T6TR7IPZ/Screenshot from 2022-06-01 15-21-39.png}
}

@online{victorLearnableProgramming,
  title = {Learnable {{Programming}}},
  author = {Victor, Bret},
  url = {http://worrydream.com/#!/LearnableProgramming},
  urldate = {2022-06-24},
  abstract = {Bret Victor has been provided by the management for your protection.},
  keywords = {programming pedagogy}
}

@inproceedings{vilaImpactHardwareVariability2024,
  title = {The {{Impact}} of {{Hardware Variability}} on {{Applications Packaged}} with {{Docker}} and {{Guix}}: A {{Case Study}} in {{Neuroimaging}}},
  shorttitle = {The {{Impact}} of {{Hardware Variability}} on {{Applications Packaged}} with {{Docker}} and {{Guix}}},
  booktitle = {Proceedings of the 2nd {{ACM Conference}} on {{Reproducibility}} and {{Replicability}}},
  author = {Vila, Gael and Medernach, Emmanuel and Gonzalez Pepe, Ines and Bonnet, Axel and Chatelain, Yohan and Sdika, Michael and Glatard, Tristan and Camarasu Pop, Sorina},
  date = {2024-06-18},
  pages = {75--84},
  publisher = {ACM},
  location = {Rennes France},
  doi = {10.1145/3641525.3663626},
  url = {https://dl.acm.org/doi/10.1145/3641525.3663626},
  urldate = {2024-10-05},
  eventtitle = {{{ACM REP}} '24: {{ACM Conference}} on {{Reproducibility}} and {{Replicability}}},
  isbn = {979-8-4007-0530-4},
  langid = {english}
}

@article{villanuevaNoEvidencePhosphine2021,
  title = {No Evidence of Phosphine in the Atmosphere of {{Venus}} from Independent Analyses},
  author = {Villanueva, G. L. and Cordiner, M. and Irwin, P. G. J. and family=Pater, given=I., prefix=de, useprefix=true and Butler, B. and Gurwell, M. and Milam, S. N. and Nixon, C. A. and Luszcz-Cook, S. H. and Wilson, C. F. and Kofman, V. and Liuzzi, G. and Faggi, S. and Fauchez, T. J. and Lippi, M. and Cosentino, R. and Thelen, A. E. and Moullet, A. and Hartogh, P. and Molter, E. M. and Charnley, S. and Arney, G. N. and Mandell, A. M. and Biver, N. and Vandaele, A. C. and family=Kleer, given=K. R., prefix=de, useprefix=true and Kopparapu, R.},
  date = {2021-07},
  journaltitle = {Nature Astronomy},
  shortjournal = {Nat Astron},
  volume = {5},
  number = {7},
  pages = {631--635},
  publisher = {Nature Publishing Group},
  issn = {2397-3366},
  doi = {10.1038/s41550-021-01422-z},
  url = {https://www.nature.com/articles/s41550-021-01422-z},
  urldate = {2023-01-19},
  issue = {7},
  langid = {english},
  keywords = {astronomical observations,project-acm-rep,retraction},
  file = {/home/sam/Zotero/storage/3JWNRC2A/Villanueva et al. - 2021 - No evidence of phosphine in the atmosphere of Venu.pdf}
}

@article{vitekR3RepeatabilityReproducibility2012,
  title = {R3: Repeatability, Reproducibility and Rigor},
  shorttitle = {R3},
  author = {Vitek, Jan and Kalibera, Tomas},
  date = {2012-06-18},
  journaltitle = {ACM SIGPLAN Notices},
  shortjournal = {SIGPLAN Not.},
  volume = {47},
  pages = {30--36},
  issn = {0362-1340, 1558-1160},
  doi = {10.1145/2442776.2442781},
  url = {https://dl.acm.org/doi/10.1145/2442776.2442781},
  urldate = {2022-06-30},
  abstract = {Computer systems research spans sub-disciplines that include embedded systems, programming languages and compilers, networking, and operating systems. Our contention is that a number of structural factors inhibit quality systems research. We highlight some of the factors we have encountered in our own work and observed in published papers and propose solutions that could both increase the productivity of researchers and the quality of their output.},
  issue = {4a},
  langid = {english},
  keywords = {reproducibility engineering,software benchmarking},
  annotation = {interest: 76}
}

@article{vogelsbergerIntroducingIllustrisProject2014,
  title = {Introducing the {{Illustris Project}}: Simulating the Coevolution of Dark and Visible Matter in the {{Universe}}},
  shorttitle = {Introducing the {{Illustris Project}}},
  author = {Vogelsberger, Mark and Genel, Shy and Springel, Volker and Torrey, Paul and Sijacki, Debora and Xu, Dandan and Snyder, Greg and Nelson, Dylan and Hernquist, Lars},
  date = {2014-10-21},
  journaltitle = {Monthly Notices of the Royal Astronomical Society},
  shortjournal = {Monthly Notices of the Royal Astronomical Society},
  volume = {444},
  number = {2},
  pages = {1518--1547},
  issn = {0035-8711},
  doi = {10.1093/mnras/stu1536},
  url = {https://doi.org/10.1093/mnras/stu1536},
  urldate = {2022-04-11},
  abstract = {We introduce the Illustris Project, a series of large-scale hydrodynamical simulations of galaxy formation. The highest resolution simulation, Illustris-1, covers a volume of (106.5舁Mpc)3, has a dark mass resolution of 6.26 × 106舁M⊙, and an initial baryonic matter mass resolution of 1.26 × 106舁M⊙. At z~=~0 gravitational forces are softened on scales of 710舁pc, and the smallest hydrodynamical gas cells have an extent of 48舁pc. We follow the dynamical evolution of 2~×~18203 resolution elements and in addition passively evolve 18203 Monte Carlo tracer particles reaching a total particle count of more than 18 billion. The galaxy formation model includes: primordial and metal-line cooling with self-shielding corrections, stellar evolution, stellar feedback, gas recycling, chemical enrichment, supermassive black hole growth, and feedback from active galactic nuclei. Here we describe the simulation suite, and contrast basic predictions of our model for the present-day galaxy population with observations of the local universe. At z~=~0 our simulation volume contains about 40舁000 well-resolved galaxies covering a diverse range of morphologies and colours including early-type, late-type and irregular galaxies. The simulation reproduces reasonably well the cosmic star formation rate density, the galaxy luminosity function, and baryon conversion efficiency at z~=~0. It also qualitatively captures the impact of galaxy environment on the red fractions of galaxies. The internal velocity structure of selected well-resolved disc galaxies obeys the stellar and baryonic Tully–Fisher relation together with flat circular velocity curves. In the well-resolved regime, the simulation reproduces the observed mix of early-type and late-type galaxies. Our model predicts a halo mass dependent impact of baryonic effects on the halo mass function and the masses of haloes caused by feedback from supernova and active galactic nuclei.},
  keywords = {astrophysics,cosmological simulation,numerical methods,project-astrophysics},
  file = {/home/sam/Zotero/storage/VYKIH94P/Vogelsberger et al. - 2014 - Introducing the Illustris Project simulating the .pdf}
}

@article{vogelsbergerPropertiesGalaxiesReproduced2014,
  title = {Properties of Galaxies Reproduced by a Hydrodynamic Simulation},
  author = {Vogelsberger, M. and Genel, S. and Springel, V. and Torrey, P. and Sijacki, D. and Xu, D. and Snyder, G. and Bird, S. and Nelson, D. and Hernquist, L.},
  date = {2014-05},
  journaltitle = {Nature},
  volume = {509},
  number = {7499},
  pages = {177--182},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature13316},
  url = {https://www.nature.com/articles/nature13316},
  urldate = {2022-04-11},
  abstract = {Previous simulations of the growth of cosmic structures have broadly reproduced the ‘cosmic web’ of galaxies that we see in the Universe, but failed to create a mixed population of elliptical and spiral galaxies, because of numerical inaccuracies and incomplete physical models. Moreover, they were unable to track the small-scale evolution of gas and stars to the present epoch within a representative portion of the Universe. Here we report a simulation that starts 12 million years after the Big Bang, and traces 13 billion years of cosmic evolution with 12 billion resolution elements in a cube of 106.5\,megaparsecs a side. It yields a reasonable population of ellipticals and spirals, reproduces the observed distribution of galaxies in clusters and characteristics of hydrogen on large scales, and at the same time matches the ‘metal’ and hydrogen content of galaxies on small scales.},
  issue = {7499},
  langid = {english},
  keywords = {astrophysics,cosmological simulation,project-astrophysics},
  file = {/home/sam/Zotero/storage/3Y89RIV4/Vogelsberger et al. - 2014 - Properties of galaxies reproduced by a hydrodynami.pdf;/home/sam/Zotero/storage/PWN4DLKK/nature13316.html}
}

@inproceedings{vuOutcomePreservingInputReduction2023,
  title = {Outcome-{{Preserving Input Reduction}} for {{Scientific Data Analysis Workflows}}},
  booktitle = {Proceedings of the 37th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}}},
  author = {Vu, Anh Duc and Kehrer, Timo and Tsigkanos, Christos},
  date = {2023-01-05},
  series = {{{ASE}} '22},
  pages = {1--5},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3551349.3559558},
  url = {https://dl.acm.org/doi/10.1145/3551349.3559558},
  urldate = {2023-07-19},
  abstract = {Analysis of data is the foundation of multiple scientific disciplines, manifesting in complex and diverse scientific data analysis workflows often involving exploratory analyses. Such analyses represent a particular case for traditional data engineering workflows, as results may be hard to interpret and judge whether they are correct or not, and where experimentation is a central theme. Oftentimes, there are certain aspects of a result which are suspicious and which should be further investigated to increase the trustworthiness of the workflow’s outcome. To this end, we advocate a semi-automated approach to reducing a workflow’s input data while preserving a specified outcome of interest, facilitating irregularity localization by narrowing down the search space for spotting corrupted input data or wrong assumptions made about it. We outline our vision on building engineering support for outcome-preserving input reduction within data analysis workflows, and report on preliminary results obtained from applying an early research prototype on a computational notebook taken from an online community of data scientists and machine learning practitioners.},
  isbn = {978-1-4503-9475-8},
  keywords = {input reduction,workflow managers},
  file = {/home/sam/Zotero/storage/JBB9FV27/Vu et al. - 2023 - Outcome-Preserving Input Reduction for Scientific .pdf}
}

@inproceedings{vuUsingSourceCode2020,
  title = {Towards {{Using Source Code Repositories}} to {{Identify Software Supply Chain Attacks}}},
  booktitle = {Proceedings of the 2020 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Vu, Duc Ly and Pashchenko, Ivan and Massacci, Fabio and Plate, Henrik and Sabetta, Antonino},
  date = {2020-10-30},
  pages = {2093--2095},
  publisher = {ACM},
  location = {Virtual Event},
  doi = {10.1145/3372297.3420015},
  url = {https://dl.acm.org/doi/10.1145/3372297.3420015},
  urldate = {2022-08-10},
  eventtitle = {{{CCS}} '20: 2020 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  isbn = {978-1-4503-7089-9},
  langid = {english},
  keywords = {security,software supply chain}
}

@online{walshAreDockerContainers2014,
  title = {Are {{Docker}} Containers Really Secure?},
  shorttitle = {Are {{Docker}} Containers Really Secure?},
  author = {Walsh, Daniel J.},
  date = {2014-07-22},
  url = {https://opensource.com/business/14/7/docker-security-selinux},
  urldate = {2024-04-15},
  abstract = {This article is based on a talk I gave at DockerCon this year. It will discuss Docker container security, where we are currently, and where we are headed. This is part of a series on Docker security, read part two.},
  langid = {english},
  organization = {Opensource.com},
  keywords = {containers,operating systems,security},
  file = {/home/sam/Zotero/storage/M5Y7FL7N/docker-security-selinux.html}
}

@online{walshBringingNewSecurity2014,
  title = {Bringing New Security Features to {{Docker}}},
  author = {Walsh, Daniel J.},
  date = {2014-09-03},
  url = {https://opensource.com/business/14/9/security-for-docker},
  urldate = {2024-04-15},
  abstract = {In my previous on article on Docker Security, I wrote that containers do not contain. In this second part, I cover the security features that have been added to Docker to attempt to control processes within a container.},
  langid = {english},
  organization = {Opensource.com},
  keywords = {containers,operating systems,security},
  file = {/home/sam/Zotero/storage/PS946GMB/security-for-docker.html}
}

@inproceedings{wangAssessingRestoringReproducibility2021,
  title = {Assessing and Restoring Reproducibility of {{Jupyter}} Notebooks},
  booktitle = {Proceedings of the 35th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}}},
  author = {Wang, Jiawei and Kuo, Tzu-yang and Li, Li and Zeller, Andreas},
  date = {2021-01-27},
  series = {{{ASE}} '20},
  pages = {138--149},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3324884.3416585},
  url = {https://doi.org/10.1145/3324884.3416585},
  urldate = {2022-11-14},
  abstract = {Jupyter notebooks---documents that contain live code, equations, visualizations, and narrative text---now are among the most popular means to compute, present, discuss and disseminate scientific findings. In principle, Jupyter notebooks should easily allow to reproduce and extend scientific computations and their findings; but in practice, this is not the case. The individual code cells in Jupyter notebooks can be executed in any order, with identifier usages preceding their definitions and results preceding their computations. In a sample of 936 published notebooks that would be executable in principle, we found that 73\% of them would not be reproducible with straightforward approaches, requiring humans to infer (and often guess) the order in which the authors created the cells. In this paper, we present an approach to (1) automatically satisfy dependencies between code cells to reconstruct possible execution orders of the cells; and (2) instrument code cells to mitigate the impact of non-reproducible statements (i.e., random functions) in Jupyter notebooks. Our Osiris prototype takes a notebook as input and outputs the possible execution schemes that reproduce the exact notebook results. In our sample, Osiris was able to reconstruct such schemes for 82.23\% of all executable notebooks, which has more than three times better than the state-of-the-art; the resulting reordered code is valid program code and thus available for further testing and analysis.},
  isbn = {978-1-4503-6768-4},
  keywords = {project-acm-rep,reproducibility engineering},
  file = {/home/sam/Zotero/storage/7QEKL8VE/Wang et al. - 2021 - Assessing and restoring reproducibility of Jupyter.pdf}
}

@inproceedings{wangLprovPracticalLibraryaware2018,
  title = {Lprov: {{Practical Library-aware Provenance Tracing}}},
  shorttitle = {Lprov},
  booktitle = {Proceedings of the 34th {{Annual Computer Security Applications Conference}}},
  author = {Wang, Fei and Kwon, Yonghwi and Ma, Shiqing and Zhang, Xiangyu and Xu, Dongyan},
  date = {2018-12-03},
  series = {{{ACSAC}} '18},
  pages = {605--617},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3274694.3274751},
  url = {https://dl.acm.org/doi/10.1145/3274694.3274751},
  urldate = {2023-08-24},
  abstract = {With the continuing evolution of sophisticated APT attacks, provenance tracking is becoming an important technique for efficient attack investigation in enterprise networks. Most of existing provenance techniques are operating on system event auditing that discloses dependence relationships by scrutinizing syscall traces. Unfortunately, such auditing-based provenance is not able to track the causality of another important dimension in provenance, the shared libraries. Different from other data-only system entities like files and sockets, dynamic libraries are linked at runtime and may get executed, which poses new challenges in provenance tracking. For example, library provenance cannot be tracked by syscalls and mapping; whether a library function is called and how it is called within an execution context is invisible at syscall level; linking a library does not promise their execution at runtime. Addressing these challenges is critical to tracking sophisticated attacks leveraging libraries. In this paper, to facilitate fine-grained investigation inside the execution of library binaries, we develop Lprov, a novel provenance tracking system which combines library tracing and syscall tracing. Upon a syscall, Lprov identifies the library calls together with the stack which induces it so that the library execution provenance can be accurately revealed. Our evaluation shows that Lprov can precisely identify attack provenance involving libraries, including malicious library attack and library vulnerability exploitation, while syscall-based provenance tools fail to identify. It only incurs 7.0\% (in geometric mean) runtime overhead and consumes 3 times less storage space of a state-of-the-art provenance tool.},
  isbn = {978-1-4503-6569-7},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/3YFG9E7U/Wang et al. - 2018 - Lprov Practical Library-aware Provenance Tracing.pdf}
}

@article{wangRetractedPredictionPosttranslational2012,
  title = {Retracted: {{Prediction}} of Posttranslational Modification Sites from Sequences with Kernel Methods},
  shorttitle = {Retracted},
  author = {Wang, Xiaobo and Wang, Yongcui and Tiang, Yingjie and Shao, Xiaojian and Wu, Ling-Yun and Deng, Naiyang},
  date = {2012},
  journaltitle = {Journal of Computational Chemistry},
  volume = {33},
  number = {17},
  pages = {1524--1524},
  issn = {1096-987X},
  doi = {10.1002/jcc.21526},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jcc.21526},
  urldate = {2023-01-19},
  abstract = {The following article from the Journal of Computational Chemistry, “Prediction of Posttranslational Modification Sites from Sequences with Kernel Methods,” by Xiaobo Wang, Yongcui Wang, Yingjie Tian, Xiaojian Shao, Ling-Yun Wu, and Naiyang Deng, published online on 21 April 2010 in Wiley Online Library (wileyonlinelibrary.com), DOI: 10.1002.jcc.21526, has been retracted by agreement between the authors, the journal's editors, and Wiley Periodicals, Inc. The retraction has been agreed because a computational error produced results that led the authors to overstate the level of performance of their computing model.},
  langid = {english},
  keywords = {project-acm-rep,retraction},
  file = {/home/sam/Zotero/storage/EVAU4HSD/2012 - Retracted Prediction of posttranslational modific.pdf;/home/sam/Zotero/storage/UXPUFZEJ/jcc.html}
}

@article{wardjr.HierarchicalGroupingOptimize1963,
  title = {Hierarchical {{Grouping}} to {{Optimize}} an {{Objective Function}}},
  author = {Ward Jr., Joe H.},
  date = {1963-03-01},
  journaltitle = {Journal of the American Statistical Association},
  volume = {58},
  number = {301},
  pages = {236--244},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1080/01621459.1963.10500845},
  url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1963.10500845},
  urldate = {2024-02-10},
  abstract = {A procedure for forming hierarchical groups of mutually exclusive subsets, each of which has members that are maximally similar with respect to specified characteristics, is suggested for use in large-scale (n {$>$} 100) studies when a precise optimal solution for a specified number of groups is not practical. Given n sets, this procedure permits their reduction to n − 1 mutually exclusive sets by considering the union of all possible n(n − 1)/2 pairs and selecting a union having a maximal value for the functional relation, or objective function, that reflects the criterion chosen by the investigator. By repeating this process until only one group remains, the complete hierarchical structure and a quantitative estimate of the loss associated with each stage in the grouping can be obtained. A general flowchart helpful in computer programming and a numerical example are included.},
  keywords = {machine learning,project-provenance-pp},
  file = {/home/sam/Zotero/storage/83MVHXS6/Ward Jr. - 1963 - Hierarchical Grouping to Optimize an Objective Fun.pdf}
}

@online{washingtonPolitiFactViceWhat,
  title = {{{PolitiFact}} - {{Vice}}: {{What}} the Movie Gets Right and Wrong about {{Dick Cheney}}},
  shorttitle = {{{PolitiFact}} - {{Vice}}},
  author = {Washington, District of Columbia 1800 I. Street NW and Dc 20006},
  url = {https://www.politifact.com/article/2019/feb/08/vice-what-movie-gets-right-and-wrong/},
  urldate = {2022-08-29},
  abstract = {Editor's note: Have you ever wondered if the movie you just saw \&mdash; that claimed to be based on a real story or},
  langid = {american},
  organization = {@politifact},
  file = {/home/sam/Zotero/storage/TSFRZ7HS/vice-what-movie-gets-right-and-wrong.html}
}

@article{weibelDublinCoreMetadata2000,
  title = {The {{Dublin Core Metadata Initiative}}: {{Mission}}, {{Current Activities}}, and {{Future Directions}}},
  shorttitle = {The {{Dublin Core Metadata Initiative}}},
  author = {Weibel, Stuart L. and Koch, Traugott},
  date = {2000-12},
  journaltitle = {D-Lib Magazine},
  shortjournal = {D-Lib Magazine},
  volume = {6},
  number = {12},
  issn = {1082-9873},
  doi = {10.1045/december2000-weibel},
  url = {http://www.dlib.org/dlib/december00/weibel/12weibel.html},
  urldate = {2023-05-25},
  langid = {english},
  keywords = {project-provenance-pp,semantic web}
}

@online{wheelerSecureProgrammingHOWTO2015,
  title = {Secure {{Programming HOWTO}} - {{Creating Secure Software}}},
  author = {Wheeler, David A.},
  date = {2015-09-19},
  url = {https://dwheeler.com/secure-programs/},
  abstract = {This book provides a set of design and implementation guidelines for writing secure programs. Such programs include application programs used as viewers of remote data, web applications (including CGI scripts), network servers, and setuid/setgid programs. Specific guidelines for C, C++, Java, Perl, PHP, Python, Tcl, and Ada95 are included. It especially covers Linux and Unix based systems, but much of its material applies to any system.},
  keywords = {industry practices,security,software engineering}
}

@inproceedings{widyasariBugsInPyDatabaseExisting2020,
  title = {{{BugsInPy}}: A Database of Existing Bugs in {{Python}} Programs to Enable Controlled Testing and Debugging Studies},
  shorttitle = {{{BugsInPy}}},
  booktitle = {Proceedings of the 28th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Widyasari, Ratnadira and Sim, Sheng Qin and Lok, Camellia and Qi, Haodi and Phan, Jack and Tay, Qijin and Tan, Constance and Wee, Fiona and Tan, Jodie Ethelda and Yieh, Yuheng and Goh, Brian and Thung, Ferdian and Kang, Hong Jin and Hoang, Thong and Lo, David and Ouh, Eng Lieh},
  date = {2020-11-08},
  series = {{{ESEC}}/{{FSE}} 2020},
  pages = {1556--1560},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3368089.3417943},
  url = {https://doi.org/10.1145/3368089.3417943},
  urldate = {2023-07-08},
  abstract = {The 2019 edition of Stack Overflow developer survey highlights that, for the first time, Python outperformed Java in terms of popularity. The gap between Python and Java further widened in the 2020 edition of the survey. Unfortunately, despite the rapid increase in Python's popularity, there are not many testing and debugging tools that are designed for Python. This is in stark contrast with the abundance of testing and debugging tools for Java. Thus, there is a need to push research on tools that can help Python developers. One factor that contributed to the rapid growth of Java testing and debugging tools is the availability of benchmarks. A popular benchmark is the Defects4J benchmark; its initial version contained 357 real bugs from 5 real-world Java programs. Each bug comes with a test suite that can expose the bug. Defects4J has been used by hundreds of testing and debugging studies and has helped to push the frontier of research in these directions. In this project, inspired by Defects4J, we create another benchmark database and tool that contain 493 real bugs from 17 real-world Python programs. We hope our benchmark can help catalyze future work on testing and debugging tools that work on Python programs.},
  isbn = {978-1-4503-7043-1},
  keywords = {automated program repair,project-bugsinpy,software engineering}
}

@article{wieseNamingPainDeveloping2020,
  title = {Naming the {{Pain}} in {{Developing Scientific Software}}},
  author = {Wiese, Igor and Polato, Ivanilton and Pinto, Gustavo},
  date = {2020-07},
  journaltitle = {IEEE Software},
  volume = {37},
  number = {4},
  pages = {75--82},
  issn = {1937-4194},
  doi = {10.1109/MS.2019.2899838},
  abstract = {The scientific software community's lack of computer science background and software engineering training takes a toll on scientists who need to develop software. We built a taxonomy of 2,110 reported problems and grouped them into three major axes: technical-related, socialrelated, and scientific-related problems.},
  eventtitle = {{{IEEE Software}}},
  keywords = {research software engineering},
  annotation = {interest: 86},
  file = {/home/sam/Zotero/storage/QIE8EIZ7/8664473.html}
}

@online{wilder-jamesDescriptionProjectWiki2017,
  title = {Description of a {{Project}} Wiki},
  author = {Wilder-James, Edd},
  date = {2017-01-13},
  url = {https://github.com/ewilderj/doap/wiki},
  urldate = {2023-05-26},
  abstract = {DOAP is a project to create an XML/RDF vocabulary to describe software projects, and in particular open source projects. In addition to developing an RDF schema and examples, the DOAP project aims to provide tool support in all the popular programming languages.},
  keywords = {project-provenance-pp,semantic web},
  file = {/home/sam/Zotero/storage/6H2FHPPR/wiki.html}
}

@inproceedings{wilkinsonWorkflowsWhenParts2022,
  title = {F*** Workflows: When Parts of {{FAIR}} Are Missing},
  shorttitle = {F*** Workflows},
  booktitle = {2022 {{IEEE}} 18th {{International Conference}} on E-{{Science}} (e-{{Science}})},
  author = {Wilkinson, Sean R. and Eisenhauer, Greg and Kapadia, Anuj J. and Knight, Kathryn and Logan, Jeremy and Widener, Patrick and Wolf, Matthew},
  date = {2022-10},
  eprint = {2209.09022},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {507--512},
  doi = {10.1109/eScience55777.2022.00090},
  url = {http://arxiv.org/abs/2209.09022},
  urldate = {2022-12-18},
  abstract = {The FAIR principles for scientific data (Findable, Accessible, Interoperable, Reusable) are also relevant to other digital objects such as research software and scientific workflows that operate on scientific data. The FAIR principles can be applied to the data being handled by a scientific workflow as well as the processes, software, and other infrastructure which are necessary to specify and execute a workflow. The FAIR principles were designed as guidelines, rather than rules, that would allow for differences in standards for different communities and for different degrees of compliance. There are many practical considerations which impact the level of FAIR-ness that can actually be achieved, including policies, traditions, and technologies. Because of these considerations, obstacles are often encountered during the workflow lifecycle that trace directly to shortcomings in the implementation of the FAIR principles. Here, we detail some cases, without naming names, in which data and workflows were Findable but otherwise lacking in areas commonly needed and expected by modern FAIR methods, tools, and users. We describe how some of these problems, all of which were overcome successfully, have motivated us to push on systems and approaches for fully FAIR workflows.},
  annotation = {interest: 95},
  file = {/home/sam/Zotero/storage/UHFLUS4R/Wilkinson et al. - 2022 - F workflows when parts of FAIR are missing.pdf;/home/sam/Zotero/storage/NXI4YTNM/2209.html}
}

@book{wilsonArchitectureOpenSource,
  title = {The {{Architecture}} of {{Open Source Applications}}, {{Volume II}}: {{Structure}}, {{Scale}}, and a {{Few More Fearless Hacks}}},
  shorttitle = {The {{Architecture}} of {{Open Source Applications}}, {{Volume II}}},
  editor = {Wilson, Greg and Brown, Amy},
  url = {http://aosabook.org/en/index.html},
  langid = {english},
  pagetotal = {392}
}

@article{wilsonSoftwareCarpentryGetting2006,
  title = {Software {{Carpentry}}: {{Getting Scientists}} to {{Write Better Code}} by {{Making Them More Productive}}},
  shorttitle = {Software {{Carpentry}}},
  author = {Wilson, G.},
  date = {2006-11},
  journaltitle = {Computing in Science \& Engineering},
  shortjournal = {Comput. Sci. Eng.},
  volume = {8},
  number = {6},
  pages = {66--69},
  issn = {1521-9615},
  doi = {10.1109/MCSE.2006.122},
  url = {http://ieeexplore.ieee.org/document/1717319/},
  urldate = {2022-06-01},
  abstract = {For the past years, my colleagues and I have developed a one-semester course that teaches scientists and engineers the "common core" of modern software development. Our experience shows that an investment of 150 hours-25 of lectures and the rest of practical work-can improve productivity by roughly 20 percent. That's one day a week, one less semester in a master's degree, or one less year for a typical PhD. The course is called software carpentry, rather than software engineering, to emphasize the fact that it focuses on small-scale and immediately practical issues. All of the material is freely available under an open-source license at www.swc.scipy.org and can be used both for self-study and in the classroom. This article describes what the course contains, and why.},
  keywords = {internship-project,research software engineering},
  file = {/home/sam/Zotero/storage/TI2W6BFI/Software_Carpentry_Getting_Scientists_to_Write_Better_Code_by_Making_Them_More_Productive.pdf}
}

@article{wilsonSoftwareCarpentryLessons2016,
  title = {Software {{Carpentry}}: Lessons Learned},
  shorttitle = {Software {{Carpentry}}},
  author = {Wilson, Greg},
  date = {2016-01-28},
  journaltitle = {F1000Research},
  shortjournal = {F1000Res},
  volume = {3},
  eprint = {24715981},
  eprinttype = {pmid},
  pages = {62},
  issn = {2046-1402},
  doi = {10.12688/f1000research.3-62.v2},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3976103/},
  urldate = {2024-02-23},
  abstract = {Since its start in 1998, Software Carpentry has evolved from a~week-long training course at the US national laboratories into a~worldwide volunteer effort to improve researchers' computing~skills. This paper explains what we have learned along the way, the~challenges we now face, and our plans for the future.},
  pmcid = {PMC3976103},
  keywords = {project-repro-py,research software engineering},
  file = {/home/sam/Zotero/storage/R94SZHSZ/Wilson - 2016 - Software Carpentry lessons learned.pdf}
}

@unpublished{wilsonTestingDistributedSystems2014,
  title = {Testing {{Distributed Systems}} with {{Deterministic Simulation}}},
  author = {Wilson, Will},
  date = {2014-09-20},
  url = {https://www.youtube.com/watch?v=4fFDFbi3toc},
  urldate = {2024-02-22},
  abstract = {Debugging highly concurrent distributed systems in a noisy network environment is an exceptionally challenging endeavor. On the one hand, evaluating all possible orders in which program events can occur is a task ill-suited to human cognition, rendering a pure analytic understanding of the control flow of such a system beyond the reach of any individual programmer. On the other hand, a more “empirical” approach to the task is also fraught with difficulty, as the dependence of severe bugs on precise timings or transient network conditions makes every part of the debugging cycle – from bug replication to verification of a fix – a Sisyphean labor bordering on the impossible. One approach which has been developed to ameliorate this situation is that of deterministic simulation, wherein the hardware components of the system – including hard disks, network links, and the machines themselves – are replaced in testing with software which fulfills the contracts of those systems, but whose state is completely transparent to the developer. This enables the simulation of a wide diversity of failure modes including network failures, disk failures or space exhaustion, unexpected machine shutdown or reboot, IP address changes, and even entire datacenter failures. Moreover, once a particular pattern of failures has been identified which uncovers a bug, the determinism property of the simulation means that the exact same series of events can be replayed an indefinite number of times, greatly facilitating the debugging process, and providing confidence when a bug has been fixed. Attendees of this talk will gain an understanding of the benefits, drawbacks, and tradeoffs involved in implementing a deterministic simulation framework, with frequent reference both to theory and to real-world engineering experience gleaned from applying this method to a complex distributed system. Attendees will also learn about language features which aid in the development of such a framework. Will Wilson FoundationDB Will Wilson works on the engineering team at FoundationDB (https://foundationdb.com). Will started his career in biotechnology, leading a successful R\&D effort in spinal cord injury diagnostics, currently undergoing commercialization by a company he co-founded. Since then, Will has worked in a variety of technical and business roles at data science and data virtualization startups. Will has a degree in math and philosophy from Yale.},
  eventtitle = {Strange {{Loop Conference}}},
  keywords = {industry practices,software testing}
}

@online{winestockEternalMainframe,
  title = {The {{Eternal Mainframe}}},
  author = {Winestock, Rudolf},
  url = {http://www.winestockwebdesign.com/Essays/Eternal_Mainframe.html},
  urldate = {2022-06-24},
  abstract = {In the computer industry, the Wheel of Reincarnation is a pattern whereby specialized hardware gets spun out from the “main” system, becomes more powerful, then gets folded back into the main system. As the linked Jargon File entry points out, several generations of this effect have been observed in graphics and floating-point coprocessors. In this essay, I note an analogous pattern taking place, not in peripherals of a computing platform, but in the most basic kinds of “computing platform.” And this pattern is being driven as much by the desire for “freedom” as by any technical consideration.},
  keywords = {academia,operating systems}
}

@online{winestockLispCurse,
  title = {The {{Lisp Curse}}},
  author = {Winestock, Rudolf},
  url = {http://www.winestockwebdesign.com/Essays/Lisp_Curse.html},
  urldate = {2022-06-24},
  abstract = {The expressive power of Lisp has drawbacks.},
  keywords = {programming languages}
}

@inproceedings{winterRetrospectiveStudyOne2022,
  title = {A Retrospective Study of One Decade of Artifact Evaluations},
  booktitle = {Proceedings of the 30th {{ACM Joint European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Winter, Stefan and Timperley, Christopher S. and Hermann, Ben and Cito, Jürgen and Bell, Jonathan and Hilton, Michael and Beyer, Dirk},
  date = {2022-11-09},
  series = {{{ESEC}}/{{FSE}} 2022},
  pages = {145--156},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3540250.3549172},
  url = {https://doi.org/10.1145/3540250.3549172},
  urldate = {2023-01-31},
  abstract = {Most software engineering research involves the development of a prototype, a proof of concept, or a measurement apparatus. Together with the data collected in the research process, they are collectively referred to as research artifacts and are subject to artifact evaluation (AE) at scientific conferences. Since its initiation in the SE community at ESEC/FSE 2011, both the goals and the process of AE have evolved and today expectations towards AE are strongly linked with reproducible research results and reusable tools that other researchers can build their work on. However, to date little evidence has been provided that artifacts which have passed AE actually live up to these high expectations, i.e., to which degree AE processes contribute to AE's goals and whether the overhead they impose is justified. We aim to fill this gap by providing an in-depth analysis of research artifacts from a decade of software engineering (SE) and programming languages (PL) conferences, based on which we reflect on the goals and mechanisms of AE in our community. In summary, our analyses (1) suggest that articles with artifacts do not generally have better visibility in the community, (2) provide evidence how evaluated and not evaluated artifacts differ with respect to different quality criteria, and (3) highlight opportunities for further improving AE processes.},
  isbn = {978-1-4503-9413-0},
  keywords = {reproducibility engineering},
  annotation = {interest: 90},
  file = {/home/sam/Zotero/storage/52U5VHNC/Winter et al. - 2022 - A retrospective study of one decade of artifact evaluations.pdf;/home/sam/Zotero/storage/T2IXPCTD/Winter et al. - 2022 - A retrospective study of one decade of artifact ev.pdf}
}

@thesis{wirickLojbanToolEncoding2005,
  type = {Master of Science in Computer Science},
  title = {Lojban as a Tool for Encoding Prose on the Semantic Web},
  author = {Wirick, Brandon},
  date = {2005-11},
  institution = {California Polytechnic State University},
  url = {https://mw.lojban.org/images/e/e1/wirick.pdf},
  abstract = {As the Web evolves, the problem of enabling software to extract semantics from prose becomes increasingly important. Current solutions for representing semantic concepts mostly involve large concept hierarchies called ontologies in which the classes have more or less arbitrary names based on English words. Parsing English prose into documents that reference these ontologies is problematic and would rely heavily on inadequately accurate techniques based on artifical intelligence. I explore a new avenue, which involves an artificial, logical, spoken language called Lojban, a language that is special in that it can be used to write both prose and unambiguous logical statements. I demonstrate that, for at least a simple but non-trivial subset of Lojban, parsing prose into documents that reference a Lojban ontology is automatic, even trivial by some comparisons. Work that builds on this would include, but not be limited to, expanding the subset of Lojban I present and finding ways to use Lojban as an intermediate step for semantic parsing of certain types of prose in natural languages.},
  langid = {english},
  annotation = {interest: 85},
  file = {/home/sam/Zotero/storage/QRXQLNDU/Wirick - Master of Science in Computer Science.pdf}
}

@online{woodruffStaticAnalysisScale2019,
  title = {Static {{Analysis}} at {{Scale}}: {{An Instagram Story}}},
  shorttitle = {Static {{Analysis}} at {{Scale}}},
  author = {Woodruff, Benjamin},
  date = {2019-08-16T19:54:41},
  url = {https://instagram-engineering.com/static-analysis-at-scale-an-instagram-story-8f498ab71a0c},
  urldate = {2022-04-18},
  abstract = {How Instagram develops and uses linting and codemod tools based on LibCST to maintain a modern codebase at scale.},
  langid = {english},
  organization = {Medium},
  file = {/home/sam/Zotero/storage/BAI8WIJH/static-analysis-at-scale-an-instagram-story-8f498ab71a0c.html}
}

@article{wordenSelfLickingIceCream1992,
  title = {On {{Self-Licking Ice Cream Cones}}},
  author = {Worden, S. P.},
  date = {1992-01-01},
  series = {Proceedings of the 7th {{Cambridge Workshop}}, {{ASP Conference Series}}},
  volume = {26},
  pages = {599},
  url = {https://ui.adsabs.harvard.edu/abs/1992ASPC...26..599W},
  urldate = {2024-04-18},
  eventtitle = {Cool {{Stars}}, {{Stellar Systems}}, and the {{Sun}}},
  keywords = {astronomy,organizational theory,science policy},
  annotation = {ADS Bibcode: 1992ASPC...26..599W},
  file = {/home/sam/Zotero/storage/IUPHTILN/Worden - 1992 - On Self-Licking Ice Cream Cones.pdf}
}

@article{wrattenReproducibleScalableShareable2021,
  title = {Reproducible, Scalable, and Shareable Analysis Pipelines with Bioinformatics Workflow Managers},
  author = {Wratten, Laura and Wilm, Andreas and Göke, Jonathan},
  date = {2021-10},
  journaltitle = {Nature Methods},
  shortjournal = {Nat Methods},
  volume = {18},
  number = {10},
  pages = {1161--1168},
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/s41592-021-01254-9},
  url = {https://www.nature.com/articles/s41592-021-01254-9},
  urldate = {2022-07-07},
  abstract = {The rapid growth of high-throughput technologies has transformed biomedical research. With the increasing amount and complexity of data, scalability and reproducibility have become essential not just for experiments, but also for computational analysis. However, transforming data into information involves running a large number of tools, optimizing parameters, and integrating dynamically changing reference data. Workflow managers were developed in response to such challenges. They simplify pipeline development, optimize resource usage, handle software installation and versions, and run on different compute platforms, enabling workflow portability and sharing. In this Perspective, we highlight key features of workflow managers, compare commonly used approaches for bioinformatics workflows, and provide a guide for computational and noncomputational users. We outline community-curated pipeline initiatives that enable novice and experienced users to perform complex, best-practice analyses without having to manually assemble workflows. In sum, we illustrate how workflow managers contribute to making computational analysis in biomedical research shareable, scalable, and reproducible.},
  langid = {english},
  keywords = {workflow managers}
}

@article{wroeRecyclingWorkflowsServices2007,
  title = {Recycling Workflows and Services through Discovery and Reuse},
  author = {Wroe, Chris and Goble, Carole and Goderis, Antoon and Lord, Phillip and Miles, Simon and Papay, Juri and Alper, Pinar and Moreau, Luc},
  date = {2007},
  journaltitle = {Concurrency and Computation: Practice and Experience},
  volume = {19},
  number = {2},
  pages = {181--194},
  issn = {1532-0634},
  doi = {10.1002/cpe.1050},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.1050},
  urldate = {2022-09-06},
  abstract = {Scientific workflows are becoming a valuable tool for scientists to capture and automate e-Science procedures. Their success brings the opportunity to publish, share, reuse and re-purpose this explicitly captured knowledge. Within the \$\textasciicircum my\$Grid project, we have identified key resources that can be shared including complete workflows, fragments of workflows and constituent services. We have examined the alternative ways that these resources can be described by their authors (and subsequent users) and developed a unified descriptive model to support their later discovery. By basing this model on existing standards, we have been able to extend existing Web service and Semantic Web service infrastructure whilst still supporting the specific needs of the e-Scientist. The \$\textasciicircum my\$Grid components enable a workflow life-cycle that extends beyond execution to include the discovery of previous relevant designs, the reuse of those designs and their subsequent publication. Experience with example groups of scientists indicates that this cycle is valuable. The growing number of workflows and services mean more work is needed to support the user in effective ranking of search results and to support the re-purposing process. Copyright © 2006 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {research software engineering,software publishing,workflow managers},
  annotation = {interest: 92},
  file = {/home/sam/Zotero/storage/AMM2WJ9C/Wroe et al. - 2007 - Recycling workflows and services through discovery.pdf;/home/sam/Zotero/storage/98YBSSSL/cpe.html}
}

@online{XDOCInterestingapplications,
  title = {{{XDOC}} — {{Interesting-applications}}},
  url = {https://www.cs.utexas.edu/users/moore/acl2/current/manual/index.html?topic=ACL2____INTERESTING-APPLICATIONS},
  urldate = {2022-09-06},
  keywords = {formal verification,software},
  annotation = {interest: 56},
  file = {/home/sam/Zotero/storage/REYNT89Q/index.html}
}

@unpublished{xingKnuthPlassLineWrapping,
  title = {Knuth-{{Plass Line Wrapping Algorithm}}},
  author = {Xing, A},
  url = {https://www.students.cs.ubc.ca/~cs-490/2015W2/lectures/Knuth.pdf},
  abstract = {Where do we break lines to fit them to a page?},
  file = {/home/sam/Zotero/storage/KR86PC4H/Knuth.pdf}
}

@report{xuDXTDarshanEXtended2017,
  title = {{{DXT}}: {{Darshan eXtended Tracing}}},
  shorttitle = {{{DXT}}},
  author = {Xu, Cong and Snyder, Shane and Venkatesan, Vishwanath and Carns, Philip and Kulkarni, Omkar and Byna, Suren and Sisneros, Roberto and Chadalavada, Kalyana},
  date = {2017-05-08},
  institution = {Argonne National Lab. (ANL), Argonne, IL (United States)},
  url = {https://www.osti.gov/biblio/1392598},
  urldate = {2023-10-27},
  abstract = {The U.S. Department of Energy's Office of Scientific and Technical Information},
  langid = {english},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/3WW37QBM/Xu et al. - 2017 - DXT Darshan eXtended Tracing.pdf}
}

@inproceedings{xuEfficientCheckpointingJava2007,
  title = {Efficient Checkpointing of Java Software Using Context-Sensitive Capture and Replay},
  booktitle = {Proceedings of the the 6th Joint Meeting of the {{European}} Software Engineering Conference and the {{ACM SIGSOFT}} Symposium on {{The}} Foundations of Software Engineering},
  author = {Xu, Guoqing and Rountev, Atanas and Tang, Yan and Qin, Feng},
  date = {2007-09-07},
  series = {{{ESEC-FSE}} '07},
  pages = {85--94},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1287624.1287638},
  url = {https://doi.org/10.1145/1287624.1287638},
  urldate = {2022-09-06},
  abstract = {Checkpointing and replaying is an attractive technique that has been used widely at the operating/runtime system level to provide fault tolerance. Applying such a technique at the application level can benefit a range of software engineering tasks such as testing of long-running programs, automated debugging, and dynamic slicing. We propose a checkpointing/replaying technique for Java that operates purely at the language level, without the need for JVM-level or OS-level support. At the core of our approach are static analyses that select, at certain program points, a safe subset of the program state to capture and replay. Irrelevant statements before the checkpoint are eliminated using control-dependence-based slicing; the remaining statements together with the captured run-time values are used to indirectly recreate the call stack of the original program at the checkpoint. At the checkpoint itself and at certain subsequent program points, the replaying version restores parts of the program state that are necessary for execution of the surrounding method. Our experimental studies indicate that the proposed static and dynamic analyses have the potential to reduce significantly the execution time for replaying, with low run-time overhead for checkpointing.},
  isbn = {978-1-59593-811-4},
  keywords = {incremental computation},
  annotation = {interest: 51}
}

@incollection{yangDEEPProvenanceAwareExecutable2012,
  title = {{{DEEP}}: {{A Provenance-Aware Executable Document System}}},
  shorttitle = {{{DEEP}}},
  booktitle = {Provenance and {{Annotation}} of {{Data}} and {{Processes}}},
  author = {Yang, Huanjia and Michaelides, Danius T. and Charlton, Chris and Browne, William J. and Moreau, Luc},
  editor = {Groth, Paul and Frew, James},
  editora = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
  editoratype = {redactor},
  date = {2012},
  volume = {7525},
  pages = {24--38},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-34222-6_3},
  url = {http://link.springer.com/10.1007/978-3-642-34222-6_3},
  urldate = {2022-07-08},
  abstract = {The concept of executable documents is attracting growing interest from both academics and publishers since it is a promising technology for the the dissemination of scientific results. Provenance is a kind of metadata that provides a rich description of the derivation history of data products starting from their original sources. It has been used in many different e-Science domains and has shown great potential in enabling reproducibility of scientific results. However, while both executable documents and provenance are aimed at enhancing the dissemination of scientific results, little has been done to explore the integration of both techniques. In this paper, we introduce the design and development of Deep, an executable document environment that generates scientific results dynamically and interactively, and also records the provenance for these results in the document. In this system, provenance is exposed to users via an interface that provides them with an alternative way of navigating the executable document. In addition, we make use of the provenance to offer a document rollback facility to users and help to manage the system’s dynamic resources.},
  isbn = {978-3-642-34221-9 978-3-642-34222-6},
  keywords = {academic publishing,provenance}
}

@inproceedings{yangDeterminismOverratedWhat2013,
  title = {Determinism Is Overrated: What Really Makes Multithreaded Programs Hard to Get Right and What Can Be Done about It?},
  shorttitle = {Determinism Is Overrated},
  booktitle = {Proceedings of the 5th {{USENIX Conference}} on {{Hot Topics}} in {{Parallelism}}},
  author = {Yang, Junfeng and Cui, Heming and Wu, Jingyue},
  date = {2013-06-24},
  series = {{{HotPar}}'13},
  pages = {11},
  publisher = {USENIX Association},
  location = {USA},
  abstract = {Our accelerating computational demand and the rise of multicore hardware have made parallel programs, especially shared-memory multithreaded programs, increasingly pervasive and critical. Yet, these programs remain extremely difficult to write, test, analyze, debug, and verify. Conventional wisdom has attributed these difficulties to nondeterminism, and researchers have recently dedicated much effort to bringing determinism into multithreading. In this paper, we argue that determinism is not as useful as commonly perceived: it is neither sufficient nor necessary for reliability. We present our view on why multithreaded programs are difficult to get right, describe a promising approach we call stable multithreading to dramatically improve reliability, and summarize our last four years' research on building and applying stable multithreading systems.},
  keywords = {operating systems},
  annotation = {interest: 87},
  file = {/home/sam/Zotero/storage/2JSRH2FK/Yang et al. - 2013 - Determinism is overrated what really makes multit.pdf}
}

@inproceedings{yangUISCOPEAccurateInstrumentationfree2020,
  title = {{{UISCOPE}}: {{Accurate}}, {{Instrumentation-free}}, and {{Visible Attack Investigation}} for {{GUI Applications}}},
  shorttitle = {{{UISCOPE}}},
  booktitle = {Proceedings 2020 {{Network}} and {{Distributed System Security Symposium}}},
  author = {Yang, Runqing and Ma, Shiqing and Xu, Haitao and Zhang, Xiangyu and Chen, Yan},
  date = {2020},
  publisher = {Internet Society},
  location = {San Diego, CA},
  doi = {10.14722/ndss.2020.24329},
  url = {https://www.ndss-symposium.org/wp-content/uploads/2020/02/24329.pdf},
  urldate = {2023-08-23},
  abstract = {Existing attack investigation solutions for GUI applications suffer from a few limitations such as inaccuracy (because of the dependence explosion problem), requiring instrumentation, and providing very low visibility. Such limitations have hindered their widespread and practical deployment. In this paper, we present UISCOPE, a novel accurate, instrumentationfree, and visible attack investigation system for GUI applications. The core idea of UISCOPE is to perform causality analysis on both UI elements/events which represent users’ perspective and low-level system events which provide detailed information of what happens under the hood, and then correlate system events with UI events to provide high accuracy and visibility. Long running processes are partitioned to individual UI transitions, to which low-level system events are attributed, making the results accurate. The produced graphs contain (causally related) UI elements with which users are very familiar, making them easily accessible. We deployed UISCOPE on 7 machines for a week, and also utilized UISCOPE to conduct an investigation of 6 realworld attacks. Our evaluation shows that compared to existing works, UISCOPE introduces neglibible overhead (less than 1\% runtime overhead and 3.05 MB event logs per hour on average) while UISCOPE can precisely identify attack provenance while offering users thorough visibility into the attack context.},
  eventtitle = {Network and {{Distributed System Security Symposium}}},
  isbn = {978-1-891562-61-7},
  langid = {english},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/IUQN7XVC/Yang et al. - 2020 - UISCOPE Accurate, Instrumentation-free, and Visib.pdf}
}

@inproceedings{yasukataZpolineSystemCall2023,
  title = {Zpoline: A System Call Hook Mechanism Based on Binary Rewriting},
  shorttitle = {Zpoline},
  author = {Yasukata, Kenichi and Tazaki, Hajime and Aublin, Pierre-Louis and Ishiguro, Kenta},
  date = {2023},
  pages = {293--300},
  url = {https://www.usenix.org/conference/atc23/presentation/yasukata},
  urldate = {2025-01-14},
  eventtitle = {2023 {{USENIX Annual Technical Conference}} ({{USENIX ATC}} 23)},
  isbn = {978-1-939133-35-9},
  langid = {english},
  file = {/home/sam/Zotero/storage/FQN7W93C/Yasukata et al. - 2023 - zpoline a system call hook mechanism based on binary rewriting.pdf}
}

@inproceedings{yiEvaluatingBenchmarkSubsetting2006,
  title = {Evaluating {{Benchmark Subsetting Approaches}}},
  booktitle = {2006 {{IEEE International Symposium}} on {{Workload Characterization}}},
  author = {Yi, Joshua J. and Sendag, Resit and Eeckhout, Lieven and Joshi, Ajay and Lilja, David J. and John, Lizy K.},
  date = {2006-10},
  pages = {93--104},
  doi = {10.1109/IISWC.2006.302733},
  url = {https://ieeexplore.ieee.org/document/4086137},
  urldate = {2024-01-22},
  abstract = {To reduce the simulation time to a tractable amount or due to compilation (or other related) problems, computer architects often simulate only a subset of the benchmarks in a benchmark suite. However, if the architect chooses a subset of benchmarks that is not representative, the subsequent simulation results will, at best, be misleading or, at worst, yield incorrect conclusions. To address this problem, computer architects have recently proposed several statistically-based approaches to subset a benchmark suite. While some of these approaches are well-grounded statistically, what has not yet been thoroughly evaluated is the: 1) absolute accuracy; 2) relative accuracy across a range of processor and memory subsystem enhancements; and 3) representativeness and coverage of each approach for a range of subset sizes. Specifically, this paper evaluates statistically-based subsetting approaches based on principal components analysis (PCA) and the Plackett and Burman (P\&B) design, in addition to prevailing approaches such as integer vs. floating-point, core vs. memory-bound, by language, and at random. Our results show that the two statistically-based approaches, PCA and P\&B, have the best absolute and relative accuracy for CPI and energy-delay product (EDP), produce subsets that are the most representative, and choose benchmark and input set pairs that are most well-distributed across the benchmark space. To achieve a 5\% absolute CPI and EDP error, across a wide range of configurations, PCA and P\&B typically need about 17 benchmark and input set pairs, while the other five approaches often choose more than 30 benchmark and input set pairs},
  eventtitle = {2006 {{IEEE International Symposium}} on {{Workload Characterization}}},
  keywords = {project-provenance-pp,software benchmarking},
  file = {/home/sam/Zotero/storage/ITHSVQR4/Yi et al. - 2006 - Evaluating Benchmark Subsetting Approaches.pdf;/home/sam/Zotero/storage/E3UB24EQ/4086137.html}
}

@inproceedings{yinPanoramaCapturingSystemwide2007,
  title = {Panorama: Capturing System-Wide Information Flow for Malware Detection and Analysis},
  shorttitle = {Panorama},
  booktitle = {Proceedings of the 14th {{ACM}} Conference on {{Computer}} and Communications Security},
  author = {Yin, Heng and Song, Dawn and Egele, Manuel and Kruegel, Christopher and Kirda, Engin},
  date = {2007-10-28},
  series = {{{CCS}} '07},
  pages = {116--127},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1315245.1315261},
  url = {https://dl.acm.org/doi/10.1145/1315245.1315261},
  urldate = {2023-08-23},
  abstract = {Malicious programs spy on users' behavior and compromise their privacy. Even software from reputable vendors, such as Google Desktop and Sony DRM media player, may perform undesirable actions. Unfortunately, existing techniques for detecting malware and analyzing unknown code samples are insufficient and have significant shortcomings. We observe that malicious information access and processing behavior is the fundamental trait of numerous malware categories breaching users' privacy (including keyloggers, password thieves, network sniffers, stealth backdoors, spyware and rootkits), which separates these malicious applications from benign software. We propose a system, Panorama, to detect and analyze malware by capturing this fundamental trait. In our extensive experiments, Panorama successfully detected all the malware samples and had very few false positives. Furthermore, by using Google Desktop as a case study, we show that our system can accurately capture its information access and processing behavior, and we can confirm that it does send back sensitive information to remote servers in certain settings. We believe that a system such as Panorama will offer indispensable assistance to code analysts and malware researchers by enabling them to quickly comprehend the behavior and innerworkings of an unknown sample.},
  isbn = {978-1-59593-703-2},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/L7K5I9UH/Yin et al. - 2007 - Panorama capturing system-wide information flow f.pdf}
}

@article{yooRegressionTestingMinimization2012,
  title = {Regression Testing Minimization, Selection and Prioritization: A Survey},
  shorttitle = {Regression Testing Minimization, Selection and Prioritization},
  author = {Yoo, S. and Harman, M.},
  date = {2012-03-01},
  journaltitle = {Software Testing, Verification \& Reliability},
  shortjournal = {Softw. Test. Verif. Reliab.},
  volume = {22},
  number = {2},
  pages = {67--120},
  issn = {0960-0833},
  doi = {10.1002/stv.430},
  url = {https://doi.org/10.1002/stv.430},
  urldate = {2022-09-06},
  abstract = {Regression testing is a testing activity that is performed to provide confidence that changes do not harm the existing behaviour of the software. Test suites tend to grow in size as software evolves, often making it too costly to execute entire test suites. A number of different approaches have been studied to maximize the value of the accrued test suite: minimization, selection and prioritization. Test suite minimization seeks to eliminate redundant test cases in order to reduce the number of tests to run. Test case selection seeks to identify the test cases that are relevant to some set of recent changes. Test case prioritization seeks to order test cases in such a way that early fault detection is maximized. This paper surveys each area of minimization, selection and prioritization technique and discusses open problems and potential directions for future research. Copyright © 2010 John Wiley \& Sons, Ltd.},
  keywords = {software testing},
  annotation = {interest: 76}
}

@article{yooRegressionTestingMinimization2013,
  title = {Regression Testing Minimization, Selection and Prioritization: A Survey},
  shorttitle = {Regression Testing Minimization, Selection and Prioritization},
  author = {Yoo, S. and Harman, M.},
  date = {2013-10-11},
  journaltitle = {Software Testing, Verification and Reliability},
  volume = {22},
  number = {2},
  pages = {67--120},
  issn = {1099-1689},
  doi = {10.1002/stvr.430},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.430},
  urldate = {2022-04-07},
  abstract = {Regression testing is a testing activity that is performed to provide confidence that changes do not harm the existing behaviour of the software. Test suites tend to grow in size as software evolves, often making it too costly to execute entire test suites. A number of different approaches have been studied to maximize the value of the accrued test suite: minimization, selection and prioritization. Test suite minimization seeks to eliminate redundant test cases in order to reduce the number of tests to run. Test case selection seeks to identify the test cases that are relevant to some set of recent changes. Test case prioritization seeks to order test cases in such a way that early fault detection is maximized. This paper surveys each area of minimization, selection and prioritization technique and discusses open problems and potential directions for future research.},
  langid = {english},
  keywords = {software engineering,software testing},
  file = {/home/sam/Zotero/storage/7SDJT536/Software Testing Verif Rel - 2013 - Yoo - Regression testing minimization selection and prioritization a survey.pdf}
}

@online{yorkBuildCloudAccessing2011,
  title = {Build in the {{Cloud}}: {{Accessing Source Code}}},
  author = {York, Nathan},
  date = {2011-06-21},
  url = {https://google-engtools.blogspot.com/2011/06/build-in-cloud-accessing-source-code.html},
  organization = {Google Engineering Tools},
  keywords = {continuous integration,software engineering},
  file = {/home/sam/Zotero/storage/AZI26DG6/build-in-cloud-accessing-source-code.html}
}

@online{yorkBuildCloudDistributing2011,
  type = {Google Engineering Tools},
  title = {Build in the {{Cloud}}: {{Distributing Build Steps}}},
  author = {York, Nathan},
  date = {2011-09-22},
  url = {https://google-engtools.blogspot.com/2011/09/build-in-cloud-distributing-build-steps.html},
  keywords = {build systems,continuous integration,industry practices,software engineering},
  file = {/home/sam/Zotero/storage/ADG5ULHP/build-in-cloud-distributing-build-steps.html}
}

@article{youngGoDirectlyJail2006,
  title = {Go {{Directly}} to {{Jail}}: {{The Criminalization}} of {{Almost Everything}}. - {{Free Online Library}}},
  author = {Young, Malcom C.},
  date = {2006-06-22},
  journaltitle = {Journal of Criminal Law and Criminology},
  url = {https://www.thefreelibrary.com/Go+Directly+to+Jail%3a+The+Criminalization+of+Almost+Everything.-a0157035815},
  urldate = {2023-06-07},
  keywords = {civil rights},
  file = {/home/sam/Zotero/storage/V57KHISL/Go+Directly+to+Jail+The+Criminalization+of+Almost+Everything.html}
}

@article{youngRecommendedRequirementsGathering2002,
  title = {Recommended {{Requirements Gathering Practices}}},
  author = {Young, Ralph R.},
  date = {2002-04},
  journaltitle = {Crosstalk: The Journal of Defense Software Engineering},
  pages = {9--12},
  abstract = {This article provides suggested conditions for performing requirements gathering and recommended requirements gathering practices. The author has conducted an extensive review of industry literature and combined this with the practical experiences of a set of requirements analysts who have supported dozens of projects. The sidebar on page 10 summarizes a set of recommended requirements gathering practices. Involving customers and users throughout the development effort results in a better understanding of the real needs. Requirements activities should be performed throughout the development effort, not just at the beginning of a project},
  keywords = {internship-project,requirements gathering,software engineering process}
}

@article{youngWhyCurrentPublication2008,
  title = {Why {{Current Publication Practices May Distort Science}}},
  author = {Young, Neal S. and Ioannidis, John P. A. and Al-Ubaydli, Omar},
  date = {2008-10-07},
  journaltitle = {PLOS Medicine},
  shortjournal = {PLOS Medicine},
  volume = {5},
  number = {10},
  pages = {e201},
  publisher = {Public Library of Science},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.0050201},
  url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0050201},
  urldate = {2022-09-06},
  abstract = {John Ioannidis and colleagues argue that the current system of publication in biomedical research provides a distorted view of the reality of scientific data.},
  langid = {english},
  keywords = {academic publishing,metascience},
  annotation = {interest: 83},
  file = {/home/sam/Zotero/storage/EUGHN8B2/Young et al. - 2008 - Why Current Publication Practices May Distort Scie.pdf;/home/sam/Zotero/storage/EC8DU7BD/article.html}
}

@inproceedings{yuanSimpleTestingCan2014,
  title = {Simple {{Testing Can Prevent Most Critical Failures}}: {{An Analysis}} of {{Production Failures}} in {{Distributed Data-Intensive Systems}}},
  booktitle = {11th {{USENIX Symposium}} on {{Operating Systems Design}} and {{Implementation}} ({{OSDI}} 14)},
  author = {Yuan, Ding and Luo, Yu and Zhuang, Xin and Rodrigues, Guilherme Renna and Zhao, Xu and Zhang, Yongle and Jain, Pranay U. and Stumm, Michael},
  date = {2014-10},
  pages = {249--265},
  publisher = {USENIX Association},
  location = {Broomfield, CO},
  url = {https://www.usenix.org/conference/osdi14/technical-sessions/presentation/yuan},
  abstract = {Large, production quality distributed systems still fail periodically, and do so sometimes catastrophically, where most or all users experience an outage or data loss. We present the result of a comprehensive study investigating 198 randomly selected, user-reported failures that occurred on Cassandra, HBase, Hadoop Distributed File System (HDFS), Hadoop MapReduce, and Redis, with the goal of understanding how one or multiple faults eventually evolve into a user-visible failure. We found that from a testing point of view, almost all failures require only 3 or fewer nodes to reproduce, which is good news considering that these services typically run on a very large number of nodes. However, multiple inputs are needed to trigger the failures with the order between them being important. Finally, we found the error logs of these systems typically contain sufficient data on both the errors and the input events that triggered the failure, enabling the diagnose and the reproduction of the production failures. We found the majority of catastrophic failures could easily have been prevented by performing simple testing on error handling code – the last line of defense – even without an understanding of the software design. We extracted three simple rules from the bugs that have lead to some of the catastrophic failures, and developed a static checker, Aspirator, capable of locating these bugs. Over 30\% of the catastrophic failures would have been prevented had Aspirator been used and the identified bugs fixed. Running Aspirator on the code of 9 distributed systems located 143 bugs and bad practices that have been fixed or confirmed by the developers.},
  isbn = {978-1-931971-16-4},
  keywords = {distributed systems,software engineering,software testing},
  file = {/home/sam/Zotero/storage/4PGMPKVF/osdi14_slides_yuan-ding.pdf;/home/sam/Zotero/storage/9HCWPLTD/Yuan et al. - Simple Testing Can Prevent Most Critical Failures.pdf}
}

@inproceedings{zhangLocalizingFailureinducingProgram2011,
  title = {Localizing Failure-Inducing Program Edits Based on Spectrum Information},
  booktitle = {2011 27th {{IEEE International Conference}} on {{Software Maintenance}} ({{ICSM}})},
  author = {Zhang, Lingming and Kim, Miryung and Khurshid, Sarfraz},
  date = {2011-09},
  pages = {23--32},
  issn = {1063-6773},
  doi = {10.1109/ICSM.2011.6080769},
  abstract = {Keeping evolving systems fault free is hard. Change impact analysis is a well-studied methodology for finding faults in evolving systems. For example, in order to help developers identify failure-inducing edits, Chianti extracts program edits as atomic changes between different program versions, selects affected tests, and determines a subset of those changes that might induce test failures. However, identifying real regression faults is challenging for developers since the number of affecting changes related to each test failure may still be too large for manual inspection. This paper presents a novel approach FAULTTRACER which ranks program edits in order to reduce developers' effort in manually inspecting all affecting changes. FAULTTRACER adapts spectrum-based fault localization techniques and applies them in tandem with an enhanced change impact analysis that uses Extended Call Graphs to identify failure-inducing edits more precisely. We evaluate FAULTTRACER using 23 versions of 4 real-world Java programs from the Software Infrastructure Repository. The experimental results show that FAULTTRACER outperforms Chianti in selecting affected tests (slightly better, but handles safety problems of Chianti) as well as in determining affecting changes (with an improvement of approximately 20\%). By ranking the affecting changes using spectrum-based test behavior profile, for 14 out of 22 studied failures, FAULTTRACER places a real regression fault within top 3 atomic changes, significantly reducing developers' effort in inspecting potential failure-inducing edits.},
  eventtitle = {2011 27th {{IEEE International Conference}} on {{Software Maintenance}} ({{ICSM}})},
  keywords = {debugging,software engineering},
  file = {/home/sam/Zotero/storage/2FZME8DA/Zhang et al. - 2011 - Localizing failure-inducing program edits based on.pdf}
}

@inproceedings{zhaoApplyingVirtualData2006,
  title = {Applying the {{Virtual Data Provenance Model}}},
  booktitle = {Provenance and {{Annotation}} of {{Data}}},
  author = {Zhao, Yong and Wilde, Michael and Foster, Ian},
  editor = {Moreau, Luc and Foster, Ian},
  date = {2006},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {148--161},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/11890850_16},
  abstract = {In many domains of science, engineering, and commerce, data analysis systems are employed to derive new data (and ultimately, one hopes, knowledge) from datasets describing experimental results or simulated phenomena. To support such analyses, we have developed a “virtual data system” that allows users first to define, then to invoke, and finally explore the provenance of procedures (and workflows comprising multiple procedure calls) that perform such data derivations. The underlying execution model is “functional” in the sense that procedures read (but do not modify) their input and produce output via deterministic computations. This property makes it straightforward for the virtual data system to record not only the recipe for producing any given data object but also sufficient information about the environment in which the recipe has been executed, all with sufficient fidelity that the steps used to create a data object can be re-executed to reproduce the data object at a later time or a different location. The virtual data system maintains this information in an integrated schema alongside semantic annotations, and thus enables a powerful query capability in which the rich semantic information implied by knowledge of the structure of data derivation procedures can be exploited to provide an information environment that fuses recipe, history, and application-specific semantics. We provide here an overview of this integration, the queries and transformations that it enables, and examples of how these capabilities can serve scientific processes.},
  isbn = {978-3-540-46303-0},
  langid = {english},
  keywords = {project-provenance-pp,provenance},
  file = {/home/sam/Zotero/storage/XFYDNEA7/Zhao et al. - 2006 - Applying the Virtual Data Provenance Model.pdf}
}

@inproceedings{zhaoWhyWorkflowsBreak2012,
  title = {Why Workflows Break — Understanding and Combating Decay in {{Taverna}} Workflows},
  booktitle = {2012 {{IEEE}} 8th {{International Conference}} on {{E-Science}} (e-{{Science}})},
  author = {Zhao, Jun and Gomez-Perez, Jose Manuel and Belhajjame, Khalid and Klyne, Graham and Garcia-cuesta, Esteban and Garrido, Aleix and Hettne, Kristina and Roos, Marco and De Roure, David and Goble, Carole},
  date = {2012-10},
  pages = {9},
  publisher = {IEEE},
  location = {Chicago, IL},
  doi = {10.1109/eScience.2012.6404482},
  url = {https://www.research.manchester.ac.uk/portal/en/publications/why-workflows-break--understanding-and-combating-decay-in-taverna-workflows(cba81ca4-e92c-408e-8442-383d1f15fcdf)/export.html},
  abstract = {Workflows provide a popular means for preserving scientific methods by explicitly encoding their process. However, some of them are subject to a decay in their ability to be re-executed or reproduce the same results over time, largely due to the volatility of the resources required for workflow executions. This paper provides an analysis of the root causes of workflow decay based on an empirical study of a collection of Taverna workflows from the myExperiment repository. Although our analysis was based on a specific type of workflow, the outcomes and methodology should be applicable to workflows from other systems, at least those whose executions also rely largely on accessing third-party resources. Based on our understanding about decay we recommend a minimal set of auxiliary resources to be preserved together with the workflows as an aggregation object and provide a software tool for end-users to create such aggregations and to assess their completeness},
  eventtitle = {2012 {{IEEE}} 8th {{International Conference}} on {{E-Science}} (e-{{Science}})},
  keywords = {internship-project,project-acm-rep,project-provenance-pp,workflow managers},
  file = {/home/sam/Zotero/storage/2BQSSKJF/Why_workflows_break__Understanding_and_combating_decay_in_Taverna_workflows.pdf}
}

@inproceedings{zhuReproducibilitySoftwareDefect2023,
  title = {On the {{Reproducibility}} of {{Software Defect Datasets}}},
  booktitle = {2023 {{IEEE}}/{{ACM}} 45th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Zhu, Hao-Nan and Rubio-González, Cindy},
  date = {2023-05},
  pages = {2324--2335},
  issn = {1558-1225},
  doi = {10.1109/ICSE48619.2023.00195},
  url = {https://ieeexplore.ieee.org/document/10172645},
  urldate = {2023-11-07},
  abstract = {Software defect datasets are crucial to facilitating the evaluation and comparison of techniques in fields such as fault localization, test generation, and automated program repair. However, the reproducibility of software defect artifacts is not immune to breakage. In this paper, we conduct a study on the reproducibility of software defect artifacts. First, we study five state-of-the-art Java defect datasets. Despite the multiple strategies applied by dataset maintainers to ensure reproducibility, all datasets are prone to breakages. Second, we conduct a case study in which we systematically test the reproducibility of 1,795 software artifacts during a 13-month period. We find that 62.6\% of the artifacts break at least once, and 15.3\% artifacts break multiple times. We manually investigate the root causes of breakages and handcraft 10 patches, which are automatically applied to 1,055 distinct artifacts in 2,948 fixes. Based on the nature of the root causes, we propose automated dependency caching and artifact isolation to prevent further breakage. In particular, we show that isolating artifacts to eliminate external dependencies increases reproducibility to 95\% or higher, which is on par with the level of reproducibility exhibited by the most reliable manually curated dataset.},
  eventtitle = {2023 {{IEEE}}/{{ACM}} 45th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  keywords = {reproducibility,software defects},
  file = {/home/sam/Zotero/storage/FYTJZWU3/slides-icse-23-reproducibility.pdf;/home/sam/Zotero/storage/GILA5WFE/Zhu and Rubio-González - 2023 - On the Reproducibility of Software Defect Datasets.pdf;/home/sam/Zotero/storage/8EZ6ENBX/10172645.html}
}

@article{zilbermanThoughtsArtifactBadging2020,
  title = {Thoughts about {{Artifact Badging}}},
  author = {Zilberman, Noa and Moore, Andrew W.},
  date = {2020-05-23},
  journaltitle = {SIGCOMM Comput. Commun. Rev.},
  volume = {50},
  number = {2},
  pages = {60--63},
  issn = {0146-4833},
  doi = {10.1145/3402413.3402422},
  url = {https://dl.acm.org/doi/10.1145/3402413.3402422},
  urldate = {2024-10-23},
  abstract = {Reproducibility: the extent to which consistent results are obtained when an experiment is repeated, is important as a means to validate experimental results, promote integrity of research, and accelerate follow up work. Commitment to artifact reviewing and badging seeks to promote reproducibility and rank the quality of submitted artifacts.However, as illustrated in this issue, the current badging scheme, with its focus upon an artifact being reusable, may not identify limitations of architecture, implementation, or evaluation.We propose that to improve the insight into artifact reproducibility, the depth and nature of artifact evaluation must move beyond simply considering if an artifact is reusable. Artifact evaluation should consider the methods of that evaluation alongside the varying of inputs to that evaluation. To achieve this, we suggest an extension to the scope of artifact badging, and describe both approaches and best practice arising in other communities. We seek to promote conversation and make a call to action intended to strengthen the scientific method within our domain.},
  file = {/home/sam/Zotero/storage/UZ23VYXL/Zilberman and Moore - 2020 - Thoughts about Artifact Badging.pdf}
}

@article{zipperleProvenancebasedIntrusionDetection2022,
  title = {Provenance-Based {{Intrusion Detection Systems}}: {{A Survey}}},
  shorttitle = {Provenance-Based {{Intrusion Detection Systems}}},
  author = {Zipperle, Michael and Gottwalt, Florian and Chang, Elizabeth and Dillon, Tharam},
  date = {2022-12-15},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {55},
  number = {7},
  pages = {135:1--135:36},
  issn = {0360-0300},
  doi = {10.1145/3539605},
  url = {https://dl.acm.org/doi/10.1145/3539605},
  urldate = {2023-08-23},
  abstract = {Traditional Intrusion Detection Systems (IDS) cannot cope with the increasing number and sophistication of cyberattacks such as Advanced Persistent Threats (APT). Due to their high false-positive rate and the required effort of security experts to validate them, incidents can remain undetected for up to several months. As a result, enterprises suffer from data loss and severe financial damage. Recent research explored data provenance for Host-based Intrusion Detection Systems (HIDS) as one promising data source to tackle this issue. Data provenance represents information flows between system entities as Direct Acyclic Graph (DAG). Provenance-based Intrusion Detection Systems (PIDS) utilize data provenance to enhance the detection performance of intrusions and reduce false-alarm rates compared to traditional IDS. This survey demonstrates the potential of PIDS by providing a detailed evaluation of recent research in the field, proposing a novel taxonomy for PIDS, discussing current issues, and potential future research directions. This survey aims to help and motivate researchers to get started in the field of PIDS by tackling issues of data collection, graph summarization, intrusion detection, and developing real-world benchmark datasets.},
  keywords = {project-provenance-pp,provenance-tool},
  file = {/home/sam/Zotero/storage/AZFUMA8Z/Zipperle et al. - 2022 - Provenance-based Intrusion Detection Systems A Su.pdf}
}

@misc{zurbuchenSMDPolicyDocument2022,
  title = {{{SMD Policy Document SPD-41a}}},
  author = {Zurbuchen, Thomas H.},
  date = {2022-09-26},
  organization = {NASA Scientific Information Policy for the Science Mission Directorate},
  keywords = {open data,opensource software,project-acm-rep},
  file = {/home/sam/Zotero/storage/FI2HJWSE/Zurbuchen - 2022 - SMD Policy Document SPD-41a.pdf}
}
